{
  "$schema": "http://cyclonedx.org/schema/bom-1.6.schema.json",
  "bomFormat": "CycloneDX",
  "specVersion": "1.6",
  "serialNumber": "urn:uuid:0cceedc4-24f8-409f-b7fb-5a049575b936",
  "version": 1,
  "metadata": {
    "timestamp": "2025-07-14T13:16:19-04:00",
    "tools": {
      "components": [
        {
          "type": "application",
          "author": "anchore",
          "name": "syft",
          "version": "1.28.0"
        }
      ]
    },
    "component": {
      "bom-ref": "b4ea604df3cad629",
      "type": "file",
      "name": "dulwich-dulwich-0.23.2/"
    }
  },
  "components": [
    {
      "bom-ref": "pkg:github/actions/checkout@v4?package-id=685f80a1cdde97b6",
      "type": "library",
      "name": "actions/checkout",
      "version": "v4",
      "cpe": "cpe:2.3:a:actions\\/checkout:actions\\/checkout:v4:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/checkout@v4",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/disperse.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/checkout@v4?package-id=08014e0742182224",
      "type": "library",
      "name": "actions/checkout",
      "version": "v4",
      "cpe": "cpe:2.3:a:actions\\/checkout:actions\\/checkout:v4:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/checkout@v4",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/docs.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/checkout@v4?package-id=b8d9c450e7680938",
      "type": "library",
      "name": "actions/checkout",
      "version": "v4",
      "cpe": "cpe:2.3:a:actions\\/checkout:actions\\/checkout:v4:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/checkout@v4",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/python-distributions.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/checkout@v4?package-id=7bc06457f681f7cd",
      "type": "library",
      "name": "actions/checkout",
      "version": "v4",
      "cpe": "cpe:2.3:a:actions\\/checkout:actions\\/checkout:v4:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/checkout@v4",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/pythontest.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/download-artifact@v4?package-id=0b6b813b2c8f1dbc",
      "type": "library",
      "name": "actions/download-artifact",
      "version": "v4",
      "cpe": "cpe:2.3:a:actions\\/download-artifact:actions\\/download-artifact:v4:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/download-artifact@v4",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/download-artifact:actions\\/download_artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/download_artifact:actions\\/download-artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/download_artifact:actions\\/download_artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/download:actions\\/download-artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/download:actions\\/download_artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/python-distributions.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/setup-python@v5?package-id=40a03abc6c2c9249",
      "type": "library",
      "name": "actions/setup-python",
      "version": "v5",
      "cpe": "cpe:2.3:a:actions\\/setup-python:actions\\/setup-python:v5:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/setup-python@v5",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup-python:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup_python:actions\\/setup-python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup_python:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup:actions\\/setup-python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/docs.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/setup-python@v5?package-id=f72ff3d5f52dd5e0",
      "type": "library",
      "name": "actions/setup-python",
      "version": "v5",
      "cpe": "cpe:2.3:a:actions\\/setup-python:actions\\/setup-python:v5:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/setup-python@v5",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup-python:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup_python:actions\\/setup-python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup_python:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup:actions\\/setup-python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/python-distributions.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/setup-python@v5?package-id=d26a1d212b443d23",
      "type": "library",
      "name": "actions/setup-python",
      "version": "v5",
      "cpe": "cpe:2.3:a:actions\\/setup-python:actions\\/setup-python:v5:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/setup-python@v5",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup-python:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup_python:actions\\/setup-python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup_python:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup:actions\\/setup-python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/setup:actions\\/setup_python:v5:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/pythontest.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/actions/upload-artifact@v4?package-id=5d3ac7f04bc8db63",
      "type": "library",
      "name": "actions/upload-artifact",
      "version": "v4",
      "cpe": "cpe:2.3:a:actions\\/upload-artifact:actions\\/upload-artifact:v4:*:*:*:*:*:*:*",
      "purl": "pkg:github/actions/upload-artifact@v4",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/upload-artifact:actions\\/upload_artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/upload_artifact:actions\\/upload-artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/upload_artifact:actions\\/upload_artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/upload:actions\\/upload-artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:actions\\/upload:actions\\/upload_artifact:v4:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/python-distributions.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/autocfg@1.5.0?package-id=2be63ae407b2ad54",
      "type": "library",
      "name": "autocfg",
      "version": "1.5.0",
      "cpe": "cpe:2.3:a:autocfg:autocfg:1.5.0:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/autocfg@1.5.0",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/dependabot/fetch-metadata@v2?package-id=119ebfa2e0d091dc",
      "type": "library",
      "name": "dependabot/fetch-metadata",
      "version": "v2",
      "cpe": "cpe:2.3:a:dependabot\\/fetch-metadata:dependabot\\/fetch-metadata:v2:*:*:*:*:*:*:*",
      "purl": "pkg:github/dependabot/fetch-metadata@v2",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:dependabot\\/fetch-metadata:dependabot\\/fetch_metadata:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:dependabot\\/fetch_metadata:dependabot\\/fetch-metadata:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:dependabot\\/fetch_metadata:dependabot\\/fetch_metadata:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:dependabot\\/fetch:dependabot\\/fetch-metadata:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:dependabot\\/fetch:dependabot\\/fetch_metadata:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/auto-merge.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/diff-tree-py@0.23.1?package-id=aa335deb52eec26c",
      "type": "library",
      "name": "diff-tree-py",
      "version": "0.23.1",
      "cpe": "cpe:2.3:a:diff-tree-py:diff-tree-py:0.23.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/diff-tree-py@0.23.1",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff-tree-py:diff_tree_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff_tree_py:diff-tree-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff_tree_py:diff_tree_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff-tree:diff-tree-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff-tree:diff_tree_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff_tree:diff-tree-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff_tree:diff_tree_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff:diff-tree-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:diff:diff_tree_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/docker/setup-qemu-action@v3?package-id=0e10730c622ce633",
      "type": "library",
      "name": "docker/setup-qemu-action",
      "version": "v3",
      "cpe": "cpe:2.3:a:docker\\/setup-qemu-action:docker\\/setup-qemu-action:v3:*:*:*:*:*:*:*",
      "purl": "pkg:github/docker/setup-qemu-action@v3",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup-qemu-action:docker\\/setup_qemu_action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup_qemu_action:docker\\/setup-qemu-action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup_qemu_action:docker\\/setup_qemu_action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup-qemu:docker\\/setup-qemu-action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup-qemu:docker\\/setup_qemu_action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup_qemu:docker\\/setup-qemu-action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup_qemu:docker\\/setup_qemu_action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup:docker\\/setup-qemu-action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:docker\\/setup:docker\\/setup_qemu_action:v3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/python-distributions.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/heck@0.5.0?package-id=3f963bd9124a3bde",
      "type": "library",
      "name": "heck",
      "version": "0.5.0",
      "cpe": "cpe:2.3:a:heck:heck:0.5.0:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/heck@0.5.0",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/indoc@2.0.6?package-id=d71209a8c5d7f44a",
      "type": "library",
      "name": "indoc",
      "version": "2.0.6",
      "cpe": "cpe:2.3:a:indoc:indoc:2.0.6:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/indoc@2.0.6",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/jelmer/action-disperse-validate@v2?package-id=5c02521fb7305da0",
      "type": "library",
      "name": "jelmer/action-disperse-validate",
      "version": "v2",
      "cpe": "cpe:2.3:a:jelmer\\/action-disperse-validate:jelmer\\/action-disperse-validate:v2:*:*:*:*:*:*:*",
      "purl": "pkg:github/jelmer/action-disperse-validate@v2",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action-disperse-validate:jelmer\\/action_disperse_validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action_disperse_validate:jelmer\\/action-disperse-validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action_disperse_validate:jelmer\\/action_disperse_validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action-disperse:jelmer\\/action-disperse-validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action-disperse:jelmer\\/action_disperse_validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action_disperse:jelmer\\/action-disperse-validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action_disperse:jelmer\\/action_disperse_validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action:jelmer\\/action-disperse-validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:jelmer\\/action:jelmer\\/action_disperse_validate:v2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/disperse.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/libc@0.2.174?package-id=475513522dfb2caf",
      "type": "library",
      "name": "libc",
      "version": "0.2.174",
      "cpe": "cpe:2.3:a:libc:libc:0.2.174:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/libc@0.2.174",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/memchr@2.7.5?package-id=96f4389eb32f342d",
      "type": "library",
      "name": "memchr",
      "version": "2.7.5",
      "cpe": "cpe:2.3:a:memchr:memchr:2.7.5:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/memchr@2.7.5",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/memoffset@0.9.1?package-id=7cc86bd1ecae6934",
      "type": "library",
      "name": "memoffset",
      "version": "0.9.1",
      "cpe": "cpe:2.3:a:memoffset:memoffset:0.9.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/memoffset@0.9.1",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/objects-py@0.23.1?package-id=bfc114352568184e",
      "type": "library",
      "name": "objects-py",
      "version": "0.23.1",
      "cpe": "cpe:2.3:a:objects-py:objects-py:0.23.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/objects-py@0.23.1",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:objects-py:objects_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:objects_py:objects-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:objects_py:objects_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:objects:objects-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:objects:objects_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/once_cell@1.21.3?package-id=33e67327358cdff4",
      "type": "library",
      "name": "once_cell",
      "version": "1.21.3",
      "cpe": "cpe:2.3:a:once-cell:once-cell:1.21.3:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/once_cell@1.21.3",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:once-cell:once_cell:1.21.3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:once_cell:once-cell:1.21.3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:once_cell:once_cell:1.21.3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:once:once-cell:1.21.3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:once:once_cell:1.21.3:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/pack-py@0.23.1?package-id=1712f688dfdb7531",
      "type": "library",
      "name": "pack-py",
      "version": "0.23.1",
      "cpe": "cpe:2.3:a:pack-py:pack-py:0.23.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/pack-py@0.23.1",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pack-py:pack_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pack_py:pack-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pack_py:pack_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pack:pack-py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pack:pack_py:0.23.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/portable-atomic@1.11.1?package-id=557cc84b57485a9e",
      "type": "library",
      "name": "portable-atomic",
      "version": "1.11.1",
      "cpe": "cpe:2.3:a:portable-atomic:portable-atomic:1.11.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/portable-atomic@1.11.1",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:portable-atomic:portable_atomic:1.11.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:portable_atomic:portable-atomic:1.11.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:portable_atomic:portable_atomic:1.11.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:portable:portable-atomic:1.11.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:portable:portable_atomic:1.11.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/proc-macro2@1.0.95?package-id=af9bec92e851c556",
      "type": "library",
      "name": "proc-macro2",
      "version": "1.0.95",
      "cpe": "cpe:2.3:a:proc-macro2:proc-macro2:1.0.95:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/proc-macro2@1.0.95",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:proc-macro2:proc_macro2:1.0.95:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:proc_macro2:proc-macro2:1.0.95:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:proc_macro2:proc_macro2:1.0.95:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:proc:proc-macro2:1.0.95:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:proc:proc_macro2:1.0.95:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/pyo3@0.25.1?package-id=91e2320002f62b2c",
      "type": "library",
      "name": "pyo3",
      "version": "0.25.1",
      "cpe": "cpe:2.3:a:pyo3:pyo3:0.25.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/pyo3@0.25.1",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/pyo3-build-config@0.25.1?package-id=c7912994b46dc11a",
      "type": "library",
      "name": "pyo3-build-config",
      "version": "0.25.1",
      "cpe": "cpe:2.3:a:pyo3-build-config:pyo3-build-config:0.25.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/pyo3-build-config@0.25.1",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-build-config:pyo3_build_config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_build_config:pyo3-build-config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_build_config:pyo3_build_config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-build:pyo3-build-config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-build:pyo3_build_config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_build:pyo3-build-config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_build:pyo3_build_config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3-build-config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3_build_config:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/pyo3-ffi@0.25.1?package-id=666f004c6e4db935",
      "type": "library",
      "name": "pyo3-ffi",
      "version": "0.25.1",
      "cpe": "cpe:2.3:a:pyo3-ffi:pyo3-ffi:0.25.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/pyo3-ffi@0.25.1",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-ffi:pyo3_ffi:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_ffi:pyo3-ffi:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_ffi:pyo3_ffi:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3-ffi:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3_ffi:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/pyo3-macros@0.25.1?package-id=258fce6a4271a123",
      "type": "library",
      "name": "pyo3-macros",
      "version": "0.25.1",
      "cpe": "cpe:2.3:a:pyo3-macros:pyo3-macros:0.25.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/pyo3-macros@0.25.1",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-macros:pyo3_macros:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_macros:pyo3-macros:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_macros:pyo3_macros:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3-macros:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3_macros:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/pyo3-macros-backend@0.25.1?package-id=a0e10e625e56292c",
      "type": "library",
      "name": "pyo3-macros-backend",
      "version": "0.25.1",
      "cpe": "cpe:2.3:a:pyo3-macros-backend:pyo3-macros-backend:0.25.1:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/pyo3-macros-backend@0.25.1",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-macros-backend:pyo3_macros_backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_macros_backend:pyo3-macros-backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_macros_backend:pyo3_macros_backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-macros:pyo3-macros-backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3-macros:pyo3_macros_backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_macros:pyo3-macros-backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3_macros:pyo3_macros_backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3-macros-backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pyo3:pyo3_macros_backend:0.25.1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:github/pypa/gh-action-pypi-publish@release%2Fv1?package-id=a6861a3b225dfd85",
      "type": "library",
      "name": "pypa/gh-action-pypi-publish",
      "version": "release/v1",
      "cpe": "cpe:2.3:a:pypa\\/gh-action-pypi-publish:pypa\\/gh-action-pypi-publish:release\\/v1:*:*:*:*:*:*:*",
      "purl": "pkg:github/pypa/gh-action-pypi-publish@release%2Fv1",
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "github-actions-usage-cataloger"
        },
        {
          "name": "syft:package:type",
          "value": "github-action"
        },
        {
          "name": "syft:package:metadataType",
          "value": "github-actions-use-statement"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh-action-pypi-publish:pypa\\/gh_action_pypi_publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh_action_pypi_publish:pypa\\/gh-action-pypi-publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh_action_pypi_publish:pypa\\/gh_action_pypi_publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh-action-pypi:pypa\\/gh-action-pypi-publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh-action-pypi:pypa\\/gh_action_pypi_publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh_action_pypi:pypa\\/gh-action-pypi-publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh_action_pypi:pypa\\/gh_action_pypi_publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh-action:pypa\\/gh-action-pypi-publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh-action:pypa\\/gh_action_pypi_publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh_action:pypa\\/gh-action-pypi-publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh_action:pypa\\/gh_action_pypi_publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh:pypa\\/gh-action-pypi-publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:pypa\\/gh:pypa\\/gh_action_pypi_publish:release\\/v1:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/.github/workflows/python-distributions.yml"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/quote@1.0.40?package-id=73ea498fe1ca745d",
      "type": "library",
      "name": "quote",
      "version": "1.0.40",
      "cpe": "cpe:2.3:a:quote:quote:1.0.40:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/quote@1.0.40",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/syn@2.0.104?package-id=fc80f6d1c7e5cbab",
      "type": "library",
      "name": "syn",
      "version": "2.0.104",
      "cpe": "cpe:2.3:a:syn:syn:2.0.104:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/syn@2.0.104",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/target-lexicon@0.13.2?package-id=60404038d91d4353",
      "type": "library",
      "name": "target-lexicon",
      "version": "0.13.2",
      "cpe": "cpe:2.3:a:target-lexicon:target-lexicon:0.13.2:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/target-lexicon@0.13.2",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:target-lexicon:target_lexicon:0.13.2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:target_lexicon:target-lexicon:0.13.2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:target_lexicon:target_lexicon:0.13.2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:target:target-lexicon:0.13.2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:target:target_lexicon:0.13.2:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/unicode-ident@1.0.18?package-id=59af8bb67e9b9d97",
      "type": "library",
      "name": "unicode-ident",
      "version": "1.0.18",
      "cpe": "cpe:2.3:a:unicode-ident:unicode-ident:1.0.18:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/unicode-ident@1.0.18",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:unicode-ident:unicode_ident:1.0.18:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:unicode_ident:unicode-ident:1.0.18:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:unicode_ident:unicode_ident:1.0.18:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:unicode:unicode-ident:1.0.18:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:cpe23",
          "value": "cpe:2.3:a:unicode:unicode_ident:1.0.18:*:*:*:*:*:*:*"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "pkg:cargo/unindent@0.2.4?package-id=583d107056b8801c",
      "type": "library",
      "name": "unindent",
      "version": "0.2.4",
      "cpe": "cpe:2.3:a:unindent:unindent:0.2.4:*:*:*:*:*:*:*",
      "purl": "pkg:cargo/unindent@0.2.4",
      "externalReferences": [
        {
          "url": "registry+https://github.com/rust-lang/crates.io-index",
          "type": "distribution"
        }
      ],
      "properties": [
        {
          "name": "syft:package:foundBy",
          "value": "rust-cargo-lock-cataloger"
        },
        {
          "name": "syft:package:language",
          "value": "rust"
        },
        {
          "name": "syft:package:type",
          "value": "rust-crate"
        },
        {
          "name": "syft:package:metadataType",
          "value": "rust-cargo-lock-entry"
        },
        {
          "name": "syft:location:0:path",
          "value": "/Cargo.lock"
        }
      ]
    },
    {
      "bom-ref": "2a2ba656b9bef836",
      "type": "file",
      "name": "/home/dulwich-dulwich-0.23.2/.github/workflows/auto-merge.yml",
      "hashes": [
        {
          "alg": "SHA-1",
          "content": "57036507de05a8837a4a039f801f78061a45eb10"
        },
        {
          "alg": "SHA-256",
          "content": "38c2d36214733f58a486913335bfab5420e39e4dbbb7c0a84b47a63b9726d96d"
        }
      ]
    },
    {
      "bom-ref": "1526706c6a05dea6",
      "type": "file",
      "name": "/home/dulwich-dulwich-0.23.2/.github/workflows/disperse.yml",
      "hashes": [
        {
          "alg": "SHA-1",
          "content": "c4b8517d91a253cb6d081d3725e8b93e9ff59cbf"
        },
        {
          "alg": "SHA-256",
          "content": "883acc4596de4f4dca4875c9420e5995a1e9dec495a6b296883af5d652d7b6a2"
        }
      ]
    },
    {
      "bom-ref": "63d1046ff1e32645",
      "type": "file",
      "name": "/home/dulwich-dulwich-0.23.2/.github/workflows/docs.yml",
      "hashes": [
        {
          "alg": "SHA-1",
          "content": "53f80d2de211973cf94ba9affc74eb45fa67e141"
        },
        {
          "alg": "SHA-256",
          "content": "c39558903c0ff4d3471d367790ecdc1b1c0eaa0999e1f677bf6994a3635ab05d"
        }
      ]
    },
    {
      "bom-ref": "88c418a84757a107",
      "type": "file",
      "name": "/home/dulwich-dulwich-0.23.2/.github/workflows/python-distributions.yml",
      "hashes": [
        {
          "alg": "SHA-1",
          "content": "50bf040fc6478b627259dc7ae748c1ef05ff1b03"
        },
        {
          "alg": "SHA-256",
          "content": "411a20fe39e097ff25a166b342085f35a9ed1238d8953478ef63c0b5f3dd1daa"
        }
      ]
    },
    {
      "bom-ref": "19480d69c3b16537",
      "type": "file",
      "name": "/home/dulwich-dulwich-0.23.2/.github/workflows/pythontest.yml",
      "hashes": [
        {
          "alg": "SHA-1",
          "content": "62f89f37d66340de88b053fffa4bc4fb4d7aa188"
        },
        {
          "alg": "SHA-256",
          "content": "3924dac3c919eccceee4be57f2a622be1e92e17cead9bff2a0c25178f2aea004"
        }
      ]
    },
    {
      "bom-ref": "c6bea2c24af05bc1",
      "type": "file",
      "name": "/home/dulwich-dulwich-0.23.2/Cargo.lock",
      "hashes": [
        {
          "alg": "SHA-1",
          "content": "30f64c989b11912127b1266913a37f96f3daad59"
        },
        {
          "alg": "SHA-256",
          "content": "3a60d6d19783d7a105cdc17bba71f22e1823f2e30cde19719023e9c6907cba8c"
        }
      ]
    }
  ],
  "dependencies": [
    {
      "ref": "pkg:cargo/diff-tree-py@0.23.1?package-id=aa335deb52eec26c",
      "dependsOn": [
        "pkg:cargo/pyo3@0.25.1?package-id=91e2320002f62b2c"
      ]
    },
    {
      "ref": "pkg:cargo/memoffset@0.9.1?package-id=7cc86bd1ecae6934",
      "dependsOn": [
        "pkg:cargo/autocfg@1.5.0?package-id=2be63ae407b2ad54"
      ]
    },
    {
      "ref": "pkg:cargo/objects-py@0.23.1?package-id=bfc114352568184e",
      "dependsOn": [
        "pkg:cargo/memchr@2.7.5?package-id=96f4389eb32f342d",
        "pkg:cargo/pyo3@0.25.1?package-id=91e2320002f62b2c"
      ]
    },
    {
      "ref": "pkg:cargo/pack-py@0.23.1?package-id=1712f688dfdb7531",
      "dependsOn": [
        "pkg:cargo/memchr@2.7.5?package-id=96f4389eb32f342d",
        "pkg:cargo/pyo3@0.25.1?package-id=91e2320002f62b2c"
      ]
    },
    {
      "ref": "pkg:cargo/proc-macro2@1.0.95?package-id=af9bec92e851c556",
      "dependsOn": [
        "pkg:cargo/unicode-ident@1.0.18?package-id=59af8bb67e9b9d97"
      ]
    },
    {
      "ref": "pkg:cargo/pyo3-build-config@0.25.1?package-id=c7912994b46dc11a",
      "dependsOn": [
        "pkg:cargo/once_cell@1.21.3?package-id=33e67327358cdff4",
        "pkg:cargo/target-lexicon@0.13.2?package-id=60404038d91d4353"
      ]
    },
    {
      "ref": "pkg:cargo/pyo3-ffi@0.25.1?package-id=666f004c6e4db935",
      "dependsOn": [
        "pkg:cargo/libc@0.2.174?package-id=475513522dfb2caf",
        "pkg:cargo/pyo3-build-config@0.25.1?package-id=c7912994b46dc11a"
      ]
    },
    {
      "ref": "pkg:cargo/pyo3-macros-backend@0.25.1?package-id=a0e10e625e56292c",
      "dependsOn": [
        "pkg:cargo/heck@0.5.0?package-id=3f963bd9124a3bde",
        "pkg:cargo/proc-macro2@1.0.95?package-id=af9bec92e851c556",
        "pkg:cargo/pyo3-build-config@0.25.1?package-id=c7912994b46dc11a",
        "pkg:cargo/quote@1.0.40?package-id=73ea498fe1ca745d",
        "pkg:cargo/syn@2.0.104?package-id=fc80f6d1c7e5cbab"
      ]
    },
    {
      "ref": "pkg:cargo/pyo3-macros@0.25.1?package-id=258fce6a4271a123",
      "dependsOn": [
        "pkg:cargo/proc-macro2@1.0.95?package-id=af9bec92e851c556",
        "pkg:cargo/pyo3-macros-backend@0.25.1?package-id=a0e10e625e56292c",
        "pkg:cargo/quote@1.0.40?package-id=73ea498fe1ca745d",
        "pkg:cargo/syn@2.0.104?package-id=fc80f6d1c7e5cbab"
      ]
    },
    {
      "ref": "pkg:cargo/pyo3@0.25.1?package-id=91e2320002f62b2c",
      "dependsOn": [
        "pkg:cargo/indoc@2.0.6?package-id=d71209a8c5d7f44a",
        "pkg:cargo/libc@0.2.174?package-id=475513522dfb2caf",
        "pkg:cargo/memoffset@0.9.1?package-id=7cc86bd1ecae6934",
        "pkg:cargo/once_cell@1.21.3?package-id=33e67327358cdff4",
        "pkg:cargo/portable-atomic@1.11.1?package-id=557cc84b57485a9e",
        "pkg:cargo/pyo3-build-config@0.25.1?package-id=c7912994b46dc11a",
        "pkg:cargo/pyo3-ffi@0.25.1?package-id=666f004c6e4db935",
        "pkg:cargo/pyo3-macros@0.25.1?package-id=258fce6a4271a123",
        "pkg:cargo/unindent@0.2.4?package-id=583d107056b8801c"
      ]
    },
    {
      "ref": "pkg:cargo/quote@1.0.40?package-id=73ea498fe1ca745d",
      "dependsOn": [
        "pkg:cargo/proc-macro2@1.0.95?package-id=af9bec92e851c556"
      ]
    },
    {
      "ref": "pkg:cargo/syn@2.0.104?package-id=fc80f6d1c7e5cbab",
      "dependsOn": [
        "pkg:cargo/proc-macro2@1.0.95?package-id=af9bec92e851c556",
        "pkg:cargo/quote@1.0.40?package-id=73ea498fe1ca745d",
        "pkg:cargo/unicode-ident@1.0.18?package-id=59af8bb67e9b9d97"
      ]
    }
  ],
  "upgrade": {
    "file_changes": {
      "old_version": "/home/dulwich-dulwich-0.22.7",
      "New file": [
        "/home/dulwich-dulwich-0.23.2/devscripts/fix-history.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/annotate.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/attrs.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/bisect.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/commit_graph.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/dumb.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/filter_branch.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/filters.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/gc.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/merge.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/notes.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/rebase.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/reftable.py",
        "/home/dulwich-dulwich-0.23.2/dulwich/sparse_patterns.py",
        "/home/dulwich-dulwich-0.23.2/examples/filter_branch.py",
        "/home/dulwich-dulwich-0.23.2/tests/compat/test_check_ignore.py",
        "/home/dulwich-dulwich-0.23.2/tests/compat/test_commit_graph.py",
        "/home/dulwich-dulwich-0.23.2/tests/compat/test_dumb.py",
        "/home/dulwich-dulwich-0.23.2/tests/compat/test_index.py",
        "/home/dulwich-dulwich-0.23.2/tests/compat/test_reftable.py",
        "/home/dulwich-dulwich-0.23.2/tests/contrib/test_diffstat.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_annotate.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_attrs.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_bisect.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_cli.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_cli_cherry_pick.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_cli_merge.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_cloud_gcs.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_commit_graph.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_dumb.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_filter_branch.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_filters.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_gc.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_lfs_integration.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_log_utils.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_merge.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_notes.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_porcelain_cherry_pick.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_porcelain_merge.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_porcelain_notes.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_rebase.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_reftable.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_sparse_patterns.py",
        "/home/dulwich-dulwich-0.23.2/tests/test_submodule.py"
      ],
      "Deleted file": [],
      "Modified file": [
        {
          "file": "/home/dulwich-dulwich-0.23.2/crates/diff-tree/src/lib.rs",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/crates/diff-tree/src/lib.rs",
            "+++ /home/dulwich-dulwich-0.23.2/crates/diff-tree/src/lib.rs",
            "@@ -139,15 +139,15 @@",
            "     tree1: &Bound<PyAny>,",
            "     tree2: &Bound<PyAny>,",
            " ) -> PyResult<PyObject> {",
            "     let entries1 = tree_entries(path, tree1, py)?;",
            "     let entries2 = tree_entries(path, tree2, py)?;",
            " ",
            "     let pym = py.import(\"dulwich.diff_tree\")?;",
            "-    let null_entry = pym.getattr(\"_NULL_ENTRY\")?.to_object(py);",
            "+    let null_entry = pym.getattr(\"_NULL_ENTRY\")?.unbind();",
            " ",
            "     let mut result = Vec::new();",
            " ",
            "     let mut i1 = 0;",
            "     let mut i2 = 0;",
            "     while i1 < entries1.len() && i2 < entries2.len() {",
            "         let cmp = entry_path_cmp(entries1[i1].bind(py), entries2[i2].bind(py))?;",
            "@@ -182,15 +182,15 @@",
            "     while i2 < entries2.len() {",
            "         let pair =",
            "             PyTuple::new(py, &[null_entry.clone_ref(py), entries2[i2].clone_ref(py)]).unwrap();",
            "         result.push(pair);",
            "         i2 += 1;",
            "     }",
            " ",
            "-    Ok(PyList::new(py, &result).unwrap().to_object(py))",
            "+    Ok(PyList::new(py, &result).unwrap().unbind().into())",
            " }",
            " ",
            " #[pymodule]",
            " fn _diff_tree(_py: Python, m: &Bound<PyModule>) -> PyResult<()> {",
            "     m.add_function(wrap_pyfunction!(_count_blocks, m)?)?;",
            "     m.add_function(wrap_pyfunction!(_is_tree, m)?)?;",
            "     m.add_function(wrap_pyfunction!(_merge_entries, m)?)?;"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/crates/objects/src/lib.rs",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/crates/objects/src/lib.rs",
            "+++ /home/dulwich-dulwich-0.23.2/crates/objects/src/lib.rs",
            "@@ -24,14 +24,15 @@",
            " use pyo3::import_exception;",
            " use pyo3::prelude::*;",
            " use pyo3::types::{PyBytes, PyDict};",
            " ",
            " import_exception!(dulwich.errors, ObjectFormatException);",
            " ",
            " const S_IFDIR: u32 = 0o40000;",
            "+const S_IFMT: u32 = 0o170000;  // File type mask",
            " ",
            " #[inline]",
            " fn bytehex(byte: u8) -> u8 {",
            "     match byte {",
            "         0..=9 => byte + b'0',",
            "         10..=15 => byte - 10 + b'a',",
            "         _ => unreachable!(),",
            "@@ -74,15 +75,15 @@",
            "         let name = &text[..namelen];",
            "         if namelen + 20 >= text.len() {",
            "             return Err(ObjectFormatException::new_err((\"SHA truncated\",)));",
            "         }",
            "         text = &text[namelen + 1..];",
            "         let sha = &text[..20];",
            "         entries.push((",
            "-            PyBytes::new(py, name).to_object(py),",
            "+            PyBytes::new(py, name).into_pyobject(py)?.unbind().into(),",
            "             mode,",
            "             sha_to_pyhex(py, sha)?,",
            "         ));",
            "         text = &text[20..];",
            "     }",
            "     Ok(entries)",
            " }",
            "@@ -92,18 +93,18 @@",
            "     let cmp = a.1[..len].cmp(&b.1[..len]);",
            "     if cmp != std::cmp::Ordering::Equal {",
            "         return cmp;",
            "     }",
            " ",
            "     let c1 =",
            "         a.1.get(len)",
            "-            .map_or_else(|| if a.0 & S_IFDIR != 0 { b'/' } else { 0 }, |&c| c);",
            "+            .map_or_else(|| if (a.0 & S_IFMT) == S_IFDIR { b'/' } else { 0 }, |&c| c);",
            "     let c2 =",
            "         b.1.get(len)",
            "-            .map_or_else(|| if b.0 & S_IFDIR != 0 { b'/' } else { 0 }, |&c| c);",
            "+            .map_or_else(|| if (b.0 & S_IFMT) == S_IFDIR { b'/' } else { 0 }, |&c| c);",
            "     c1.cmp(&c2)",
            " }",
            " ",
            " /// Iterate over a tree entries dictionary.",
            " ///",
            " /// # Arguments",
            " ///",
            "@@ -136,19 +137,26 @@",
            "     let objectsm = py.import(\"dulwich.objects\")?;",
            "     let tree_entry_cls = objectsm.getattr(\"TreeEntry\")?;",
            "     qsort_entries",
            "         .into_iter()",
            "         .map(|(name, mode, hexsha)| -> PyResult<PyObject> {",
            "             Ok(tree_entry_cls",
            "                 .call1((",
            "-                    PyBytes::new(py, name.as_slice()).to_object(py),",
            "+                    PyBytes::new(py, name.as_slice())",
            "+                        .into_pyobject(py)?",
            "+                        .unbind()",
            "+                        .into_any(),",
            "                     mode,",
            "-                    PyBytes::new(py, hexsha.as_slice()).to_object(py),",
            "+                    PyBytes::new(py, hexsha.as_slice())",
            "+                        .into_pyobject(py)?",
            "+                        .unbind()",
            "+                        .into_any(),",
            "                 ))?",
            "-                .to_object(py))",
            "+                .unbind()",
            "+                .into())",
            "         })",
            "         .collect::<PyResult<Vec<PyObject>>>()",
            " }",
            " ",
            " #[pymodule]",
            " fn _objects(_py: Python, m: &Bound<PyModule>) -> PyResult<()> {",
            "     m.add_function(wrap_pyfunction!(sorted_tree_items, m)?)?;"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/docs/conf.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/docs/conf.py",
            "+++ /home/dulwich-dulwich-0.23.2/docs/conf.py",
            "@@ -128,15 +128,15 @@",
            " # docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32",
            " # pixels large.",
            " # html_favicon = None",
            " ",
            " # Add any paths that contain custom static files (such as style sheets) here,",
            " # relative to this directory. They are copied after the builtin static files,",
            " # so a file named \"default.css\" will overwrite the builtin \"default.css\".",
            "-html_static_path = []",
            "+html_static_path: list[str] = []",
            " ",
            " # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,",
            " # using the given strftime format.",
            " # html_last_updated_fmt = '%b %d, %Y'",
            " ",
            " # If true, SmartyPants will be used to convert quotes and dashes to",
            " # typographically correct entities."
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/__init__.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/__init__.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/__init__.py",
            "@@ -19,8 +19,57 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " ",
            " \"\"\"Python implementation of the Git file formats and protocols.\"\"\"",
            " ",
            "-__version__ = (0, 22, 7)",
            "+import sys",
            "+from typing import Any, Callable, Optional, TypeVar",
            "+",
            "+if sys.version_info >= (3, 10):",
            "+    from typing import ParamSpec",
            "+else:",
            "+    from typing_extensions import ParamSpec",
            "+",
            "+__version__ = (0, 23, 2)",
            "+",
            "+__all__ = [\"replace_me\"]",
            "+",
            "+P = ParamSpec(\"P\")",
            "+R = TypeVar(\"R\")",
            "+F = TypeVar(\"F\", bound=Callable[..., Any])",
            "+",
            "+try:",
            "+    from dissolve import replace_me",
            "+except ImportError:",
            "+    # if dissolve is not installed, then just provide a basic implementation",
            "+    # of its replace_me decorator",
            "+    def replace_me(",
            "+        since: Optional[str] = None, remove_in: Optional[str] = None",
            "+    ) -> Callable[[Callable[P, R]], Callable[P, R]]:",
            "+        def decorator(func: Callable[P, R]) -> Callable[P, R]:",
            "+            import functools",
            "+            import warnings",
            "+",
            "+            m = f\"{func.__name__} is deprecated\"",
            "+            if since is not None and remove_in is not None:",
            "+                m += f\" since {since} and will be removed in {remove_in}\"",
            "+            elif since is not None:",
            "+                m += f\" since {since}\"",
            "+            elif remove_in is not None:",
            "+                m += f\" and will be removed in {remove_in}\"",
            "+            else:",
            "+                m += \" and will be removed in a future version\"",
            "+",
            "+            @functools.wraps(func)",
            "+            def _wrapped_func(*args: P.args, **kwargs: P.kwargs) -> R:",
            "+                warnings.warn(",
            "+                    m,",
            "+                    DeprecationWarning,",
            "+                    stacklevel=2,",
            "+                )",
            "+                return func(*args, **kwargs)",
            "+",
            "+            return _wrapped_func",
            "+",
            "+        return decorator"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/archive.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/archive.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/archive.py",
            "@@ -22,57 +22,75 @@",
            " ",
            " \"\"\"Generates tarballs for Git trees.\"\"\"",
            " ",
            " import posixpath",
            " import stat",
            " import struct",
            " import tarfile",
            "+from collections.abc import Generator",
            " from contextlib import closing",
            " from io import BytesIO",
            " from os import SEEK_END",
            "+from typing import TYPE_CHECKING, Optional",
            "+",
            "+if TYPE_CHECKING:",
            "+    from .object_store import BaseObjectStore",
            "+    from .objects import TreeEntry",
            "+",
            "+from .objects import Tree",
            " ",
            " ",
            " class ChunkedBytesIO:",
            "     \"\"\"Turn a list of bytestrings into a file-like object.",
            " ",
            "     This is similar to creating a `BytesIO` from a concatenation of the",
            "     bytestring list, but saves memory by NOT creating one giant bytestring",
            "     first::",
            " ",
            "         BytesIO(b''.join(list_of_bytestrings)) =~= ChunkedBytesIO(",
            "             list_of_bytestrings)",
            "     \"\"\"",
            " ",
            "-    def __init__(self, contents) -> None:",
            "+    def __init__(self, contents: list[bytes]) -> None:",
            "         self.contents = contents",
            "         self.pos = (0, 0)",
            " ",
            "-    def read(self, maxbytes=None):",
            "-        if maxbytes < 0:",
            "-            maxbytes = float(\"inf\")",
            "+    def read(self, maxbytes: Optional[int] = None) -> bytes:",
            "+        if maxbytes is None or maxbytes < 0:",
            "+            remaining = None",
            "+        else:",
            "+            remaining = maxbytes",
            " ",
            "         buf = []",
            "         chunk, cursor = self.pos",
            " ",
            "         while chunk < len(self.contents):",
            "-            if maxbytes < len(self.contents[chunk]) - cursor:",
            "-                buf.append(self.contents[chunk][cursor : cursor + maxbytes])",
            "-                cursor += maxbytes",
            "+            chunk_remainder = len(self.contents[chunk]) - cursor",
            "+            if remaining is not None and remaining < chunk_remainder:",
            "+                buf.append(self.contents[chunk][cursor : cursor + remaining])",
            "+                cursor += remaining",
            "                 self.pos = (chunk, cursor)",
            "                 break",
            "             else:",
            "                 buf.append(self.contents[chunk][cursor:])",
            "-                maxbytes -= len(self.contents[chunk]) - cursor",
            "+                if remaining is not None:",
            "+                    remaining -= chunk_remainder",
            "                 chunk += 1",
            "                 cursor = 0",
            "                 self.pos = (chunk, cursor)",
            "         return b\"\".join(buf)",
            " ",
            " ",
            "-def tar_stream(store, tree, mtime, prefix=b\"\", format=\"\"):",
            "+def tar_stream(",
            "+    store: \"BaseObjectStore\",",
            "+    tree: \"Tree\",",
            "+    mtime: int,",
            "+    prefix: bytes = b\"\",",
            "+    format: str = \"\",",
            "+) -> Generator[bytes, None, None]:",
            "     \"\"\"Generate a tar stream for the contents of a Git tree.",
            " ",
            "     Returns a generator that lazily assembles a .tar.gz archive, yielding it in",
            "     pieces (bytestrings). To obtain the complete .tar.gz binary file, simply",
            "     concatenate these chunks.",
            " ",
            "     Args:",
            "@@ -81,15 +99,19 @@",
            "       mtime: UNIX timestamp that is assigned as the modification time for",
            "         all files, and the gzip header modification time if format='gz'",
            "       format: Optional compression format for tarball",
            "     Returns:",
            "       Bytestrings",
            "     \"\"\"",
            "     buf = BytesIO()",
            "-    with closing(tarfile.open(None, f\"w:{format}\", buf)) as tar:",
            "+    mode = \"w:\" + format if format else \"w\"",
            "+    from typing import Any, cast",
            "+",
            "+    # The tarfile.open overloads are complex; cast to Any to avoid issues",
            "+    with closing(cast(Any, tarfile.open)(name=None, mode=mode, fileobj=buf)) as tar:",
            "         if format == \"gz\":",
            "             # Manually correct the gzip header file modification time so that",
            "             # archives created from the same Git tree are always identical.",
            "             # The gzip header file modification time is not currently",
            "             # accessible from the tarfile API, see:",
            "             # https://bugs.python.org/issue31526",
            "             buf.seek(0)",
            "@@ -101,15 +123,19 @@",
            "         for entry_abspath, entry in _walk_tree(store, tree, prefix):",
            "             try:",
            "                 blob = store[entry.sha]",
            "             except KeyError:",
            "                 # Entry probably refers to a submodule, which we don't yet",
            "                 # support.",
            "                 continue",
            "-            data = ChunkedBytesIO(blob.chunked)",
            "+            if hasattr(blob, \"chunked\"):",
            "+                data = ChunkedBytesIO(blob.chunked)",
            "+            else:",
            "+                # Fallback for objects without chunked attribute",
            "+                data = ChunkedBytesIO([blob.as_raw_string()])",
            " ",
            "             info = tarfile.TarInfo()",
            "             # tarfile only works with ascii.",
            "             info.name = entry_abspath.decode(\"utf-8\", \"surrogateescape\")",
            "             info.size = blob.raw_length()",
            "             info.mode = entry.mode",
            "             info.mtime = mtime",
            "@@ -117,17 +143,21 @@",
            "             tar.addfile(info, data)",
            "             yield buf.getvalue()",
            "             buf.truncate(0)",
            "             buf.seek(0)",
            "     yield buf.getvalue()",
            " ",
            " ",
            "-def _walk_tree(store, tree, root=b\"\"):",
            "+def _walk_tree(",
            "+    store: \"BaseObjectStore\", tree: \"Tree\", root: bytes = b\"\"",
            "+) -> Generator[tuple[bytes, \"TreeEntry\"], None, None]:",
            "     \"\"\"Recursively walk a dulwich Tree, yielding tuples of",
            "     (absolute path, TreeEntry) along the way.",
            "     \"\"\"",
            "     for entry in tree.iteritems():",
            "         entry_abspath = posixpath.join(root, entry.path)",
            "         if stat.S_ISDIR(entry.mode):",
            "-            yield from _walk_tree(store, store[entry.sha], entry_abspath)",
            "+            subtree = store[entry.sha]",
            "+            if isinstance(subtree, Tree):",
            "+                yield from _walk_tree(store, subtree, entry_abspath)",
            "         else:",
            "             yield (entry_abspath, entry)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/bundle.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/bundle.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/bundle.py",
            "@@ -17,37 +17,36 @@",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Bundle format support.\"\"\"",
            " ",
            "-from collections.abc import Sequence",
            "-from typing import Optional, Union",
            "+from typing import BinaryIO, Optional",
            " ",
            " from .pack import PackData, write_pack_data",
            " ",
            " ",
            " class Bundle:",
            "     version: Optional[int]",
            " ",
            "-    capabilities: dict[str, str]",
            "+    capabilities: dict[str, Optional[str]]",
            "     prerequisites: list[tuple[bytes, str]]",
            "-    references: dict[str, bytes]",
            "-    pack_data: Union[PackData, Sequence[bytes]]",
            "+    references: dict[bytes, bytes]",
            "+    pack_data: PackData",
            " ",
            "     def __repr__(self) -> str:",
            "         return (",
            "             f\"<{type(self).__name__}(version={self.version}, \"",
            "             f\"capabilities={self.capabilities}, \"",
            "             f\"prerequisites={self.prerequisites}, \"",
            "             f\"references={self.references})>\"",
            "         )",
            " ",
            "-    def __eq__(self, other):",
            "+    def __eq__(self, other: object) -> bool:",
            "         if not isinstance(other, type(self)):",
            "             return False",
            "         if self.version != other.version:",
            "             return False",
            "         if self.capabilities != other.capabilities:",
            "             return False",
            "         if self.prerequisites != other.prerequisites:",
            "@@ -55,29 +54,28 @@",
            "         if self.references != other.references:",
            "             return False",
            "         if self.pack_data != other.pack_data:",
            "             return False",
            "         return True",
            " ",
            " ",
            "-def _read_bundle(f, version):",
            "+def _read_bundle(f: BinaryIO, version: int) -> Bundle:",
            "     capabilities = {}",
            "     prerequisites = []",
            "     references = {}",
            "     line = f.readline()",
            "     if version >= 3:",
            "         while line.startswith(b\"@\"):",
            "             line = line[1:].rstrip(b\"\\n\")",
            "             try:",
            "-                key, value = line.split(b\"=\", 1)",
            "+                key, value_bytes = line.split(b\"=\", 1)",
            "+                value = value_bytes.decode(\"utf-8\")",
            "             except ValueError:",
            "                 key = line",
            "                 value = None",
            "-            else:",
            "-                value = value.decode(\"utf-8\")",
            "             capabilities[key.decode(\"utf-8\")] = value",
            "             line = f.readline()",
            "     while line.startswith(b\"-\"):",
            "         (obj_id, comment) = line[1:].rstrip(b\"\\n\").split(b\" \", 1)",
            "         prerequisites.append((obj_id, comment.decode(\"utf-8\")))",
            "         line = f.readline()",
            "     while line != b\"\\n\":",
            "@@ -90,25 +88,25 @@",
            "     ret.capabilities = capabilities",
            "     ret.prerequisites = prerequisites",
            "     ret.pack_data = pack_data",
            "     ret.version = version",
            "     return ret",
            " ",
            " ",
            "-def read_bundle(f):",
            "+def read_bundle(f: BinaryIO) -> Bundle:",
            "     \"\"\"Read a bundle file.\"\"\"",
            "     firstline = f.readline()",
            "     if firstline == b\"# v2 git bundle\\n\":",
            "         return _read_bundle(f, 2)",
            "     if firstline == b\"# v3 git bundle\\n\":",
            "         return _read_bundle(f, 3)",
            "     raise AssertionError(f\"unsupported bundle format header: {firstline!r}\")",
            " ",
            " ",
            "-def write_bundle(f, bundle) -> None:",
            "+def write_bundle(f: BinaryIO, bundle: Bundle) -> None:",
            "     version = bundle.version",
            "     if version is None:",
            "         if bundle.capabilities:",
            "             version = 3",
            "         else:",
            "             version = 2",
            "     if version == 2:"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/cli.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/cli.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/cli.py",
            "@@ -25,45 +25,106 @@",
            " ",
            " This is a very simple command-line wrapper for Dulwich. It is by",
            " no means intended to be a full-blown Git command-line interface but just",
            " a way to test Dulwich.",
            " \"\"\"",
            " ",
            " import argparse",
            "-import optparse",
            " import os",
            " import signal",
            " import sys",
            "-from getopt import getopt",
            "-from typing import TYPE_CHECKING, ClassVar, Optional",
            "+from pathlib import Path",
            "+from typing import ClassVar, Optional",
            " ",
            " from dulwich import porcelain",
            " ",
            " from .client import GitProtocolError, get_transport_and_path",
            " from .errors import ApplyDeltaError",
            " from .index import Index",
            "+from .objects import valid_hexsha",
            " from .objectspec import parse_commit",
            " from .pack import Pack, sha_to_hex",
            " from .repo import Repo",
            " ",
            "-if TYPE_CHECKING:",
            "-    from .objects import ObjectID",
            "-    from .refs import Ref",
            "-",
            " ",
            " def signal_int(signal, frame) -> None:",
            "     sys.exit(1)",
            " ",
            " ",
            " def signal_quit(signal, frame) -> None:",
            "     import pdb",
            " ",
            "     pdb.set_trace()",
            " ",
            " ",
            "+def parse_relative_time(time_str):",
            "+    \"\"\"Parse a relative time string like '2 weeks ago' into seconds.",
            "+",
            "+    Args:",
            "+        time_str: String like '2 weeks ago' or 'now'",
            "+",
            "+    Returns:",
            "+        Number of seconds",
            "+",
            "+    Raises:",
            "+        ValueError: If the time string cannot be parsed",
            "+    \"\"\"",
            "+    if time_str == \"now\":",
            "+        return 0",
            "+",
            "+    if not time_str.endswith(\" ago\"):",
            "+        raise ValueError(f\"Invalid relative time format: {time_str}\")",
            "+",
            "+    parts = time_str[:-4].split()",
            "+    if len(parts) != 2:",
            "+        raise ValueError(f\"Invalid relative time format: {time_str}\")",
            "+",
            "+    try:",
            "+        num = int(parts[0])",
            "+        unit = parts[1]",
            "+",
            "+        multipliers = {",
            "+            \"second\": 1,",
            "+            \"seconds\": 1,",
            "+            \"minute\": 60,",
            "+            \"minutes\": 60,",
            "+            \"hour\": 3600,",
            "+            \"hours\": 3600,",
            "+            \"day\": 86400,",
            "+            \"days\": 86400,",
            "+            \"week\": 604800,",
            "+            \"weeks\": 604800,",
            "+        }",
            "+",
            "+        if unit in multipliers:",
            "+            return num * multipliers[unit]",
            "+        else:",
            "+            raise ValueError(f\"Unknown time unit: {unit}\")",
            "+    except ValueError as e:",
            "+        if \"invalid literal\" in str(e):",
            "+            raise ValueError(f\"Invalid number in relative time: {parts[0]}\")",
            "+        raise",
            "+",
            "+",
            "+def format_bytes(bytes):",
            "+    \"\"\"Format bytes as human-readable string.",
            "+",
            "+    Args:",
            "+        bytes: Number of bytes",
            "+",
            "+    Returns:",
            "+        Human-readable string like \"1.5 MB\"",
            "+    \"\"\"",
            "+    for unit in [\"B\", \"KB\", \"MB\", \"GB\"]:",
            "+        if bytes < 1024.0:",
            "+            return f\"{bytes:.1f} {unit}\"",
            "+        bytes /= 1024.0",
            "+    return f\"{bytes:.1f} TB\"",
            "+",
            "+",
            " class Command:",
            "     \"\"\"A Dulwich subcommand.\"\"\"",
            " ",
            "     def run(self, args) -> Optional[int]:",
            "         \"\"\"Run the command.\"\"\"",
            "         raise NotImplementedError(self.run)",
            " ",
            "@@ -83,33 +144,81 @@",
            "             client.archive(",
            "                 path,",
            "                 args.committish,",
            "                 sys.stdout.write,",
            "                 write_error=sys.stderr.write,",
            "             )",
            "         else:",
            "+            # Use buffer if available (for binary output), otherwise use stdout",
            "+            outstream = getattr(sys.stdout, \"buffer\", sys.stdout)",
            "             porcelain.archive(",
            "-                \".\", args.committish, outstream=sys.stdout.buffer, errstream=sys.stderr",
            "+                \".\", args.committish, outstream=outstream, errstream=sys.stderr",
            "             )",
            " ",
            " ",
            " class cmd_add(Command):",
            "     def run(self, argv) -> None:",
            "         parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"path\", nargs=\"+\")",
            "+        args = parser.parse_args(argv)",
            "+",
            "+        # Convert '.' to None to add all files",
            "+        paths = args.path",
            "+        if len(paths) == 1 and paths[0] == \".\":",
            "+            paths = None",
            "+",
            "+        porcelain.add(\".\", paths=paths)",
            "+",
            "+",
            "+class cmd_annotate(Command):",
            "+    def run(self, argv) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"path\", help=\"Path to file to annotate\")",
            "+        parser.add_argument(\"committish\", nargs=\"?\", help=\"Commit to start from\")",
            "         args = parser.parse_args(argv)",
            " ",
            "-        porcelain.add(\".\", paths=args)",
            "+        results = porcelain.annotate(\".\", args.path, args.committish)",
            "+        for (commit, entry), line in results:",
            "+            # Show shortened commit hash and line content",
            "+            commit_hash = commit.id[:8]",
            "+            print(f\"{commit_hash.decode()} {line.decode()}\")",
            "+",
            "+",
            "+class cmd_blame(Command):",
            "+    def run(self, argv) -> None:",
            "+        # blame is an alias for annotate",
            "+        cmd_annotate().run(argv)",
            " ",
            " ",
            " class cmd_rm(Command):",
            "     def run(self, argv) -> None:",
            "         parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--cached\", action=\"store_true\", help=\"Remove from index only\"",
            "+        )",
            "+        parser.add_argument(\"path\", type=Path, nargs=\"+\")",
            "         args = parser.parse_args(argv)",
            " ",
            "-        porcelain.rm(\".\", paths=args)",
            "+        porcelain.remove(\".\", paths=args.path, cached=args.cached)",
            "+",
            "+",
            "+class cmd_mv(Command):",
            "+    def run(self, argv) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"-f\",",
            "+            \"--force\",",
            "+            action=\"store_true\",",
            "+            help=\"Force move even if destination exists\",",
            "+        )",
            "+        parser.add_argument(\"source\", type=Path)",
            "+        parser.add_argument(\"destination\", type=Path)",
            "+        args = parser.parse_args(argv)",
            "+",
            "+        porcelain.mv(\".\", args.source, args.destination, force=args.force)",
            " ",
            " ",
            " class cmd_fetch_pack(Command):",
            "     def run(self, argv) -> None:",
            "         parser = argparse.ArgumentParser()",
            "         parser.add_argument(\"--all\", action=\"store_true\")",
            "         parser.add_argument(\"location\", nargs=\"?\", type=str)",
            "@@ -117,27 +226,26 @@",
            "         args = parser.parse_args(argv)",
            "         client, path = get_transport_and_path(args.location)",
            "         r = Repo(\".\")",
            "         if args.all:",
            "             determine_wants = r.object_store.determine_wants_all",
            "         else:",
            " ",
            "-            def determine_wants(",
            "-                refs: dict[Ref, ObjectID], depth: Optional[int] = None",
            "-            ) -> list[ObjectID]:",
            "+            def determine_wants(refs, depth: Optional[int] = None):",
            "                 return [y.encode(\"utf-8\") for y in args.refs if y not in r.object_store]",
            " ",
            "         client.fetch(path, r, determine_wants)",
            " ",
            " ",
            " class cmd_fetch(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-        dict(opts)",
            "-        client, path = get_transport_and_path(args.pop(0))",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"location\", help=\"Remote location to fetch from\")",
            "+        args = parser.parse_args(args)",
            "+        client, path = get_transport_and_path(args.location)",
            "         r = Repo(\".\")",
            " ",
            "         def progress(msg: bytes) -> None:",
            "             sys.stdout.buffer.write(msg)",
            " ",
            "         refs = client.fetch(path, r, progress=progress)",
            "         print(\"Remote refs:\")",
            "@@ -152,206 +260,218 @@",
            "         args = parser.parse_args(args)",
            "         for sha, object_type, ref in porcelain.for_each_ref(\".\", args.pattern):",
            "             print(f\"{sha.decode()} {object_type.decode()}\\t{ref.decode()}\")",
            " ",
            " ",
            " class cmd_fsck(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-        dict(opts)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "         for obj, msg in porcelain.fsck(\".\"):",
            "             print(f\"{obj}: {msg}\")",
            " ",
            " ",
            " class cmd_log(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        parser.add_option(",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "             \"--reverse\",",
            "-            dest=\"reverse\",",
            "             action=\"store_true\",",
            "             help=\"Reverse order in which entries are printed\",",
            "         )",
            "-        parser.add_option(",
            "+        parser.add_argument(",
            "             \"--name-status\",",
            "-            dest=\"name_status\",",
            "             action=\"store_true\",",
            "             help=\"Print name/status for each changed file\",",
            "         )",
            "-        options, args = parser.parse_args(args)",
            "+        parser.add_argument(\"paths\", nargs=\"*\", help=\"Paths to show log for\")",
            "+        args = parser.parse_args(args)",
            " ",
            "         porcelain.log(",
            "             \".\",",
            "-            paths=args,",
            "-            reverse=options.reverse,",
            "-            name_status=options.name_status,",
            "+            paths=args.paths,",
            "+            reverse=args.reverse,",
            "+            name_status=args.name_status,",
            "             outstream=sys.stdout,",
            "         )",
            " ",
            " ",
            " class cmd_diff(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"commit\", nargs=\"?\", default=\"HEAD\", help=\"Commit to show diff for\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            " ",
            "         r = Repo(\".\")",
            "-        if args == []:",
            "-            commit_id = b\"HEAD\"",
            "-        else:",
            "-            commit_id = args[0]",
            "+        commit_id = (",
            "+            args.commit.encode() if isinstance(args.commit, str) else args.commit",
            "+        )",
            "         commit = parse_commit(r, commit_id)",
            "         parent_commit = r[commit.parents[0]]",
            "         porcelain.diff_tree(",
            "             r, parent_commit.tree, commit.tree, outstream=sys.stdout.buffer",
            "         )",
            " ",
            " ",
            " class cmd_dump_pack(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-",
            "-        if args == []:",
            "-            print(\"Usage: dulwich dump-pack FILENAME\")",
            "-            sys.exit(1)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"filename\", help=\"Pack file to dump\")",
            "+        args = parser.parse_args(args)",
            " ",
            "-        basename, _ = os.path.splitext(args[0])",
            "+        basename, _ = os.path.splitext(args.filename)",
            "         x = Pack(basename)",
            "         print(f\"Object names checksum: {x.name()}\")",
            "-        print(f\"Checksum: {sha_to_hex(x.get_stored_checksum())}\")",
            "+        print(f\"Checksum: {sha_to_hex(x.get_stored_checksum())!r}\")",
            "         x.check()",
            "         print(f\"Length: {len(x)}\")",
            "         for name in x:",
            "             try:",
            "                 print(f\"\\t{x[name]}\")",
            "             except KeyError as k:",
            "                 print(f\"\\t{name}: Unable to resolve base {k}\")",
            "             except ApplyDeltaError as e:",
            "                 print(f\"\\t{name}: Unable to apply delta: {e!r}\")",
            " ",
            " ",
            " class cmd_dump_index(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-",
            "-        if args == []:",
            "-            print(\"Usage: dulwich dump-index FILENAME\")",
            "-            sys.exit(1)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"filename\", help=\"Index file to dump\")",
            "+        args = parser.parse_args(args)",
            " ",
            "-        filename = args[0]",
            "-        idx = Index(filename)",
            "+        idx = Index(args.filename)",
            " ",
            "         for o in idx:",
            "             print(o, idx[o])",
            " ",
            " ",
            " class cmd_init(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [\"bare\"])",
            "-        kwopts = dict(opts)",
            "-",
            "-        if args == []:",
            "-            path = os.getcwd()",
            "-        else:",
            "-            path = args[0]",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--bare\", action=\"store_true\", help=\"Create a bare repository\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"path\", nargs=\"?\", default=os.getcwd(), help=\"Repository path\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            " ",
            "-        porcelain.init(path, bare=(\"--bare\" in kwopts))",
            "+        porcelain.init(args.path, bare=args.bare)",
            " ",
            " ",
            " class cmd_clone(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        parser.add_option(",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "             \"--bare\",",
            "-            dest=\"bare\",",
            "             help=\"Whether to create a bare repository.\",",
            "             action=\"store_true\",",
            "         )",
            "-        parser.add_option(",
            "-            \"--depth\", dest=\"depth\", type=int, help=\"Depth at which to fetch\"",
            "-        )",
            "-        parser.add_option(",
            "+        parser.add_argument(\"--depth\", type=int, help=\"Depth at which to fetch\")",
            "+        parser.add_argument(",
            "             \"-b\",",
            "             \"--branch\",",
            "-            dest=\"branch\",",
            "             type=str,",
            "-            help=(\"Check out branch instead of branch pointed to by remote \" \"HEAD\"),",
            "+            help=\"Check out branch instead of branch pointed to by remote HEAD\",",
            "         )",
            "-        parser.add_option(",
            "+        parser.add_argument(",
            "             \"--refspec\",",
            "-            dest=\"refspec\",",
            "             type=str,",
            "             help=\"References to fetch\",",
            "             action=\"append\",",
            "         )",
            "-        parser.add_option(",
            "+        parser.add_argument(",
            "             \"--filter\",",
            "             dest=\"filter_spec\",",
            "             type=str,",
            "             help=\"git-rev-list-style object filter\",",
            "         )",
            "-        parser.add_option(",
            "-            \"--protocol\", dest=\"protocol\", type=int, help=\"Git protocol version to use\"",
            "+        parser.add_argument(",
            "+            \"--protocol\",",
            "+            type=int,",
            "+            help=\"Git protocol version to use\",",
            "         )",
            "-        options, args = parser.parse_args(args)",
            "-",
            "-        if args == []:",
            "-            print(\"usage: dulwich clone host:path [PATH]\")",
            "-            sys.exit(1)",
            "-",
            "-        source = args.pop(0)",
            "-        if len(args) > 0:",
            "-            target = args.pop(0)",
            "-        else:",
            "-            target = None",
            "+        parser.add_argument(",
            "+            \"--recurse-submodules\",",
            "+            action=\"store_true\",",
            "+            help=\"Initialize and clone submodules\",",
            "+        )",
            "+        parser.add_argument(\"source\", help=\"Repository to clone from\")",
            "+        parser.add_argument(\"target\", nargs=\"?\", help=\"Directory to clone into\")",
            "+        args = parser.parse_args(args)",
            " ",
            "         try:",
            "             porcelain.clone(",
            "-                source,",
            "-                target,",
            "-                bare=options.bare,",
            "-                depth=options.depth,",
            "-                branch=options.branch,",
            "-                refspec=options.refspec,",
            "-                filter_spec=options.filter_spec,",
            "-                protocol_version=options.protocol,",
            "+                args.source,",
            "+                args.target,",
            "+                bare=args.bare,",
            "+                depth=args.depth,",
            "+                branch=args.branch,",
            "+                refspec=args.refspec,",
            "+                filter_spec=args.filter_spec,",
            "+                protocol_version=args.protocol,",
            "+                recurse_submodules=args.recurse_submodules,",
            "             )",
            "         except GitProtocolError as e:",
            "             print(f\"{e}\")",
            " ",
            " ",
            " class cmd_commit(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [\"message\"])",
            "-        kwopts = dict(opts)",
            "-        porcelain.commit(\".\", message=kwopts[\"--message\"])",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"--message\", \"-m\", required=True, help=\"Commit message\")",
            "+        args = parser.parse_args(args)",
            "+        porcelain.commit(\".\", message=args.message)",
            " ",
            " ",
            " class cmd_commit_tree(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [\"message\"])",
            "-        if args == []:",
            "-            print(\"usage: dulwich commit-tree tree\")",
            "-            sys.exit(1)",
            "-        kwopts = dict(opts)",
            "-        porcelain.commit_tree(\".\", tree=args[0], message=kwopts[\"--message\"])",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"--message\", \"-m\", required=True, help=\"Commit message\")",
            "+        parser.add_argument(\"tree\", help=\"Tree SHA to commit\")",
            "+        args = parser.parse_args(args)",
            "+        porcelain.commit_tree(\".\", tree=args.tree, message=args.message)",
            " ",
            " ",
            " class cmd_update_server_info(Command):",
            "     def run(self, args) -> None:",
            "         porcelain.update_server_info(\".\")",
            " ",
            " ",
            " class cmd_symbolic_ref(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [\"ref-name\", \"force\"])",
            "-        if not args:",
            "-            print(\"Usage: dulwich symbolic-ref REF_NAME [--force]\")",
            "-            sys.exit(1)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"name\", help=\"Symbolic reference name\")",
            "+        parser.add_argument(\"ref\", nargs=\"?\", help=\"Target reference\")",
            "+        parser.add_argument(\"--force\", action=\"store_true\", help=\"Force update\")",
            "+        args = parser.parse_args(args)",
            " ",
            "-        ref_name = args.pop(0)",
            "-        porcelain.symbolic_ref(\".\", ref_name=ref_name, force=\"--force\" in args)",
            "+        # If ref is provided, we're setting; otherwise we're reading",
            "+        if args.ref:",
            "+            # Set symbolic reference",
            "+            from .repo import Repo",
            "+",
            "+            with Repo(\".\") as repo:",
            "+                repo.refs.set_symbolic_ref(args.name.encode(), args.ref.encode())",
            "+        else:",
            "+            # Read symbolic reference",
            "+            from .repo import Repo",
            "+",
            "+            with Repo(\".\") as repo:",
            "+                try:",
            "+                    target = repo.refs.read_ref(args.name.encode())",
            "+                    if target.startswith(b\"ref: \"):",
            "+                        print(target[5:].decode())",
            "+                    else:",
            "+                        print(target.decode())",
            "+                except KeyError:",
            "+                    print(f\"fatal: ref '{args.name}' is not a symbolic ref\")",
            " ",
            " ",
            " class cmd_pack_refs(Command):",
            "     def run(self, argv) -> None:",
            "         parser = argparse.ArgumentParser()",
            "         parser.add_argument(\"--all\", action=\"store_true\")",
            "         # ignored, we never prune",
            "@@ -363,176 +483,194 @@",
            " ",
            " ",
            " class cmd_show(Command):",
            "     def run(self, argv) -> None:",
            "         parser = argparse.ArgumentParser()",
            "         parser.add_argument(\"objectish\", type=str, nargs=\"*\")",
            "         args = parser.parse_args(argv)",
            "-        porcelain.show(\".\", args.objectish or None)",
            "+        porcelain.show(\".\", args.objectish or None, outstream=sys.stdout)",
            " ",
            " ",
            " class cmd_diff_tree(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-        if len(args) < 2:",
            "-            print(\"Usage: dulwich diff-tree OLD-TREE NEW-TREE\")",
            "-            sys.exit(1)",
            "-        porcelain.diff_tree(\".\", args[0], args[1])",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"old_tree\", help=\"Old tree SHA\")",
            "+        parser.add_argument(\"new_tree\", help=\"New tree SHA\")",
            "+        args = parser.parse_args(args)",
            "+        porcelain.diff_tree(\".\", args.old_tree, args.new_tree)",
            " ",
            " ",
            " class cmd_rev_list(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-        if len(args) < 1:",
            "-            print(\"Usage: dulwich rev-list COMMITID...\")",
            "-            sys.exit(1)",
            "-        porcelain.rev_list(\".\", args)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"commits\", nargs=\"+\", help=\"Commit IDs to list\")",
            "+        args = parser.parse_args(args)",
            "+        porcelain.rev_list(\".\", args.commits)",
            " ",
            " ",
            " class cmd_tag(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        parser.add_option(",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "             \"-a\",",
            "             \"--annotated\",",
            "             help=\"Create an annotated tag.\",",
            "             action=\"store_true\",",
            "         )",
            "-        parser.add_option(",
            "+        parser.add_argument(",
            "             \"-s\", \"--sign\", help=\"Sign the annotated tag.\", action=\"store_true\"",
            "         )",
            "-        options, args = parser.parse_args(args)",
            "+        parser.add_argument(\"tag_name\", help=\"Name of the tag to create\")",
            "+        args = parser.parse_args(args)",
            "         porcelain.tag_create(",
            "-            \".\", args[0], annotated=options.annotated, sign=options.sign",
            "+            \".\", args.tag_name, annotated=args.annotated, sign=args.sign",
            "         )",
            " ",
            " ",
            " class cmd_repack(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-        dict(opts)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "         porcelain.repack(\".\")",
            " ",
            " ",
            " class cmd_reset(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [\"hard\", \"soft\", \"mixed\"])",
            "-        kwopts = dict(opts)",
            "-        mode = \"\"",
            "-        if \"--hard\" in kwopts:",
            "+        parser = argparse.ArgumentParser()",
            "+        mode_group = parser.add_mutually_exclusive_group()",
            "+        mode_group.add_argument(",
            "+            \"--hard\", action=\"store_true\", help=\"Reset working tree and index\"",
            "+        )",
            "+        mode_group.add_argument(\"--soft\", action=\"store_true\", help=\"Reset only HEAD\")",
            "+        mode_group.add_argument(",
            "+            \"--mixed\", action=\"store_true\", help=\"Reset HEAD and index\"",
            "+        )",
            "+        parser.add_argument(\"treeish\", nargs=\"?\", help=\"Commit/tree to reset to\")",
            "+        args = parser.parse_args(args)",
            "+",
            "+        if args.hard:",
            "             mode = \"hard\"",
            "-        elif \"--soft\" in kwopts:",
            "+        elif args.soft:",
            "             mode = \"soft\"",
            "-        elif \"--mixed\" in kwopts:",
            "+        elif args.mixed:",
            "+            mode = \"mixed\"",
            "+        else:",
            "+            # Default to mixed behavior",
            "             mode = \"mixed\"",
            "-        porcelain.reset(\".\", mode=mode)",
            "+",
            "+        # Use the porcelain.reset function for all modes",
            "+        porcelain.reset(\".\", mode=mode, treeish=args.treeish)",
            "+",
            "+",
            "+class cmd_revert(Command):",
            "+    def run(self, args) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--no-commit\",",
            "+            \"-n\",",
            "+            action=\"store_true\",",
            "+            help=\"Apply changes but don't create a commit\",",
            "+        )",
            "+        parser.add_argument(\"-m\", \"--message\", help=\"Custom commit message\")",
            "+        parser.add_argument(\"commits\", nargs=\"+\", help=\"Commits to revert\")",
            "+        args = parser.parse_args(args)",
            "+",
            "+        result = porcelain.revert(",
            "+            \".\", commits=args.commits, no_commit=args.no_commit, message=args.message",
            "+        )",
            "+",
            "+        if result and not args.no_commit:",
            "+            print(f\"[{result.decode('ascii')[:7]}] Revert completed\")",
            " ",
            " ",
            " class cmd_daemon(Command):",
            "     def run(self, args) -> None:",
            "         from dulwich import log_utils",
            " ",
            "         from .protocol import TCP_GIT_PORT",
            " ",
            "-        parser = optparse.OptionParser()",
            "-        parser.add_option(",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "             \"-l\",",
            "             \"--listen_address\",",
            "-            dest=\"listen_address\",",
            "             default=\"localhost\",",
            "             help=\"Binding IP address.\",",
            "         )",
            "-        parser.add_option(",
            "+        parser.add_argument(",
            "             \"-p\",",
            "             \"--port\",",
            "-            dest=\"port\",",
            "             type=int,",
            "             default=TCP_GIT_PORT,",
            "             help=\"Binding TCP port.\",",
            "         )",
            "-        options, args = parser.parse_args(args)",
            "+        parser.add_argument(",
            "+            \"gitdir\", nargs=\"?\", default=\".\", help=\"Git directory to serve\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            " ",
            "         log_utils.default_logging_config()",
            "-        if len(args) >= 1:",
            "-            gitdir = args[0]",
            "-        else:",
            "-            gitdir = \".\"",
            "-",
            "-        porcelain.daemon(gitdir, address=options.listen_address, port=options.port)",
            "+        porcelain.daemon(args.gitdir, address=args.listen_address, port=args.port)",
            " ",
            " ",
            " class cmd_web_daemon(Command):",
            "     def run(self, args) -> None:",
            "         from dulwich import log_utils",
            " ",
            "-        parser = optparse.OptionParser()",
            "-        parser.add_option(",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "             \"-l\",",
            "             \"--listen_address\",",
            "-            dest=\"listen_address\",",
            "             default=\"\",",
            "             help=\"Binding IP address.\",",
            "         )",
            "-        parser.add_option(",
            "+        parser.add_argument(",
            "             \"-p\",",
            "             \"--port\",",
            "-            dest=\"port\",",
            "             type=int,",
            "             default=8000,",
            "             help=\"Binding TCP port.\",",
            "         )",
            "-        options, args = parser.parse_args(args)",
            "+        parser.add_argument(",
            "+            \"gitdir\", nargs=\"?\", default=\".\", help=\"Git directory to serve\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            " ",
            "         log_utils.default_logging_config()",
            "-        if len(args) >= 1:",
            "-            gitdir = args[0]",
            "-        else:",
            "-            gitdir = \".\"",
            "-",
            "-        porcelain.web_daemon(gitdir, address=options.listen_address, port=options.port)",
            "+        porcelain.web_daemon(args.gitdir, address=args.listen_address, port=args.port)",
            " ",
            " ",
            " class cmd_write_tree(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "-        sys.stdout.write(\"{}\\n\".format(porcelain.write_tree(\".\")))",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "+        sys.stdout.write(\"{}\\n\".format(porcelain.write_tree(\".\").decode()))",
            " ",
            " ",
            " class cmd_receive_pack(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "-        if len(args) >= 1:",
            "-            gitdir = args[0]",
            "-        else:",
            "-            gitdir = \".\"",
            "-        porcelain.receive_pack(gitdir)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"gitdir\", nargs=\"?\", default=\".\", help=\"Git directory\")",
            "+        args = parser.parse_args(args)",
            "+        porcelain.receive_pack(args.gitdir)",
            " ",
            " ",
            " class cmd_upload_pack(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "-        if len(args) >= 1:",
            "-            gitdir = args[0]",
            "-        else:",
            "-            gitdir = \".\"",
            "-        porcelain.upload_pack(gitdir)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"gitdir\", nargs=\"?\", default=\".\", help=\"Git directory\")",
            "+        args = parser.parse_args(args)",
            "+        porcelain.upload_pack(args.gitdir)",
            " ",
            " ",
            " class cmd_status(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "-        if len(args) >= 1:",
            "-            gitdir = args[0]",
            "-        else:",
            "-            gitdir = \".\"",
            "-        status = porcelain.status(gitdir)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"gitdir\", nargs=\"?\", default=\".\", help=\"Git directory\")",
            "+        args = parser.parse_args(args)",
            "+        status = porcelain.status(args.gitdir)",
            "         if any(names for (kind, names) in status.staged.items()):",
            "             sys.stdout.write(\"Changes to be committed:\\n\\n\")",
            "             for kind, names in status.staged.items():",
            "                 for name in names:",
            "                     sys.stdout.write(",
            "                         f\"\\t{kind}: {name.decode(sys.getfilesystemencoding())}\\n\"",
            "                     )",
            "@@ -547,91 +685,177 @@",
            "             for name in status.untracked:",
            "                 sys.stdout.write(f\"\\t{name}\\n\")",
            "             sys.stdout.write(\"\\n\")",
            " ",
            " ",
            " class cmd_ls_remote(Command):",
            "     def run(self, args) -> None:",
            "-        opts, args = getopt(args, \"\", [])",
            "-        if len(args) < 1:",
            "-            print(\"Usage: dulwich ls-remote URL\")",
            "-            sys.exit(1)",
            "-        refs = porcelain.ls_remote(args[0])",
            "-        for ref in sorted(refs):",
            "-            sys.stdout.write(f\"{ref}\\t{refs[ref]}\\n\")",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--symref\", action=\"store_true\", help=\"Show symbolic references\"",
            "+        )",
            "+        parser.add_argument(\"url\", help=\"Remote URL to list references from\")",
            "+        args = parser.parse_args(args)",
            "+        result = porcelain.ls_remote(args.url)",
            "+",
            "+        if args.symref:",
            "+            # Show symrefs first, like git does",
            "+            for ref, target in sorted(result.symrefs.items()):",
            "+                sys.stdout.write(f\"ref: {target.decode()}\\t{ref.decode()}\\n\")",
            "+",
            "+        # Show regular refs",
            "+        for ref in sorted(result.refs):",
            "+            sys.stdout.write(f\"{result.refs[ref].decode()}\\t{ref.decode()}\\n\")",
            " ",
            " ",
            " class cmd_ls_tree(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        parser.add_option(",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "             \"-r\",",
            "             \"--recursive\",",
            "             action=\"store_true\",",
            "             help=\"Recursively list tree contents.\",",
            "         )",
            "-        parser.add_option(\"--name-only\", action=\"store_true\", help=\"Only display name.\")",
            "-        options, args = parser.parse_args(args)",
            "-        try:",
            "-            treeish = args.pop(0)",
            "-        except IndexError:",
            "-            treeish = None",
            "+        parser.add_argument(",
            "+            \"--name-only\", action=\"store_true\", help=\"Only display name.\"",
            "+        )",
            "+        parser.add_argument(\"treeish\", nargs=\"?\", help=\"Tree-ish to list\")",
            "+        args = parser.parse_args(args)",
            "         porcelain.ls_tree(",
            "             \".\",",
            "-            treeish,",
            "+            args.treeish,",
            "             outstream=sys.stdout,",
            "-            recursive=options.recursive,",
            "-            name_only=options.name_only,",
            "+            recursive=args.recursive,",
            "+            name_only=args.name_only,",
            "         )",
            " ",
            " ",
            " class cmd_pack_objects(Command):",
            "     def run(self, args) -> None:",
            "-        deltify = False",
            "-        reuse_deltas = True",
            "-        opts, args = getopt(args, \"\", [\"stdout\", \"deltify\", \"no-reuse-deltas\"])",
            "-        kwopts = dict(opts)",
            "-        if len(args) < 1 and \"--stdout\" not in kwopts.keys():",
            "-            print(\"Usage: dulwich pack-objects basename\")",
            "-            sys.exit(1)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--stdout\", action=\"store_true\", help=\"Write pack to stdout\"",
            "+        )",
            "+        parser.add_argument(\"--deltify\", action=\"store_true\", help=\"Create deltas\")",
            "+        parser.add_argument(",
            "+            \"--no-reuse-deltas\", action=\"store_true\", help=\"Don't reuse existing deltas\"",
            "+        )",
            "+        parser.add_argument(\"basename\", nargs=\"?\", help=\"Base name for pack files\")",
            "+        args = parser.parse_args(args)",
            "+",
            "+        if not args.stdout and not args.basename:",
            "+            parser.error(\"basename required when not using --stdout\")",
            "+",
            "         object_ids = [line.strip() for line in sys.stdin.readlines()]",
            "-        if \"--deltify\" in kwopts.keys():",
            "-            deltify = True",
            "-        if \"--no-reuse-deltas\" in kwopts.keys():",
            "-            reuse_deltas = False",
            "-        if \"--stdout\" in kwopts.keys():",
            "+        deltify = args.deltify",
            "+        reuse_deltas = not args.no_reuse_deltas",
            "+",
            "+        if args.stdout:",
            "             packf = getattr(sys.stdout, \"buffer\", sys.stdout)",
            "             idxf = None",
            "             close = []",
            "         else:",
            "-            basename = args[0]",
            "-            packf = open(basename + \".pack\", \"wb\")",
            "-            idxf = open(basename + \".idx\", \"wb\")",
            "+            packf = open(args.basename + \".pack\", \"wb\")",
            "+            idxf = open(args.basename + \".idx\", \"wb\")",
            "             close = [packf, idxf]",
            "+",
            "         porcelain.pack_objects(",
            "             \".\", object_ids, packf, idxf, deltify=deltify, reuse_deltas=reuse_deltas",
            "         )",
            "         for f in close:",
            "             f.close()",
            " ",
            " ",
            "+class cmd_unpack_objects(Command):",
            "+    def run(self, args) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"pack_file\", help=\"Pack file to unpack\")",
            "+        args = parser.parse_args(args)",
            "+",
            "+        count = porcelain.unpack_objects(args.pack_file)",
            "+        print(f\"Unpacked {count} objects\")",
            "+",
            "+",
            "+class cmd_prune(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        import datetime",
            "+        import time",
            "+",
            "+        from dulwich.object_store import DEFAULT_TEMPFILE_GRACE_PERIOD",
            "+",
            "+        parser = argparse.ArgumentParser(",
            "+            description=\"Remove temporary pack files left behind by interrupted operations\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--expire\",",
            "+            nargs=\"?\",",
            "+            const=\"2.weeks.ago\",",
            "+            help=\"Only prune files older than the specified date (default: 2.weeks.ago)\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--dry-run\",",
            "+            \"-n\",",
            "+            action=\"store_true\",",
            "+            help=\"Only report what would be removed\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--verbose\",",
            "+            \"-v\",",
            "+            action=\"store_true\",",
            "+            help=\"Report all actions\",",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        # Parse expire grace period",
            "+        grace_period = DEFAULT_TEMPFILE_GRACE_PERIOD",
            "+        if args.expire:",
            "+            try:",
            "+                grace_period = parse_relative_time(args.expire)",
            "+            except ValueError:",
            "+                # Try to parse as absolute date",
            "+                try:",
            "+                    date = datetime.datetime.strptime(args.expire, \"%Y-%m-%d\")",
            "+                    grace_period = int(time.time() - date.timestamp())",
            "+                except ValueError:",
            "+                    print(f\"Error: Invalid expire date: {args.expire}\", file=sys.stderr)",
            "+                    return 1",
            "+",
            "+        # Progress callback",
            "+        def progress(msg):",
            "+            if args.verbose:",
            "+                print(msg)",
            "+",
            "+        try:",
            "+            porcelain.prune(",
            "+                \".\",",
            "+                grace_period=grace_period,",
            "+                dry_run=args.dry_run,",
            "+                progress=progress if args.verbose else None,",
            "+            )",
            "+            return None",
            "+        except porcelain.Error as e:",
            "+            print(f\"Error: {e}\", file=sys.stderr)",
            "+            return 1",
            "+",
            "+",
            " class cmd_pull(Command):",
            "     def run(self, args) -> None:",
            "         parser = argparse.ArgumentParser()",
            "         parser.add_argument(\"from_location\", type=str)",
            "         parser.add_argument(\"refspec\", type=str, nargs=\"*\")",
            "         parser.add_argument(\"--filter\", type=str, nargs=1)",
            "-        parser.add_argument(\"--protocol\", type=int, nargs=1)",
            "+        parser.add_argument(\"--protocol\", type=int)",
            "         args = parser.parse_args(args)",
            "         porcelain.pull(",
            "             \".\",",
            "             args.from_location or None,",
            "             args.refspec or None,",
            "             filter_spec=args.filter,",
            "-            protocol_version=args.protocol_version or None,",
            "+            protocol_version=args.protocol or None,",
            "         )",
            " ",
            " ",
            " class cmd_push(Command):",
            "     def run(self, argv) -> Optional[int]:",
            "         parser = argparse.ArgumentParser()",
            "         parser.add_argument(\"-f\", \"--force\", action=\"store_true\", help=\"Force\")",
            "@@ -647,29 +871,36 @@",
            "             return 1",
            " ",
            "         return None",
            " ",
            " ",
            " class cmd_remote_add(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "-        porcelain.remote_add(\".\", args[0], args[1])",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"name\", help=\"Name of the remote\")",
            "+        parser.add_argument(\"url\", help=\"URL of the remote\")",
            "+        args = parser.parse_args(args)",
            "+        porcelain.remote_add(\".\", args.name, args.url)",
            " ",
            " ",
            " class SuperCommand(Command):",
            "     subcommands: ClassVar[dict[str, type[Command]]] = {}",
            "     default_command: ClassVar[Optional[type[Command]]] = None",
            " ",
            "     def run(self, args):",
            "-        if not args and not self.default_command:",
            "-            print(",
            "-                \"Supported subcommands: {}\".format(\", \".join(self.subcommands.keys()))",
            "-            )",
            "-            return False",
            "+        if not args:",
            "+            if self.default_command:",
            "+                return self.default_command().run(args)",
            "+            else:",
            "+                print(",
            "+                    \"Supported subcommands: {}\".format(",
            "+                        \", \".join(self.subcommands.keys())",
            "+                    )",
            "+                )",
            "+                return False",
            "         cmd = args[0]",
            "         try:",
            "             cmd_kls = self.subcommands[cmd]",
            "         except KeyError:",
            "             print(f\"No such subcommand: {args[0]}\")",
            "             return False",
            "         return cmd_kls().run(args[1:])",
            "@@ -692,102 +923,998 @@",
            " class cmd_submodule_init(Command):",
            "     def run(self, argv) -> None:",
            "         parser = argparse.ArgumentParser()",
            "         parser.parse_args(argv)",
            "         porcelain.submodule_init(\".\")",
            " ",
            " ",
            "+class cmd_submodule_add(Command):",
            "+    def run(self, argv) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"url\", help=\"URL of repository to add as submodule\")",
            "+        parser.add_argument(\"path\", nargs=\"?\", help=\"Path where submodule should live\")",
            "+        parser.add_argument(\"--name\", help=\"Name for the submodule\")",
            "+        args = parser.parse_args(argv)",
            "+        porcelain.submodule_add(\".\", args.url, args.path, args.name)",
            "+",
            "+",
            "+class cmd_submodule_update(Command):",
            "+    def run(self, argv) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--init\", action=\"store_true\", help=\"Initialize submodules first\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--force\",",
            "+            action=\"store_true\",",
            "+            help=\"Force update even if local changes exist\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"paths\", nargs=\"*\", help=\"Specific submodule paths to update\"",
            "+        )",
            "+        args = parser.parse_args(argv)",
            "+        paths = args.paths if args.paths else None",
            "+        porcelain.submodule_update(\".\", paths=paths, init=args.init, force=args.force)",
            "+",
            "+",
            " class cmd_submodule(SuperCommand):",
            "     subcommands: ClassVar[dict[str, type[Command]]] = {",
            "+        \"add\": cmd_submodule_add,",
            "         \"init\": cmd_submodule_init,",
            "+        \"list\": cmd_submodule_list,",
            "+        \"update\": cmd_submodule_update,",
            "     }",
            " ",
            "-    default_command = cmd_submodule_init",
            "+    default_command = cmd_submodule_list",
            " ",
            " ",
            " class cmd_check_ignore(Command):",
            "     def run(self, args):",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"paths\", nargs=\"+\", help=\"Paths to check\")",
            "+        args = parser.parse_args(args)",
            "         ret = 1",
            "-        for path in porcelain.check_ignore(\".\", args):",
            "+        for path in porcelain.check_ignore(\".\", args.paths):",
            "             print(path)",
            "             ret = 0",
            "         return ret",
            " ",
            " ",
            " class cmd_check_mailmap(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "-        for arg in args:",
            "-            canonical_identity = porcelain.check_mailmap(\".\", arg)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"identities\", nargs=\"+\", help=\"Identities to check\")",
            "+        args = parser.parse_args(args)",
            "+        for identity in args.identities:",
            "+            canonical_identity = porcelain.check_mailmap(\".\", identity)",
            "             print(canonical_identity)",
            " ",
            " ",
            "+class cmd_branch(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"branch\",",
            "+            type=str,",
            "+            help=\"Name of the branch\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"-d\",",
            "+            \"--delete\",",
            "+            action=\"store_true\",",
            "+            help=\"Delete branch\",",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+        if not args.branch:",
            "+            print(\"Usage: dulwich branch [-d] BRANCH_NAME\")",
            "+            return 1",
            "+",
            "+        if args.delete:",
            "+            porcelain.branch_delete(\".\", name=args.branch)",
            "+        else:",
            "+            try:",
            "+                porcelain.branch_create(\".\", name=args.branch)",
            "+            except porcelain.Error as e:",
            "+                sys.stderr.write(f\"{e}\")",
            "+                return 1",
            "+        return 0",
            "+",
            "+",
            "+class cmd_checkout(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"target\",",
            "+            type=str,",
            "+            help=\"Name of the branch, tag, or commit to checkout\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"-f\",",
            "+            \"--force\",",
            "+            action=\"store_true\",",
            "+            help=\"Force checkout\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"-b\",",
            "+            \"--new-branch\",",
            "+            type=str,",
            "+            help=\"Create a new branch at the target and switch to it\",",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+        if not args.target:",
            "+            print(\"Usage: dulwich checkout TARGET [--force] [-b NEW_BRANCH]\")",
            "+            return 1",
            "+",
            "+        try:",
            "+            porcelain.checkout(",
            "+                \".\", target=args.target, force=args.force, new_branch=args.new_branch",
            "+            )",
            "+        except porcelain.CheckoutError as e:",
            "+            sys.stderr.write(f\"{e}\\n\")",
            "+            return 1",
            "+        return 0",
            "+",
            "+",
            " class cmd_stash_list(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "         for i, entry in porcelain.stash_list(\".\"):",
            "             print(\"stash@{{{}}}: {}\".format(i, entry.message.rstrip(\"\\n\")))",
            " ",
            " ",
            " class cmd_stash_push(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "         porcelain.stash_push(\".\")",
            "         print(\"Saved working directory and index state\")",
            " ",
            " ",
            " class cmd_stash_pop(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "         porcelain.stash_pop(\".\")",
            "-        print(\"Restrored working directory and index state\")",
            "+        print(\"Restored working directory and index state\")",
            "+",
            "+",
            "+class cmd_bisect(SuperCommand):",
            "+    \"\"\"Git bisect command implementation.\"\"\"",
            "+",
            "+    subcommands: ClassVar[dict[str, type[Command]]] = {}",
            "+",
            "+    def run(self, args):",
            "+        parser = argparse.ArgumentParser(prog=\"dulwich bisect\")",
            "+        subparsers = parser.add_subparsers(dest=\"subcommand\", help=\"bisect subcommands\")",
            "+",
            "+        # bisect start",
            "+        start_parser = subparsers.add_parser(\"start\", help=\"Start a new bisect session\")",
            "+        start_parser.add_argument(\"bad\", nargs=\"?\", help=\"Bad commit\")",
            "+        start_parser.add_argument(\"good\", nargs=\"*\", help=\"Good commit(s)\")",
            "+        start_parser.add_argument(",
            "+            \"--no-checkout\",",
            "+            action=\"store_true\",",
            "+            help=\"Don't checkout commits during bisect\",",
            "+        )",
            "+        start_parser.add_argument(",
            "+            \"--term-bad\", default=\"bad\", help=\"Term to use for bad commits\"",
            "+        )",
            "+        start_parser.add_argument(",
            "+            \"--term-good\", default=\"good\", help=\"Term to use for good commits\"",
            "+        )",
            "+        start_parser.add_argument(",
            "+            \"--\", dest=\"paths\", nargs=\"*\", help=\"Paths to limit bisect to\"",
            "+        )",
            "+",
            "+        # bisect bad",
            "+        bad_parser = subparsers.add_parser(\"bad\", help=\"Mark a commit as bad\")",
            "+        bad_parser.add_argument(\"rev\", nargs=\"?\", help=\"Commit to mark as bad\")",
            "+",
            "+        # bisect good",
            "+        good_parser = subparsers.add_parser(\"good\", help=\"Mark a commit as good\")",
            "+        good_parser.add_argument(\"rev\", nargs=\"?\", help=\"Commit to mark as good\")",
            "+",
            "+        # bisect skip",
            "+        skip_parser = subparsers.add_parser(\"skip\", help=\"Skip commits\")",
            "+        skip_parser.add_argument(\"revs\", nargs=\"*\", help=\"Commits to skip\")",
            "+",
            "+        # bisect reset",
            "+        reset_parser = subparsers.add_parser(\"reset\", help=\"Reset bisect state\")",
            "+        reset_parser.add_argument(\"commit\", nargs=\"?\", help=\"Commit to reset to\")",
            "+",
            "+        # bisect log",
            "+        subparsers.add_parser(\"log\", help=\"Show bisect log\")",
            "+",
            "+        # bisect replay",
            "+        replay_parser = subparsers.add_parser(\"replay\", help=\"Replay bisect log\")",
            "+        replay_parser.add_argument(\"logfile\", help=\"Log file to replay\")",
            "+",
            "+        # bisect help",
            "+        subparsers.add_parser(\"help\", help=\"Show help\")",
            "+",
            "+        parsed_args = parser.parse_args(args)",
            "+",
            "+        if not parsed_args.subcommand:",
            "+            parser.print_help()",
            "+            return 1",
            "+",
            "+        try:",
            "+            if parsed_args.subcommand == \"start\":",
            "+                next_sha = porcelain.bisect_start(",
            "+                    bad=parsed_args.bad,",
            "+                    good=parsed_args.good if parsed_args.good else None,",
            "+                    paths=parsed_args.paths,",
            "+                    no_checkout=parsed_args.no_checkout,",
            "+                    term_bad=parsed_args.term_bad,",
            "+                    term_good=parsed_args.term_good,",
            "+                )",
            "+                if next_sha:",
            "+                    print(f\"Bisecting: checking out '{next_sha.decode('ascii')}'\")",
            "+",
            "+            elif parsed_args.subcommand == \"bad\":",
            "+                next_sha = porcelain.bisect_bad(rev=parsed_args.rev)",
            "+                if next_sha:",
            "+                    print(f\"Bisecting: checking out '{next_sha.decode('ascii')}'\")",
            "+                else:",
            "+                    # Bisect complete - find the first bad commit",
            "+                    with porcelain.open_repo_closing(\".\") as r:",
            "+                        bad_ref = os.path.join(r.controldir(), \"refs\", \"bisect\", \"bad\")",
            "+                        with open(bad_ref, \"rb\") as f:",
            "+                            bad_sha = f.read().strip()",
            "+                        commit = r.object_store[bad_sha]",
            "+                        message = commit.message.decode(",
            "+                            \"utf-8\", errors=\"replace\"",
            "+                        ).split(\"\\n\")[0]",
            "+                        print(f\"{bad_sha.decode('ascii')} is the first bad commit\")",
            "+                        print(f\"commit {bad_sha.decode('ascii')}\")",
            "+                        print(f\"    {message}\")",
            "+",
            "+            elif parsed_args.subcommand == \"good\":",
            "+                next_sha = porcelain.bisect_good(rev=parsed_args.rev)",
            "+                if next_sha:",
            "+                    print(f\"Bisecting: checking out '{next_sha.decode('ascii')}'\")",
            "+",
            "+            elif parsed_args.subcommand == \"skip\":",
            "+                next_sha = porcelain.bisect_skip(",
            "+                    revs=parsed_args.revs if parsed_args.revs else None",
            "+                )",
            "+                if next_sha:",
            "+                    print(f\"Bisecting: checking out '{next_sha.decode('ascii')}'\")",
            "+",
            "+            elif parsed_args.subcommand == \"reset\":",
            "+                porcelain.bisect_reset(commit=parsed_args.commit)",
            "+                print(\"Bisect reset\")",
            "+",
            "+            elif parsed_args.subcommand == \"log\":",
            "+                log = porcelain.bisect_log()",
            "+                print(log, end=\"\")",
            "+",
            "+            elif parsed_args.subcommand == \"replay\":",
            "+                porcelain.bisect_replay(log_file=parsed_args.logfile)",
            "+                print(f\"Replayed bisect log from {parsed_args.logfile}\")",
            "+",
            "+            elif parsed_args.subcommand == \"help\":",
            "+                parser.print_help()",
            "+",
            "+        except porcelain.Error as e:",
            "+            print(f\"Error: {e}\", file=sys.stderr)",
            "+            return 1",
            "+        except ValueError as e:",
            "+            print(f\"Error: {e}\", file=sys.stderr)",
            "+            return 1",
            "+",
            "+        return 0",
            " ",
            " ",
            " class cmd_stash(SuperCommand):",
            "     subcommands: ClassVar[dict[str, type[Command]]] = {",
            "         \"list\": cmd_stash_list,",
            "         \"pop\": cmd_stash_pop,",
            "         \"push\": cmd_stash_push,",
            "     }",
            " ",
            " ",
            " class cmd_ls_files(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "         for name in porcelain.ls_files(\".\"):",
            "             print(name)",
            " ",
            " ",
            " class cmd_describe(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        options, args = parser.parse_args(args)",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.parse_args(args)",
            "         print(porcelain.describe(\".\"))",
            " ",
            " ",
            "+class cmd_merge(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"commit\", type=str, help=\"Commit to merge\")",
            "+        parser.add_argument(",
            "+            \"--no-commit\", action=\"store_true\", help=\"Do not create a merge commit\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--no-ff\", action=\"store_true\", help=\"Force create a merge commit\"",
            "+        )",
            "+        parser.add_argument(\"-m\", \"--message\", type=str, help=\"Merge commit message\")",
            "+        args = parser.parse_args(args)",
            "+",
            "+        try:",
            "+            merge_commit_id, conflicts = porcelain.merge(",
            "+                \".\",",
            "+                args.commit,",
            "+                no_commit=args.no_commit,",
            "+                no_ff=args.no_ff,",
            "+                message=args.message,",
            "+            )",
            "+",
            "+            if conflicts:",
            "+                print(f\"Merge conflicts in {len(conflicts)} file(s):\")",
            "+                for conflict_path in conflicts:",
            "+                    print(f\"  {conflict_path.decode()}\")",
            "+                print(",
            "+                    \"\\nAutomatic merge failed; fix conflicts and then commit the result.\"",
            "+                )",
            "+                return 1",
            "+            elif merge_commit_id is None and not args.no_commit:",
            "+                print(\"Already up to date.\")",
            "+            elif args.no_commit:",
            "+                print(\"Automatic merge successful; not committing as requested.\")",
            "+            else:",
            "+                print(",
            "+                    f\"Merge successful. Created merge commit {merge_commit_id.decode()}\"",
            "+                )",
            "+            return 0",
            "+        except porcelain.Error as e:",
            "+            print(f\"Error: {e}\")",
            "+            return 1",
            "+",
            "+",
            "+class cmd_notes_add(Command):",
            "+    def run(self, args) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"object\", help=\"Object to annotate\")",
            "+        parser.add_argument(\"-m\", \"--message\", help=\"Note message\", required=True)",
            "+        parser.add_argument(",
            "+            \"--ref\", default=\"commits\", help=\"Notes ref (default: commits)\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        porcelain.notes_add(\".\", args.object, args.message, ref=args.ref)",
            "+",
            "+",
            "+class cmd_notes_show(Command):",
            "+    def run(self, args) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"object\", help=\"Object to show notes for\")",
            "+        parser.add_argument(",
            "+            \"--ref\", default=\"commits\", help=\"Notes ref (default: commits)\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        note = porcelain.notes_show(\".\", args.object, ref=args.ref)",
            "+        if note:",
            "+            sys.stdout.buffer.write(note)",
            "+        else:",
            "+            print(f\"No notes found for object {args.object}\")",
            "+",
            "+",
            "+class cmd_notes_remove(Command):",
            "+    def run(self, args) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(\"object\", help=\"Object to remove notes from\")",
            "+        parser.add_argument(",
            "+            \"--ref\", default=\"commits\", help=\"Notes ref (default: commits)\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        result = porcelain.notes_remove(\".\", args.object, ref=args.ref)",
            "+        if result:",
            "+            print(f\"Removed notes for object {args.object}\")",
            "+        else:",
            "+            print(f\"No notes found for object {args.object}\")",
            "+",
            "+",
            "+class cmd_notes_list(Command):",
            "+    def run(self, args) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--ref\", default=\"commits\", help=\"Notes ref (default: commits)\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        notes = porcelain.notes_list(\".\", ref=args.ref)",
            "+        for object_sha, note_content in notes:",
            "+            print(f\"{object_sha.hex()}\")",
            "+",
            "+",
            "+class cmd_notes(SuperCommand):",
            "+    subcommands: ClassVar[dict[str, type[Command]]] = {",
            "+        \"add\": cmd_notes_add,",
            "+        \"show\": cmd_notes_show,",
            "+        \"remove\": cmd_notes_remove,",
            "+        \"list\": cmd_notes_list,",
            "+    }",
            "+",
            "+    default_command = cmd_notes_list",
            "+",
            "+",
            "+class cmd_cherry_pick(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        parser = argparse.ArgumentParser(",
            "+            description=\"Apply the changes introduced by some existing commits\"",
            "+        )",
            "+        parser.add_argument(\"commit\", nargs=\"?\", help=\"Commit to cherry-pick\")",
            "+        parser.add_argument(",
            "+            \"-n\",",
            "+            \"--no-commit\",",
            "+            action=\"store_true\",",
            "+            help=\"Apply changes without making a commit\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--continue\",",
            "+            dest=\"continue_\",",
            "+            action=\"store_true\",",
            "+            help=\"Continue after resolving conflicts\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--abort\",",
            "+            action=\"store_true\",",
            "+            help=\"Abort the current cherry-pick operation\",",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        # Check argument validity",
            "+        if args.continue_ or args.abort:",
            "+            if args.commit is not None:",
            "+                parser.error(\"Cannot specify commit with --continue or --abort\")",
            "+                return 1",
            "+        else:",
            "+            if args.commit is None:",
            "+                parser.error(\"Commit argument is required\")",
            "+                return 1",
            "+",
            "+        try:",
            "+            commit_arg = args.commit",
            "+",
            "+            result = porcelain.cherry_pick(",
            "+                \".\",",
            "+                commit_arg,",
            "+                no_commit=args.no_commit,",
            "+                continue_=args.continue_,",
            "+                abort=args.abort,",
            "+            )",
            "+",
            "+            if args.abort:",
            "+                print(\"Cherry-pick aborted.\")",
            "+            elif args.continue_:",
            "+                if result:",
            "+                    print(f\"Cherry-pick completed: {result.decode()}\")",
            "+                else:",
            "+                    print(\"Cherry-pick completed.\")",
            "+            elif result is None:",
            "+                if args.no_commit:",
            "+                    print(\"Cherry-pick applied successfully (no commit created).\")",
            "+                else:",
            "+                    # This shouldn't happen unless there were conflicts",
            "+                    print(\"Cherry-pick resulted in conflicts.\")",
            "+            else:",
            "+                print(f\"Cherry-pick successful: {result.decode()}\")",
            "+",
            "+            return None",
            "+        except porcelain.Error as e:",
            "+            print(f\"Error: {e}\", file=sys.stderr)",
            "+            return 1",
            "+",
            "+",
            "+class cmd_merge_tree(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        parser = argparse.ArgumentParser(",
            "+            description=\"Perform a tree-level merge without touching the working directory\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"base_tree\",",
            "+            nargs=\"?\",",
            "+            help=\"The common ancestor tree (optional, defaults to empty tree)\",",
            "+        )",
            "+        parser.add_argument(\"our_tree\", help=\"Our side of the merge\")",
            "+        parser.add_argument(\"their_tree\", help=\"Their side of the merge\")",
            "+        parser.add_argument(",
            "+            \"-z\",",
            "+            \"--name-only\",",
            "+            action=\"store_true\",",
            "+            help=\"Output only conflict paths, null-terminated\",",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        try:",
            "+            # Determine base tree - if only two args provided, base is None",
            "+            if args.base_tree is None:",
            "+                # Only two arguments provided",
            "+                base_tree = None",
            "+                our_tree = args.our_tree",
            "+                their_tree = args.their_tree",
            "+            else:",
            "+                # Three arguments provided",
            "+                base_tree = args.base_tree",
            "+                our_tree = args.our_tree",
            "+                their_tree = args.their_tree",
            "+",
            "+            merged_tree_id, conflicts = porcelain.merge_tree(",
            "+                \".\", base_tree, our_tree, their_tree",
            "+            )",
            "+",
            "+            if args.name_only:",
            "+                # Output only conflict paths, null-terminated",
            "+                for conflict_path in conflicts:",
            "+                    sys.stdout.buffer.write(conflict_path)",
            "+                    sys.stdout.buffer.write(b\"\\0\")",
            "+            else:",
            "+                # Output the merged tree SHA",
            "+                print(merged_tree_id.decode(\"ascii\"))",
            "+",
            "+                # Output conflict information",
            "+                if conflicts:",
            "+                    print(f\"\\nConflicts in {len(conflicts)} file(s):\")",
            "+                    for conflict_path in conflicts:",
            "+                        print(f\"  {conflict_path.decode()}\")",
            "+",
            "+            return None",
            "+",
            "+        except porcelain.Error as e:",
            "+            print(f\"Error: {e}\", file=sys.stderr)",
            "+            return 1",
            "+        except KeyError as e:",
            "+            print(f\"Error: Object not found: {e}\", file=sys.stderr)",
            "+            return 1",
            "+",
            "+",
            "+class cmd_gc(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        import datetime",
            "+        import time",
            "+",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"--auto\",",
            "+            action=\"store_true\",",
            "+            help=\"Only run gc if needed\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--aggressive\",",
            "+            action=\"store_true\",",
            "+            help=\"Use more aggressive settings\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--no-prune\",",
            "+            action=\"store_true\",",
            "+            help=\"Do not prune unreachable objects\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--prune\",",
            "+            nargs=\"?\",",
            "+            const=\"now\",",
            "+            help=\"Prune unreachable objects older than date (default: 2 weeks ago)\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--dry-run\",",
            "+            \"-n\",",
            "+            action=\"store_true\",",
            "+            help=\"Only report what would be done\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--quiet\",",
            "+            \"-q\",",
            "+            action=\"store_true\",",
            "+            help=\"Only report errors\",",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        # Parse prune grace period",
            "+        grace_period = None",
            "+        if args.prune:",
            "+            try:",
            "+                grace_period = parse_relative_time(args.prune)",
            "+            except ValueError:",
            "+                # Try to parse as absolute date",
            "+                try:",
            "+                    date = datetime.datetime.strptime(args.prune, \"%Y-%m-%d\")",
            "+                    grace_period = int(time.time() - date.timestamp())",
            "+                except ValueError:",
            "+                    print(f\"Error: Invalid prune date: {args.prune}\")",
            "+                    return 1",
            "+        elif not args.no_prune:",
            "+            # Default to 2 weeks",
            "+            grace_period = 1209600",
            "+",
            "+        # Progress callback",
            "+        def progress(msg):",
            "+            if not args.quiet:",
            "+                print(msg)",
            "+",
            "+        try:",
            "+            stats = porcelain.gc(",
            "+                \".\",",
            "+                auto=args.auto,",
            "+                aggressive=args.aggressive,",
            "+                prune=not args.no_prune,",
            "+                grace_period=grace_period,",
            "+                dry_run=args.dry_run,",
            "+                progress=progress if not args.quiet else None,",
            "+            )",
            "+",
            "+            # Report results",
            "+            if not args.quiet:",
            "+                if args.dry_run:",
            "+                    print(\"\\nDry run results:\")",
            "+                else:",
            "+                    print(\"\\nGarbage collection complete:\")",
            "+",
            "+                if stats.pruned_objects:",
            "+                    print(f\"  Pruned {len(stats.pruned_objects)} unreachable objects\")",
            "+                    print(f\"  Freed {format_bytes(stats.bytes_freed)}\")",
            "+",
            "+                if stats.packs_before != stats.packs_after:",
            "+                    print(",
            "+                        f\"  Reduced pack files from {stats.packs_before} to {stats.packs_after}\"",
            "+                    )",
            "+",
            "+        except porcelain.Error as e:",
            "+            print(f\"Error: {e}\")",
            "+            return 1",
            "+        return None",
            "+",
            "+",
            "+class cmd_count_objects(Command):",
            "+    def run(self, args) -> None:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"-v\",",
            "+            \"--verbose\",",
            "+            action=\"store_true\",",
            "+            help=\"Display verbose information.\",",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        if args.verbose:",
            "+            stats = porcelain.count_objects(\".\", verbose=True)",
            "+            # Display verbose output",
            "+            print(f\"count: {stats.count}\")",
            "+            print(f\"size: {stats.size // 1024}\")  # Size in KiB",
            "+            assert stats.in_pack is not None",
            "+            print(f\"in-pack: {stats.in_pack}\")",
            "+            assert stats.packs is not None",
            "+            print(f\"packs: {stats.packs}\")",
            "+            assert stats.size_pack is not None",
            "+            print(f\"size-pack: {stats.size_pack // 1024}\")  # Size in KiB",
            "+        else:",
            "+            # Simple output",
            "+            stats = porcelain.count_objects(\".\", verbose=False)",
            "+            print(f\"{stats.count} objects, {stats.size // 1024} kilobytes\")",
            "+",
            "+",
            "+class cmd_rebase(Command):",
            "+    def run(self, args) -> int:",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "+            \"upstream\", nargs=\"?\", help=\"Upstream branch to rebase onto\"",
            "+        )",
            "+        parser.add_argument(\"--onto\", type=str, help=\"Rebase onto specific commit\")",
            "+        parser.add_argument(",
            "+            \"--branch\", type=str, help=\"Branch to rebase (default: current)\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--abort\", action=\"store_true\", help=\"Abort an in-progress rebase\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--continue\",",
            "+            dest=\"continue_rebase\",",
            "+            action=\"store_true\",",
            "+            help=\"Continue an in-progress rebase\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--skip\", action=\"store_true\", help=\"Skip current commit and continue\"",
            "+        )",
            "+        args = parser.parse_args(args)",
            "+",
            "+        # Handle abort/continue/skip first",
            "+        if args.abort:",
            "+            try:",
            "+                porcelain.rebase(\".\", args.upstream or \"HEAD\", abort=True)",
            "+                print(\"Rebase aborted.\")",
            "+            except porcelain.Error as e:",
            "+                print(f\"Error: {e}\")",
            "+                return 1",
            "+            return 0",
            "+",
            "+        if args.continue_rebase:",
            "+            try:",
            "+                new_shas = porcelain.rebase(",
            "+                    \".\", args.upstream or \"HEAD\", continue_rebase=True",
            "+                )",
            "+                print(\"Rebase complete.\")",
            "+            except porcelain.Error as e:",
            "+                print(f\"Error: {e}\")",
            "+                return 1",
            "+            return 0",
            "+",
            "+        # Normal rebase requires upstream",
            "+        if not args.upstream:",
            "+            print(\"Error: Missing required argument 'upstream'\")",
            "+            return 1",
            "+",
            "+        try:",
            "+            new_shas = porcelain.rebase(",
            "+                \".\",",
            "+                args.upstream,",
            "+                onto=args.onto,",
            "+                branch=args.branch,",
            "+            )",
            "+",
            "+            if new_shas:",
            "+                print(f\"Successfully rebased {len(new_shas)} commits.\")",
            "+            else:",
            "+                print(\"Already up to date.\")",
            "+            return 0",
            "+",
            "+        except porcelain.Error as e:",
            "+            print(f\"Error: {e}\")",
            "+            return 1",
            "+",
            "+",
            "+class cmd_filter_branch(Command):",
            "+    def run(self, args) -> Optional[int]:",
            "+        import subprocess",
            "+",
            "+        parser = argparse.ArgumentParser(description=\"Rewrite branches\")",
            "+",
            "+        # Supported Git-compatible options",
            "+        parser.add_argument(",
            "+            \"--subdirectory-filter\",",
            "+            type=str,",
            "+            help=\"Only include history for subdirectory\",",
            "+        )",
            "+        parser.add_argument(\"--env-filter\", type=str, help=\"Environment filter command\")",
            "+        parser.add_argument(\"--tree-filter\", type=str, help=\"Tree filter command\")",
            "+        parser.add_argument(\"--index-filter\", type=str, help=\"Index filter command\")",
            "+        parser.add_argument(\"--parent-filter\", type=str, help=\"Parent filter command\")",
            "+        parser.add_argument(\"--msg-filter\", type=str, help=\"Message filter command\")",
            "+        parser.add_argument(\"--commit-filter\", type=str, help=\"Commit filter command\")",
            "+        parser.add_argument(",
            "+            \"--tag-name-filter\", type=str, help=\"Tag name filter command\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--prune-empty\", action=\"store_true\", help=\"Remove empty commits\"",
            "+        )",
            "+        parser.add_argument(",
            "+            \"--original\",",
            "+            type=str,",
            "+            default=\"refs/original\",",
            "+            help=\"Namespace for original refs\",",
            "+        )",
            "+        parser.add_argument(",
            "+            \"-f\",",
            "+            \"--force\",",
            "+            action=\"store_true\",",
            "+            help=\"Force operation even if refs/original/* exists\",",
            "+        )",
            "+",
            "+        # Branch/ref to rewrite (defaults to HEAD)",
            "+        parser.add_argument(",
            "+            \"branch\", nargs=\"?\", default=\"HEAD\", help=\"Branch or ref to rewrite\"",
            "+        )",
            "+",
            "+        args = parser.parse_args(args)",
            "+",
            "+        # Track if any filter fails",
            "+        filter_error = False",
            "+",
            "+        # Setup environment for filters",
            "+        env = os.environ.copy()",
            "+",
            "+        # Helper function to run shell commands",
            "+        def run_filter(cmd, input_data=None, cwd=None, extra_env=None):",
            "+            nonlocal filter_error",
            "+            filter_env = env.copy()",
            "+            if extra_env:",
            "+                filter_env.update(extra_env)",
            "+            result = subprocess.run(",
            "+                cmd,",
            "+                shell=True,",
            "+                input=input_data,",
            "+                cwd=cwd,",
            "+                env=filter_env,",
            "+                capture_output=True,",
            "+            )",
            "+            if result.returncode != 0:",
            "+                filter_error = True",
            "+                return None",
            "+            return result.stdout",
            "+",
            "+        # Create filter functions based on arguments",
            "+        filter_message = None",
            "+        if args.msg_filter:",
            "+",
            "+            def filter_message(message):",
            "+                result = run_filter(args.msg_filter, input_data=message)",
            "+                return result if result is not None else message",
            "+",
            "+        tree_filter = None",
            "+        if args.tree_filter:",
            "+",
            "+            def tree_filter(tree_sha, tmpdir):",
            "+                from dulwich.objects import Blob, Tree",
            "+",
            "+                # Export tree to tmpdir",
            "+                with Repo(\".\") as r:",
            "+                    tree = r.object_store[tree_sha]",
            "+                    for entry in tree.items():",
            "+                        path = Path(tmpdir) / entry.path.decode()",
            "+                        if entry.mode & 0o040000:  # Directory",
            "+                            path.mkdir(exist_ok=True)",
            "+                        else:",
            "+                            obj = r.object_store[entry.sha]",
            "+                            path.write_bytes(obj.data)",
            "+",
            "+                    # Run the filter command in the temp directory",
            "+                    run_filter(args.tree_filter, cwd=tmpdir)",
            "+",
            "+                    # Rebuild tree from modified temp directory",
            "+                    def build_tree_from_dir(dir_path):",
            "+                        tree = Tree()",
            "+                        for name in sorted(os.listdir(dir_path)):",
            "+                            if name.startswith(\".\"):",
            "+                                continue",
            "+                            path = os.path.join(dir_path, name)",
            "+                            if os.path.isdir(path):",
            "+                                subtree_sha = build_tree_from_dir(path)",
            "+                                tree.add(name.encode(), 0o040000, subtree_sha)",
            "+                            else:",
            "+                                with open(path, \"rb\") as f:",
            "+                                    data = f.read()",
            "+                                blob = Blob.from_string(data)",
            "+                                r.object_store.add_object(blob)",
            "+                                # Use appropriate file mode",
            "+                                mode = os.stat(path).st_mode",
            "+                                if mode & 0o100:",
            "+                                    file_mode = 0o100755",
            "+                                else:",
            "+                                    file_mode = 0o100644",
            "+                                tree.add(name.encode(), file_mode, blob.id)",
            "+                        r.object_store.add_object(tree)",
            "+                        return tree.id",
            "+",
            "+                    return build_tree_from_dir(tmpdir)",
            "+",
            "+        index_filter = None",
            "+        if args.index_filter:",
            "+",
            "+            def index_filter(tree_sha, index_path):",
            "+                run_filter(args.index_filter, extra_env={\"GIT_INDEX_FILE\": index_path})",
            "+                return None  # Read back from index",
            "+",
            "+        parent_filter = None",
            "+        if args.parent_filter:",
            "+",
            "+            def parent_filter(parents):",
            "+                parent_str = \" \".join(p.hex() for p in parents)",
            "+                result = run_filter(args.parent_filter, input_data=parent_str.encode())",
            "+                if result is None:",
            "+                    return parents",
            "+",
            "+                output = result.decode().strip()",
            "+                if not output:",
            "+                    return []",
            "+                new_parents = []",
            "+                for sha in output.split():",
            "+                    if valid_hexsha(sha):",
            "+                        new_parents.append(sha)",
            "+                return new_parents",
            "+",
            "+        commit_filter = None",
            "+        if args.commit_filter:",
            "+",
            "+            def commit_filter(commit_obj, tree_sha):",
            "+                # The filter receives: tree parent1 parent2...",
            "+                cmd_input = tree_sha.hex()",
            "+                for parent in commit_obj.parents:",
            "+                    cmd_input += \" \" + parent.hex()",
            "+",
            "+                result = run_filter(",
            "+                    args.commit_filter,",
            "+                    input_data=cmd_input.encode(),",
            "+                    extra_env={\"GIT_COMMIT\": commit_obj.id.hex()},",
            "+                )",
            "+                if result is None:",
            "+                    return None",
            "+",
            "+                output = result.decode().strip()",
            "+                if not output:",
            "+                    return None  # Skip commit",
            "+",
            "+                if valid_hexsha(output):",
            "+                    return output",
            "+                return None",
            "+",
            "+        tag_name_filter = None",
            "+        if args.tag_name_filter:",
            "+",
            "+            def tag_name_filter(tag_name):",
            "+                result = run_filter(args.tag_name_filter, input_data=tag_name)",
            "+                return result.strip() if result is not None else tag_name",
            "+",
            "+        # Open repo once",
            "+        with Repo(\".\") as r:",
            "+            # Check for refs/original if not forcing",
            "+            if not args.force:",
            "+                original_prefix = args.original.encode() + b\"/\"",
            "+                for ref in r.refs.allkeys():",
            "+                    if ref.startswith(original_prefix):",
            "+                        print(\"Cannot create a new backup.\")",
            "+                        print(f\"A previous backup already exists in {args.original}/\")",
            "+                        print(\"Force overwriting the backup with -f\")",
            "+                        return 1",
            "+",
            "+            try:",
            "+                # Call porcelain.filter_branch with the repo object",
            "+                result = porcelain.filter_branch(",
            "+                    r,",
            "+                    args.branch,",
            "+                    filter_message=filter_message,",
            "+                    tree_filter=tree_filter if args.tree_filter else None,",
            "+                    index_filter=index_filter if args.index_filter else None,",
            "+                    parent_filter=parent_filter if args.parent_filter else None,",
            "+                    commit_filter=commit_filter if args.commit_filter else None,",
            "+                    subdirectory_filter=args.subdirectory_filter,",
            "+                    prune_empty=args.prune_empty,",
            "+                    tag_name_filter=tag_name_filter if args.tag_name_filter else None,",
            "+                    force=args.force,",
            "+                    keep_original=True,  # Always keep original with git",
            "+                )",
            "+",
            "+                # Check if any filter failed",
            "+                if filter_error:",
            "+                    print(\"Error: Filter command failed\", file=sys.stderr)",
            "+                    return 1",
            "+",
            "+                # Git filter-branch shows progress",
            "+                if result:",
            "+                    print(f\"Rewrite {args.branch} ({len(result)} commits)\")",
            "+                    # Git shows: Ref 'refs/heads/branch' was rewritten",
            "+                    if args.branch != \"HEAD\":",
            "+                        ref_name = (",
            "+                            args.branch",
            "+                            if args.branch.startswith(\"refs/\")",
            "+                            else f\"refs/heads/{args.branch}\"",
            "+                        )",
            "+                        print(f\"Ref '{ref_name}' was rewritten\")",
            "+",
            "+                return 0",
            "+",
            "+            except porcelain.Error as e:",
            "+                print(f\"Error: {e}\", file=sys.stderr)",
            "+                return 1",
            "+",
            "+",
            " class cmd_help(Command):",
            "     def run(self, args) -> None:",
            "-        parser = optparse.OptionParser()",
            "-        parser.add_option(",
            "+        parser = argparse.ArgumentParser()",
            "+        parser.add_argument(",
            "             \"-a\",",
            "             \"--all\",",
            "-            dest=\"all\",",
            "             action=\"store_true\",",
            "             help=\"List all commands.\",",
            "         )",
            "-        options, args = parser.parse_args(args)",
            "+        args = parser.parse_args(args)",
            " ",
            "-        if options.all:",
            "+        if args.all:",
            "             print(\"Available commands:\")",
            "             for cmd in sorted(commands):",
            "                 print(f\"  {cmd}\")",
            "         else:",
            "             print(",
            "                 \"\"\"\\",
            " The dulwich command line tool is currently a very basic frontend for the",
            "@@ -796,60 +1923,77 @@",
            " For a list of supported commands, see 'dulwich help -a'.",
            " \"\"\"",
            "             )",
            " ",
            " ",
            " commands = {",
            "     \"add\": cmd_add,",
            "+    \"annotate\": cmd_annotate,",
            "     \"archive\": cmd_archive,",
            "+    \"bisect\": cmd_bisect,",
            "+    \"blame\": cmd_blame,",
            "+    \"branch\": cmd_branch,",
            "     \"check-ignore\": cmd_check_ignore,",
            "     \"check-mailmap\": cmd_check_mailmap,",
            "+    \"checkout\": cmd_checkout,",
            "+    \"cherry-pick\": cmd_cherry_pick,",
            "     \"clone\": cmd_clone,",
            "     \"commit\": cmd_commit,",
            "     \"commit-tree\": cmd_commit_tree,",
            "+    \"count-objects\": cmd_count_objects,",
            "     \"describe\": cmd_describe,",
            "     \"daemon\": cmd_daemon,",
            "     \"diff\": cmd_diff,",
            "     \"diff-tree\": cmd_diff_tree,",
            "     \"dump-pack\": cmd_dump_pack,",
            "     \"dump-index\": cmd_dump_index,",
            "     \"fetch-pack\": cmd_fetch_pack,",
            "     \"fetch\": cmd_fetch,",
            "+    \"filter-branch\": cmd_filter_branch,",
            "     \"for-each-ref\": cmd_for_each_ref,",
            "     \"fsck\": cmd_fsck,",
            "+    \"gc\": cmd_gc,",
            "     \"help\": cmd_help,",
            "     \"init\": cmd_init,",
            "     \"log\": cmd_log,",
            "     \"ls-files\": cmd_ls_files,",
            "     \"ls-remote\": cmd_ls_remote,",
            "     \"ls-tree\": cmd_ls_tree,",
            "+    \"merge\": cmd_merge,",
            "+    \"merge-tree\": cmd_merge_tree,",
            "+    \"notes\": cmd_notes,",
            "     \"pack-objects\": cmd_pack_objects,",
            "     \"pack-refs\": cmd_pack_refs,",
            "+    \"prune\": cmd_prune,",
            "     \"pull\": cmd_pull,",
            "     \"push\": cmd_push,",
            "+    \"rebase\": cmd_rebase,",
            "     \"receive-pack\": cmd_receive_pack,",
            "     \"remote\": cmd_remote,",
            "     \"repack\": cmd_repack,",
            "     \"reset\": cmd_reset,",
            "+    \"revert\": cmd_revert,",
            "     \"rev-list\": cmd_rev_list,",
            "     \"rm\": cmd_rm,",
            "+    \"mv\": cmd_mv,",
            "     \"show\": cmd_show,",
            "     \"stash\": cmd_stash,",
            "     \"status\": cmd_status,",
            "     \"symbolic-ref\": cmd_symbolic_ref,",
            "     \"submodule\": cmd_submodule,",
            "     \"tag\": cmd_tag,",
            "+    \"unpack-objects\": cmd_unpack_objects,",
            "     \"update-server-info\": cmd_update_server_info,",
            "     \"upload-pack\": cmd_upload_pack,",
            "     \"web-daemon\": cmd_web_daemon,",
            "     \"write-tree\": cmd_write_tree,",
            " }",
            " ",
            " ",
            "-def main(argv=None):",
            "+def main(argv=None) -> Optional[int]:",
            "     if argv is None:",
            "         argv = sys.argv[1:]",
            " ",
            "     if len(argv) < 1:",
            "         print(\"Usage: dulwich <{}> [OPTIONS...]\".format(\"|\".join(commands.keys())))",
            "         return 1"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/client.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/client.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/client.py",
            "@@ -36,14 +36,15 @@",
            " Known capabilities that are not supported:",
            " ",
            "  * no-progress",
            "  * include-tag",
            " \"\"\"",
            " ",
            " import copy",
            "+import functools",
            " import logging",
            " import os",
            " import select",
            " import socket",
            " import subprocess",
            " import sys",
            " from collections.abc import Iterable, Iterator",
            "@@ -117,23 +118,24 @@",
            "     filter_ref_prefix,",
            "     parse_capability,",
            "     pkt_line,",
            "     pkt_seq,",
            " )",
            " from .refs import (",
            "     PEELED_TAG_SUFFIX,",
            "+    SYMREF,",
            "     Ref,",
            "     _import_remote_refs,",
            "     _set_default_branch,",
            "     _set_head,",
            "     _set_origin_head,",
            "     read_info_refs,",
            "     split_peeled_refs,",
            " )",
            "-from .repo import Repo",
            "+from .repo import BaseRepo, Repo",
            " ",
            " # Default ref prefix, used if none is specified.",
            " # GitHub defaults to just sending HEAD if no ref-prefix is",
            " # specified, so explicitly request all refs to match",
            " # behaviour with v1 when no ref-prefix is specified.",
            " DEFAULT_REF_PREFIX = [b\"HEAD\", b\"refs/\"]",
            " ",
            "@@ -311,22 +313,16 @@",
            "         return {}, set()",
            "     if refs == {CAPABILITIES_REF: ZERO_SHA}:",
            "         refs = {}",
            "     assert server_capabilities is not None",
            "     return refs, set(server_capabilities)",
            " ",
            " ",
            "-class FetchPackResult:",
            "-    \"\"\"Result of a fetch-pack operation.",
            "-",
            "-    Attributes:",
            "-      refs: Dictionary with all remote refs",
            "-      symrefs: Dictionary with remote symrefs",
            "-      agent: User agent string",
            "-    \"\"\"",
            "+class _DeprecatedDictProxy:",
            "+    \"\"\"Base class for result objects that provide deprecated dict-like interface.\"\"\"",
            " ",
            "     _FORWARDED_ATTRS: ClassVar[set[str]] = {",
            "         \"clear\",",
            "         \"copy\",",
            "         \"fromkeys\",",
            "         \"get\",",
            "         \"items\",",
            "@@ -337,42 +333,23 @@",
            "         \"update\",",
            "         \"values\",",
            "         \"viewitems\",",
            "         \"viewkeys\",",
            "         \"viewvalues\",",
            "     }",
            " ",
            "-    def __init__(",
            "-        self, refs, symrefs, agent, new_shallow=None, new_unshallow=None",
            "-    ) -> None:",
            "-        self.refs = refs",
            "-        self.symrefs = symrefs",
            "-        self.agent = agent",
            "-        self.new_shallow = new_shallow",
            "-        self.new_unshallow = new_unshallow",
            "-",
            "     def _warn_deprecated(self) -> None:",
            "         import warnings",
            " ",
            "         warnings.warn(",
            "-            \"Use FetchPackResult.refs instead.\",",
            "+            f\"Use {self.__class__.__name__}.refs instead.\",",
            "             DeprecationWarning,",
            "             stacklevel=3,",
            "         )",
            " ",
            "-    def __eq__(self, other):",
            "-        if isinstance(other, dict):",
            "-            self._warn_deprecated()",
            "-            return self.refs == other",
            "-        return (",
            "-            self.refs == other.refs",
            "-            and self.symrefs == other.symrefs",
            "-            and self.agent == other.agent",
            "-        )",
            "-",
            "     def __contains__(self, name) -> bool:",
            "         self._warn_deprecated()",
            "         return name in self.refs",
            " ",
            "     def __getitem__(self, name):",
            "         self._warn_deprecated()",
            "         return self.refs[name]",
            "@@ -382,91 +359,107 @@",
            "         return len(self.refs)",
            " ",
            "     def __iter__(self):",
            "         self._warn_deprecated()",
            "         return iter(self.refs)",
            " ",
            "     def __getattribute__(self, name):",
            "-        if name in type(self)._FORWARDED_ATTRS:",
            "+        # Avoid infinite recursion by checking against class variable directly",
            "+        if name != \"_FORWARDED_ATTRS\" and name in type(self)._FORWARDED_ATTRS:",
            "             self._warn_deprecated()",
            "-            return getattr(self.refs, name)",
            "+            # Direct attribute access to avoid recursion",
            "+            refs = object.__getattribute__(self, \"refs\")",
            "+            return getattr(refs, name)",
            "         return super().__getattribute__(name)",
            " ",
            "+",
            "+class FetchPackResult(_DeprecatedDictProxy):",
            "+    \"\"\"Result of a fetch-pack operation.",
            "+",
            "+    Attributes:",
            "+      refs: Dictionary with all remote refs",
            "+      symrefs: Dictionary with remote symrefs",
            "+      agent: User agent string",
            "+    \"\"\"",
            "+",
            "+    def __init__(",
            "+        self, refs, symrefs, agent, new_shallow=None, new_unshallow=None",
            "+    ) -> None:",
            "+        self.refs = refs",
            "+        self.symrefs = symrefs",
            "+        self.agent = agent",
            "+        self.new_shallow = new_shallow",
            "+        self.new_unshallow = new_unshallow",
            "+",
            "+    def __eq__(self, other):",
            "+        if isinstance(other, dict):",
            "+            self._warn_deprecated()",
            "+            return self.refs == other",
            "+        return (",
            "+            self.refs == other.refs",
            "+            and self.symrefs == other.symrefs",
            "+            and self.agent == other.agent",
            "+        )",
            "+",
            "     def __repr__(self) -> str:",
            "         return f\"{self.__class__.__name__}({self.refs!r}, {self.symrefs!r}, {self.agent!r})\"",
            " ",
            " ",
            "-class SendPackResult:",
            "-    \"\"\"Result of a upload-pack operation.",
            "+class LsRemoteResult(_DeprecatedDictProxy):",
            "+    \"\"\"Result of a ls-remote operation.",
            " ",
            "     Attributes:",
            "       refs: Dictionary with all remote refs",
            "-      agent: User agent string",
            "-      ref_status: Optional dictionary mapping ref name to error message (if it",
            "-        failed to update), or None if it was updated successfully",
            "+      symrefs: Dictionary with remote symrefs",
            "     \"\"\"",
            " ",
            "-    _FORWARDED_ATTRS: ClassVar[set[str]] = {",
            "-        \"clear\",",
            "-        \"copy\",",
            "-        \"fromkeys\",",
            "-        \"get\",",
            "-        \"items\",",
            "-        \"keys\",",
            "-        \"pop\",",
            "-        \"popitem\",",
            "-        \"setdefault\",",
            "-        \"update\",",
            "-        \"values\",",
            "-        \"viewitems\",",
            "-        \"viewkeys\",",
            "-        \"viewvalues\",",
            "-    }",
            "-",
            "-    def __init__(self, refs, agent=None, ref_status=None) -> None:",
            "+    def __init__(self, refs, symrefs) -> None:",
            "         self.refs = refs",
            "-        self.agent = agent",
            "-        self.ref_status = ref_status",
            "+        self.symrefs = symrefs",
            " ",
            "     def _warn_deprecated(self) -> None:",
            "         import warnings",
            " ",
            "         warnings.warn(",
            "-            \"Use SendPackResult.refs instead.\",",
            "+            \"Treating LsRemoteResult as a dictionary is deprecated. \"",
            "+            \"Use result.refs instead.\",",
            "             DeprecationWarning,",
            "             stacklevel=3,",
            "         )",
            " ",
            "     def __eq__(self, other):",
            "         if isinstance(other, dict):",
            "             self._warn_deprecated()",
            "             return self.refs == other",
            "-        return self.refs == other.refs and self.agent == other.agent",
            "+        return self.refs == other.refs and self.symrefs == other.symrefs",
            " ",
            "-    def __contains__(self, name) -> bool:",
            "-        self._warn_deprecated()",
            "-        return name in self.refs",
            "+    def __repr__(self) -> str:",
            "+        return f\"{self.__class__.__name__}({self.refs!r}, {self.symrefs!r})\"",
            " ",
            "-    def __getitem__(self, name):",
            "-        self._warn_deprecated()",
            "-        return self.refs[name]",
            " ",
            "-    def __len__(self) -> int:",
            "-        self._warn_deprecated()",
            "-        return len(self.refs)",
            "+class SendPackResult(_DeprecatedDictProxy):",
            "+    \"\"\"Result of a upload-pack operation.",
            " ",
            "-    def __iter__(self):",
            "-        self._warn_deprecated()",
            "-        return iter(self.refs)",
            "+    Attributes:",
            "+      refs: Dictionary with all remote refs",
            "+      agent: User agent string",
            "+      ref_status: Optional dictionary mapping ref name to error message (if it",
            "+        failed to update), or None if it was updated successfully",
            "+    \"\"\"",
            " ",
            "-    def __getattribute__(self, name):",
            "-        if name in type(self)._FORWARDED_ATTRS:",
            "+    def __init__(self, refs, agent=None, ref_status=None) -> None:",
            "+        self.refs = refs",
            "+        self.agent = agent",
            "+        self.ref_status = ref_status",
            "+",
            "+    def __eq__(self, other):",
            "+        if isinstance(other, dict):",
            "             self._warn_deprecated()",
            "-            return getattr(self.refs, name)",
            "-        return super().__getattribute__(name)",
            "+            return self.refs == other",
            "+        return self.refs == other.refs and self.agent == other.agent",
            " ",
            "     def __repr__(self) -> str:",
            "         return f\"{self.__class__.__name__}({self.refs!r}, {self.agent!r})\"",
            " ",
            " ",
            " def _read_shallow_updates(pkt_seq):",
            "     new_shallow = set()",
            "@@ -571,15 +564,15 @@",
            " ",
            " def _handle_upload_pack_head(",
            "     proto,",
            "     capabilities,",
            "     graph_walker,",
            "     wants,",
            "     can_read,",
            "-    depth,",
            "+    depth: Optional[int],",
            "     protocol_version,",
            " ):",
            "     \"\"\"Handle the head of a 'git-upload-pack' request.",
            " ",
            "     Args:",
            "       proto: Protocol object to read from",
            "       capabilities: List of negotiated capabilities",
            "@@ -596,23 +589,23 @@",
            "         protocol_version = DEFAULT_GIT_PROTOCOL_VERSION_SEND",
            "     if protocol_version != 2:",
            "         wantcmd += b\" \" + b\" \".join(sorted(capabilities))",
            "     wantcmd += b\"\\n\"",
            "     proto.write_pkt_line(wantcmd)",
            "     for want in wants[1:]:",
            "         proto.write_pkt_line(COMMAND_WANT + b\" \" + want + b\"\\n\")",
            "-    if depth not in (0, None) or getattr(graph_walker, \"shallow\", None):",
            "+    if depth not in (0, None) or graph_walker.shallow:",
            "         if protocol_version == 2:",
            "             if not find_capability(capabilities, CAPABILITY_FETCH, CAPABILITY_SHALLOW):",
            "                 raise GitProtocolError(",
            "-                    \"server does not support shallow capability required for \" \"depth\"",
            "+                    \"server does not support shallow capability required for depth\"",
            "                 )",
            "         elif CAPABILITY_SHALLOW not in capabilities:",
            "             raise GitProtocolError(",
            "-                \"server does not support shallow capability required for \" \"depth\"",
            "+                \"server does not support shallow capability required for depth\"",
            "             )",
            "         for sha in graph_walker.shallow:",
            "             proto.write_pkt_line(COMMAND_SHALLOW + b\" \" + sha + b\"\\n\")",
            "         if depth is not None:",
            "             proto.write_pkt_line(",
            "                 COMMAND_DEEPEN + b\" \" + str(depth).encode(\"ascii\") + b\"\\n\"",
            "             )",
            "@@ -827,15 +820,15 @@",
            "         target_path,",
            "         mkdir: bool = True,",
            "         bare: bool = False,",
            "         origin: Optional[str] = \"origin\",",
            "         checkout=None,",
            "         branch=None,",
            "         progress=None,",
            "-        depth=None,",
            "+        depth: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "         filter_spec=None,",
            "         protocol_version: Optional[int] = None,",
            "     ) -> Repo:",
            "         \"\"\"Clone a repository.\"\"\"",
            "         if mkdir:",
            "             os.mkdir(target_path)",
            "@@ -918,15 +911,15 @@",
            "                 shutil.rmtree(target_path)",
            "             raise",
            "         return target",
            " ",
            "     def fetch(",
            "         self,",
            "         path: str,",
            "-        target: Repo,",
            "+        target: BaseRepo,",
            "         determine_wants: Optional[",
            "             Callable[[dict[bytes, bytes], Optional[int]], list[bytes]]",
            "         ] = None,",
            "         progress: Optional[Callable[[bytes], None]] = None,",
            "         depth: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "         filter_spec: Optional[bytes] = None,",
            "@@ -1037,19 +1030,24 @@",
            "         raise NotImplementedError(self.fetch_pack)",
            " ",
            "     def get_refs(",
            "         self,",
            "         path,",
            "         protocol_version: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "-    ) -> dict[Ref, ObjectID]:",
            "+    ) -> LsRemoteResult:",
            "         \"\"\"Retrieve the current refs from a git smart server.",
            " ",
            "         Args:",
            "           path: Path to the repo to fetch from. (as bytestring)",
            "+          protocol_version: Desired Git protocol version.",
            "+          ref_prefix: Prefix filter for refs.",
            "+",
            "+        Returns:",
            "+          LsRemoteResult object with refs and symrefs",
            "         \"\"\"",
            "         raise NotImplementedError(self.get_refs)",
            " ",
            "     @staticmethod",
            "     def _should_send_pack(new_refs):",
            "         # The packfile MUST NOT be sent if the only command used is delete.",
            "         return any(sha != ZERO_SHA for sha in new_refs.values())",
            "@@ -1310,15 +1308,15 @@",
            "     def fetch_pack(",
            "         self,",
            "         path,",
            "         determine_wants,",
            "         graph_walker,",
            "         pack_data,",
            "         progress=None,",
            "-        depth=None,",
            "+        depth: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "         filter_spec=None,",
            "         protocol_version: Optional[int] = None,",
            "     ):",
            "         \"\"\"Retrieve a pack from a git smart server.",
            " ",
            "         Args:",
            "@@ -1411,14 +1409,16 @@",
            "                 wants = [cid for cid in wants if cid != ZERO_SHA]",
            "             if not wants:",
            "                 proto.write_pkt_line(None)",
            "                 return FetchPackResult(refs, symrefs, agent)",
            "             if self.protocol_version == 2:",
            "                 proto.write_pkt_line(b\"command=fetch\\n\")",
            "                 proto.write(b\"0001\")  # delim-pkt",
            "+                if CAPABILITY_THIN_PACK in self._fetch_capabilities:",
            "+                    proto.write(pkt_line(b\"thin-pack\\n\"))",
            "                 if (",
            "                     find_capability(",
            "                         negotiated_capabilities, CAPABILITY_FETCH, CAPABILITY_FILTER",
            "                     )",
            "                     and filter_spec",
            "                 ):",
            "                     proto.write(pkt_line(b\"filter %s\\n\" % filter_spec))",
            "@@ -1478,32 +1478,32 @@",
            "             if ref_prefix is None:",
            "                 ref_prefix = DEFAULT_REF_PREFIX",
            "             for prefix in ref_prefix:",
            "                 proto.write_pkt_line(b\"ref-prefix \" + prefix)",
            "             proto.write_pkt_line(None)",
            "             with proto:",
            "                 try:",
            "-                    refs, _symrefs, peeled = read_pkt_refs_v2(proto.read_pkt_seq())",
            "+                    refs, symrefs, peeled = read_pkt_refs_v2(proto.read_pkt_seq())",
            "                 except HangupException as exc:",
            "                     raise _remote_error_from_stderr(stderr) from exc",
            "                 proto.write_pkt_line(None)",
            "                 for refname, refvalue in peeled.items():",
            "                     refs[refname + PEELED_TAG_SUFFIX] = refvalue",
            "-                return refs",
            "+                return LsRemoteResult(refs, symrefs)",
            "         else:",
            "             with proto:",
            "                 try:",
            "                     refs, server_capabilities = read_pkt_refs_v1(proto.read_pkt_seq())",
            "                 except HangupException as exc:",
            "                     raise _remote_error_from_stderr(stderr) from exc",
            "                 proto.write_pkt_line(None)",
            "-                (_symrefs, _agent) = _extract_symrefs_and_agent(server_capabilities)",
            "+                (symrefs, _agent) = _extract_symrefs_and_agent(server_capabilities)",
            "                 if ref_prefix is not None:",
            "                     refs = filter_ref_prefix(refs, ref_prefix)",
            "-                return refs",
            "+                return LsRemoteResult(refs, symrefs)",
            " ",
            "     def archive(",
            "         self,",
            "         path,",
            "         committish,",
            "         write_data,",
            "         progress=None,",
            "@@ -1527,15 +1527,15 @@",
            "                 pkt = proto.read_pkt_line()",
            "             except HangupException as exc:",
            "                 raise _remote_error_from_stderr(stderr) from exc",
            "             if pkt == b\"NACK\\n\" or pkt == b\"NACK\":",
            "                 return",
            "             elif pkt == b\"ACK\\n\" or pkt == b\"ACK\":",
            "                 pass",
            "-            elif pkt.startswith(b\"ERR \"):",
            "+            elif pkt and pkt.startswith(b\"ERR \"):",
            "                 raise GitProtocolError(pkt[4:].rstrip(b\"\\n\").decode(\"utf-8\", \"replace\"))",
            "             else:",
            "                 raise AssertionError(f\"invalid response {pkt!r}\")",
            "             ret = proto.read_pkt_line()",
            "             if ret is not None:",
            "                 raise AssertionError(\"expected pkt tail\")",
            "             for chan, data in _read_side_band64k_data(proto.read_pkt_seq()):",
            "@@ -1655,20 +1655,27 @@",
            "             from msvcrt import get_osfhandle",
            " ",
            "             handle = get_osfhandle(self.proc.stdout.fileno())",
            "             return _win32_peek_avail(handle) != 0",
            "         else:",
            "             return _fileno_can_read(self.proc.stdout.fileno())",
            " ",
            "-    def close(self) -> None:",
            "+    def close(self, timeout: Optional[int] = 60) -> None:",
            "         self.proc.stdin.close()",
            "         self.proc.stdout.close()",
            "         if self.proc.stderr:",
            "             self.proc.stderr.close()",
            "-        self.proc.wait()",
            "+        try:",
            "+            self.proc.wait(timeout=timeout)",
            "+        except subprocess.TimeoutExpired as e:",
            "+            self.proc.kill()",
            "+            self.proc.wait()",
            "+            raise GitProtocolError(",
            "+                f\"Git subprocess did not terminate within {timeout} seconds; killed it.\"",
            "+            ) from e",
            " ",
            " ",
            " def find_git_command() -> list[str]:",
            "     \"\"\"Find command to run for system Git (usually C Git).\"\"\"",
            "     if sys.platform == \"win32\":  # support .exe, .bat and .cmd",
            "         try:  # to avoid overhead",
            "             import pywintypes",
            "@@ -1818,15 +1825,15 @@",
            "                         ref_status[refname] = \"unable to remove\"",
            " ",
            "         return SendPackResult(new_refs, ref_status=ref_status)",
            " ",
            "     def fetch(",
            "         self,",
            "         path: str,",
            "-        target: Repo,",
            "+        target: BaseRepo,",
            "         determine_wants: Optional[",
            "             Callable[[dict[bytes, bytes], Optional[int]], list[bytes]]",
            "         ] = None,",
            "         progress: Optional[Callable[[bytes], None]] = None,",
            "         depth: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "         filter_spec: Optional[bytes] = None,",
            "@@ -1866,15 +1873,15 @@",
            "     def fetch_pack(",
            "         self,",
            "         path,",
            "         determine_wants,",
            "         graph_walker,",
            "         pack_data,",
            "         progress=None,",
            "-        depth=None,",
            "+        depth: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "         filter_spec: Optional[bytes] = None,",
            "         protocol_version: Optional[int] = None,",
            "     ) -> FetchPackResult:",
            "         \"\"\"Retrieve a pack from a local on-disk repository.",
            " ",
            "         Args:",
            "@@ -1919,15 +1926,28 @@",
            "         self,",
            "         path,",
            "         protocol_version: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "     ):",
            "         \"\"\"Retrieve the current refs from a local on-disk repository.\"\"\"",
            "         with self._open_repo(path) as target:",
            "-            return target.get_refs()",
            "+            refs = target.get_refs()",
            "+            # Extract symrefs from the local repository",
            "+            symrefs = {}",
            "+            for ref in refs:",
            "+                try:",
            "+                    # Check if this ref is symbolic by reading it directly",
            "+                    ref_value = target.refs.read_ref(ref)",
            "+                    if ref_value and ref_value.startswith(SYMREF):",
            "+                        # Extract the target from the symref",
            "+                        symrefs[ref] = ref_value[len(SYMREF) :]",
            "+                except (KeyError, ValueError):",
            "+                    # Not a symbolic ref or error reading it",
            "+                    pass",
            "+            return LsRemoteResult(refs, symrefs)",
            " ",
            " ",
            " # What Git client to use for local access",
            " default_local_git_client_cls = LocalGitClient",
            " ",
            " ",
            " class SSHVendor:",
            "@@ -2098,15 +2118,15 @@",
            "     )",
            "     from .contrib.paramiko_vendor import ParamikoSSHVendor",
            " ",
            "     return ParamikoSSHVendor(**kwargs)",
            " ",
            " ",
            " # Can be overridden by users",
            "-get_ssh_vendor = SubprocessSSHVendor",
            "+get_ssh_vendor: Callable[[], SSHVendor] = SubprocessSSHVendor",
            " ",
            " ",
            " class SSHGitClient(TraditionalGitClient):",
            "     def __init__(",
            "         self,",
            "         host,",
            "         port=None,",
            "@@ -2119,17 +2139,33 @@",
            "         **kwargs,",
            "     ) -> None:",
            "         self.host = host",
            "         self.port = port",
            "         self.username = username",
            "         self.password = password",
            "         self.key_filename = key_filename",
            "-        self.ssh_command = ssh_command or os.environ.get(",
            "-            \"GIT_SSH_COMMAND\", os.environ.get(\"GIT_SSH\")",
            "-        )",
            "+        # Priority: ssh_command parameter, then env vars, then core.sshCommand config",
            "+        if ssh_command:",
            "+            self.ssh_command = ssh_command",
            "+        else:",
            "+            # Check environment variables first",
            "+            self.ssh_command = os.environ.get(",
            "+                \"GIT_SSH_COMMAND\", os.environ.get(\"GIT_SSH\")",
            "+            )",
            "+",
            "+            # Fall back to config if no environment variable set",
            "+            if not self.ssh_command and config is not None:",
            "+                try:",
            "+                    config_ssh_command = config.get((b\"core\",), b\"sshCommand\")",
            "+                    self.ssh_command = (",
            "+                        config_ssh_command.decode() if config_ssh_command else None",
            "+                    )",
            "+                except KeyError:",
            "+                    pass",
            "+",
            "         super().__init__(**kwargs)",
            "         self.alternative_paths: dict[bytes, bytes] = {}",
            "         if vendor is not None:",
            "             self.ssh_vendor = vendor",
            "         else:",
            "             self.ssh_vendor = get_ssh_vendor()",
            " ",
            "@@ -2210,22 +2246,24 @@",
            " ",
            " ",
            " def default_urllib3_manager(",
            "     config,",
            "     pool_manager_cls=None,",
            "     proxy_manager_cls=None,",
            "     base_url=None,",
            "+    timeout=None,",
            "     **override_kwargs,",
            " ) -> Union[\"urllib3.ProxyManager\", \"urllib3.PoolManager\"]:",
            "     \"\"\"Return urllib3 connection pool manager.",
            " ",
            "     Honour detected proxy configurations.",
            " ",
            "     Args:",
            "       config: `dulwich.config.ConfigDict` instance with Git configuration.",
            "+      timeout: Timeout for HTTP requests in seconds",
            "       override_kwargs: Additional arguments for `urllib3.ProxyManager`",
            " ",
            "     Returns:",
            "       Either pool_manager_cls (defaults to `urllib3.ProxyManager`) instance for",
            "       proxy configurations, proxy_manager_cls",
            "       (defaults to `urllib3.PoolManager`) instance otherwise",
            " ",
            "@@ -2261,22 +2299,35 @@",
            "             ssl_verify = True",
            " ",
            "         try:",
            "             ca_certs = config.get(b\"http\", b\"sslCAInfo\")",
            "         except KeyError:",
            "             ca_certs = None",
            " ",
            "+        # Check for timeout configuration",
            "+        if timeout is None:",
            "+            try:",
            "+                timeout = config.get(b\"http\", b\"timeout\")",
            "+                if timeout is not None:",
            "+                    timeout = int(timeout)",
            "+            except KeyError:",
            "+                pass",
            "+",
            "     if user_agent is None:",
            "         user_agent = default_user_agent_string()",
            " ",
            "     headers = {\"User-agent\": user_agent}",
            " ",
            "     kwargs = {",
            "         \"ca_certs\": ca_certs,",
            "     }",
            "+",
            "+    # Add timeout if specified",
            "+    if timeout is not None:",
            "+        kwargs[\"timeout\"] = timeout",
            "     if ssl_verify is True:",
            "         kwargs[\"cert_reqs\"] = \"CERT_REQUIRED\"",
            "     elif ssl_verify is False:",
            "         kwargs[\"cert_reqs\"] = \"CERT_NONE\"",
            "     else:",
            "         # Default to SSL verification",
            "         kwargs[\"cert_reqs\"] = \"CERT_REQUIRED\"",
            "@@ -2372,15 +2423,15 @@",
            "     \"\"\"",
            " ",
            "     def __init__(self, base_url, dumb=False, **kwargs) -> None:",
            "         self._base_url = base_url.rstrip(\"/\") + \"/\"",
            "         self.dumb = dumb",
            "         GitClient.__init__(self, **kwargs)",
            " ",
            "-    def _http_request(self, url, headers=None, data=None):",
            "+    def _http_request(self, url, headers=None, data=None, raise_for_status=True):",
            "         \"\"\"Perform HTTP request.",
            " ",
            "         Args:",
            "           url: Request URL.",
            "           headers: Optional custom headers to override defaults.",
            "           data: Request data.",
            " ",
            "@@ -2464,15 +2515,15 @@",
            " ",
            "                     resp, read = self._smart_request(",
            "                         service.decode(\"ascii\"), base_url, body",
            "                     )",
            "                     proto = Protocol(read, None)",
            "                     return server_capabilities, resp, read, proto",
            " ",
            "-                proto = Protocol(read, None)",
            "+                proto = Protocol(read, None)  # type: ignore",
            "                 server_protocol_version = negotiate_protocol_version(proto)",
            "                 if server_protocol_version not in GIT_PROTOCOL_VERSIONS:",
            "                     raise ValueError(",
            "                         f\"unknown Git protocol version {server_protocol_version} used by server\"",
            "                     )",
            "                 if protocol_version and server_protocol_version > protocol_version:",
            "                     raise ValueError(",
            "@@ -2522,15 +2573,22 @@",
            "                             server_capabilities",
            "                         )",
            "                         if ref_prefix is not None:",
            "                             refs = filter_ref_prefix(refs, ref_prefix)",
            "                     return refs, server_capabilities, base_url, symrefs, peeled",
            "             else:",
            "                 self.protocol_version = 0  # dumb servers only support protocol v0",
            "-                (refs, peeled) = split_peeled_refs(read_info_refs(resp))",
            "+                # Read all the response data",
            "+                data = b\"\"",
            "+                while True:",
            "+                    chunk = read(4096)",
            "+                    if not chunk:",
            "+                        break",
            "+                    data += chunk",
            "+                (refs, peeled) = split_peeled_refs(read_info_refs(BytesIO(data)))",
            "                 if ref_prefix is not None:",
            "                     refs = filter_ref_prefix(refs, ref_prefix)",
            "                 return refs, set(), base_url, {}, peeled",
            "         finally:",
            "             resp.close()",
            " ",
            "     def _smart_request(self, service, url, data):",
            "@@ -2625,15 +2683,15 @@",
            "     def fetch_pack(",
            "         self,",
            "         path,",
            "         determine_wants,",
            "         graph_walker,",
            "         pack_data,",
            "         progress=None,",
            "-        depth=None,",
            "+        depth: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "         filter_spec=None,",
            "         protocol_version: Optional[int] = None,",
            "     ):",
            "         \"\"\"Retrieve a pack from a git smart server.",
            " ",
            "         Args:",
            "@@ -2672,31 +2730,62 @@",
            "             symrefs = capa_symrefs",
            "         if depth is not None:",
            "             wants = determine_wants(refs, depth=depth)",
            "         else:",
            "             wants = determine_wants(refs)",
            "         if wants is not None:",
            "             wants = [cid for cid in wants if cid != ZERO_SHA]",
            "-        if not wants:",
            "+        if not wants and not self.dumb:",
            "+            return FetchPackResult(refs, symrefs, agent)",
            "+        elif self.dumb:",
            "+            # Use dumb HTTP protocol",
            "+            from .dumb import DumbRemoteHTTPRepo",
            "+",
            "+            # Pass http_request function",
            "+            dumb_repo = DumbRemoteHTTPRepo(",
            "+                url, functools.partial(self._http_request, raise_for_status=False)",
            "+            )",
            "+",
            "+            # Fetch pack data from dumb remote",
            "+            pack_data_list = list(",
            "+                dumb_repo.fetch_pack_data(",
            "+                    graph_walker, lambda refs: wants, progress=progress, depth=depth",
            "+                )",
            "+            )",
            "+",
            "+            symrefs[b\"HEAD\"] = dumb_repo.get_head()",
            "+",
            "+            # Write pack data",
            "+            if pack_data:",
            "+                from .pack import write_pack_data",
            "+",
            "+                # Write pack data directly using the unpacked objects",
            "+                write_pack_data(",
            "+                    pack_data,",
            "+                    iter(pack_data_list),",
            "+                    num_records=len(pack_data_list),",
            "+                    progress=progress,",
            "+                )",
            "+",
            "             return FetchPackResult(refs, symrefs, agent)",
            "-        if self.dumb:",
            "-            raise NotImplementedError(self.fetch_pack)",
            "         req_data = BytesIO()",
            "-        req_proto = Protocol(None, req_data.write)",
            "+        req_proto = Protocol(None, req_data.write)  # type: ignore",
            "         (new_shallow, new_unshallow) = _handle_upload_pack_head(",
            "             req_proto,",
            "             negotiated_capabilities,",
            "             graph_walker,",
            "             wants,",
            "             can_read=None,",
            "             depth=depth,",
            "             protocol_version=self.protocol_version,",
            "         )",
            "         if self.protocol_version == 2:",
            "             data = pkt_line(b\"command=fetch\\n\") + b\"0001\"",
            "+            if CAPABILITY_THIN_PACK in self._fetch_capabilities:",
            "+                data += pkt_line(b\"thin-pack\\n\")",
            "             if (",
            "                 find_capability(",
            "                     negotiated_capabilities, CAPABILITY_FETCH, CAPABILITY_FILTER",
            "                 )",
            "                 and filter_spec",
            "             ):",
            "                 data += pkt_line(b\"filter %s\\n\" % filter_spec)",
            "@@ -2705,15 +2794,15 @@",
            "             data += req_data.getvalue()",
            "         else:",
            "             if filter_spec:",
            "                 self._warn_filter_objects()",
            "             data = req_data.getvalue()",
            "         resp, read = self._smart_request(\"git-upload-pack\", url, data)",
            "         try:",
            "-            resp_proto = Protocol(read, None)",
            "+            resp_proto = Protocol(read, None)  # type: ignore",
            "             if new_shallow is None and new_unshallow is None:",
            "                 (new_shallow, new_unshallow) = _read_shallow_updates(",
            "                     resp_proto.read_pkt_seq()",
            "                 )",
            "             _handle_upload_pack_tail(",
            "                 resp_proto,",
            "                 negotiated_capabilities,",
            "@@ -2730,23 +2819,23 @@",
            "         self,",
            "         path,",
            "         protocol_version: Optional[int] = None,",
            "         ref_prefix: Optional[list[Ref]] = None,",
            "     ):",
            "         \"\"\"Retrieve the current refs from a git smart server.\"\"\"",
            "         url = self._get_url(path)",
            "-        refs, _, _, _, peeled = self._discover_references(",
            "+        refs, _, _, symrefs, peeled = self._discover_references(",
            "             b\"git-upload-pack\",",
            "             url,",
            "             protocol_version=protocol_version,",
            "             ref_prefix=ref_prefix,",
            "         )",
            "         for refname, refvalue in peeled.items():",
            "             refs[refname + PEELED_TAG_SUFFIX] = refvalue",
            "-        return refs",
            "+        return LsRemoteResult(refs, symrefs)",
            " ",
            "     def get_url(self, path):",
            "         return self._get_url(path).rstrip(\"/\")",
            " ",
            "     def _get_url(self, path):",
            "         return urljoin(self._base_url, path).rstrip(\"/\") + \"/\"",
            " ",
            "@@ -2781,21 +2870,25 @@",
            "         self,",
            "         base_url,",
            "         dumb=None,",
            "         pool_manager=None,",
            "         config=None,",
            "         username=None,",
            "         password=None,",
            "+        timeout=None,",
            "         **kwargs,",
            "     ) -> None:",
            "         self._username = username",
            "         self._password = password",
            "+        self._timeout = timeout",
            " ",
            "         if pool_manager is None:",
            "-            self.pool_manager = default_urllib3_manager(config, base_url=base_url)",
            "+            self.pool_manager = default_urllib3_manager(",
            "+                config, base_url=base_url, timeout=timeout",
            "+            )",
            "         else:",
            "             self.pool_manager = pool_manager",
            " ",
            "         if username is not None:",
            "             # No escaping needed: \":\" is not allowed in username:",
            "             # https://tools.ietf.org/html/rfc2617#section-2",
            "             credentials = f\"{username}:{password or ''}\"",
            "@@ -2811,42 +2904,47 @@",
            "     def _get_url(self, path):",
            "         if not isinstance(path, str):",
            "             # urllib3.util.url._encode_invalid_chars() converts the path back",
            "             # to bytes using the utf-8 codec.",
            "             path = path.decode(\"utf-8\")",
            "         return urljoin(self._base_url, path).rstrip(\"/\") + \"/\"",
            " ",
            "-    def _http_request(self, url, headers=None, data=None):",
            "+    def _http_request(self, url, headers=None, data=None, raise_for_status=True):",
            "         import urllib3.exceptions",
            " ",
            "         req_headers = self.pool_manager.headers.copy()",
            "         if headers is not None:",
            "             req_headers.update(headers)",
            "         req_headers[\"Pragma\"] = \"no-cache\"",
            " ",
            "         try:",
            "+            request_kwargs = {",
            "+                \"headers\": req_headers,",
            "+                \"preload_content\": False,",
            "+            }",
            "+            if self._timeout is not None:",
            "+                request_kwargs[\"timeout\"] = self._timeout",
            "+",
            "             if data is None:",
            "-                resp = self.pool_manager.request(",
            "-                    \"GET\", url, headers=req_headers, preload_content=False",
            "-                )",
            "+                resp = self.pool_manager.request(\"GET\", url, **request_kwargs)",
            "             else:",
            "-                resp = self.pool_manager.request(",
            "-                    \"POST\", url, headers=req_headers, body=data, preload_content=False",
            "-                )",
            "+                request_kwargs[\"body\"] = data",
            "+                resp = self.pool_manager.request(\"POST\", url, **request_kwargs)",
            "         except urllib3.exceptions.HTTPError as e:",
            "             raise GitProtocolError(str(e)) from e",
            " ",
            "-        if resp.status == 404:",
            "-            raise NotGitRepository",
            "-        if resp.status == 401:",
            "-            raise HTTPUnauthorized(resp.headers.get(\"WWW-Authenticate\"), url)",
            "-        if resp.status == 407:",
            "-            raise HTTPProxyUnauthorized(resp.headers.get(\"Proxy-Authenticate\"), url)",
            "-        if resp.status != 200:",
            "-            raise GitProtocolError(f\"unexpected http resp {resp.status} for {url}\")",
            "+        if raise_for_status:",
            "+            if resp.status == 404:",
            "+                raise NotGitRepository",
            "+            if resp.status == 401:",
            "+                raise HTTPUnauthorized(resp.headers.get(\"WWW-Authenticate\"), url)",
            "+            if resp.status == 407:",
            "+                raise HTTPProxyUnauthorized(resp.headers.get(\"Proxy-Authenticate\"), url)",
            "+            if resp.status != 200:",
            "+                raise GitProtocolError(f\"unexpected http resp {resp.status} for {url}\")",
            " ",
            "         resp.content_type = resp.headers.get(\"Content-Type\")",
            "         # Check if geturl() is available (urllib3 version >= 1.23)",
            "         try:",
            "             resp_url = resp.geturl()",
            "         except AttributeError:",
            "             # get_redirect_location() is available for urllib3 >= 1.1",
            "@@ -2891,14 +2989,16 @@",
            " ) -> tuple[GitClient, str]:",
            "     \"\"\"Obtain a git client from a URL.",
            " ",
            "     Args:",
            "       url: URL to open (a unicode string)",
            "       config: Optional config object",
            "       operation: Kind of operation that'll be performed; \"pull\" or \"push\"",
            "+",
            "+    Keyword Args:",
            "       thin_packs: Whether or not thin packs should be retrieved",
            "       report_activity: Optional callback for reporting transport",
            "         activity.",
            " ",
            "     Returns:",
            "       Tuple with client instance and relative path.",
            " ",
            "@@ -2912,15 +3012,15 @@",
            " ",
            " ",
            " def _get_transport_and_path_from_url(url, config, operation, **kwargs):",
            "     parsed = urlparse(url)",
            "     if parsed.scheme == \"git\":",
            "         return (TCPGitClient.from_parsedurl(parsed, **kwargs), parsed.path)",
            "     elif parsed.scheme in (\"git+ssh\", \"ssh\"):",
            "-        return SSHGitClient.from_parsedurl(parsed, **kwargs), parsed.path",
            "+        return SSHGitClient.from_parsedurl(parsed, config=config, **kwargs), parsed.path",
            "     elif parsed.scheme in (\"http\", \"https\"):",
            "         return (",
            "             HttpGitClient.from_parsedurl(parsed, config=config, **kwargs),",
            "             parsed.path,",
            "         )",
            "     elif parsed.scheme == \"file\":",
            "         if sys.platform == \"win32\" or os.name == \"nt\":",
            "@@ -2960,14 +3060,16 @@",
            " ) -> tuple[GitClient, str]:",
            "     \"\"\"Obtain a git client from a URL.",
            " ",
            "     Args:",
            "       location: URL or path (a string)",
            "       config: Optional config object",
            "       operation: Kind of operation that'll be performed; \"pull\" or \"push\"",
            "+",
            "+    Keyword Args:",
            "       thin_packs: Whether or not thin packs should be retrieved",
            "       report_activity: Optional callback for reporting transport",
            "         activity.",
            " ",
            "     Returns:",
            "       Tuple with client instance and relative path.",
            " ",
            "@@ -2989,15 +3091,15 @@",
            " ",
            "     try:",
            "         (username, hostname, path) = parse_rsync_url(location)",
            "     except ValueError:",
            "         # Otherwise, assume it's a local path.",
            "         return default_local_git_client_cls(**kwargs), location",
            "     else:",
            "-        return SSHGitClient(hostname, username=username, **kwargs), path",
            "+        return SSHGitClient(hostname, username=username, config=config, **kwargs), path",
            " ",
            " ",
            " DEFAULT_GIT_CREDENTIALS_PATHS = [",
            "     os.path.expanduser(\"~/.git-credentials\"),",
            "     get_xdg_config_home_path(\"git\", \"credentials\"),",
            " ]"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/config.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/config.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/config.py",
            "@@ -19,56 +19,172 @@",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Reading and writing Git configuration files.",
            " ",
            " Todo:",
            "  * preserve formatting when updating configuration files",
            "- * treat subsection names as case-insensitive for [branch.foo] style",
            "-   subsections",
            " \"\"\"",
            " ",
            "+import logging",
            " import os",
            "+import re",
            " import sys",
            "-from collections.abc import Iterable, Iterator, KeysView, MutableMapping",
            "+from collections.abc import (",
            "+    ItemsView,",
            "+    Iterable,",
            "+    Iterator,",
            "+    KeysView,",
            "+    MutableMapping,",
            "+    ValuesView,",
            "+)",
            " from contextlib import suppress",
            "+from pathlib import Path",
            " from typing import (",
            "     Any,",
            "     BinaryIO,",
            "+    Callable,",
            "+    Generic,",
            "     Optional,",
            "+    TypeVar,",
            "     Union,",
            "     overload,",
            " )",
            " ",
            " from .file import GitFile",
            " ",
            "-SENTINEL = object()",
            "+ConfigKey = Union[str, bytes, tuple[Union[str, bytes], ...]]",
            "+ConfigValue = Union[str, bytes, bool, int]",
            " ",
            "+logger = logging.getLogger(__name__)",
            " ",
            "-def lower_key(key):",
            "-    if isinstance(key, (bytes, str)):",
            "-        return key.lower()",
            "+# Type for file opener callback",
            "+FileOpener = Callable[[Union[str, os.PathLike]], BinaryIO]",
            "+",
            "+# Type for includeIf condition matcher",
            "+# Takes the condition value (e.g., \"main\" for onbranch:main) and returns bool",
            "+ConditionMatcher = Callable[[str], bool]",
            "+",
            "+# Security limits for include files",
            "+MAX_INCLUDE_FILE_SIZE = 1024 * 1024  # 1MB max for included config files",
            "+DEFAULT_MAX_INCLUDE_DEPTH = 10  # Maximum recursion depth for includes",
            " ",
            "-    if isinstance(key, Iterable):",
            "-        return type(key)(map(lower_key, key))  # type: ignore",
            " ",
            "-    return key",
            "+def _match_gitdir_pattern(",
            "+    path: bytes, pattern: bytes, ignorecase: bool = False",
            "+) -> bool:",
            "+    \"\"\"Simple gitdir pattern matching for includeIf conditions.",
            "+",
            "+    This handles the basic gitdir patterns used in includeIf directives.",
            "+    \"\"\"",
            "+    # Convert to strings for easier manipulation",
            "+    path_str = path.decode(\"utf-8\", errors=\"replace\")",
            "+    pattern_str = pattern.decode(\"utf-8\", errors=\"replace\")",
            "+",
            "+    # Normalize paths to use forward slashes for consistent matching",
            "+    path_str = path_str.replace(\"\\\\\", \"/\")",
            "+    pattern_str = pattern_str.replace(\"\\\\\", \"/\")",
            "+",
            "+    if ignorecase:",
            "+        path_str = path_str.lower()",
            "+        pattern_str = pattern_str.lower()",
            "+",
            "+    # Handle the common cases for gitdir patterns",
            "+    if pattern_str.startswith(\"**/\") and pattern_str.endswith(\"/**\"):",
            "+        # Pattern like **/dirname/** should match any path containing dirname",
            "+        dirname = pattern_str[3:-3]  # Remove **/ and /**",
            "+        # Check if path contains the directory name as a path component",
            "+        return (\"/\" + dirname + \"/\") in path_str or path_str.endswith(\"/\" + dirname)",
            "+    elif pattern_str.startswith(\"**/\"):",
            "+        # Pattern like **/filename",
            "+        suffix = pattern_str[3:]  # Remove **/",
            "+        return suffix in path_str or path_str.endswith(\"/\" + suffix)",
            "+    elif pattern_str.endswith(\"/**\"):",
            "+        # Pattern like /path/to/dir/** should match /path/to/dir and any subdirectory",
            "+        base_pattern = pattern_str[:-3]  # Remove /**",
            "+        return path_str == base_pattern or path_str.startswith(base_pattern + \"/\")",
            "+    elif \"**\" in pattern_str:",
            "+        # Handle patterns with ** in the middle",
            "+        parts = pattern_str.split(\"**\")",
            "+        if len(parts) == 2:",
            "+            prefix, suffix = parts",
            "+            # Path must start with prefix and end with suffix (if any)",
            "+            if prefix and not path_str.startswith(prefix):",
            "+                return False",
            "+            if suffix and not path_str.endswith(suffix):",
            "+                return False",
            "+            return True",
            "+",
            "+    # Direct match or simple glob pattern",
            "+    if \"*\" in pattern_str or \"?\" in pattern_str or \"[\" in pattern_str:",
            "+        import fnmatch",
            "+",
            "+        return fnmatch.fnmatch(path_str, pattern_str)",
            "+    else:",
            "+        return path_str == pattern_str",
            "+",
            "+",
            "+def match_glob_pattern(value: str, pattern: str) -> bool:",
            "+    r\"\"\"Match a value against a glob pattern.",
            "+",
            "+    Supports simple glob patterns like ``*`` and ``**``.",
            "+",
            "+    Raises:",
            "+        ValueError: If the pattern is invalid",
            "+    \"\"\"",
            "+    # Convert glob pattern to regex",
            "+    pattern_escaped = re.escape(pattern)",
            "+    # Replace escaped \\*\\* with .* (match anything)",
            "+    pattern_escaped = pattern_escaped.replace(r\"\\*\\*\", \".*\")",
            "+    # Replace escaped \\* with [^/]* (match anything except /)",
            "+    pattern_escaped = pattern_escaped.replace(r\"\\*\", \"[^/]*\")",
            "+    # Anchor the pattern",
            "+    pattern_regex = f\"^{pattern_escaped}$\"",
            "+",
            "+    try:",
            "+        return bool(re.match(pattern_regex, value))",
            "+    except re.error as e:",
            "+        raise ValueError(f\"Invalid glob pattern {pattern!r}: {e}\")",
            "+",
            " ",
            "+def lower_key(key: ConfigKey) -> ConfigKey:",
            "+    if isinstance(key, (bytes, str)):",
            "+        return key.lower()",
            " ",
            "-class CaseInsensitiveOrderedMultiDict(MutableMapping):",
            "-    def __init__(self) -> None:",
            "-        self._real: list[Any] = []",
            "-        self._keyed: dict[Any, Any] = {}",
            "+    if isinstance(key, tuple):",
            "+        # For config sections, only lowercase the section name (first element)",
            "+        # but preserve the case of subsection names (remaining elements)",
            "+        if len(key) > 0:",
            "+            first = key[0]",
            "+            assert isinstance(first, (bytes, str))",
            "+            return (first.lower(), *key[1:])",
            "+        return key",
            "+",
            "+    raise TypeError(key)",
            "+",
            "+",
            "+K = TypeVar(\"K\", bound=ConfigKey)  # Key type must be ConfigKey",
            "+V = TypeVar(\"V\")  # Value type",
            "+_T = TypeVar(\"_T\")  # For get() default parameter",
            "+",
            "+",
            "+class CaseInsensitiveOrderedMultiDict(MutableMapping[K, V], Generic[K, V]):",
            "+    def __init__(self, default_factory: Optional[Callable[[], V]] = None) -> None:",
            "+        self._real: list[tuple[K, V]] = []",
            "+        self._keyed: dict[Any, V] = {}",
            "+        self._default_factory = default_factory",
            " ",
            "     @classmethod",
            "-    def make(cls, dict_in=None):",
            "+    def make(",
            "+        cls, dict_in=None, default_factory=None",
            "+    ) -> \"CaseInsensitiveOrderedMultiDict[K, V]\":",
            "         if isinstance(dict_in, cls):",
            "             return dict_in",
            " ",
            "-        out = cls()",
            "+        out = cls(default_factory=default_factory)",
            " ",
            "         if dict_in is None:",
            "             return out",
            " ",
            "         if not isinstance(dict_in, MutableMapping):",
            "             raise TypeError",
            " ",
            "@@ -76,64 +192,94 @@",
            "             out[key] = value",
            " ",
            "         return out",
            " ",
            "     def __len__(self) -> int:",
            "         return len(self._keyed)",
            " ",
            "-    def keys(self) -> KeysView[tuple[bytes, ...]]:",
            "-        return self._keyed.keys()",
            "+    def keys(self) -> KeysView[K]:",
            "+        return self._keyed.keys()  # type: ignore[return-value]",
            "+",
            "+    def items(self) -> ItemsView[K, V]:",
            "+        # Return a view that iterates over the real list to preserve order",
            "+        class OrderedItemsView(ItemsView[K, V]):",
            "+            def __init__(self, mapping: CaseInsensitiveOrderedMultiDict[K, V]):",
            "+                self._mapping = mapping",
            "+",
            "+            def __iter__(self) -> Iterator[tuple[K, V]]:",
            "+                return iter(self._mapping._real)",
            "+",
            "+            def __len__(self) -> int:",
            "+                return len(self._mapping._real)",
            " ",
            "-    def items(self):",
            "-        return iter(self._real)",
            "+            def __contains__(self, item: object) -> bool:",
            "+                if not isinstance(item, tuple) or len(item) != 2:",
            "+                    return False",
            "+                key, value = item",
            "+                return any(k == key and v == value for k, v in self._mapping._real)",
            " ",
            "-    def __iter__(self):",
            "-        return self._keyed.__iter__()",
            "+        return OrderedItemsView(self)",
            " ",
            "-    def values(self):",
            "+    def __iter__(self) -> Iterator[K]:",
            "+        return iter(self._keyed)",
            "+",
            "+    def values(self) -> ValuesView[V]:",
            "         return self._keyed.values()",
            " ",
            "     def __setitem__(self, key, value) -> None:",
            "         self._real.append((key, value))",
            "         self._keyed[lower_key(key)] = value",
            " ",
            "+    def set(self, key, value) -> None:",
            "+        # This method replaces all existing values for the key",
            "+        lower = lower_key(key)",
            "+        self._real = [(k, v) for k, v in self._real if lower_key(k) != lower]",
            "+        self._real.append((key, value))",
            "+        self._keyed[lower] = value",
            "+",
            "     def __delitem__(self, key) -> None:",
            "         key = lower_key(key)",
            "         del self._keyed[key]",
            "         for i, (actual, unused_value) in reversed(list(enumerate(self._real))):",
            "             if lower_key(actual) == key:",
            "                 del self._real[i]",
            " ",
            "-    def __getitem__(self, item):",
            "+    def __getitem__(self, item: K) -> V:",
            "         return self._keyed[lower_key(item)]",
            " ",
            "-    def get(self, key, default=SENTINEL):",
            "+    def get(self, key: K, /, default: Union[V, _T, None] = None) -> Union[V, _T, None]:  # type: ignore[override]",
            "         try:",
            "             return self[key]",
            "         except KeyError:",
            "-            pass",
            "-",
            "-        if default is SENTINEL:",
            "-            return type(self)()",
            "-",
            "-        return default",
            "+            if default is not None:",
            "+                return default",
            "+            elif self._default_factory is not None:",
            "+                return self._default_factory()",
            "+            else:",
            "+                return None",
            " ",
            "-    def get_all(self, key):",
            "-        key = lower_key(key)",
            "+    def get_all(self, key: K) -> Iterator[V]:",
            "+        lowered_key = lower_key(key)",
            "         for actual, value in self._real:",
            "-            if lower_key(actual) == key:",
            "+            if lower_key(actual) == lowered_key:",
            "                 yield value",
            " ",
            "-    def setdefault(self, key, default=SENTINEL):",
            "+    def setdefault(self, key: K, default: Optional[V] = None) -> V:",
            "         try:",
            "             return self[key]",
            "         except KeyError:",
            "-            self[key] = self.get(key, default)",
            "-",
            "-        return self[key]",
            "+            if default is not None:",
            "+                self[key] = default",
            "+                return default",
            "+            elif self._default_factory is not None:",
            "+                value = self._default_factory()",
            "+                self[key] = value",
            "+                return value",
            "+            else:",
            "+                raise",
            " ",
            " ",
            " Name = bytes",
            " NameLike = Union[bytes, str]",
            " Section = tuple[bytes, ...]",
            " SectionLike = Union[bytes, str, tuple[Union[bytes, str], ...]]",
            " Value = bytes",
            "@@ -237,53 +383,60 @@",
            "           name: Name of section to check for",
            "         Returns:",
            "           boolean indicating whether the section exists",
            "         \"\"\"",
            "         return name in self.sections()",
            " ",
            " ",
            "-class ConfigDict(Config, MutableMapping[Section, MutableMapping[Name, Value]]):",
            "+class ConfigDict(Config):",
            "     \"\"\"Git configuration stored in a dictionary.\"\"\"",
            " ",
            "     def __init__(",
            "         self,",
            "         values: Union[",
            "             MutableMapping[Section, MutableMapping[Name, Value]], None",
            "         ] = None,",
            "         encoding: Union[str, None] = None,",
            "     ) -> None:",
            "         \"\"\"Create a new ConfigDict.\"\"\"",
            "         if encoding is None:",
            "             encoding = sys.getdefaultencoding()",
            "         self.encoding = encoding",
            "-        self._values = CaseInsensitiveOrderedMultiDict.make(values)",
            "+        self._values: CaseInsensitiveOrderedMultiDict[",
            "+            Section, CaseInsensitiveOrderedMultiDict[Name, Value]",
            "+        ] = CaseInsensitiveOrderedMultiDict.make(",
            "+            values, default_factory=CaseInsensitiveOrderedMultiDict",
            "+        )",
            " ",
            "     def __repr__(self) -> str:",
            "         return f\"{self.__class__.__name__}({self._values!r})\"",
            " ",
            "     def __eq__(self, other: object) -> bool:",
            "         return isinstance(other, self.__class__) and other._values == self._values",
            " ",
            "-    def __getitem__(self, key: Section) -> MutableMapping[Name, Value]:",
            "+    def __getitem__(self, key: Section) -> CaseInsensitiveOrderedMultiDict[Name, Value]:",
            "         return self._values.__getitem__(key)",
            " ",
            "     def __setitem__(self, key: Section, value: MutableMapping[Name, Value]) -> None:",
            "         return self._values.__setitem__(key, value)",
            " ",
            "     def __delitem__(self, key: Section) -> None:",
            "         return self._values.__delitem__(key)",
            " ",
            "     def __iter__(self) -> Iterator[Section]:",
            "         return self._values.__iter__()",
            " ",
            "     def __len__(self) -> int:",
            "         return self._values.__len__()",
            " ",
            "+    def keys(self) -> KeysView[Section]:",
            "+        return self._values.keys()",
            "+",
            "     @classmethod",
            "-    def _parse_setting(cls, name: str):",
            "+    def _parse_setting(cls, name: str) -> tuple[str, Optional[str], str]:",
            "         parts = name.split(\".\")",
            "         if len(parts) == 3:",
            "             return (parts[0], parts[1], parts[2])",
            "         else:",
            "             return (parts[0], None, parts[1])",
            " ",
            "     def _check_section_and_name(",
            "@@ -313,15 +466,15 @@",
            "             try:",
            "                 return self._values[section].get_all(name)",
            "             except KeyError:",
            "                 pass",
            " ",
            "         return self._values[(section[0],)].get_all(name)",
            " ",
            "-    def get(  # type: ignore[override]",
            "+    def get(",
            "         self,",
            "         section: SectionLike,",
            "         name: NameLike,",
            "     ) -> Value:",
            "         section, name = self._check_section_and_name(section, name)",
            " ",
            "         if len(section) > 1:",
            "@@ -342,23 +495,46 @@",
            " ",
            "         if isinstance(value, bool):",
            "             value = b\"true\" if value else b\"false\"",
            " ",
            "         if not isinstance(value, bytes):",
            "             value = value.encode(self.encoding)",
            " ",
            "+        section_dict = self._values.setdefault(section)",
            "+        if hasattr(section_dict, \"set\"):",
            "+            section_dict.set(name, value)",
            "+        else:",
            "+            section_dict[name] = value",
            "+",
            "+    def add(",
            "+        self,",
            "+        section: SectionLike,",
            "+        name: NameLike,",
            "+        value: Union[ValueLike, bool],",
            "+    ) -> None:",
            "+        \"\"\"Add a value to a configuration setting, creating a multivar if needed.\"\"\"",
            "+        section, name = self._check_section_and_name(section, name)",
            "+",
            "+        if isinstance(value, bool):",
            "+            value = b\"true\" if value else b\"false\"",
            "+",
            "+        if not isinstance(value, bytes):",
            "+            value = value.encode(self.encoding)",
            "+",
            "         self._values.setdefault(section)[name] = value",
            " ",
            "-    def items(  # type: ignore[override]",
            "-        self, section: Section",
            "-    ) -> Iterator[tuple[Name, Value]]:",
            "-        return self._values.get(section).items()",
            "+    def items(self, section: SectionLike) -> Iterator[tuple[Name, Value]]:",
            "+        section_bytes, _ = self._check_section_and_name(section, b\"\")",
            "+        section_dict = self._values.get(section_bytes)",
            "+        if section_dict is not None:",
            "+            return iter(section_dict.items())",
            "+        return iter([])",
            " ",
            "     def sections(self) -> Iterator[Section]:",
            "-        return self._values.keys()",
            "+        return iter(self._values.keys())",
            " ",
            " ",
            " def _format_string(value: bytes) -> bytes:",
            "     if (",
            "         value.startswith((b\" \", b\"\\t\"))",
            "         or value.endswith((b\" \", b\"\\t\"))",
            "         or b\"#\" in value",
            "@@ -385,28 +561,34 @@",
            "     whitespace = bytearray()",
            "     in_quotes = False",
            "     i = 0",
            "     while i < len(value):",
            "         c = value[i]",
            "         if c == ord(b\"\\\\\"):",
            "             i += 1",
            "-            try:",
            "-                v = _ESCAPE_TABLE[value[i]]",
            "-            except IndexError as exc:",
            "-                raise ValueError(",
            "-                    f\"escape character in {value!r} at {i} before end of string\"",
            "-                ) from exc",
            "-            except KeyError as exc:",
            "-                raise ValueError(",
            "-                    f\"escape character followed by unknown character {value[i]!r} at {i} in {value!r}\"",
            "-                ) from exc",
            "-            if whitespace:",
            "-                ret.extend(whitespace)",
            "-                whitespace = bytearray()",
            "-            ret.append(v)",
            "+            if i >= len(value):",
            "+                # Backslash at end of string - treat as literal backslash",
            "+                if whitespace:",
            "+                    ret.extend(whitespace)",
            "+                    whitespace = bytearray()",
            "+                ret.append(ord(b\"\\\\\"))",
            "+            else:",
            "+                try:",
            "+                    v = _ESCAPE_TABLE[value[i]]",
            "+                    if whitespace:",
            "+                        ret.extend(whitespace)",
            "+                        whitespace = bytearray()",
            "+                    ret.append(v)",
            "+                except KeyError:",
            "+                    # Unknown escape sequence - treat backslash as literal and process next char normally",
            "+                    if whitespace:",
            "+                        ret.extend(whitespace)",
            "+                        whitespace = bytearray()",
            "+                    ret.append(ord(b\"\\\\\"))",
            "+                    i -= 1  # Reprocess the character after the backslash",
            "         elif c == ord(b'\"'):",
            "             in_quotes = not in_quotes",
            "         elif c in _COMMENT_CHARS and not in_quotes:",
            "             # the rest of the line is a comment",
            "             break",
            "         elif c in _WHITESPACE_CHARS:",
            "             whitespace.append(c)",
            "@@ -459,14 +641,52 @@",
            "         if character == quote:",
            "             string_open = not string_open",
            "         elif not string_open and character in comment_bytes:",
            "             return line[:i]",
            "     return line",
            " ",
            " ",
            "+def _is_line_continuation(value: bytes) -> bool:",
            "+    \"\"\"Check if a value ends with a line continuation backslash.",
            "+",
            "+    A line continuation occurs when a line ends with a backslash that is:",
            "+    1. Not escaped (not preceded by another backslash)",
            "+    2. Not within quotes",
            "+",
            "+    Args:",
            "+        value: The value to check",
            "+",
            "+    Returns:",
            "+        True if the value ends with a line continuation backslash",
            "+    \"\"\"",
            "+    if not value.endswith((b\"\\\\\\n\", b\"\\\\\\r\\n\")):",
            "+        return False",
            "+",
            "+    # Remove only the newline characters, keep the content including the backslash",
            "+    if value.endswith(b\"\\\\\\r\\n\"):",
            "+        content = value[:-2]  # Remove \\r\\n, keep the \\",
            "+    else:",
            "+        content = value[:-1]  # Remove \\n, keep the \\",
            "+",
            "+    if not content.endswith(b\"\\\\\"):",
            "+        return False",
            "+",
            "+    # Count consecutive backslashes at the end",
            "+    backslash_count = 0",
            "+    for i in range(len(content) - 1, -1, -1):",
            "+        if content[i : i + 1] == b\"\\\\\":",
            "+            backslash_count += 1",
            "+        else:",
            "+            break",
            "+",
            "+    # If we have an odd number of backslashes, the last one is a line continuation",
            "+    # If we have an even number, they are all escaped and there's no continuation",
            "+    return backslash_count % 2 == 1",
            "+",
            "+",
            " def _parse_section_header_line(line: bytes) -> tuple[Section, bytes]:",
            "     # Parse section header (\"[bla]\")",
            "     line = _strip_comments(line).rstrip()",
            "     in_quotes = False",
            "     escaped = False",
            "     for i, c in enumerate(line):",
            "         if escaped:",
            "@@ -481,18 +701,27 @@",
            "             break",
            "     else:",
            "         raise ValueError(\"expected trailing ]\")",
            "     pts = line[1:last].split(b\" \", 1)",
            "     line = line[last + 1 :]",
            "     section: Section",
            "     if len(pts) == 2:",
            "-        if pts[1][:1] != b'\"' or pts[1][-1:] != b'\"':",
            "-            raise ValueError(f\"Invalid subsection {pts[1]!r}\")",
            "-        else:",
            "+        # Handle subsections - Git allows more complex syntax for certain sections like includeIf",
            "+        if pts[1][:1] == b'\"' and pts[1][-1:] == b'\"':",
            "+            # Standard quoted subsection",
            "             pts[1] = pts[1][1:-1]",
            "+        elif pts[0] == b\"includeIf\":",
            "+            # Special handling for includeIf sections which can have complex conditions",
            "+            # Git allows these without strict quote validation",
            "+            pts[1] = pts[1].strip()",
            "+            if pts[1][:1] == b'\"' and pts[1][-1:] == b'\"':",
            "+                pts[1] = pts[1][1:-1]",
            "+        else:",
            "+            # Other sections must have quoted subsections",
            "+            raise ValueError(f\"Invalid subsection {pts[1]!r}\")",
            "         if not _check_section_name(pts[0]):",
            "             raise ValueError(f\"invalid section name {pts[0]!r}\")",
            "         section = (pts[0], pts[1])",
            "     else:",
            "         if not _check_section_name(pts[0]):",
            "             raise ValueError(f\"invalid section name {pts[0]!r}\")",
            "         pts = pts[0].split(b\".\", 1)",
            "@@ -511,19 +740,47 @@",
            "         values: Union[",
            "             MutableMapping[Section, MutableMapping[Name, Value]], None",
            "         ] = None,",
            "         encoding: Union[str, None] = None,",
            "     ) -> None:",
            "         super().__init__(values=values, encoding=encoding)",
            "         self.path: Optional[str] = None",
            "+        self._included_paths: set[str] = set()  # Track included files to prevent cycles",
            " ",
            "     @classmethod",
            "-    def from_file(cls, f: BinaryIO) -> \"ConfigFile\":",
            "-        \"\"\"Read configuration from a file-like object.\"\"\"",
            "+    def from_file(",
            "+        cls,",
            "+        f: BinaryIO,",
            "+        *,",
            "+        config_dir: Optional[str] = None,",
            "+        included_paths: Optional[set[str]] = None,",
            "+        include_depth: int = 0,",
            "+        max_include_depth: int = DEFAULT_MAX_INCLUDE_DEPTH,",
            "+        file_opener: Optional[FileOpener] = None,",
            "+        condition_matchers: Optional[dict[str, ConditionMatcher]] = None,",
            "+    ) -> \"ConfigFile\":",
            "+        \"\"\"Read configuration from a file-like object.",
            "+",
            "+        Args:",
            "+            f: File-like object to read from",
            "+            config_dir: Directory containing the config file (for relative includes)",
            "+            included_paths: Set of already included paths (to prevent cycles)",
            "+            include_depth: Current include depth (to prevent infinite recursion)",
            "+            max_include_depth: Maximum allowed include depth",
            "+            file_opener: Optional callback to open included files",
            "+            condition_matchers: Optional dict of condition matchers for includeIf",
            "+        \"\"\"",
            "+        if include_depth > max_include_depth:",
            "+            # Prevent excessive recursion",
            "+            raise ValueError(f\"Maximum include depth ({max_include_depth}) exceeded\")",
            "+",
            "         ret = cls()",
            "+        if included_paths is not None:",
            "+            ret._included_paths = included_paths.copy()",
            "+",
            "         section: Optional[Section] = None",
            "         setting = None",
            "         continuation = None",
            "         for lineno, line in enumerate(f.readlines()):",
            "             if lineno == 0 and line.startswith(b\"\\xef\\xbb\\xbf\"):",
            "                 line = line[3:]",
            "             line = line.lstrip()",
            "@@ -539,49 +796,324 @@",
            "                     setting, value = line.split(b\"=\", 1)",
            "                 except ValueError:",
            "                     setting = line",
            "                     value = b\"true\"",
            "                 setting = setting.strip()",
            "                 if not _check_variable_name(setting):",
            "                     raise ValueError(f\"invalid variable name {setting!r}\")",
            "-                if value.endswith(b\"\\\\\\n\"):",
            "-                    continuation = value[:-2]",
            "-                elif value.endswith(b\"\\\\\\r\\n\"):",
            "-                    continuation = value[:-3]",
            "+                if _is_line_continuation(value):",
            "+                    if value.endswith(b\"\\\\\\r\\n\"):",
            "+                        continuation = value[:-3]",
            "+                    else:",
            "+                        continuation = value[:-2]",
            "                 else:",
            "                     continuation = None",
            "                     value = _parse_string(value)",
            "                     ret._values[section][setting] = value",
            "+",
            "+                    # Process include/includeIf directives",
            "+                    ret._handle_include_directive(",
            "+                        section,",
            "+                        setting,",
            "+                        value,",
            "+                        config_dir=config_dir,",
            "+                        include_depth=include_depth,",
            "+                        max_include_depth=max_include_depth,",
            "+                        file_opener=file_opener,",
            "+                        condition_matchers=condition_matchers,",
            "+                    )",
            "+",
            "                     setting = None",
            "             else:  # continuation line",
            "-                if line.endswith(b\"\\\\\\n\"):",
            "-                    continuation += line[:-2]",
            "-                elif line.endswith(b\"\\\\\\r\\n\"):",
            "-                    continuation += line[:-3]",
            "+                assert continuation is not None",
            "+                if _is_line_continuation(line):",
            "+                    if line.endswith(b\"\\\\\\r\\n\"):",
            "+                        continuation += line[:-3]",
            "+                    else:",
            "+                        continuation += line[:-2]",
            "                 else:",
            "                     continuation += line",
            "                     value = _parse_string(continuation)",
            "+                    assert section is not None  # Already checked above",
            "                     ret._values[section][setting] = value",
            "+",
            "+                    # Process include/includeIf directives",
            "+                    ret._handle_include_directive(",
            "+                        section,",
            "+                        setting,",
            "+                        value,",
            "+                        config_dir=config_dir,",
            "+                        include_depth=include_depth,",
            "+                        max_include_depth=max_include_depth,",
            "+                        file_opener=file_opener,",
            "+                        condition_matchers=condition_matchers,",
            "+                    )",
            "+",
            "                     continuation = None",
            "                     setting = None",
            "         return ret",
            " ",
            "+    def _handle_include_directive(",
            "+        self,",
            "+        section: Optional[Section],",
            "+        setting: bytes,",
            "+        value: bytes,",
            "+        *,",
            "+        config_dir: Optional[str],",
            "+        include_depth: int,",
            "+        max_include_depth: int,",
            "+        file_opener: Optional[FileOpener],",
            "+        condition_matchers: Optional[dict[str, ConditionMatcher]],",
            "+    ) -> None:",
            "+        \"\"\"Handle include/includeIf directives during config parsing.\"\"\"",
            "+        if (",
            "+            section is not None",
            "+            and setting == b\"path\"",
            "+            and (",
            "+                section[0].lower() == b\"include\"",
            "+                or (len(section) > 1 and section[0].lower() == b\"includeif\")",
            "+            )",
            "+        ):",
            "+            self._process_include(",
            "+                section,",
            "+                value,",
            "+                config_dir=config_dir,",
            "+                include_depth=include_depth,",
            "+                max_include_depth=max_include_depth,",
            "+                file_opener=file_opener,",
            "+                condition_matchers=condition_matchers,",
            "+            )",
            "+",
            "+    def _process_include(",
            "+        self,",
            "+        section: Section,",
            "+        path_value: bytes,",
            "+        *,",
            "+        config_dir: Optional[str],",
            "+        include_depth: int,",
            "+        max_include_depth: int,",
            "+        file_opener: Optional[FileOpener],",
            "+        condition_matchers: Optional[dict[str, ConditionMatcher]],",
            "+    ) -> None:",
            "+        \"\"\"Process an include or includeIf directive.\"\"\"",
            "+        path_str = path_value.decode(self.encoding, errors=\"replace\")",
            "+",
            "+        # Handle includeIf conditions",
            "+        if len(section) > 1 and section[0].lower() == b\"includeif\":",
            "+            condition = section[1].decode(self.encoding, errors=\"replace\")",
            "+            if not self._evaluate_includeif_condition(",
            "+                condition, config_dir, condition_matchers",
            "+            ):",
            "+                return",
            "+",
            "+        # Resolve the include path",
            "+        include_path = self._resolve_include_path(path_str, config_dir)",
            "+        if not include_path:",
            "+            return",
            "+",
            "+        # Check for circular includes",
            "+        try:",
            "+            abs_path = str(Path(include_path).resolve())",
            "+        except (OSError, ValueError) as e:",
            "+            # Invalid path - log and skip",
            "+            logger.debug(\"Invalid include path %r: %s\", include_path, e)",
            "+            return",
            "+        if abs_path in self._included_paths:",
            "+            return",
            "+",
            "+        # Load and merge the included file",
            "+        try:",
            "+            # Use provided file opener or default to GitFile",
            "+            if file_opener is None:",
            "+",
            "+                def opener(path):",
            "+                    return GitFile(path, \"rb\")",
            "+            else:",
            "+                opener = file_opener",
            "+",
            "+            f = opener(include_path)",
            "+        except (OSError, ValueError) as e:",
            "+            # Git silently ignores missing or unreadable include files",
            "+            # Log for debugging purposes",
            "+            logger.debug(\"Invalid include path %r: %s\", include_path, e)",
            "+        else:",
            "+            with f as included_file:",
            "+                # Track this path to prevent cycles",
            "+                self._included_paths.add(abs_path)",
            "+",
            "+                # Parse the included file",
            "+                included_config = ConfigFile.from_file(",
            "+                    included_file,",
            "+                    config_dir=os.path.dirname(include_path),",
            "+                    included_paths=self._included_paths,",
            "+                    include_depth=include_depth + 1,",
            "+                    max_include_depth=max_include_depth,",
            "+                    file_opener=file_opener,",
            "+                    condition_matchers=condition_matchers,",
            "+                )",
            "+",
            "+                # Merge the included configuration",
            "+                self._merge_config(included_config)",
            "+",
            "+    def _merge_config(self, other: \"ConfigFile\") -> None:",
            "+        \"\"\"Merge another config file into this one.\"\"\"",
            "+        for section, values in other._values.items():",
            "+            if section not in self._values:",
            "+                self._values[section] = CaseInsensitiveOrderedMultiDict()",
            "+            for key, value in values.items():",
            "+                self._values[section][key] = value",
            "+",
            "+    def _resolve_include_path(",
            "+        self, path: str, config_dir: Optional[str]",
            "+    ) -> Optional[str]:",
            "+        \"\"\"Resolve an include path to an absolute path.\"\"\"",
            "+        # Expand ~ to home directory",
            "+        path = os.path.expanduser(path)",
            "+",
            "+        # If path is relative and we have a config directory, make it relative to that",
            "+        if not os.path.isabs(path) and config_dir:",
            "+            path = os.path.join(config_dir, path)",
            "+",
            "+        return path",
            "+",
            "+    def _evaluate_includeif_condition(",
            "+        self,",
            "+        condition: str,",
            "+        config_dir: Optional[str] = None,",
            "+        condition_matchers: Optional[dict[str, ConditionMatcher]] = None,",
            "+    ) -> bool:",
            "+        \"\"\"Evaluate an includeIf condition.\"\"\"",
            "+        # Try custom matchers first if provided",
            "+        if condition_matchers:",
            "+            for prefix, matcher in condition_matchers.items():",
            "+                if condition.startswith(prefix):",
            "+                    return matcher(condition[len(prefix) :])",
            "+",
            "+        # Fall back to built-in matchers",
            "+        if condition.startswith(\"hasconfig:\"):",
            "+            return self._evaluate_hasconfig_condition(condition[10:])",
            "+        else:",
            "+            # Unknown condition type - log and ignore (Git behavior)",
            "+            logger.debug(\"Unknown includeIf condition: %r\", condition)",
            "+            return False",
            "+",
            "+    def _evaluate_hasconfig_condition(self, condition: str) -> bool:",
            "+        \"\"\"Evaluate a hasconfig condition.",
            "+",
            "+        Format: hasconfig:config.key:pattern",
            "+        Example: hasconfig:remote.*.url:ssh://org-*@github.com/**",
            "+        \"\"\"",
            "+        # Split on the first colon to separate config key from pattern",
            "+        parts = condition.split(\":\", 1)",
            "+        if len(parts) != 2:",
            "+            logger.debug(\"Invalid hasconfig condition format: %r\", condition)",
            "+            return False",
            "+",
            "+        config_key, pattern = parts",
            "+",
            "+        # Parse the config key to get section and name",
            "+        key_parts = config_key.split(\".\", 2)",
            "+        if len(key_parts) < 2:",
            "+            logger.debug(\"Invalid hasconfig config key: %r\", config_key)",
            "+            return False",
            "+",
            "+        # Handle wildcards in section names (e.g., remote.*)",
            "+        if len(key_parts) == 3 and key_parts[1] == \"*\":",
            "+            # Match any subsection",
            "+            section_prefix = key_parts[0].encode(self.encoding)",
            "+            name = key_parts[2].encode(self.encoding)",
            "+",
            "+            # Check all sections that match the pattern",
            "+            for section in self.sections():",
            "+                if len(section) == 2 and section[0] == section_prefix:",
            "+                    try:",
            "+                        values = list(self.get_multivar(section, name))",
            "+                        for value in values:",
            "+                            if self._match_hasconfig_pattern(value, pattern):",
            "+                                return True",
            "+                    except KeyError:",
            "+                        continue",
            "+        else:",
            "+            # Direct section lookup",
            "+            if len(key_parts) == 2:",
            "+                section = (key_parts[0].encode(self.encoding),)",
            "+                name = key_parts[1].encode(self.encoding)",
            "+            else:",
            "+                section = (",
            "+                    key_parts[0].encode(self.encoding),",
            "+                    key_parts[1].encode(self.encoding),",
            "+                )",
            "+                name = key_parts[2].encode(self.encoding)",
            "+",
            "+            try:",
            "+                values = list(self.get_multivar(section, name))",
            "+                for value in values:",
            "+                    if self._match_hasconfig_pattern(value, pattern):",
            "+                        return True",
            "+            except KeyError:",
            "+                pass",
            "+",
            "+        return False",
            "+",
            "+    def _match_hasconfig_pattern(self, value: bytes, pattern: str) -> bool:",
            "+        \"\"\"Match a config value against a hasconfig pattern.",
            "+",
            "+        Supports simple glob patterns like ``*`` and ``**``.",
            "+        \"\"\"",
            "+        value_str = value.decode(self.encoding, errors=\"replace\")",
            "+        return match_glob_pattern(value_str, pattern)",
            "+",
            "     @classmethod",
            "-    def from_path(cls, path: str) -> \"ConfigFile\":",
            "-        \"\"\"Read configuration from a file on disk.\"\"\"",
            "-        with GitFile(path, \"rb\") as f:",
            "-            ret = cls.from_file(f)",
            "-            ret.path = path",
            "+    def from_path(",
            "+        cls,",
            "+        path: Union[str, os.PathLike],",
            "+        *,",
            "+        max_include_depth: int = DEFAULT_MAX_INCLUDE_DEPTH,",
            "+        file_opener: Optional[FileOpener] = None,",
            "+        condition_matchers: Optional[dict[str, ConditionMatcher]] = None,",
            "+    ) -> \"ConfigFile\":",
            "+        \"\"\"Read configuration from a file on disk.",
            "+",
            "+        Args:",
            "+            path: Path to the configuration file",
            "+            max_include_depth: Maximum allowed include depth",
            "+            file_opener: Optional callback to open included files",
            "+            condition_matchers: Optional dict of condition matchers for includeIf",
            "+        \"\"\"",
            "+        abs_path = os.fspath(path)",
            "+        config_dir = os.path.dirname(abs_path)",
            "+",
            "+        # Use provided file opener or default to GitFile",
            "+        if file_opener is None:",
            "+",
            "+            def opener(p):",
            "+                return GitFile(p, \"rb\")",
            "+        else:",
            "+            opener = file_opener",
            "+",
            "+        with opener(abs_path) as f:",
            "+            ret = cls.from_file(",
            "+                f,",
            "+                config_dir=config_dir,",
            "+                max_include_depth=max_include_depth,",
            "+                file_opener=file_opener,",
            "+                condition_matchers=condition_matchers,",
            "+            )",
            "+            ret.path = abs_path",
            "             return ret",
            " ",
            "-    def write_to_path(self, path: Optional[str] = None) -> None:",
            "+    def write_to_path(self, path: Optional[Union[str, os.PathLike]] = None) -> None:",
            "         \"\"\"Write configuration to a file on disk.\"\"\"",
            "         if path is None:",
            "-            path = self.path",
            "-        with GitFile(path, \"wb\") as f:",
            "+            if self.path is None:",
            "+                raise ValueError(\"No path specified and no default path available\")",
            "+            path_to_use: Union[str, os.PathLike] = self.path",
            "+        else:",
            "+            path_to_use = path",
            "+        with GitFile(path_to_use, \"wb\") as f:",
            "             self.write_to_file(f)",
            " ",
            "     def write_to_file(self, f: BinaryIO) -> None:",
            "         \"\"\"Write configuration to a file-like object.\"\"\"",
            "         for section, values in self._values.items():",
            "             try:",
            "                 section_name, subsection_name = section",
            "@@ -593,23 +1125,23 @@",
            "             else:",
            "                 f.write(b\"[\" + section_name + b' \"' + subsection_name + b'\"]\\n')",
            "             for key, value in values.items():",
            "                 value = _format_string(value)",
            "                 f.write(b\"\\t\" + key + b\" = \" + value + b\"\\n\")",
            " ",
            " ",
            "-def get_xdg_config_home_path(*path_segments):",
            "+def get_xdg_config_home_path(*path_segments: str) -> str:",
            "     xdg_config_home = os.environ.get(",
            "         \"XDG_CONFIG_HOME\",",
            "         os.path.expanduser(\"~/.config/\"),",
            "     )",
            "     return os.path.join(xdg_config_home, *path_segments)",
            " ",
            " ",
            "-def _find_git_in_win_path():",
            "+def _find_git_in_win_path() -> Iterator[str]:",
            "     for exe in (\"git.exe\", \"git.cmd\"):",
            "         for path in os.environ.get(\"PATH\", \"\").split(\";\"):",
            "             if os.path.exists(os.path.join(path, exe)):",
            "                 # in windows native shells (powershell/cmd) exe path is",
            "                 # .../Git/bin/git.exe or .../Git/cmd/git.exe",
            "                 #",
            "                 # in git-bash exe path is .../Git/mingw64/bin/git.exe",
            "@@ -617,25 +1149,25 @@",
            "                 yield git_dir",
            "                 parent_dir, basename = os.path.split(git_dir)",
            "                 if basename == \"mingw32\" or basename == \"mingw64\":",
            "                     yield parent_dir",
            "                 break",
            " ",
            " ",
            "-def _find_git_in_win_reg():",
            "+def _find_git_in_win_reg() -> Iterator[str]:",
            "     import platform",
            "     import winreg",
            " ",
            "     if platform.machine() == \"AMD64\":",
            "         subkey = (",
            "             \"SOFTWARE\\\\Wow6432Node\\\\Microsoft\\\\Windows\\\\\"",
            "             \"CurrentVersion\\\\Uninstall\\\\Git_is1\"",
            "         )",
            "     else:",
            "-        subkey = \"SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\\" \"Uninstall\\\\Git_is1\"",
            "+        subkey = \"SOFTWARE\\\\Microsoft\\\\Windows\\\\CurrentVersion\\\\Uninstall\\\\Git_is1\"",
            " ",
            "     for key in (winreg.HKEY_CURRENT_USER, winreg.HKEY_LOCAL_MACHINE):  # type: ignore",
            "         with suppress(OSError):",
            "             with winreg.OpenKey(key, subkey) as k:  # type: ignore",
            "                 val, typ = winreg.QueryValueEx(k, \"InstallLocation\")  # type: ignore",
            "                 if typ == winreg.REG_SZ:  # type: ignore",
            "                     yield val",
            "@@ -643,15 +1175,15 @@",
            " ",
            " # There is no set standard for system config dirs on windows. We try the",
            " # following:",
            " #   - %PROGRAMDATA%/Git/config - (deprecated) Windows config dir per CGit docs",
            " #   - %PROGRAMFILES%/Git/etc/gitconfig - Git for Windows (msysgit) config dir",
            " #     Used if CGit installation (Git/bin/git.exe) is found in PATH in the",
            " #     system registry",
            "-def get_win_system_paths():",
            "+def get_win_system_paths() -> Iterator[str]:",
            "     if \"PROGRAMDATA\" in os.environ:",
            "         yield os.path.join(os.environ[\"PROGRAMDATA\"], \"Git\", \"config\")",
            " ",
            "     for git_dir in _find_git_in_win_path():",
            "         yield os.path.join(git_dir, \"etc\", \"gitconfig\")",
            "     for git_dir in _find_git_in_win_reg():",
            "         yield os.path.join(git_dir, \"etc\", \"gitconfig\")",
            "@@ -728,30 +1260,32 @@",
            "         for backend in self.backends:",
            "             for section in backend.sections():",
            "                 if section not in seen:",
            "                     seen.add(section)",
            "                     yield section",
            " ",
            " ",
            "-def read_submodules(path: str) -> Iterator[tuple[bytes, bytes, bytes]]:",
            "+def read_submodules(",
            "+    path: Union[str, os.PathLike],",
            "+) -> Iterator[tuple[bytes, bytes, bytes]]:",
            "     \"\"\"Read a .gitmodules file.\"\"\"",
            "     cfg = ConfigFile.from_path(path)",
            "     return parse_submodules(cfg)",
            " ",
            " ",
            " def parse_submodules(config: ConfigFile) -> Iterator[tuple[bytes, bytes, bytes]]:",
            "     \"\"\"Parse a gitmodules GitConfig file, returning submodules.",
            " ",
            "     Args:",
            "       config: A `ConfigFile`",
            "     Returns:",
            "       list of tuples (submodule path, url, name),",
            "         where name is quoted part of the section's name.",
            "     \"\"\"",
            "-    for section in config.keys():",
            "+    for section in config.sections():",
            "         section_kind, section_name = section",
            "         if section_kind == b\"submodule\":",
            "             try:",
            "                 sm_path = config.get(section, b\"path\")",
            "                 sm_url = config.get(section, b\"url\")",
            "                 yield (sm_path, sm_url, section_name)",
            "             except KeyError:"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/contrib/diffstat.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/contrib/diffstat.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/contrib/diffstat.py",
            "@@ -107,15 +107,15 @@",
            "         nametypes.append(binaryfile)",
            "         counts.append((added, deleted))",
            "     return names, nametypes, counts",
            " ",
            " ",
            " # note must all done using bytes not string because on linux filenames",
            " # may not be encodable even to utf-8",
            "-def diffstat(lines, max_width=80):",
            "+def diffstat(lines: list[bytes], max_width: int = 80) -> bytes:",
            "     \"\"\"Generate summary statistics from a git style diff ala",
            "        (git diff tag1 tag2 --stat).",
            " ",
            "     Args:",
            "       lines: list of byte string \"lines\" from the diff to be parsed",
            "       max_width: maximum line length for generating the summary",
            "                  statistics (default 80)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/contrib/paramiko_vendor.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/contrib/paramiko_vendor.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/contrib/paramiko_vendor.py",
            "@@ -27,38 +27,43 @@",
            "   >>> from dulwich import client as _mod_client",
            "   >>> from dulwich.contrib.paramiko_vendor import ParamikoSSHVendor",
            "   >>> _mod_client.get_ssh_vendor = ParamikoSSHVendor",
            " ",
            " This implementation is experimental and does not have any tests.",
            " \"\"\"",
            " ",
            "+import os",
            "+import warnings",
            "+from typing import Any, BinaryIO, Optional, cast",
            "+",
            " import paramiko",
            " import paramiko.client",
            "+import paramiko.config",
            " ",
            " ",
            " class _ParamikoWrapper:",
            "-    def __init__(self, client, channel) -> None:",
            "+    def __init__(self, client: paramiko.SSHClient, channel: paramiko.Channel) -> None:",
            "         self.client = client",
            "         self.channel = channel",
            " ",
            "         # Channel must block",
            "         self.channel.setblocking(True)",
            " ",
            "     @property",
            "-    def stderr(self):",
            "-        return self.channel.makefile_stderr(\"rb\")",
            "+    def stderr(self) -> BinaryIO:",
            "+        return cast(BinaryIO, self.channel.makefile_stderr(\"rb\"))",
            " ",
            "-    def can_read(self):",
            "+    def can_read(self) -> bool:",
            "         return self.channel.recv_ready()",
            " ",
            "-    def write(self, data):",
            "+    def write(self, data: bytes) -> None:",
            "         return self.channel.sendall(data)",
            " ",
            "-    def read(self, n=None):",
            "-        data = self.channel.recv(n)",
            "+    def read(self, n: Optional[int] = None) -> bytes:",
            "+        data = self.channel.recv(n or 4096)",
            "         data_len = len(data)",
            " ",
            "         # Closed socket",
            "         if not data:",
            "             return b\"\"",
            " ",
            "         # Read more if needed",
            "@@ -70,51 +75,91 @@",
            "     def close(self) -> None:",
            "         self.channel.close()",
            " ",
            " ",
            " class ParamikoSSHVendor:",
            "     # http://docs.paramiko.org/en/2.4/api/client.html",
            " ",
            "-    def __init__(self, **kwargs) -> None:",
            "+    def __init__(self, **kwargs: object) -> None:",
            "         self.kwargs = kwargs",
            "+        self.ssh_config = self._load_ssh_config()",
            "+",
            "+    def _load_ssh_config(self) -> paramiko.config.SSHConfig:",
            "+        \"\"\"Load SSH configuration from ~/.ssh/config.\"\"\"",
            "+        ssh_config = paramiko.config.SSHConfig()",
            "+        config_path = os.path.expanduser(\"~/.ssh/config\")",
            "+        try:",
            "+            with open(config_path) as config_file:",
            "+                ssh_config.parse(config_file)",
            "+        except FileNotFoundError:",
            "+            # Config file doesn't exist - this is normal, ignore silently",
            "+            pass",
            "+        except (OSError, PermissionError) as e:",
            "+            # Config file exists but can't be read - warn user",
            "+            warnings.warn(f\"Could not read SSH config file {config_path}: {e}\")",
            "+        return ssh_config",
            " ",
            "     def run_command(",
            "         self,",
            "-        host,",
            "-        command,",
            "-        username=None,",
            "-        port=None,",
            "-        password=None,",
            "-        pkey=None,",
            "-        key_filename=None,",
            "-        protocol_version=None,",
            "-        **kwargs,",
            "-    ):",
            "+        host: str,",
            "+        command: str,",
            "+        username: Optional[str] = None,",
            "+        port: Optional[int] = None,",
            "+        password: Optional[str] = None,",
            "+        pkey: Optional[paramiko.PKey] = None,",
            "+        key_filename: Optional[str] = None,",
            "+        protocol_version: Optional[int] = None,",
            "+        **kwargs: object,",
            "+    ) -> _ParamikoWrapper:",
            "         client = paramiko.SSHClient()",
            " ",
            "-        connection_kwargs = {\"hostname\": host}",
            "+        # Get SSH config for this host",
            "+        host_config = self.ssh_config.lookup(host)",
            "+",
            "+        connection_kwargs: dict[str, Any] = {",
            "+            \"hostname\": host_config.get(\"hostname\", host)",
            "+        }",
            "         connection_kwargs.update(self.kwargs)",
            "+",
            "+        # Use SSH config values if not explicitly provided",
            "         if username:",
            "             connection_kwargs[\"username\"] = username",
            "+        elif \"user\" in host_config:",
            "+            connection_kwargs[\"username\"] = host_config[\"user\"]",
            "+",
            "         if port:",
            "             connection_kwargs[\"port\"] = port",
            "+        elif \"port\" in host_config:",
            "+            connection_kwargs[\"port\"] = int(host_config[\"port\"])",
            "+",
            "         if password:",
            "             connection_kwargs[\"password\"] = password",
            "         if pkey:",
            "             connection_kwargs[\"pkey\"] = pkey",
            "         if key_filename:",
            "             connection_kwargs[\"key_filename\"] = key_filename",
            "+        elif \"identityfile\" in host_config:",
            "+            # Use the first identity file from SSH config",
            "+            identity_files = host_config[\"identityfile\"]",
            "+            if isinstance(identity_files, list) and identity_files:",
            "+                connection_kwargs[\"key_filename\"] = identity_files[0]",
            "+            elif isinstance(identity_files, str):",
            "+                connection_kwargs[\"key_filename\"] = identity_files",
            "+",
            "         connection_kwargs.update(kwargs)",
            " ",
            "         policy = paramiko.client.MissingHostKeyPolicy()",
            "         client.set_missing_host_key_policy(policy)",
            "         client.connect(**connection_kwargs)",
            " ",
            "         # Open SSH session",
            "-        channel = client.get_transport().open_session()",
            "+        transport = client.get_transport()",
            "+        if transport is None:",
            "+            raise RuntimeError(\"Transport is None\")",
            "+        channel = transport.open_session()",
            " ",
            "         if protocol_version is None or protocol_version == 2:",
            "             channel.set_environment_variable(name=\"GIT_PROTOCOL\", value=\"version=2\")",
            " ",
            "         # Run commands",
            "         channel.exec_command(command)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/contrib/release_robot.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/contrib/release_robot.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/contrib/release_robot.py",
            "@@ -42,26 +42,28 @@",
            "       |",
            "       * __init__.py  <-- put __version__ here",
            " ",
            " ",
            " \"\"\"",
            " ",
            " import datetime",
            "+import logging",
            " import re",
            " import sys",
            " import time",
            "+from typing import Any, Optional, cast",
            " ",
            " from ..repo import Repo",
            " ",
            " # CONSTANTS",
            " PROJDIR = \".\"",
            " PATTERN = r\"[ a-zA-Z_\\-]*([\\d\\.]+[\\-\\w\\.]*)\"",
            " ",
            " ",
            "-def get_recent_tags(projdir=PROJDIR):",
            "+def get_recent_tags(projdir: str = PROJDIR) -> list[tuple[str, list[Any]]]:",
            "     \"\"\"Get list of tags in order from newest to oldest and their datetimes.",
            " ",
            "     Args:",
            "       projdir: path to ``.git``",
            "     Returns:",
            "       list of tags sorted by commit time from newest to oldest",
            " ",
            "@@ -70,52 +72,58 @@",
            "     Otherwise the tag meta is a tuple containing the tag time, tag id and tag",
            "     name. Time is in UTC.",
            "     \"\"\"",
            "     with Repo(projdir) as project:  # dulwich repository object",
            "         refs = project.get_refs()  # dictionary of refs and their SHA-1 values",
            "         tags = {}  # empty dictionary to hold tags, commits and datetimes",
            "         # iterate over refs in repository",
            "-        for key, value in refs.items():",
            "-            key = key.decode(\"utf-8\")  # compatible with Python-3",
            "+        for key_bytes, value in refs.items():",
            "+            key = key_bytes.decode(\"utf-8\")  # compatible with Python-3",
            "             obj = project.get_object(value)  # dulwich object from SHA-1",
            "             # don't just check if object is \"tag\" b/c it could be a \"commit\"",
            "             # instead check if \"tags\" is in the ref-name",
            "             if \"tags\" not in key:",
            "                 # skip ref if not a tag",
            "                 continue",
            "             # strip the leading text from refs to get \"tag name\"",
            "             _, tag = key.rsplit(\"/\", 1)",
            "             # check if tag object is \"commit\" or \"tag\" pointing to a \"commit\"",
            "-            try:",
            "-                commit = obj.object  # a tuple (commit class, commit id)",
            "-            except AttributeError:",
            "-                commit = obj",
            "-                tag_meta = None",
            "-            else:",
            "+            from ..objects import Commit, Tag",
            "+",
            "+            if isinstance(obj, Tag):",
            "+                commit_info = obj.object  # a tuple (commit class, commit id)",
            "                 tag_meta = (",
            "                     datetime.datetime(*time.gmtime(obj.tag_time)[:6]),",
            "                     obj.id.decode(\"utf-8\"),",
            "                     obj.name.decode(\"utf-8\"),",
            "                 )  # compatible with Python-3",
            "-                commit = project.get_object(commit[1])  # commit object",
            "+                commit = project.get_object(commit_info[1])  # commit object",
            "+            else:",
            "+                commit = obj",
            "+                tag_meta = None",
            "             # get tag commit datetime, but dulwich returns seconds since",
            "             # beginning of epoch, so use Python time module to convert it to",
            "             # timetuple then convert to datetime",
            "+            commit_obj = cast(Commit, commit)",
            "             tags[tag] = [",
            "-                datetime.datetime(*time.gmtime(commit.commit_time)[:6]),",
            "-                commit.id.decode(\"utf-8\"),",
            "-                commit.author.decode(\"utf-8\"),",
            "+                datetime.datetime(*time.gmtime(commit_obj.commit_time)[:6]),",
            "+                commit_obj.id.decode(\"utf-8\"),",
            "+                commit_obj.author.decode(\"utf-8\"),",
            "                 tag_meta,",
            "             ]  # compatible with Python-3",
            " ",
            "     # return list of tags sorted by their datetimes from newest to oldest",
            "     return sorted(tags.items(), key=lambda tag: tag[1][0], reverse=True)",
            " ",
            " ",
            "-def get_current_version(projdir=PROJDIR, pattern=PATTERN, logger=None):",
            "+def get_current_version(",
            "+    projdir: str = PROJDIR,",
            "+    pattern: str = PATTERN,",
            "+    logger: Optional[logging.Logger] = None,",
            "+) -> Optional[str]:",
            "     \"\"\"Return the most recent tag, using an options regular expression pattern.",
            " ",
            "     The default pattern will strip any characters preceding the first semantic",
            "     version. *EG*: \"Release-0.2.1-rc.1\" will be come \"0.2.1-rc.1\". If no match",
            "     is found, then the most recent tag is return without modification.",
            " ",
            "     Args:",
            "@@ -125,23 +133,28 @@",
            "     Returns:",
            "       tag matching first group in regular expression pattern",
            "     \"\"\"",
            "     tags = get_recent_tags(projdir)",
            "     try:",
            "         tag = tags[0][0]",
            "     except IndexError:",
            "-        return",
            "+        return None",
            "     matches = re.match(pattern, tag)",
            "-    try:",
            "-        current_version = matches.group(1)",
            "-    except (IndexError, AttributeError) as err:",
            "+    if matches:",
            "+        try:",
            "+            current_version = matches.group(1)",
            "+            return current_version",
            "+        except IndexError as err:",
            "+            if logger:",
            "+                logger.debug(\"Pattern %r didn't match tag %r: %s\", pattern, tag, err)",
            "+            return tag",
            "+    else:",
            "         if logger:",
            "-            logger.exception(err)",
            "+            logger.debug(\"Pattern %r didn't match tag %r\", pattern, tag)",
            "         return tag",
            "-    return current_version",
            " ",
            " ",
            " if __name__ == \"__main__\":",
            "     if len(sys.argv) > 1:",
            "         _PROJDIR = sys.argv[1]",
            "     else:",
            "         _PROJDIR = PROJDIR"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/contrib/requests_vendor.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/contrib/requests_vendor.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/contrib/requests_vendor.py",
            "@@ -28,42 +28,60 @@",
            "   >>> from dulwich.contrib.requests_vendor import RequestsHttpGitClient",
            "   >>> _mod_client.HttpGitClient = RequestsHttpGitClient",
            " ",
            " This implementation is experimental and does not have any tests.",
            " \"\"\"",
            " ",
            " from io import BytesIO",
            "+from typing import TYPE_CHECKING, Any, Callable, Optional",
            "+",
            "+if TYPE_CHECKING:",
            "+    from ..config import ConfigFile",
            " ",
            " from requests import Session",
            " ",
            " from ..client import (",
            "     AbstractHttpGitClient,",
            "     HTTPProxyUnauthorized,",
            "     HTTPUnauthorized,",
            "     default_user_agent_string,",
            " )",
            " from ..errors import GitProtocolError, NotGitRepository",
            " ",
            " ",
            " class RequestsHttpGitClient(AbstractHttpGitClient):",
            "     def __init__(",
            "-        self, base_url, dumb=None, config=None, username=None, password=None, **kwargs",
            "+        self,",
            "+        base_url: str,",
            "+        dumb: Optional[bool] = None,",
            "+        config: Optional[\"ConfigFile\"] = None,",
            "+        username: Optional[str] = None,",
            "+        password: Optional[str] = None,",
            "+        **kwargs: object,",
            "     ) -> None:",
            "         self._username = username",
            "         self._password = password",
            " ",
            "         self.session = get_session(config)",
            " ",
            "         if username is not None:",
            "-            self.session.auth = (username, password)",
            "-",
            "-        super().__init__(base_url=base_url, dumb=dumb, **kwargs)",
            "+            self.session.auth = (username, password)  # type: ignore[assignment]",
            " ",
            "-    def _http_request(self, url, headers=None, data=None, allow_compression=False):",
            "-        req_headers = self.session.headers.copy()",
            "+        super().__init__(",
            "+            base_url=base_url, dumb=bool(dumb) if dumb is not None else False, **kwargs",
            "+        )",
            "+",
            "+    def _http_request(",
            "+        self,",
            "+        url: str,",
            "+        headers: Optional[dict[str, str]] = None,",
            "+        data: Optional[bytes] = None,",
            "+        allow_compression: bool = False,",
            "+    ) -> tuple[Any, Callable[[int], bytes]]:",
            "+        req_headers = self.session.headers.copy()  # type: ignore[attr-defined]",
            "         if headers is not None:",
            "             req_headers.update(headers)",
            " ",
            "         if allow_compression:",
            "             req_headers[\"Accept-Encoding\"] = \"gzip\"",
            "         else:",
            "             req_headers[\"Accept-Encoding\"] = \"identity\"",
            "@@ -79,62 +97,66 @@",
            "             raise HTTPUnauthorized(resp.headers.get(\"WWW-Authenticate\"), url)",
            "         if resp.status_code == 407:",
            "             raise HTTPProxyUnauthorized(resp.headers.get(\"Proxy-Authenticate\"), url)",
            "         if resp.status_code != 200:",
            "             raise GitProtocolError(f\"unexpected http resp {resp.status_code} for {url}\")",
            " ",
            "         # Add required fields as stated in AbstractHttpGitClient._http_request",
            "-        resp.content_type = resp.headers.get(\"Content-Type\")",
            "-        resp.redirect_location = \"\"",
            "+        resp.content_type = resp.headers.get(\"Content-Type\")  # type: ignore[attr-defined]",
            "+        resp.redirect_location = \"\"  # type: ignore[attr-defined]",
            "         if resp.history:",
            "-            resp.redirect_location = resp.url",
            "+            resp.redirect_location = resp.url  # type: ignore[attr-defined]",
            " ",
            "         read = BytesIO(resp.content).read",
            " ",
            "         return resp, read",
            " ",
            " ",
            "-def get_session(config):",
            "+def get_session(config: Optional[\"ConfigFile\"]) -> Session:",
            "     session = Session()",
            "     session.headers.update({\"Pragma\": \"no-cache\"})",
            " ",
            "-    proxy_server = user_agent = ca_certs = ssl_verify = None",
            "+    proxy_server: Optional[str] = None",
            "+    user_agent: Optional[str] = None",
            "+    ca_certs: Optional[str] = None",
            "+    ssl_verify: Optional[bool] = None",
            " ",
            "     if config is not None:",
            "         try:",
            "-            proxy_server = config.get(b\"http\", b\"proxy\")",
            "-            if isinstance(proxy_server, bytes):",
            "-                proxy_server = proxy_server.decode()",
            "+            proxy_bytes = config.get(b\"http\", b\"proxy\")",
            "+            if isinstance(proxy_bytes, bytes):",
            "+                proxy_server = proxy_bytes.decode()",
            "         except KeyError:",
            "             pass",
            " ",
            "         try:",
            "-            user_agent = config.get(b\"http\", b\"useragent\")",
            "-            if isinstance(user_agent, bytes):",
            "-                user_agent = user_agent.decode()",
            "+            agent_bytes = config.get(b\"http\", b\"useragent\")",
            "+            if isinstance(agent_bytes, bytes):",
            "+                user_agent = agent_bytes.decode()",
            "         except KeyError:",
            "             pass",
            " ",
            "         try:",
            "             ssl_verify = config.get_boolean(b\"http\", b\"sslVerify\")",
            "         except KeyError:",
            "             ssl_verify = True",
            " ",
            "         try:",
            "-            ca_certs = config.get(b\"http\", b\"sslCAInfo\")",
            "-            if isinstance(ca_certs, bytes):",
            "-                ca_certs = ca_certs.decode()",
            "+            certs_bytes = config.get(b\"http\", b\"sslCAInfo\")",
            "+            if isinstance(certs_bytes, bytes):",
            "+                ca_certs = certs_bytes.decode()",
            "         except KeyError:",
            "             ca_certs = None",
            " ",
            "     if user_agent is None:",
            "         user_agent = default_user_agent_string()",
            "-    session.headers.update({\"User-agent\": user_agent})",
            "+    if user_agent is not None:",
            "+        session.headers.update({\"User-agent\": user_agent})",
            " ",
            "     if ca_certs:",
            "         session.verify = ca_certs",
            "     elif ssl_verify is False:",
            "         session.verify = ssl_verify",
            " ",
            "-    if proxy_server:",
            "+    if proxy_server is not None:",
            "         session.proxies.update({\"http\": proxy_server, \"https\": proxy_server})",
            "     return session"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/contrib/swift.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/contrib/swift.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/contrib/swift.py",
            "@@ -24,34 +24,37 @@",
            " \"\"\"Repo implementation atop OpenStack SWIFT.\"\"\"",
            " ",
            " # TODO: Refactor to share more code with dulwich/repo.py.",
            " # TODO(fbo): Second attempt to _send() must be notified via real log",
            " # TODO(fbo): More logs for operations",
            " ",
            " import json",
            "+import logging",
            " import os",
            " import posixpath",
            " import stat",
            " import sys",
            " import tempfile",
            " import urllib.parse as urlparse",
            " import zlib",
            "+from collections.abc import Iterator",
            " from configparser import ConfigParser",
            " from io import BytesIO",
            "-from typing import Optional",
            "+from typing import BinaryIO, Callable, Optional, Union, cast",
            " ",
            " from geventhttpclient import HTTPClient",
            " ",
            " from ..greenthreads import GreenThreadsMissingObjectFinder",
            " from ..lru_cache import LRUSizeCache",
            "-from ..object_store import INFODIR, PACKDIR, PackBasedObjectStore",
            "+from ..object_store import INFODIR, PACKDIR, ObjectContainer, PackBasedObjectStore",
            " from ..objects import S_ISGITLINK, Blob, Commit, Tag, Tree",
            " from ..pack import (",
            "     Pack,",
            "     PackData,",
            "+    PackIndex,",
            "     PackIndexer,",
            "     PackStreamCopier,",
            "     _compute_object_size,",
            "     compute_file_sha,",
            "     iter_sha1,",
            "     load_pack_index_file,",
            "     read_pack_header,",
            "@@ -59,15 +62,15 @@",
            "     write_pack_header,",
            "     write_pack_index_v2,",
            "     write_pack_object,",
            " )",
            " from ..protocol import TCP_GIT_PORT",
            " from ..refs import InfoRefsContainer, read_info_refs, split_peeled_refs, write_info_refs",
            " from ..repo import OBJECTDIR, BaseRepo",
            "-from ..server import Backend, TCPGitServer",
            "+from ..server import Backend, BackendRepo, TCPGitServer",
            " ",
            " \"\"\"",
            " # Configuration file sample",
            " [swift]",
            " # Authentication URL (Keystone or Swift)",
            " auth_url = http://127.0.0.1:5000/v2.0",
            " # Authentication version to use",
            "@@ -90,122 +93,157 @@",
            " chunk_length = 12228",
            " # Cache size (MBytes) (Default 20)",
            " cache_length = 20",
            " \"\"\"",
            " ",
            " ",
            " class PackInfoMissingObjectFinder(GreenThreadsMissingObjectFinder):",
            "-    def next(self):",
            "+    def next(self) -> Optional[tuple[bytes, int, Union[bytes, None]]]:",
            "         while True:",
            "             if not self.objects_to_send:",
            "                 return None",
            "-            (sha, name, leaf) = self.objects_to_send.pop()",
            "+            (sha, name, leaf, _) = self.objects_to_send.pop()",
            "             if sha not in self.sha_done:",
            "                 break",
            "         if not leaf:",
            "-            info = self.object_store.pack_info_get(sha)",
            "-            if info[0] == Commit.type_num:",
            "-                self.add_todo([(info[2], \"\", False)])",
            "-            elif info[0] == Tree.type_num:",
            "-                self.add_todo([tuple(i) for i in info[1]])",
            "-            elif info[0] == Tag.type_num:",
            "-                self.add_todo([(info[1], None, False)])",
            "-            if sha in self._tagged:",
            "-                self.add_todo([(self._tagged[sha], None, True)])",
            "+            try:",
            "+                obj = self.object_store[sha]",
            "+                if isinstance(obj, Commit):",
            "+                    self.add_todo([(obj.tree, b\"\", None, False)])",
            "+                elif isinstance(obj, Tree):",
            "+                    tree_items = [",
            "+                        (",
            "+                            item.sha,",
            "+                            item.path",
            "+                            if isinstance(item.path, bytes)",
            "+                            else item.path.encode(\"utf-8\"),",
            "+                            None,",
            "+                            False,",
            "+                        )",
            "+                        for item in obj.items()",
            "+                    ]",
            "+                    self.add_todo(tree_items)",
            "+                elif isinstance(obj, Tag):",
            "+                    self.add_todo([(obj.object[1], None, None, False)])",
            "+                if sha in self._tagged:",
            "+                    self.add_todo([(self._tagged[sha], None, None, True)])",
            "+            except KeyError:",
            "+                pass",
            "         self.sha_done.add(sha)",
            "         self.progress(f\"counting objects: {len(self.sha_done)}\\r\")",
            "-        return (sha, name)",
            "+        return (",
            "+            sha,",
            "+            0,",
            "+            name if isinstance(name, bytes) else name.encode(\"utf-8\") if name else None,",
            "+        )",
            " ",
            " ",
            "-def load_conf(path=None, file=None):",
            "+def load_conf(path: Optional[str] = None, file: Optional[str] = None) -> ConfigParser:",
            "     \"\"\"Load configuration in global var CONF.",
            " ",
            "     Args:",
            "       path: The path to the configuration file",
            "       file: If provided read instead the file like object",
            "     \"\"\"",
            "     conf = ConfigParser()",
            "     if file:",
            "-        try:",
            "-            conf.read_file(file, path)",
            "-        except AttributeError:",
            "-            # read_file only exists in Python3",
            "-            conf.readfp(file)",
            "-        return conf",
            "-    confpath = None",
            "-    if not path:",
            "-        try:",
            "-            confpath = os.environ[\"DULWICH_SWIFT_CFG\"]",
            "-        except KeyError as exc:",
            "-            raise Exception(\"You need to specify a configuration file\") from exc",
            "+        conf.read_file(file, path)",
            "     else:",
            "-        confpath = path",
            "-    if not os.path.isfile(confpath):",
            "-        raise Exception(f\"Unable to read configuration file {confpath}\")",
            "-    conf.read(confpath)",
            "+        confpath = None",
            "+        if not path:",
            "+            try:",
            "+                confpath = os.environ[\"DULWICH_SWIFT_CFG\"]",
            "+            except KeyError as exc:",
            "+                raise Exception(\"You need to specify a configuration file\") from exc",
            "+        else:",
            "+            confpath = path",
            "+        if not os.path.isfile(confpath):",
            "+            raise Exception(f\"Unable to read configuration file {confpath}\")",
            "+        conf.read(confpath)",
            "     return conf",
            " ",
            " ",
            "-def swift_load_pack_index(scon, filename):",
            "+def swift_load_pack_index(scon: \"SwiftConnector\", filename: str) -> \"PackIndex\":",
            "     \"\"\"Read a pack index file from Swift.",
            " ",
            "     Args:",
            "       scon: a `SwiftConnector` instance",
            "       filename: Path to the index file objectise",
            "     Returns: a `PackIndexer` instance",
            "     \"\"\"",
            "-    with scon.get_object(filename) as f:",
            "-        return load_pack_index_file(filename, f)",
            "+    f = scon.get_object(filename)",
            "+    if f is None:",
            "+        raise Exception(f\"Could not retrieve index file {filename}\")",
            "+    if isinstance(f, bytes):",
            "+        f = BytesIO(f)",
            "+    return load_pack_index_file(filename, f)",
            " ",
            " ",
            "-def pack_info_create(pack_data, pack_index):",
            "+def pack_info_create(pack_data: \"PackData\", pack_index: \"PackIndex\") -> bytes:",
            "     pack = Pack.from_objects(pack_data, pack_index)",
            "-    info = {}",
            "+    info: dict = {}",
            "     for obj in pack.iterobjects():",
            "         # Commit",
            "         if obj.type_num == Commit.type_num:",
            "-            info[obj.id] = (obj.type_num, obj.parents, obj.tree)",
            "+            commit_obj = obj",
            "+            assert isinstance(commit_obj, Commit)",
            "+            info[obj.id] = (obj.type_num, commit_obj.parents, commit_obj.tree)",
            "         # Tree",
            "         elif obj.type_num == Tree.type_num:",
            "+            tree_obj = obj",
            "+            assert isinstance(tree_obj, Tree)",
            "             shas = [",
            "                 (s, n, not stat.S_ISDIR(m))",
            "-                for n, m, s in obj.items()",
            "+                for n, m, s in tree_obj.items()",
            "                 if not S_ISGITLINK(m)",
            "             ]",
            "             info[obj.id] = (obj.type_num, shas)",
            "         # Blob",
            "         elif obj.type_num == Blob.type_num:",
            "-            info[obj.id] = None",
            "+            info[obj.id] = (obj.type_num,)",
            "         # Tag",
            "         elif obj.type_num == Tag.type_num:",
            "-            info[obj.id] = (obj.type_num, obj.object[1])",
            "-    return zlib.compress(json.dumps(info))",
            "+            tag_obj = obj",
            "+            assert isinstance(tag_obj, Tag)",
            "+            info[obj.id] = (obj.type_num, tag_obj.object[1])",
            "+    return zlib.compress(json.dumps(info).encode(\"utf-8\"))",
            " ",
            " ",
            "-def load_pack_info(filename, scon=None, file=None):",
            "+def load_pack_info(",
            "+    filename: str,",
            "+    scon: Optional[\"SwiftConnector\"] = None,",
            "+    file: Optional[BinaryIO] = None,",
            "+) -> Optional[dict]:",
            "     if not file:",
            "-        f = scon.get_object(filename)",
            "+        if scon is None:",
            "+            return None",
            "+        obj = scon.get_object(filename)",
            "+        if obj is None:",
            "+            return None",
            "+        if isinstance(obj, bytes):",
            "+            return json.loads(zlib.decompress(obj))",
            "+        else:",
            "+            f: BinaryIO = obj",
            "     else:",
            "         f = file",
            "-    if not f:",
            "-        return None",
            "     try:",
            "         return json.loads(zlib.decompress(f.read()))",
            "     finally:",
            "-        f.close()",
            "+        if hasattr(f, \"close\"):",
            "+            f.close()",
            " ",
            " ",
            " class SwiftException(Exception):",
            "     pass",
            " ",
            " ",
            " class SwiftConnector:",
            "     \"\"\"A Connector to swift that manage authentication and errors catching.\"\"\"",
            " ",
            "-    def __init__(self, root, conf) -> None:",
            "+    def __init__(self, root: str, conf: ConfigParser) -> None:",
            "         \"\"\"Initialize a SwiftConnector.",
            " ",
            "         Args:",
            "           root: The swift container that will act as Git bare repository",
            "           conf: A ConfigParser Object",
            "         \"\"\"",
            "         self.conf = conf",
            "@@ -238,15 +276,15 @@",
            "             network_timeout=self.http_timeout,",
            "             headers=token_header,",
            "         )",
            "         self.base_path = str(",
            "             posixpath.join(urlparse.urlparse(self.storage_url).path, self.root)",
            "         )",
            " ",
            "-    def swift_auth_v1(self):",
            "+    def swift_auth_v1(self) -> tuple[str, str]:",
            "         self.user = self.user.replace(\";\", \":\")",
            "         auth_httpclient = HTTPClient.from_url(",
            "             self.auth_url,",
            "             connection_timeout=self.http_timeout,",
            "             network_timeout=self.http_timeout,",
            "         )",
            "         headers = {\"X-Auth-User\": self.user, \"X-Auth-Key\": self.password}",
            "@@ -261,15 +299,15 @@",
            "                 \"AUTH v1.0 request failed on \"",
            "                 + f\"{self.auth_url} with error code {ret.status_code} ({ret.items()!s})\"",
            "             )",
            "         storage_url = ret[\"X-Storage-Url\"]",
            "         token = ret[\"X-Auth-Token\"]",
            "         return storage_url, token",
            " ",
            "-    def swift_auth_v2(self):",
            "+    def swift_auth_v2(self) -> tuple[str, str]:",
            "         self.tenant, self.user = self.user.split(\";\")",
            "         auth_dict = {}",
            "         auth_dict[\"auth\"] = {",
            "             \"passwordCredentials\": {",
            "                 \"username\": self.user,",
            "                 \"password\": self.password,",
            "             },",
            "@@ -327,15 +365,15 @@",
            "         if not self.test_root_exists():",
            "             ret = self.httpclient.request(\"PUT\", self.base_path)",
            "             if ret.status_code < 200 or ret.status_code > 300:",
            "                 raise SwiftException(",
            "                     f\"PUT request failed with error code {ret.status_code}\"",
            "                 )",
            " ",
            "-    def get_container_objects(self):",
            "+    def get_container_objects(self) -> Optional[list[dict]]:",
            "         \"\"\"Retrieve objects list in a container.",
            " ",
            "         Returns: A list of dict that describe objects",
            "                  or None if container does not exist",
            "         \"\"\"",
            "         qs = \"?format=json\"",
            "         path = self.base_path + qs",
            "@@ -345,15 +383,15 @@",
            "         if ret.status_code < 200 or ret.status_code > 300:",
            "             raise SwiftException(",
            "                 f\"GET request failed with error code {ret.status_code}\"",
            "             )",
            "         content = ret.read()",
            "         return json.loads(content)",
            " ",
            "-    def get_object_stat(self, name):",
            "+    def get_object_stat(self, name: str) -> Optional[dict]:",
            "         \"\"\"Retrieve object stat.",
            " ",
            "         Args:",
            "           name: The object name",
            "         Returns:",
            "           A dict that describe the object or None if object does not exist",
            "         \"\"\"",
            "@@ -366,45 +404,47 @@",
            "                 f\"HEAD request failed with error code {ret.status_code}\"",
            "             )",
            "         resp_headers = {}",
            "         for header, value in ret.items():",
            "             resp_headers[header.lower()] = value",
            "         return resp_headers",
            " ",
            "-    def put_object(self, name, content) -> None:",
            "+    def put_object(self, name: str, content: BinaryIO) -> None:",
            "         \"\"\"Put an object.",
            " ",
            "         Args:",
            "           name: The object name",
            "           content: A file object",
            "         Raises:",
            "           SwiftException: if unable to create",
            "         \"\"\"",
            "         content.seek(0)",
            "         data = content.read()",
            "         path = self.base_path + \"/\" + name",
            "         headers = {\"Content-Length\": str(len(data))}",
            " ",
            "-        def _send():",
            "+        def _send() -> object:",
            "             ret = self.httpclient.request(\"PUT\", path, body=data, headers=headers)",
            "             return ret",
            " ",
            "         try:",
            "             # Sometime got Broken Pipe - Dirty workaround",
            "             ret = _send()",
            "-        except Exception:",
            "+        except (BrokenPipeError, ConnectionError):",
            "             # Second attempt work",
            "             ret = _send()",
            " ",
            "-        if ret.status_code < 200 or ret.status_code > 300:",
            "+        if ret.status_code < 200 or ret.status_code > 300:  # type: ignore",
            "             raise SwiftException(",
            "-                f\"PUT request failed with error code {ret.status_code}\"",
            "+                f\"PUT request failed with error code {ret.status_code}\"  # type: ignore",
            "             )",
            " ",
            "-    def get_object(self, name, range=None):",
            "+    def get_object(",
            "+        self, name: str, range: Optional[str] = None",
            "+    ) -> Optional[Union[bytes, BytesIO]]:",
            "         \"\"\"Retrieve an object.",
            " ",
            "         Args:",
            "           name: The object name",
            "           range: A string range like \"0-10\" to",
            "                  retrieve specified bytes in object content",
            "         Returns:",
            "@@ -423,15 +463,15 @@",
            "             )",
            "         content = ret.read()",
            " ",
            "         if range:",
            "             return content",
            "         return BytesIO(content)",
            " ",
            "-    def del_object(self, name) -> None:",
            "+    def del_object(self, name: str) -> None:",
            "         \"\"\"Delete an object.",
            " ",
            "         Args:",
            "           name: The object name",
            "         Raises:",
            "           SwiftException: if unable to delete",
            "         \"\"\"",
            "@@ -444,16 +484,18 @@",
            " ",
            "     def del_root(self) -> None:",
            "         \"\"\"Delete the root container by removing container content.",
            " ",
            "         Raises:",
            "           SwiftException: if unable to delete",
            "         \"\"\"",
            "-        for obj in self.get_container_objects():",
            "-            self.del_object(obj[\"name\"])",
            "+        objects = self.get_container_objects()",
            "+        if objects:",
            "+            for obj in objects:",
            "+                self.del_object(obj[\"name\"])",
            "         ret = self.httpclient.request(\"DELETE\", self.base_path)",
            "         if ret.status_code < 200 or ret.status_code > 300:",
            "             raise SwiftException(",
            "                 f\"DELETE request failed with error code {ret.status_code}\"",
            "             )",
            " ",
            " ",
            "@@ -463,15 +505,15 @@",
            "     The reader allows to read a specified amount of bytes from",
            "     a given offset of a Swift object. A read offset is kept internally.",
            "     The reader will read from Swift a specified amount of data to complete",
            "     its internal buffer. chunk_length specify the amount of data",
            "     to read from Swift.",
            "     \"\"\"",
            " ",
            "-    def __init__(self, scon, filename, pack_length) -> None:",
            "+    def __init__(self, scon: SwiftConnector, filename: str, pack_length: int) -> None:",
            "         \"\"\"Initialize a SwiftPackReader.",
            " ",
            "         Args:",
            "           scon: a `SwiftConnector` instance",
            "           filename: the pack filename",
            "           pack_length: The size of the pack object",
            "         \"\"\"",
            "@@ -479,23 +521,28 @@",
            "         self.filename = filename",
            "         self.pack_length = pack_length",
            "         self.offset = 0",
            "         self.base_offset = 0",
            "         self.buff = b\"\"",
            "         self.buff_length = self.scon.chunk_length",
            " ",
            "-    def _read(self, more=False) -> None:",
            "+    def _read(self, more: bool = False) -> None:",
            "         if more:",
            "             self.buff_length = self.buff_length * 2",
            "         offset = self.base_offset",
            "         r = min(self.base_offset + self.buff_length, self.pack_length)",
            "         ret = self.scon.get_object(self.filename, range=f\"{offset}-{r}\")",
            "-        self.buff = ret",
            "+        if ret is None:",
            "+            self.buff = b\"\"",
            "+        elif isinstance(ret, bytes):",
            "+            self.buff = ret",
            "+        else:",
            "+            self.buff = ret.read()",
            " ",
            "-    def read(self, length):",
            "+    def read(self, length: int) -> bytes:",
            "         \"\"\"Read a specified amount of Bytes form the pack object.",
            " ",
            "         Args:",
            "           length: amount of bytes to read",
            "         Returns:",
            "           a bytestring",
            "         \"\"\"",
            "@@ -508,154 +555,179 @@",
            "             # Need to read more from swift",
            "             self._read(more=True)",
            "             return self.read(length)",
            "         data = self.buff[self.offset : end]",
            "         self.offset = end",
            "         return data",
            " ",
            "-    def seek(self, offset) -> None:",
            "+    def seek(self, offset: int) -> None:",
            "         \"\"\"Seek to a specified offset.",
            " ",
            "         Args:",
            "           offset: the offset to seek to",
            "         \"\"\"",
            "         self.base_offset = offset",
            "         self._read()",
            "         self.offset = 0",
            " ",
            "-    def read_checksum(self):",
            "+    def read_checksum(self) -> bytes:",
            "         \"\"\"Read the checksum from the pack.",
            " ",
            "         Returns: the checksum bytestring",
            "         \"\"\"",
            "-        return self.scon.get_object(self.filename, range=\"-20\")",
            "+        ret = self.scon.get_object(self.filename, range=\"-20\")",
            "+        if ret is None:",
            "+            return b\"\"",
            "+        elif isinstance(ret, bytes):",
            "+            return ret",
            "+        else:",
            "+            return ret.read()",
            " ",
            " ",
            " class SwiftPackData(PackData):",
            "     \"\"\"The data contained in a packfile.",
            " ",
            "     We use the SwiftPackReader to read bytes from packs stored in Swift",
            "     using the Range header feature of Swift.",
            "     \"\"\"",
            " ",
            "-    def __init__(self, scon, filename) -> None:",
            "+    def __init__(self, scon: SwiftConnector, filename: Union[str, os.PathLike]) -> None:",
            "         \"\"\"Initialize a SwiftPackReader.",
            " ",
            "         Args:",
            "           scon: a `SwiftConnector` instance",
            "           filename: the pack filename",
            "         \"\"\"",
            "         self.scon = scon",
            "         self._filename = filename",
            "         self._header_size = 12",
            "-        headers = self.scon.get_object_stat(self._filename)",
            "+        headers = self.scon.get_object_stat(str(self._filename))",
            "+        if headers is None:",
            "+            raise Exception(f\"Could not get stats for {self._filename}\")",
            "         self.pack_length = int(headers[\"content-length\"])",
            "-        pack_reader = SwiftPackReader(self.scon, self._filename, self.pack_length)",
            "+        pack_reader = SwiftPackReader(self.scon, str(self._filename), self.pack_length)",
            "         (version, self._num_objects) = read_pack_header(pack_reader.read)",
            "         self._offset_cache = LRUSizeCache(",
            "             1024 * 1024 * self.scon.cache_length,",
            "             compute_size=_compute_object_size,",
            "         )",
            "         self.pack = None",
            " ",
            "-    def get_object_at(self, offset):",
            "+    def get_object_at(",
            "+        self, offset: int",
            "+    ) -> tuple[int, Union[tuple[Union[bytes, int], list[bytes]], list[bytes]]]:",
            "         if offset in self._offset_cache:",
            "             return self._offset_cache[offset]",
            "         assert offset >= self._header_size",
            "-        pack_reader = SwiftPackReader(self.scon, self._filename, self.pack_length)",
            "+        pack_reader = SwiftPackReader(self.scon, str(self._filename), self.pack_length)",
            "         pack_reader.seek(offset)",
            "         unpacked, _ = unpack_object(pack_reader.read)",
            "-        return (unpacked.pack_type_num, unpacked._obj())",
            "+        obj_data = unpacked._obj()",
            "+        return (unpacked.pack_type_num, obj_data)",
            " ",
            "-    def get_stored_checksum(self):",
            "-        pack_reader = SwiftPackReader(self.scon, self._filename, self.pack_length)",
            "+    def get_stored_checksum(self) -> bytes:",
            "+        pack_reader = SwiftPackReader(self.scon, str(self._filename), self.pack_length)",
            "         return pack_reader.read_checksum()",
            " ",
            "     def close(self) -> None:",
            "         pass",
            " ",
            " ",
            " class SwiftPack(Pack):",
            "     \"\"\"A Git pack object.",
            " ",
            "     Same implementation as pack.Pack except that _idx_load and",
            "     _data_load are bounded to Swift version of load_pack_index and",
            "     PackData.",
            "     \"\"\"",
            " ",
            "-    def __init__(self, *args, **kwargs) -> None:",
            "+    def __init__(self, *args: object, **kwargs: object) -> None:",
            "         self.scon = kwargs[\"scon\"]",
            "         del kwargs[\"scon\"]",
            "-        super().__init__(*args, **kwargs)",
            "+        super().__init__(*args, **kwargs)  # type: ignore",
            "         self._pack_info_path = self._basename + \".info\"",
            "-        self._pack_info = None",
            "-        self._pack_info_load = lambda: load_pack_info(self._pack_info_path, self.scon)",
            "-        self._idx_load = lambda: swift_load_pack_index(self.scon, self._idx_path)",
            "-        self._data_load = lambda: SwiftPackData(self.scon, self._data_path)",
            "+        self._pack_info: Optional[dict] = None",
            "+        self._pack_info_load = lambda: load_pack_info(self._pack_info_path, self.scon)  # type: ignore",
            "+        self._idx_load = lambda: swift_load_pack_index(self.scon, self._idx_path)  # type: ignore",
            "+        self._data_load = lambda: SwiftPackData(self.scon, self._data_path)  # type: ignore",
            " ",
            "     @property",
            "-    def pack_info(self):",
            "+    def pack_info(self) -> Optional[dict]:",
            "         \"\"\"The pack data object being used.\"\"\"",
            "         if self._pack_info is None:",
            "             self._pack_info = self._pack_info_load()",
            "         return self._pack_info",
            " ",
            " ",
            " class SwiftObjectStore(PackBasedObjectStore):",
            "     \"\"\"A Swift Object Store.",
            " ",
            "     Allow to manage a bare Git repository from Openstack Swift.",
            "     This object store only supports pack files and not loose objects.",
            "     \"\"\"",
            " ",
            "-    def __init__(self, scon) -> None:",
            "+    def __init__(self, scon: SwiftConnector) -> None:",
            "         \"\"\"Open a Swift object store.",
            " ",
            "         Args:",
            "           scon: A `SwiftConnector` instance",
            "         \"\"\"",
            "         super().__init__()",
            "         self.scon = scon",
            "         self.root = self.scon.root",
            "         self.pack_dir = posixpath.join(OBJECTDIR, PACKDIR)",
            "         self._alternates = None",
            " ",
            "-    def _update_pack_cache(self):",
            "+    def _update_pack_cache(self) -> list:",
            "         objects = self.scon.get_container_objects()",
            "+        if objects is None:",
            "+            return []",
            "         pack_files = [",
            "             o[\"name\"].replace(\".pack\", \"\")",
            "             for o in objects",
            "             if o[\"name\"].endswith(\".pack\")",
            "         ]",
            "         ret = []",
            "         for basename in pack_files:",
            "             pack = SwiftPack(basename, scon=self.scon)",
            "             self._pack_cache[basename] = pack",
            "             ret.append(pack)",
            "         return ret",
            " ",
            "-    def _iter_loose_objects(self):",
            "+    def _iter_loose_objects(self) -> Iterator:",
            "         \"\"\"Loose objects are not supported by this repository.\"\"\"",
            "-        return []",
            "+        return iter([])",
            " ",
            "-    def pack_info_get(self, sha):",
            "+    def pack_info_get(self, sha: bytes) -> Optional[tuple]:",
            "         for pack in self.packs:",
            "             if sha in pack:",
            "-                return pack.pack_info[sha]",
            "+                if hasattr(pack, \"pack_info\"):",
            "+                    pack_info = pack.pack_info",
            "+                    if pack_info is not None:",
            "+                        return pack_info.get(sha)",
            "+        return None",
            " ",
            "-    def _collect_ancestors(self, heads, common=set()):",
            "-        def _find_parents(commit):",
            "+    def _collect_ancestors(",
            "+        self, heads: list, common: Optional[set] = None",
            "+    ) -> tuple[set, set]:",
            "+        if common is None:",
            "+            common = set()",
            "+",
            "+        def _find_parents(commit: bytes) -> list:",
            "             for pack in self.packs:",
            "                 if commit in pack:",
            "                     try:",
            "-                        parents = pack.pack_info[commit][1]",
            "+                        if hasattr(pack, \"pack_info\"):",
            "+                            pack_info = pack.pack_info",
            "+                            if pack_info is not None:",
            "+                                return pack_info[commit][1]",
            "                     except KeyError:",
            "                         # Seems to have no parents",
            "                         return []",
            "-                    return parents",
            "+            return []",
            " ",
            "         bases = set()",
            "         commits = set()",
            "         queue = []",
            "         queue.extend(heads)",
            "         while queue:",
            "             e = queue.pop(0)",
            "@@ -663,30 +735,30 @@",
            "                 bases.add(e)",
            "             elif e not in commits:",
            "                 commits.add(e)",
            "                 parents = _find_parents(e)",
            "                 queue.extend(parents)",
            "         return (commits, bases)",
            " ",
            "-    def add_pack(self):",
            "+    def add_pack(self) -> tuple[BytesIO, Callable, Callable]:",
            "         \"\"\"Add a new pack to this object store.",
            " ",
            "         Returns: Fileobject to write to and a commit function to",
            "             call when the pack is finished.",
            "         \"\"\"",
            "         f = BytesIO()",
            " ",
            "-        def commit():",
            "+        def commit() -> Optional[\"SwiftPack\"]:",
            "             f.seek(0)",
            "             pack = PackData(file=f, filename=\"\")",
            "             entries = pack.sorted_entries()",
            "             if entries:",
            "                 basename = posixpath.join(",
            "                     self.pack_dir,",
            "-                    f\"pack-{iter_sha1(entry[0] for entry in entries)}\",",
            "+                    f\"pack-{iter_sha1(entry[0] for entry in entries).decode('ascii')}\",",
            "                 )",
            "                 index = BytesIO()",
            "                 write_pack_index_v2(index, entries, pack.get_stored_checksum())",
            "                 self.scon.put_object(basename + \".pack\", f)",
            "                 f.close()",
            "                 self.scon.put_object(basename + \".idx\", index)",
            "                 index.close()",
            "@@ -698,66 +770,68 @@",
            "                 return None",
            " ",
            "         def abort() -> None:",
            "             pass",
            " ",
            "         return f, commit, abort",
            " ",
            "-    def add_object(self, obj) -> None:",
            "+    def add_object(self, obj: object) -> None:",
            "         self.add_objects(",
            "             [",
            "-                (obj, None),",
            "+                (obj, None),  # type: ignore",
            "             ]",
            "         )",
            " ",
            "     def _pack_cache_stale(self) -> bool:",
            "         return False",
            " ",
            "-    def _get_loose_object(self, sha) -> None:",
            "+    def _get_loose_object(self, sha: bytes) -> None:",
            "         return None",
            " ",
            "-    def add_thin_pack(self, read_all, read_some):",
            "+    def add_thin_pack(self, read_all: Callable, read_some: Callable) -> \"SwiftPack\":",
            "         \"\"\"Read a thin pack.",
            " ",
            "         Read it from a stream and complete it in a temporary file.",
            "         Then the pack and the corresponding index file are uploaded to Swift.",
            "         \"\"\"",
            "         fd, path = tempfile.mkstemp(prefix=\"tmp_pack_\")",
            "         f = os.fdopen(fd, \"w+b\")",
            "         try:",
            "-            indexer = PackIndexer(f, resolve_ext_ref=self.get_raw)",
            "+            indexer = PackIndexer(f, resolve_ext_ref=None)",
            "             copier = PackStreamCopier(read_all, read_some, f, delta_iter=indexer)",
            "             copier.verify()",
            "             return self._complete_thin_pack(f, path, copier, indexer)",
            "         finally:",
            "             f.close()",
            "             os.unlink(path)",
            " ",
            "-    def _complete_thin_pack(self, f, path, copier, indexer):",
            "-        entries = list(indexer)",
            "+    def _complete_thin_pack(",
            "+        self, f: BinaryIO, path: str, copier: object, indexer: object",
            "+    ) -> \"SwiftPack\":",
            "+        entries = list(indexer)  # type: ignore",
            " ",
            "         # Update the header with the new number of objects.",
            "         f.seek(0)",
            "-        write_pack_header(f, len(entries) + len(indexer.ext_refs()))",
            "+        write_pack_header(f, len(entries) + len(indexer.ext_refs()))  # type: ignore",
            " ",
            "         # Must flush before reading (http://bugs.python.org/issue3207)",
            "         f.flush()",
            " ",
            "         # Rescan the rest of the pack, computing the SHA with the new header.",
            "         new_sha = compute_file_sha(f, end_ofs=-20)",
            " ",
            "         # Must reposition before writing (http://bugs.python.org/issue3207)",
            "         f.seek(0, os.SEEK_CUR)",
            " ",
            "         # Complete the pack.",
            "-        for ext_sha in indexer.ext_refs():",
            "+        for ext_sha in indexer.ext_refs():  # type: ignore",
            "             assert len(ext_sha) == 20",
            "             type_num, data = self.get_raw(ext_sha)",
            "             offset = f.tell()",
            "-            crc32 = write_pack_object(f, type_num, data, sha=new_sha)",
            "+            crc32 = write_pack_object(f, type_num, data, sha=new_sha)  # type: ignore",
            "             entries.append((ext_sha, offset, crc32))",
            "         pack_sha = new_sha.digest()",
            "         f.write(pack_sha)",
            "         f.flush()",
            " ",
            "         # Move the pack in.",
            "         entries.sort()",
            "@@ -792,85 +866,99 @@",
            "         self._add_cached_pack(pack_base_name, final_pack)",
            "         return final_pack",
            " ",
            " ",
            " class SwiftInfoRefsContainer(InfoRefsContainer):",
            "     \"\"\"Manage references in info/refs object.\"\"\"",
            " ",
            "-    def __init__(self, scon, store) -> None:",
            "+    def __init__(self, scon: SwiftConnector, store: object) -> None:",
            "         self.scon = scon",
            "         self.filename = \"info/refs\"",
            "         self.store = store",
            "         f = self.scon.get_object(self.filename)",
            "         if not f:",
            "             f = BytesIO(b\"\")",
            "+        elif isinstance(f, bytes):",
            "+            f = BytesIO(f)",
            "         super().__init__(f)",
            " ",
            "-    def _load_check_ref(self, name, old_ref):",
            "+    def _load_check_ref(",
            "+        self, name: bytes, old_ref: Optional[bytes]",
            "+    ) -> Union[dict, bool]:",
            "         self._check_refname(name)",
            "-        f = self.scon.get_object(self.filename)",
            "-        if not f:",
            "+        obj = self.scon.get_object(self.filename)",
            "+        if not obj:",
            "             return {}",
            "+        if isinstance(obj, bytes):",
            "+            f = BytesIO(obj)",
            "+        else:",
            "+            f = obj",
            "         refs = read_info_refs(f)",
            "         (refs, peeled) = split_peeled_refs(refs)",
            "         if old_ref is not None:",
            "             if refs[name] != old_ref:",
            "                 return False",
            "         return refs",
            " ",
            "-    def _write_refs(self, refs) -> None:",
            "+    def _write_refs(self, refs: dict) -> None:",
            "         f = BytesIO()",
            "-        f.writelines(write_info_refs(refs, self.store))",
            "+        f.writelines(write_info_refs(refs, cast(\"ObjectContainer\", self.store)))",
            "         self.scon.put_object(self.filename, f)",
            " ",
            "     def set_if_equals(",
            "         self,",
            "-        name,",
            "-        old_ref,",
            "-        new_ref,",
            "-        committer=None,",
            "-        timestamp=None,",
            "-        timezone=None,",
            "-        message=None,",
            "+        name: bytes,",
            "+        old_ref: Optional[bytes],",
            "+        new_ref: bytes,",
            "+        committer: Optional[bytes] = None,",
            "+        timestamp: Optional[float] = None,",
            "+        timezone: Optional[int] = None,",
            "+        message: Optional[bytes] = None,",
            "     ) -> bool:",
            "         \"\"\"Set a refname to new_ref only if it currently equals old_ref.\"\"\"",
            "         if name == \"HEAD\":",
            "             return True",
            "         refs = self._load_check_ref(name, old_ref)",
            "         if not isinstance(refs, dict):",
            "             return False",
            "         refs[name] = new_ref",
            "         self._write_refs(refs)",
            "         self._refs[name] = new_ref",
            "         return True",
            " ",
            "     def remove_if_equals(",
            "-        self, name, old_ref, committer=None, timestamp=None, timezone=None, message=None",
            "+        self,",
            "+        name: bytes,",
            "+        old_ref: Optional[bytes],",
            "+        committer: object = None,",
            "+        timestamp: object = None,",
            "+        timezone: object = None,",
            "+        message: object = None,",
            "     ) -> bool:",
            "         \"\"\"Remove a refname only if it currently equals old_ref.\"\"\"",
            "         if name == \"HEAD\":",
            "             return True",
            "         refs = self._load_check_ref(name, old_ref)",
            "         if not isinstance(refs, dict):",
            "             return False",
            "         del refs[name]",
            "         self._write_refs(refs)",
            "         del self._refs[name]",
            "         return True",
            " ",
            "-    def allkeys(self):",
            "+    def allkeys(self) -> Iterator[bytes]:",
            "         try:",
            "-            self._refs[\"HEAD\"] = self._refs[\"refs/heads/master\"]",
            "+            self._refs[b\"HEAD\"] = self._refs[b\"refs/heads/master\"]",
            "         except KeyError:",
            "             pass",
            "-        return self._refs.keys()",
            "+        return iter(self._refs.keys())",
            " ",
            " ",
            " class SwiftRepo(BaseRepo):",
            "-    def __init__(self, root, conf) -> None:",
            "+    def __init__(self, root: str, conf: ConfigParser) -> None:",
            "         \"\"\"Init a Git bare Repository on top of a Swift container.",
            " ",
            "         References are managed in info/refs objects by",
            "         `SwiftInfoRefsContainer`. The root attribute is the Swift",
            "         container that contain the Git bare repository.",
            " ",
            "         Args:",
            "@@ -895,27 +983,27 @@",
            "     def _determine_file_mode(self) -> bool:",
            "         \"\"\"Probe the file-system to determine whether permissions can be trusted.",
            " ",
            "         Returns: True if permissions can be trusted, False otherwise.",
            "         \"\"\"",
            "         return False",
            " ",
            "-    def _put_named_file(self, filename, contents) -> None:",
            "+    def _put_named_file(self, filename: str, contents: bytes) -> None:",
            "         \"\"\"Put an object in a Swift container.",
            " ",
            "         Args:",
            "           filename: the path to the object to put on Swift",
            "           contents: the content as bytestring",
            "         \"\"\"",
            "         with BytesIO() as f:",
            "             f.write(contents)",
            "             self.scon.put_object(filename, f)",
            " ",
            "     @classmethod",
            "-    def init_bare(cls, scon, conf):",
            "+    def init_bare(cls, scon: SwiftConnector, conf: ConfigParser) -> \"SwiftRepo\":",
            "         \"\"\"Create a new bare repository.",
            " ",
            "         Args:",
            "           scon: a `SwiftConnector` instance",
            "           conf: a ConfigParser object",
            "         Returns:",
            "           a `SwiftRepo` instance",
            "@@ -928,24 +1016,24 @@",
            "             scon.put_object(obj, BytesIO(b\"\"))",
            "         ret = cls(scon.root, conf)",
            "         ret._init_files(True)",
            "         return ret",
            " ",
            " ",
            " class SwiftSystemBackend(Backend):",
            "-    def __init__(self, logger, conf) -> None:",
            "+    def __init__(self, logger: \"logging.Logger\", conf: ConfigParser) -> None:",
            "         self.conf = conf",
            "         self.logger = logger",
            " ",
            "-    def open_repository(self, path):",
            "+    def open_repository(self, path: str) -> \"BackendRepo\":",
            "         self.logger.info(\"opening repository at %s\", path)",
            "-        return SwiftRepo(path, self.conf)",
            "+        return cast(\"BackendRepo\", SwiftRepo(path, self.conf))",
            " ",
            " ",
            "-def cmd_daemon(args) -> None:",
            "+def cmd_daemon(args: list) -> None:",
            "     \"\"\"Entry point for starting a TCP git server.\"\"\"",
            "     import optparse",
            " ",
            "     parser = optparse.OptionParser()",
            "     parser.add_option(",
            "         \"-l\",",
            "         \"--listen_address\",",
            "@@ -989,15 +1077,15 @@",
            "     backend = SwiftSystemBackend(logger, conf)",
            " ",
            "     log_utils.default_logging_config()",
            "     server = TCPGitServer(backend, options.listen_address, port=options.port)",
            "     server.serve_forever()",
            " ",
            " ",
            "-def cmd_init(args) -> None:",
            "+def cmd_init(args: list) -> None:",
            "     import optparse",
            " ",
            "     parser = optparse.OptionParser()",
            "     parser.add_option(",
            "         \"-c\",",
            "         \"--swift_config\",",
            "         dest=\"swift_config\",",
            "@@ -1010,15 +1098,15 @@",
            "     if args == []:",
            "         parser.error(\"missing repository name\")",
            "     repo = args[0]",
            "     scon = SwiftConnector(repo, conf)",
            "     SwiftRepo.init_bare(scon, conf)",
            " ",
            " ",
            "-def main(argv=sys.argv) -> None:",
            "+def main(argv: list = sys.argv) -> None:",
            "     commands = {",
            "         \"init\": cmd_init,",
            "         \"daemon\": cmd_daemon,",
            "     }",
            " ",
            "     if len(sys.argv) < 2:",
            "         print("
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/diff_tree.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/diff_tree.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/diff_tree.py",
            "@@ -19,17 +19,18 @@",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Utilities for diffing files and trees.\"\"\"",
            " ",
            " import stat",
            " from collections import defaultdict, namedtuple",
            "+from collections.abc import Iterator",
            " from io import BytesIO",
            " from itertools import chain",
            "-from typing import Optional",
            "+from typing import TYPE_CHECKING, Any, Callable, Optional, TypeVar",
            " ",
            " from .object_store import BaseObjectStore",
            " from .objects import S_ISGITLINK, ObjectID, ShaFile, Tree, TreeEntry",
            " ",
            " # TreeChange type constants.",
            " CHANGE_ADD = \"add\"",
            " CHANGE_MODIFY = \"modify\"",
            "@@ -48,19 +49,19 @@",
            " REWRITE_THRESHOLD = None",
            " ",
            " ",
            " class TreeChange(namedtuple(\"TreeChange\", [\"type\", \"old\", \"new\"])):",
            "     \"\"\"Named tuple a single change between two trees.\"\"\"",
            " ",
            "     @classmethod",
            "-    def add(cls, new):",
            "+    def add(cls, new: TreeEntry) -> \"TreeChange\":",
            "         return cls(CHANGE_ADD, _NULL_ENTRY, new)",
            " ",
            "     @classmethod",
            "-    def delete(cls, old):",
            "+    def delete(cls, old: TreeEntry) -> \"TreeChange\":",
            "         return cls(CHANGE_DELETE, old, _NULL_ENTRY)",
            " ",
            " ",
            " def _tree_entries(path: bytes, tree: Tree) -> list[TreeEntry]:",
            "     result: list[TreeEntry] = []",
            "     if not tree:",
            "         return result",
            "@@ -108,22 +109,27 @@",
            "     for i in range(i1, len1):",
            "         result.append((entries1[i], _NULL_ENTRY))",
            "     for i in range(i2, len2):",
            "         result.append((_NULL_ENTRY, entries2[i]))",
            "     return result",
            " ",
            " ",
            "-def _is_tree(entry):",
            "+def _is_tree(entry: TreeEntry) -> bool:",
            "     mode = entry.mode",
            "     if mode is None:",
            "         return False",
            "     return stat.S_ISDIR(mode)",
            " ",
            " ",
            "-def walk_trees(store, tree1_id, tree2_id, prune_identical=False):",
            "+def walk_trees(",
            "+    store: BaseObjectStore,",
            "+    tree1_id: Optional[ObjectID],",
            "+    tree2_id: Optional[ObjectID],",
            "+    prune_identical: bool = False,",
            "+) -> Iterator[tuple[TreeEntry, TreeEntry]]:",
            "     \"\"\"Recursively walk all the entries of two trees.",
            " ",
            "     Iteration is depth-first pre-order, as in e.g. os.walk.",
            " ",
            "     Args:",
            "       store: An ObjectStore for looking up objects.",
            "       tree1_id: The SHA of the first Tree object to iterate, or None.",
            "@@ -148,33 +154,46 @@",
            "         is_tree2 = _is_tree(entry2)",
            "         if prune_identical and is_tree1 and is_tree2 and entry1 == entry2:",
            "             continue",
            " ",
            "         tree1 = (is_tree1 and store[entry1.sha]) or None",
            "         tree2 = (is_tree2 and store[entry2.sha]) or None",
            "         path = entry1.path or entry2.path",
            "-        todo.extend(reversed(_merge_entries(path, tree1, tree2)))",
            "+",
            "+        # Ensure trees are Tree objects before merging",
            "+        if tree1 is not None and not isinstance(tree1, Tree):",
            "+            tree1 = None",
            "+        if tree2 is not None and not isinstance(tree2, Tree):",
            "+            tree2 = None",
            "+",
            "+        if tree1 is not None or tree2 is not None:",
            "+            # Use empty trees for None values",
            "+            if tree1 is None:",
            "+                tree1 = Tree()",
            "+            if tree2 is None:",
            "+                tree2 = Tree()",
            "+            todo.extend(reversed(_merge_entries(path, tree1, tree2)))",
            "         yield entry1, entry2",
            " ",
            " ",
            "-def _skip_tree(entry, include_trees):",
            "+def _skip_tree(entry: TreeEntry, include_trees: bool) -> TreeEntry:",
            "     if entry.mode is None or (not include_trees and stat.S_ISDIR(entry.mode)):",
            "         return _NULL_ENTRY",
            "     return entry",
            " ",
            " ",
            " def tree_changes(",
            "-    store,",
            "-    tree1_id,",
            "-    tree2_id,",
            "-    want_unchanged=False,",
            "-    rename_detector=None,",
            "-    include_trees=False,",
            "-    change_type_same=False,",
            "-):",
            "+    store: BaseObjectStore,",
            "+    tree1_id: Optional[ObjectID],",
            "+    tree2_id: Optional[ObjectID],",
            "+    want_unchanged: bool = False,",
            "+    rename_detector: Optional[\"RenameDetector\"] = None,",
            "+    include_trees: bool = False,",
            "+    change_type_same: bool = False,",
            "+) -> Iterator[TreeChange]:",
            "     \"\"\"Find the differences between the contents of two trees.",
            " ",
            "     Args:",
            "       store: An ObjectStore for looking up objects.",
            "       tree1_id: The SHA of the source tree.",
            "       tree2_id: The SHA of the target tree.",
            "       want_unchanged: If True, include TreeChanges for unmodified entries",
            "@@ -227,31 +246,35 @@",
            "             change_type = CHANGE_ADD",
            "         else:",
            "             # Both were None because at least one was a tree.",
            "             continue",
            "         yield TreeChange(change_type, entry1, entry2)",
            " ",
            " ",
            "-def _all_eq(seq, key, value) -> bool:",
            "+T = TypeVar(\"T\")",
            "+U = TypeVar(\"U\")",
            "+",
            "+",
            "+def _all_eq(seq: list[T], key: Callable[[T], U], value: U) -> bool:",
            "     for e in seq:",
            "         if key(e) != value:",
            "             return False",
            "     return True",
            " ",
            " ",
            "-def _all_same(seq, key):",
            "+def _all_same(seq: list[Any], key: Callable[[Any], Any]) -> bool:",
            "     return _all_eq(seq[1:], key, key(seq[0]))",
            " ",
            " ",
            " def tree_changes_for_merge(",
            "     store: BaseObjectStore,",
            "     parent_tree_ids: list[ObjectID],",
            "     tree_id: ObjectID,",
            "-    rename_detector=None,",
            "-):",
            "+    rename_detector: Optional[\"RenameDetector\"] = None,",
            "+) -> Iterator[list[Optional[TreeChange]]]:",
            "     \"\"\"Get the tree changes for a merge tree relative to all its parents.",
            " ",
            "     Args:",
            "       store: An ObjectStore for looking up objects.",
            "       parent_tree_ids: An iterable of the SHAs of the parent trees.",
            "       tree_id: The SHA of the merge tree.",
            "       rename_detector: RenameDetector object for detecting renames.",
            "@@ -282,18 +305,18 @@",
            "         for change in parent_changes:",
            "             if change.type == CHANGE_DELETE:",
            "                 path = change.old.path",
            "             else:",
            "                 path = change.new.path",
            "             changes_by_path[path][i] = change",
            " ",
            "-    def old_sha(c):",
            "+    def old_sha(c: TreeChange) -> Optional[ObjectID]:",
            "         return c.old.sha",
            " ",
            "-    def change_type(c):",
            "+    def change_type(c: TreeChange) -> str:",
            "         return c.type",
            " ",
            "     # Yield only conflicting changes.",
            "     for _, changes in sorted(changes_by_path.items()):",
            "         assert len(changes) == num_parents",
            "         have = [c for c in changes if c is not None]",
            "         if _all_eq(have, change_type, CHANGE_DELETE):",
            "@@ -344,15 +367,15 @@",
            "             n = 0",
            "     if n > 0:",
            "         last_block = block_getvalue()",
            "         block_counts[hash(last_block)] += len(last_block)",
            "     return block_counts",
            " ",
            " ",
            "-def _common_bytes(blocks1, blocks2):",
            "+def _common_bytes(blocks1: dict[int, int], blocks2: dict[int, int]) -> int:",
            "     \"\"\"Count the number of common bytes in two block count dicts.",
            " ",
            "     Args:",
            "       blocks1: The first dict of block hashcode -> total bytes.",
            "       blocks2: The second dict of block hashcode -> total bytes.",
            " ",
            "     Returns:",
            "@@ -366,15 +389,19 @@",
            "     for block, count1 in blocks1.items():",
            "         count2 = blocks2.get(block)",
            "         if count2:",
            "             score += min(count1, count2)",
            "     return score",
            " ",
            " ",
            "-def _similarity_score(obj1, obj2, block_cache=None):",
            "+def _similarity_score(",
            "+    obj1: ShaFile,",
            "+    obj2: ShaFile,",
            "+    block_cache: Optional[dict[ObjectID, dict[int, int]]] = None,",
            "+) -> int:",
            "     \"\"\"Compute a similarity score for two objects.",
            " ",
            "     Args:",
            "       obj1: The first object to score.",
            "       obj2: The second object to score.",
            "       block_cache: An optional dict of SHA to block counts to cache",
            "         results between calls.",
            "@@ -394,15 +421,15 @@",
            "     common_bytes = _common_bytes(block_cache[obj1.id], block_cache[obj2.id])",
            "     max_size = max(obj1.raw_length(), obj2.raw_length())",
            "     if not max_size:",
            "         return _MAX_SCORE",
            "     return int(float(common_bytes) * _MAX_SCORE / max_size)",
            " ",
            " ",
            "-def _tree_change_key(entry):",
            "+def _tree_change_key(entry: TreeChange) -> tuple[bytes, bytes]:",
            "     # Sort by old path then new path. If only one exists, use it for both keys.",
            "     path1 = entry.old.path",
            "     path2 = entry.new.path",
            "     if path1 is None:",
            "         path1 = path2",
            "     if path2 is None:",
            "         path2 = path1",
            "@@ -415,19 +442,19 @@",
            "     _adds: list[TreeChange]",
            "     _deletes: list[TreeChange]",
            "     _changes: list[TreeChange]",
            "     _candidates: list[tuple[int, TreeChange]]",
            " ",
            "     def __init__(",
            "         self,",
            "-        store,",
            "-        rename_threshold=RENAME_THRESHOLD,",
            "-        max_files=MAX_FILES,",
            "-        rewrite_threshold=REWRITE_THRESHOLD,",
            "-        find_copies_harder=False,",
            "+        store: BaseObjectStore,",
            "+        rename_threshold: int = RENAME_THRESHOLD,",
            "+        max_files: Optional[int] = MAX_FILES,",
            "+        rewrite_threshold: Optional[int] = REWRITE_THRESHOLD,",
            "+        find_copies_harder: bool = False,",
            "     ) -> None:",
            "         \"\"\"Initialize the rename detector.",
            " ",
            "         Args:",
            "           store: An ObjectStore for looking up objects.",
            "           rename_threshold: The threshold similarity score for considering",
            "             an add/delete pair to be a rename/copy; see _similarity_score.",
            "@@ -450,26 +477,26 @@",
            "         self._want_unchanged = False",
            " ",
            "     def _reset(self) -> None:",
            "         self._adds = []",
            "         self._deletes = []",
            "         self._changes = []",
            " ",
            "-    def _should_split(self, change):",
            "+    def _should_split(self, change: TreeChange) -> bool:",
            "         if (",
            "             self._rewrite_threshold is None",
            "             or change.type != CHANGE_MODIFY",
            "             or change.old.sha == change.new.sha",
            "         ):",
            "             return False",
            "         old_obj = self._store[change.old.sha]",
            "         new_obj = self._store[change.new.sha]",
            "         return _similarity_score(old_obj, new_obj) < self._rewrite_threshold",
            " ",
            "-    def _add_change(self, change) -> None:",
            "+    def _add_change(self, change: TreeChange) -> None:",
            "         if change.type == CHANGE_ADD:",
            "             self._adds.append(change)",
            "         elif change.type == CHANGE_DELETE:",
            "             self._deletes.append(change)",
            "         elif self._should_split(change):",
            "             self._deletes.append(TreeChange.delete(change.old))",
            "             self._adds.append(TreeChange.add(change.new))",
            "@@ -480,26 +507,28 @@",
            "             # but don't split them (to avoid spurious renames). Setting",
            "             # find_copies_harder means we treat unchanged the same as",
            "             # modified.",
            "             self._deletes.append(change)",
            "         else:",
            "             self._changes.append(change)",
            " ",
            "-    def _collect_changes(self, tree1_id, tree2_id) -> None:",
            "+    def _collect_changes(",
            "+        self, tree1_id: Optional[ObjectID], tree2_id: Optional[ObjectID]",
            "+    ) -> None:",
            "         want_unchanged = self._find_copies_harder or self._want_unchanged",
            "         for change in tree_changes(",
            "             self._store,",
            "             tree1_id,",
            "             tree2_id,",
            "             want_unchanged=want_unchanged,",
            "             include_trees=self._include_trees,",
            "         ):",
            "             self._add_change(change)",
            " ",
            "-    def _prune(self, add_paths, delete_paths) -> None:",
            "+    def _prune(self, add_paths: set[bytes], delete_paths: set[bytes]) -> None:",
            "         self._adds = [a for a in self._adds if a.new.path not in add_paths]",
            "         self._deletes = [d for d in self._deletes if d.old.path not in delete_paths]",
            " ",
            "     def _find_exact_renames(self) -> None:",
            "         add_map = defaultdict(list)",
            "         for add in self._adds:",
            "             add_map[add.new.sha].append(add.new)",
            "@@ -528,18 +557,22 @@",
            "             old = sha_deletes[0][0]",
            "             if num_extra_adds > 0:",
            "                 for new in sha_adds[-num_extra_adds:]:",
            "                     add_paths.add(new.path)",
            "                     self._changes.append(TreeChange(CHANGE_COPY, old, new))",
            "         self._prune(add_paths, delete_paths)",
            " ",
            "-    def _should_find_content_renames(self):",
            "+    def _should_find_content_renames(self) -> bool:",
            "+        if self._max_files is None:",
            "+            return True",
            "         return len(self._adds) * len(self._deletes) <= self._max_files**2",
            " ",
            "-    def _rename_type(self, check_paths, delete, add):",
            "+    def _rename_type(",
            "+        self, check_paths: bool, delete: TreeChange, add: TreeChange",
            "+    ) -> str:",
            "         if check_paths and delete.old.path == add.new.path:",
            "             # If the paths match, this must be a split modify, so make sure it",
            "             # comes out as a modify.",
            "             return CHANGE_MODIFY",
            "         elif delete.type != CHANGE_DELETE:",
            "             # If it's in deletes but not marked as a delete, it must have been",
            "             # added due to find_copies_harder, and needs to be marked as a",
            "@@ -614,30 +647,34 @@",
            "             ):",
            "                 modifies[path] = TreeChange(CHANGE_MODIFY, delete.old, add.new)",
            " ",
            "         self._adds = [a for a in self._adds if a.new.path not in modifies]",
            "         self._deletes = [a for a in self._deletes if a.new.path not in modifies]",
            "         self._changes += modifies.values()",
            " ",
            "-    def _sorted_changes(self):",
            "+    def _sorted_changes(self) -> list[TreeChange]:",
            "         result = []",
            "         result.extend(self._adds)",
            "         result.extend(self._deletes)",
            "         result.extend(self._changes)",
            "         result.sort(key=_tree_change_key)",
            "         return result",
            " ",
            "     def _prune_unchanged(self) -> None:",
            "         if self._want_unchanged:",
            "             return",
            "         self._deletes = [d for d in self._deletes if d.type != CHANGE_UNCHANGED]",
            " ",
            "     def changes_with_renames(",
            "-        self, tree1_id, tree2_id, want_unchanged=False, include_trees=False",
            "-    ):",
            "+        self,",
            "+        tree1_id: Optional[ObjectID],",
            "+        tree2_id: Optional[ObjectID],",
            "+        want_unchanged: bool = False,",
            "+        include_trees: bool = False,",
            "+    ) -> list[TreeChange]:",
            "         \"\"\"Iterate TreeChanges between two tree SHAs, with rename detection.\"\"\"",
            "         self._reset()",
            "         self._want_unchanged = want_unchanged",
            "         self._include_trees = include_trees",
            "         self._collect_changes(tree1_id, tree2_id)",
            "         self._find_exact_renames()",
            "         self._find_content_rename_candidates()",
            "@@ -647,16 +684,31 @@",
            "         return self._sorted_changes()",
            " ",
            " ",
            " # Hold on to the pure-python implementations for testing.",
            " _is_tree_py = _is_tree",
            " _merge_entries_py = _merge_entries",
            " _count_blocks_py = _count_blocks",
            "-try:",
            "-    # Try to import Rust versions",
            "-    from dulwich._diff_tree import (  # type: ignore",
            "-        _count_blocks,",
            "-        _is_tree,",
            "-        _merge_entries,",
            "-    )",
            "-except ImportError:",
            "+",
            "+if TYPE_CHECKING:",
            "+    # For type checking, use the Python implementations",
            "     pass",
            "+else:",
            "+    # At runtime, try to import Rust extensions",
            "+    try:",
            "+        # Try to import Rust versions",
            "+        from dulwich._diff_tree import (",
            "+            _count_blocks as _rust_count_blocks,",
            "+        )",
            "+        from dulwich._diff_tree import (",
            "+            _is_tree as _rust_is_tree,",
            "+        )",
            "+        from dulwich._diff_tree import (",
            "+            _merge_entries as _rust_merge_entries,",
            "+        )",
            "+",
            "+        # Override with Rust versions",
            "+        _count_blocks = _rust_count_blocks",
            "+        _is_tree = _rust_is_tree",
            "+        _merge_entries = _rust_merge_entries",
            "+    except ImportError:",
            "+        pass"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/fastexport.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/fastexport.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/fastexport.py",
            "@@ -19,75 +19,98 @@",
            " # License, Version 2.0.",
            " #",
            " ",
            " ",
            " \"\"\"Fast export/import functionality.\"\"\"",
            " ",
            " import stat",
            "+from collections.abc import Generator",
            "+from typing import TYPE_CHECKING, Any, BinaryIO, Optional",
            " ",
            " from fastimport import commands, parser, processor",
            " from fastimport import errors as fastimport_errors",
            " ",
            " from .index import commit_tree",
            " from .object_store import iter_tree_contents",
            "-from .objects import ZERO_SHA, Blob, Commit, Tag",
            "+from .objects import ZERO_SHA, Blob, Commit, ObjectID, Tag",
            "+from .refs import Ref",
            " ",
            "+if TYPE_CHECKING:",
            "+    from .object_store import BaseObjectStore",
            "+    from .repo import BaseRepo",
            " ",
            "-def split_email(text):",
            "+",
            "+def split_email(text: bytes) -> tuple[bytes, bytes]:",
            "+    # TODO(jelmer): Dedupe this and the same functionality in",
            "+    # format_annotate_line.",
            "     (name, email) = text.rsplit(b\" <\", 1)",
            "     return (name, email.rstrip(b\">\"))",
            " ",
            " ",
            " class GitFastExporter:",
            "     \"\"\"Generate a fast-export output stream for Git objects.\"\"\"",
            " ",
            "-    def __init__(self, outf, store) -> None:",
            "+    def __init__(self, outf: BinaryIO, store: \"BaseObjectStore\") -> None:",
            "         self.outf = outf",
            "         self.store = store",
            "         self.markers: dict[bytes, bytes] = {}",
            "         self._marker_idx = 0",
            " ",
            "-    def print_cmd(self, cmd) -> None:",
            "-        self.outf.write(getattr(cmd, \"__bytes__\", cmd.__repr__)() + b\"\\n\")",
            "+    def print_cmd(self, cmd: object) -> None:",
            "+        if hasattr(cmd, \"__bytes__\"):",
            "+            output = cmd.__bytes__()",
            "+        else:",
            "+            output = cmd.__repr__().encode(\"utf-8\")",
            "+        self.outf.write(output + b\"\\n\")",
            " ",
            "-    def _allocate_marker(self):",
            "+    def _allocate_marker(self) -> bytes:",
            "         self._marker_idx += 1",
            "         return str(self._marker_idx).encode(\"ascii\")",
            " ",
            "-    def _export_blob(self, blob):",
            "+    def _export_blob(self, blob: Blob) -> tuple[Any, bytes]:",
            "         marker = self._allocate_marker()",
            "         self.markers[marker] = blob.id",
            "         return (commands.BlobCommand(marker, blob.data), marker)",
            " ",
            "-    def emit_blob(self, blob):",
            "+    def emit_blob(self, blob: Blob) -> bytes:",
            "         (cmd, marker) = self._export_blob(blob)",
            "         self.print_cmd(cmd)",
            "         return marker",
            " ",
            "-    def _iter_files(self, base_tree, new_tree):",
            "+    def _iter_files(",
            "+        self, base_tree: Optional[bytes], new_tree: Optional[bytes]",
            "+    ) -> Generator[Any, None, None]:",
            "         for (",
            "             (old_path, new_path),",
            "             (old_mode, new_mode),",
            "             (old_hexsha, new_hexsha),",
            "         ) in self.store.tree_changes(base_tree, new_tree):",
            "             if new_path is None:",
            "-                yield commands.FileDeleteCommand(old_path)",
            "+                if old_path is not None:",
            "+                    yield commands.FileDeleteCommand(old_path)",
            "                 continue",
            "-            if not stat.S_ISDIR(new_mode):",
            "-                blob = self.store[new_hexsha]",
            "-                marker = self.emit_blob(blob)",
            "+            marker = b\"\"",
            "+            if new_mode is not None and not stat.S_ISDIR(new_mode):",
            "+                if new_hexsha is not None:",
            "+                    blob = self.store[new_hexsha]",
            "+                    from .objects import Blob",
            "+",
            "+                    if isinstance(blob, Blob):",
            "+                        marker = self.emit_blob(blob)",
            "             if old_path != new_path and old_path is not None:",
            "                 yield commands.FileRenameCommand(old_path, new_path)",
            "             if old_mode != new_mode or old_hexsha != new_hexsha:",
            "                 prefixed_marker = b\":\" + marker",
            "                 yield commands.FileModifyCommand(",
            "                     new_path, new_mode, prefixed_marker, None",
            "                 )",
            " ",
            "-    def _export_commit(self, commit, ref, base_tree=None):",
            "+    def _export_commit(",
            "+        self, commit: Commit, ref: Ref, base_tree: Optional[ObjectID] = None",
            "+    ) -> tuple[Any, bytes]:",
            "         file_cmds = list(self._iter_files(base_tree, commit.tree))",
            "         marker = self._allocate_marker()",
            "         if commit.parents:",
            "             from_ = commit.parents[0]",
            "             merges = commit.parents[1:]",
            "         else:",
            "             from_ = None",
            "@@ -107,53 +130,61 @@",
            "             commit.message,",
            "             from_,",
            "             merges,",
            "             file_cmds,",
            "         )",
            "         return (cmd, marker)",
            " ",
            "-    def emit_commit(self, commit, ref, base_tree=None):",
            "+    def emit_commit(",
            "+        self, commit: Commit, ref: Ref, base_tree: Optional[ObjectID] = None",
            "+    ) -> bytes:",
            "         cmd, marker = self._export_commit(commit, ref, base_tree)",
            "         self.print_cmd(cmd)",
            "         return marker",
            " ",
            " ",
            " class GitImportProcessor(processor.ImportProcessor):",
            "     \"\"\"An import processor that imports into a Git repository using Dulwich.\"\"\"",
            " ",
            "     # FIXME: Batch creation of objects?",
            " ",
            "-    def __init__(self, repo, params=None, verbose=False, outf=None) -> None:",
            "+    def __init__(",
            "+        self,",
            "+        repo: \"BaseRepo\",",
            "+        params: Optional[Any] = None,  # noqa: ANN401",
            "+        verbose: bool = False,",
            "+        outf: Optional[BinaryIO] = None,",
            "+    ) -> None:",
            "         processor.ImportProcessor.__init__(self, params, verbose)",
            "         self.repo = repo",
            "         self.last_commit = ZERO_SHA",
            "         self.markers: dict[bytes, bytes] = {}",
            "         self._contents: dict[bytes, tuple[int, bytes]] = {}",
            " ",
            "-    def lookup_object(self, objectish):",
            "+    def lookup_object(self, objectish: bytes) -> ObjectID:",
            "         if objectish.startswith(b\":\"):",
            "             return self.markers[objectish[1:]]",
            "         return objectish",
            " ",
            "-    def import_stream(self, stream):",
            "+    def import_stream(self, stream: BinaryIO) -> dict[bytes, bytes]:",
            "         p = parser.ImportParser(stream)",
            "         self.process(p.iter_commands)",
            "         return self.markers",
            " ",
            "-    def blob_handler(self, cmd) -> None:",
            "+    def blob_handler(self, cmd: commands.BlobCommand) -> None:",
            "         \"\"\"Process a BlobCommand.\"\"\"",
            "         blob = Blob.from_string(cmd.data)",
            "         self.repo.object_store.add_object(blob)",
            "         if cmd.mark:",
            "             self.markers[cmd.mark] = blob.id",
            " ",
            "-    def checkpoint_handler(self, cmd) -> None:",
            "+    def checkpoint_handler(self, cmd: commands.CheckpointCommand) -> None:",
            "         \"\"\"Process a CheckpointCommand.\"\"\"",
            " ",
            "-    def commit_handler(self, cmd) -> None:",
            "+    def commit_handler(self, cmd: commands.CommitCommand) -> None:",
            "         \"\"\"Process a CommitCommand.\"\"\"",
            "         commit = Commit()",
            "         if cmd.author is not None:",
            "             author = cmd.author",
            "         else:",
            "             author = cmd.committer",
            "         (author_name, author_email, author_timestamp, author_timezone) = author",
            "@@ -174,15 +205,15 @@",
            "         if cmd.from_:",
            "             cmd.from_ = self.lookup_object(cmd.from_)",
            "             self._reset_base(cmd.from_)",
            "         for filecmd in cmd.iter_files():",
            "             if filecmd.name == b\"filemodify\":",
            "                 if filecmd.data is not None:",
            "                     blob = Blob.from_string(filecmd.data)",
            "-                    self.repo.object_store.add(blob)",
            "+                    self.repo.object_store.add_object(blob)",
            "                     blob_id = blob.id",
            "                 else:",
            "                     blob_id = self.lookup_object(filecmd.dataref)",
            "                 self._contents[filecmd.path] = (filecmd.mode, blob_id)",
            "             elif filecmd.name == b\"filedelete\":",
            "                 del self._contents[filecmd.path]",
            "             elif filecmd.name == b\"filecopy\":",
            "@@ -204,45 +235,50 @@",
            "             commit.parents.append(self.lookup_object(merge))",
            "         self.repo.object_store.add_object(commit)",
            "         self.repo[cmd.ref] = commit.id",
            "         self.last_commit = commit.id",
            "         if cmd.mark:",
            "             self.markers[cmd.mark] = commit.id",
            " ",
            "-    def progress_handler(self, cmd) -> None:",
            "+    def progress_handler(self, cmd: commands.ProgressCommand) -> None:",
            "         \"\"\"Process a ProgressCommand.\"\"\"",
            " ",
            "-    def _reset_base(self, commit_id) -> None:",
            "+    def _reset_base(self, commit_id: ObjectID) -> None:",
            "         if self.last_commit == commit_id:",
            "             return",
            "         self._contents = {}",
            "         self.last_commit = commit_id",
            "         if commit_id != ZERO_SHA:",
            "-            tree_id = self.repo[commit_id].tree",
            "+            from .objects import Commit",
            "+",
            "+            commit = self.repo[commit_id]",
            "+            tree_id = commit.tree if isinstance(commit, Commit) else None",
            "+            if tree_id is None:",
            "+                return",
            "             for (",
            "                 path,",
            "                 mode,",
            "                 hexsha,",
            "             ) in iter_tree_contents(self.repo.object_store, tree_id):",
            "                 self._contents[path] = (mode, hexsha)",
            " ",
            "-    def reset_handler(self, cmd) -> None:",
            "+    def reset_handler(self, cmd: commands.ResetCommand) -> None:",
            "         \"\"\"Process a ResetCommand.\"\"\"",
            "         if cmd.from_ is None:",
            "             from_ = ZERO_SHA",
            "         else:",
            "             from_ = self.lookup_object(cmd.from_)",
            "         self._reset_base(from_)",
            "         self.repo.refs[cmd.ref] = from_",
            " ",
            "-    def tag_handler(self, cmd) -> None:",
            "+    def tag_handler(self, cmd: commands.TagCommand) -> None:",
            "         \"\"\"Process a TagCommand.\"\"\"",
            "         tag = Tag()",
            "         tag.tagger = cmd.tagger",
            "         tag.message = cmd.message",
            "-        tag.name = cmd.tag",
            "-        self.repo.add_object(tag)",
            "+        tag.name = cmd.from_",
            "+        self.repo.object_store.add_object(tag)",
            "         self.repo.refs[\"refs/tags/\" + tag.name] = tag.id",
            " ",
            "-    def feature_handler(self, cmd):",
            "+    def feature_handler(self, cmd: commands.FeatureCommand) -> None:",
            "         \"\"\"Process a FeatureCommand.\"\"\"",
            "         raise fastimport_errors.UnknownFeature(cmd.feature_name)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/file.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/file.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/file.py",
            "@@ -20,59 +20,50 @@",
            " #",
            " ",
            " \"\"\"Safe access to git files.\"\"\"",
            " ",
            " import os",
            " import sys",
            " import warnings",
            "-from typing import ClassVar",
            "+from typing import ClassVar, Union",
            " ",
            " ",
            " def ensure_dir_exists(dirname) -> None:",
            "     \"\"\"Ensure a directory exists, creating if necessary.\"\"\"",
            "     try:",
            "         os.makedirs(dirname)",
            "     except FileExistsError:",
            "         pass",
            " ",
            " ",
            " def _fancy_rename(oldname, newname) -> None:",
            "     \"\"\"Rename file with temporary backup file to rollback if rename fails.\"\"\"",
            "     if not os.path.exists(newname):",
            "-        try:",
            "-            os.rename(oldname, newname)",
            "-        except OSError:",
            "-            raise",
            "+        os.rename(oldname, newname)",
            "         return",
            " ",
            "     # Defer the tempfile import since it pulls in a lot of other things.",
            "     import tempfile",
            " ",
            "     # destination file exists",
            "-    try:",
            "-        (fd, tmpfile) = tempfile.mkstemp(\".tmp\", prefix=oldname, dir=\".\")",
            "-        os.close(fd)",
            "-        os.remove(tmpfile)",
            "-    except OSError:",
            "-        # either file could not be created (e.g. permission problem)",
            "-        # or could not be deleted (e.g. rude virus scanner)",
            "-        raise",
            "-    try:",
            "-        os.rename(newname, tmpfile)",
            "-    except OSError:",
            "-        raise  # no rename occurred",
            "+    (fd, tmpfile) = tempfile.mkstemp(\".tmp\", prefix=oldname, dir=\".\")",
            "+    os.close(fd)",
            "+    os.remove(tmpfile)",
            "+    os.rename(newname, tmpfile)",
            "     try:",
            "         os.rename(oldname, newname)",
            "     except OSError:",
            "         os.rename(tmpfile, newname)",
            "         raise",
            "     os.remove(tmpfile)",
            " ",
            " ",
            "-def GitFile(filename, mode=\"rb\", bufsize=-1, mask=0o644):",
            "+def GitFile(",
            "+    filename: Union[str, bytes, os.PathLike], mode=\"rb\", bufsize=-1, mask=0o644",
            "+):",
            "     \"\"\"Create a file object that obeys the git file locking protocol.",
            " ",
            "     Returns: a builtin file object or a _GitFile object",
            " ",
            "     Note: See _GitFile for a description of the file locking protocol.",
            " ",
            "     Only read-only and write-only (binary) modes are supported; r+, w+, and a",
            "@@ -136,18 +127,21 @@",
            "         \"seek\",",
            "         \"tell\",",
            "         \"truncate\",",
            "         \"write\",",
            "         \"writelines\",",
            "     }",
            " ",
            "-    def __init__(self, filename, mode, bufsize, mask) -> None:",
            "-        self._filename = filename",
            "+    def __init__(",
            "+        self, filename: Union[str, bytes, os.PathLike], mode, bufsize, mask",
            "+    ) -> None:",
            "+        # Convert PathLike to str/bytes for our internal use",
            "+        self._filename: Union[str, bytes] = os.fspath(filename)",
            "         if isinstance(self._filename, bytes):",
            "-            self._lockfilename = self._filename + b\".lock\"",
            "+            self._lockfilename: Union[str, bytes] = self._filename + b\".lock\"",
            "         else:",
            "             self._lockfilename = self._filename + \".lock\"",
            "         try:",
            "             fd = os.open(",
            "                 self._lockfilename,",
            "                 os.O_RDWR | os.O_CREAT | os.O_EXCL | getattr(os, \"O_BINARY\", 0),",
            "                 mask,"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/graph.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/graph.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/graph.py",
            "@@ -18,17 +18,21 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " ",
            " \"\"\"Implementation of merge-base following the approach of git.\"\"\"",
            " ",
            " from collections.abc import Iterator",
            " from heapq import heappop, heappush",
            "-from typing import Generic, Optional, TypeVar",
            "+from typing import TYPE_CHECKING, Any, Callable, Generic, Optional, TypeVar",
            "+",
            "+if TYPE_CHECKING:",
            "+    from .repo import BaseRepo",
            " ",
            " from .lru_cache import LRUCache",
            "+from .objects import ObjectID",
            " ",
            " T = TypeVar(\"T\")",
            " ",
            " ",
            " # priority queue using builtin python minheap tools",
            " # why they do not have a builtin maxheap is simply ridiculous but",
            " # liveable with integer time stamps using negation",
            "@@ -48,65 +52,104 @@",
            "         return None",
            " ",
            "     def iter(self) -> Iterator[tuple[int, T]]:",
            "         for pr, cmt in self.pq:",
            "             yield (-pr, cmt)",
            " ",
            " ",
            "-def _find_lcas(lookup_parents, c1, c2s, lookup_stamp, min_stamp=0):",
            "+def _find_lcas(",
            "+    lookup_parents: Callable[[ObjectID], list[ObjectID]],",
            "+    c1: ObjectID,",
            "+    c2s: list[ObjectID],",
            "+    lookup_stamp: Callable[[ObjectID], int],",
            "+    min_stamp: int = 0,",
            "+    shallows: Optional[set[ObjectID]] = None,",
            "+) -> list[ObjectID]:",
            "     cands = []",
            "     cstates = {}",
            " ",
            "     # Flags to Record State",
            "     _ANC_OF_1 = 1  # ancestor of commit 1",
            "     _ANC_OF_2 = 2  # ancestor of commit 2",
            "     _DNC = 4  # Do Not Consider",
            "     _LCA = 8  # potential LCA (Lowest Common Ancestor)",
            " ",
            "-    def _has_candidates(wlst, cstates) -> bool:",
            "+    def _has_candidates(wlst: WorkList[ObjectID], cstates: dict[ObjectID, int]) -> bool:",
            "         for dt, cmt in wlst.iter():",
            "             if cmt in cstates:",
            "                 if not ((cstates[cmt] & _DNC) == _DNC):",
            "                     return True",
            "         return False",
            " ",
            "     # initialize the working list states with ancestry info",
            "     # note possibility of c1 being one of c2s should be handled",
            "-    wlst = WorkList()",
            "+    wlst: WorkList[bytes] = WorkList()",
            "     cstates[c1] = _ANC_OF_1",
            "-    wlst.add((lookup_stamp(c1), c1))",
            "+    try:",
            "+        wlst.add((lookup_stamp(c1), c1))",
            "+    except KeyError:",
            "+        # If c1 doesn't exist and we have shallow commits, it might be a missing parent",
            "+        if shallows is None or not shallows:",
            "+            raise",
            "+        # For missing commits in shallow repos, use a minimal timestamp",
            "+        wlst.add((0, c1))",
            "+",
            "     for c2 in c2s:",
            "         cflags = cstates.get(c2, 0)",
            "         cstates[c2] = cflags | _ANC_OF_2",
            "-        wlst.add((lookup_stamp(c2), c2))",
            "+        try:",
            "+            wlst.add((lookup_stamp(c2), c2))",
            "+        except KeyError:",
            "+            # If c2 doesn't exist and we have shallow commits, it might be a missing parent",
            "+            if shallows is None or not shallows:",
            "+                raise",
            "+            # For missing commits in shallow repos, use a minimal timestamp",
            "+            wlst.add((0, c2))",
            " ",
            "     # loop while at least one working list commit is still viable (not marked as _DNC)",
            "     # adding any parents to the list in a breadth first manner",
            "     while _has_candidates(wlst, cstates):",
            "-        dt, cmt = wlst.get()",
            "+        result = wlst.get()",
            "+        if result is None:",
            "+            break",
            "+        dt, cmt = result",
            "         # Look only at ANCESTRY and _DNC flags so that already",
            "         # found _LCAs can still be marked _DNC by lower _LCAS",
            "         cflags = cstates[cmt] & (_ANC_OF_1 | _ANC_OF_2 | _DNC)",
            "         if cflags == (_ANC_OF_1 | _ANC_OF_2):",
            "             # potential common ancestor if not already in candidates add it",
            "             if not (cstates[cmt] & _LCA) == _LCA:",
            "                 cstates[cmt] = cstates[cmt] | _LCA",
            "                 cands.append((dt, cmt))",
            "             # mark any parents of this node _DNC as all parents",
            "             # would be one generation further removed common ancestors",
            "             cflags = cflags | _DNC",
            "-        parents = lookup_parents(cmt)",
            "+        try:",
            "+            parents = lookup_parents(cmt)",
            "+        except KeyError:",
            "+            # If we can't get parents in a shallow repo, skip this node",
            "+            # This is safer than pretending it has no parents",
            "+            if shallows is not None and shallows:",
            "+                continue",
            "+            raise",
            "+",
            "         if parents:",
            "             for pcmt in parents:",
            "                 pflags = cstates.get(pcmt, 0)",
            "                 # if this parent was already visited with no new ancestry/flag information",
            "                 # do not add it to the working list again",
            "                 if (pflags & cflags) == cflags:",
            "                     continue",
            "-                pdt = lookup_stamp(pcmt)",
            "+                try:",
            "+                    pdt = lookup_stamp(pcmt)",
            "+                except KeyError:",
            "+                    # Parent doesn't exist - if we're in a shallow repo, skip it",
            "+                    if shallows is not None and shallows:",
            "+                        continue",
            "+                    raise",
            "                 if pdt < min_stamp:",
            "                     continue",
            "                 cstates[pcmt] = pflags | cflags",
            "                 wlst.add((pdt, pcmt))",
            " ",
            "     # walk final candidates removing any superseded by _DNC by later lower _LCAs",
            "     # remove any duplicates and sort it so that earliest is first",
            "@@ -116,68 +159,70 @@",
            "             results.append((dt, cmt))",
            "     results.sort(key=lambda x: x[0])",
            "     lcas = [cmt for dt, cmt in results]",
            "     return lcas",
            " ",
            " ",
            " # actual git sorts these based on commit times",
            "-def find_merge_base(repo, commit_ids):",
            "+def find_merge_base(repo: \"BaseRepo\", commit_ids: list[ObjectID]) -> list[ObjectID]:",
            "     \"\"\"Find lowest common ancestors of commit_ids[0] and *any* of commits_ids[1:].",
            " ",
            "     Args:",
            "       repo: Repository object",
            "       commit_ids: list of commit ids",
            "     Returns:",
            "       list of lowest common ancestor commit_ids",
            "     \"\"\"",
            "-    cmtcache = LRUCache(max_cache=128)",
            "+    cmtcache: LRUCache[ObjectID, Any] = LRUCache(max_cache=128)",
            "     parents_provider = repo.parents_provider()",
            " ",
            "-    def lookup_stamp(cmtid):",
            "+    def lookup_stamp(cmtid: ObjectID) -> int:",
            "         if cmtid not in cmtcache:",
            "             cmtcache[cmtid] = repo.object_store[cmtid]",
            "         return cmtcache[cmtid].commit_time",
            " ",
            "-    def lookup_parents(cmtid):",
            "+    def lookup_parents(cmtid: ObjectID) -> list[ObjectID]:",
            "         commit = None",
            "         if cmtid in cmtcache:",
            "             commit = cmtcache[cmtid]",
            "         # must use parents provider to handle grafts and shallow",
            "         return parents_provider.get_parents(cmtid, commit=commit)",
            " ",
            "     if not commit_ids:",
            "         return []",
            "     c1 = commit_ids[0]",
            "     if not len(commit_ids) > 1:",
            "         return [c1]",
            "     c2s = commit_ids[1:]",
            "     if c1 in c2s:",
            "         return [c1]",
            "-    lcas = _find_lcas(lookup_parents, c1, c2s, lookup_stamp)",
            "+    lcas = _find_lcas(",
            "+        lookup_parents, c1, c2s, lookup_stamp, shallows=parents_provider.shallows",
            "+    )",
            "     return lcas",
            " ",
            " ",
            "-def find_octopus_base(repo, commit_ids):",
            "+def find_octopus_base(repo: \"BaseRepo\", commit_ids: list[ObjectID]) -> list[ObjectID]:",
            "     \"\"\"Find lowest common ancestors of *all* provided commit_ids.",
            " ",
            "     Args:",
            "       repo: Repository",
            "       commit_ids:  list of commit ids",
            "     Returns:",
            "       list of lowest common ancestor commit_ids",
            "     \"\"\"",
            "-    cmtcache = LRUCache(max_cache=128)",
            "+    cmtcache: LRUCache[ObjectID, Any] = LRUCache(max_cache=128)",
            "     parents_provider = repo.parents_provider()",
            " ",
            "-    def lookup_stamp(cmtid):",
            "+    def lookup_stamp(cmtid: ObjectID) -> int:",
            "         if cmtid not in cmtcache:",
            "             cmtcache[cmtid] = repo.object_store[cmtid]",
            "         return cmtcache[cmtid].commit_time",
            " ",
            "-    def lookup_parents(cmtid):",
            "+    def lookup_parents(cmtid: ObjectID) -> list[ObjectID]:",
            "         commit = None",
            "         if cmtid in cmtcache:",
            "             commit = cmtcache[cmtid]",
            "         # must use parents provider to handle grafts and shallow",
            "         return parents_provider.get_parents(cmtid, commit=commit)",
            " ",
            "     if not commit_ids:",
            "@@ -185,43 +230,67 @@",
            "     if len(commit_ids) <= 2:",
            "         return find_merge_base(repo, commit_ids)",
            "     lcas = [commit_ids[0]]",
            "     others = commit_ids[1:]",
            "     for cmt in others:",
            "         next_lcas = []",
            "         for ca in lcas:",
            "-            res = _find_lcas(lookup_parents, cmt, [ca], lookup_stamp)",
            "+            res = _find_lcas(",
            "+                lookup_parents,",
            "+                cmt,",
            "+                [ca],",
            "+                lookup_stamp,",
            "+                shallows=parents_provider.shallows,",
            "+            )",
            "             next_lcas.extend(res)",
            "         lcas = next_lcas[:]",
            "     return lcas",
            " ",
            " ",
            "-def can_fast_forward(repo, c1, c2):",
            "+def can_fast_forward(repo: \"BaseRepo\", c1: bytes, c2: bytes) -> bool:",
            "     \"\"\"Is it possible to fast-forward from c1 to c2?",
            " ",
            "     Args:",
            "       repo: Repository to retrieve objects from",
            "       c1: Commit id for first commit",
            "       c2: Commit id for second commit",
            "     \"\"\"",
            "-    cmtcache = LRUCache(max_cache=128)",
            "+    cmtcache: LRUCache[ObjectID, Any] = LRUCache(max_cache=128)",
            "     parents_provider = repo.parents_provider()",
            " ",
            "-    def lookup_stamp(cmtid):",
            "+    def lookup_stamp(cmtid: ObjectID) -> int:",
            "         if cmtid not in cmtcache:",
            "             cmtcache[cmtid] = repo.object_store[cmtid]",
            "         return cmtcache[cmtid].commit_time",
            " ",
            "-    def lookup_parents(cmtid):",
            "+    def lookup_parents(cmtid: ObjectID) -> list[ObjectID]:",
            "         commit = None",
            "         if cmtid in cmtcache:",
            "             commit = cmtcache[cmtid]",
            "         # must use parents provider to handle grafts and shallow",
            "         return parents_provider.get_parents(cmtid, commit=commit)",
            " ",
            "     if c1 == c2:",
            "         return True",
            " ",
            "     # Algorithm: Find the common ancestor",
            "-    min_stamp = lookup_stamp(c1)",
            "-    lcas = _find_lcas(lookup_parents, c1, [c2], lookup_stamp, min_stamp=min_stamp)",
            "+    try:",
            "+        min_stamp = lookup_stamp(c1)",
            "+    except KeyError:",
            "+        # If c1 doesn't exist in the object store, we can't determine fast-forward",
            "+        # This can happen in shallow clones where c1 is a missing parent",
            "+        # Check if any shallow commits have c1 as a parent",
            "+        if parents_provider.shallows:",
            "+            # We're in a shallow repository and c1 doesn't exist",
            "+            # We can't determine if fast-forward is possible",
            "+            return False",
            "+        raise",
            "+",
            "+    lcas = _find_lcas(",
            "+        lookup_parents,",
            "+        c1,",
            "+        [c2],",
            "+        lookup_stamp,",
            "+        min_stamp=min_stamp,",
            "+        shallows=parents_provider.shallows,",
            "+    )",
            "     return lcas == [c1]"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/hooks.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/hooks.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/hooks.py",
            "@@ -101,15 +101,15 @@",
            "             )",
            "             if ret != 0:",
            "                 if self.post_exec_callback is not None:",
            "                     self.post_exec_callback(0, *args)",
            "                 raise HookError(f\"Hook {self.name} exited with non-zero status {ret}\")",
            "             if self.post_exec_callback is not None:",
            "                 return self.post_exec_callback(1, *args)",
            "-        except OSError:  # no file. silent failure.",
            "+        except FileNotFoundError:  # no file. silent failure.",
            "             if self.post_exec_callback is not None:",
            "                 self.post_exec_callback(0, *args)",
            " ",
            " ",
            " class PreCommitShellHook(ShellHook):",
            "     \"\"\"pre-commit shell hook.\"\"\""
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/ignore.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/ignore.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/ignore.py",
            "@@ -17,51 +17,132 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Parsing of gitignore files.",
            " ",
            " For details for the matching rules, see https://git-scm.com/docs/gitignore",
            "+",
            "+Important: When checking if directories are ignored, include a trailing slash in the path.",
            "+For example, use \"dir/\" instead of \"dir\" to check if a directory is ignored.",
            " \"\"\"",
            " ",
            " import os.path",
            " import re",
            " from collections.abc import Iterable",
            " from contextlib import suppress",
            " from typing import TYPE_CHECKING, BinaryIO, Optional, Union",
            " ",
            " if TYPE_CHECKING:",
            "     from .repo import Repo",
            " ",
            " from .config import Config, get_xdg_config_home_path",
            " ",
            " ",
            "+def _pattern_to_str(pattern: Union[\"Pattern\", bytes, str]) -> str:",
            "+    \"\"\"Convert a pattern to string, handling both Pattern objects and raw patterns.\"\"\"",
            "+    if hasattr(pattern, \"pattern\"):",
            "+        pattern_bytes = pattern.pattern",
            "+    else:",
            "+        pattern_bytes = pattern",
            "+",
            "+    return pattern_bytes.decode() if isinstance(pattern_bytes, bytes) else pattern_bytes",
            "+",
            "+",
            "+def _check_parent_exclusion(path: str, matching_patterns: list) -> bool:",
            "+    \"\"\"Check if a parent directory exclusion prevents negation patterns from taking effect.",
            "+",
            "+    Args:",
            "+        path: Path to check",
            "+        matching_patterns: List of Pattern objects that matched the path",
            "+",
            "+    Returns:",
            "+        True if parent exclusion applies (negation should be ineffective), False otherwise",
            "+    \"\"\"",
            "+    # Find the final negation pattern that would include this file",
            "+    final_negation_pattern = None",
            "+    for pattern in reversed(matching_patterns):",
            "+        if not pattern.is_exclude:  # is_exclude=False means negation/inclusion",
            "+            final_negation_pattern = pattern",
            "+            break",
            "+",
            "+    if not final_negation_pattern:",
            "+        return False  # No negation to check",
            "+",
            "+    final_pattern_str = _pattern_to_str(final_negation_pattern)",
            "+",
            "+    # Check each exclusion pattern to see if it excludes a parent directory",
            "+    for pattern in matching_patterns:",
            "+        if not pattern.is_exclude:  # Skip negations",
            "+            continue",
            "+",
            "+        pattern_str = _pattern_to_str(pattern)",
            "+",
            "+        if _pattern_excludes_parent(pattern_str, path, final_pattern_str):",
            "+            return True",
            "+",
            "+    return False  # No parent exclusion applies",
            "+",
            "+",
            "+def _pattern_excludes_parent(",
            "+    pattern_str: str, path: str, final_pattern_str: str",
            "+) -> bool:",
            "+    \"\"\"Check if a pattern excludes a parent directory of the given path.\"\"\"",
            "+    # Case 1: Direct directory exclusion (pattern ending with /)",
            "+    if pattern_str.endswith(\"/\"):",
            "+        excluded_dir = pattern_str[:-1]  # Remove trailing /",
            "+        return \"/\" in path and path.startswith(excluded_dir + \"/\")",
            "+",
            "+    # Case 2: Recursive exclusion patterns (**/dir/**)",
            "+    if pattern_str.startswith(\"**/\") and pattern_str.endswith(\"/**\"):",
            "+        dir_name = pattern_str[3:-3]  # Remove **/ and /**",
            "+        return dir_name != \"\" and (\"/\" + dir_name + \"/\") in (\"/\" + path)",
            "+",
            "+    # Case 3: Directory glob patterns (dir/**)",
            "+    if pattern_str.endswith(\"/**\") and not pattern_str.startswith(\"**/\"):",
            "+        dir_prefix = pattern_str[:-3]  # Remove /**",
            "+        if path.startswith(dir_prefix + \"/\"):",
            "+            # Check if this is a nested path (more than one level under dir_prefix)",
            "+            remaining_path = path[len(dir_prefix + \"/\") :]",
            "+            if \"/\" in remaining_path:",
            "+                # This is a nested path - parent directory exclusion applies",
            "+                # BUT only for directory negations, not file negations",
            "+                return final_pattern_str.endswith(\"/\")",
            "+",
            "+    return False",
            "+",
            "+",
            " def _translate_segment(segment: bytes) -> bytes:",
            "+    \"\"\"Translate a single path segment to regex, following Git rules exactly.\"\"\"",
            "     if segment == b\"*\":",
            "         return b\"[^/]+\"",
            "+",
            "     res = b\"\"",
            "     i, n = 0, len(segment)",
            "     while i < n:",
            "         c = segment[i : i + 1]",
            "-        i = i + 1",
            "+        i += 1",
            "         if c == b\"*\":",
            "             res += b\"[^/]*\"",
            "         elif c == b\"?\":",
            "             res += b\"[^/]\"",
            "         elif c == b\"\\\\\":",
            "-            res += re.escape(segment[i : i + 1])",
            "-            i += 1",
            "+            if i < n:",
            "+                res += re.escape(segment[i : i + 1])",
            "+                i += 1",
            "+            else:",
            "+                res += re.escape(c)",
            "         elif c == b\"[\":",
            "             j = i",
            "             if j < n and segment[j : j + 1] == b\"!\":",
            "-                j = j + 1",
            "+                j += 1",
            "             if j < n and segment[j : j + 1] == b\"]\":",
            "-                j = j + 1",
            "+                j += 1",
            "             while j < n and segment[j : j + 1] != b\"]\":",
            "-                j = j + 1",
            "+                j += 1",
            "             if j >= n:",
            "                 res += b\"\\\\[\"",
            "             else:",
            "                 stuff = segment[i:j].replace(b\"\\\\\", b\"\\\\\\\\\")",
            "                 i = j + 1",
            "                 if stuff.startswith(b\"!\"):",
            "                     stuff = b\"^\" + stuff[1:]",
            "@@ -69,43 +150,110 @@",
            "                     stuff = b\"\\\\\" + stuff",
            "                 res += b\"[\" + stuff + b\"]\"",
            "         else:",
            "             res += re.escape(c)",
            "     return res",
            " ",
            " ",
            "-def translate(pat: bytes) -> bytes:",
            "-    \"\"\"Translate a shell PATTERN to a regular expression.",
            "+def _handle_double_asterisk(segments: list[bytes], i: int) -> tuple[bytes, bool]:",
            "+    \"\"\"Handle ** segment processing, returns (regex_part, skip_next).\"\"\"",
            "+    # Check if ** is at end",
            "+    remaining = segments[i + 1 :]",
            "+    if all(s == b\"\" for s in remaining):",
            "+        # ** at end - matches everything",
            "+        return b\".*\", False",
            "+",
            "+    # Check if next segment is also **",
            "+    if i + 1 < len(segments) and segments[i + 1] == b\"**\":",
            "+        # Consecutive ** segments",
            "+        # Check if this ends with a directory pattern (trailing /)",
            "+        remaining_after_next = segments[i + 2 :]",
            "+        is_dir_pattern = (",
            "+            len(remaining_after_next) == 1 and remaining_after_next[0] == b\"\"",
            "+        )",
            "+",
            "+        if is_dir_pattern:",
            "+            # Pattern like c/**/**/ - requires at least one intermediate directory",
            "+            return b\"[^/]+/(?:[^/]+/)*\", True",
            "+        else:",
            "+            # Pattern like c/**/**/d - allows zero intermediate directories",
            "+            return b\"(?:[^/]+/)*\", True",
            "+    else:",
            "+        # ** in middle - handle differently depending on what follows",
            "+        if i == 0:",
            "+            # ** at start - any prefix",
            "+            return b\"(?:.*/)??\", False",
            "+        else:",
            "+            # ** in middle - match zero or more complete directory segments",
            "+            return b\"(?:[^/]+/)*\", False",
            " ",
            "-    There is no way to quote meta-characters.",
            " ",
            "-    Originally copied from fnmatch in Python 2.7, but modified for Dulwich",
            "-    to cope with features in Git ignore patterns.",
            "-    \"\"\"",
            "+def _handle_leading_patterns(pat: bytes, res: bytes) -> tuple[bytes, bytes]:",
            "+    \"\"\"Handle leading patterns like ``/**/``, ``**/``, or ``/``.\"\"\"",
            "+    if pat.startswith(b\"/**/\"):",
            "+        # Leading /** is same as **",
            "+        return pat[4:], b\"(.*/)?\"",
            "+    elif pat.startswith(b\"**/\"):",
            "+        # Leading **/",
            "+        return pat[3:], b\"(.*/)?\"",
            "+    elif pat.startswith(b\"/\"):",
            "+        # Leading / means relative to .gitignore location",
            "+        return pat[1:], b\"\"",
            "+    else:",
            "+        return pat, b\"\"",
            "+",
            "+",
            "+def translate(pat: bytes) -> bytes:",
            "+    \"\"\"Translate a gitignore pattern to a regular expression following Git rules exactly.\"\"\"",
            "     res = b\"(?ms)\"",
            " ",
            "-    if b\"/\" not in pat[:-1]:",
            "-        # If there's no slash, this is a filename-based match",
            "-        res += b\"(.*/)?\"",
            "+    # Check for invalid patterns with // - Git treats these as broken patterns",
            "+    if b\"//\" in pat:",
            "+        # Pattern with // doesn't match anything in Git",
            "+        return b\"(?!.*)\"  # Negative lookahead - matches nothing",
            "+",
            "+    # Don't normalize consecutive ** patterns - Git treats them specially",
            "+    # c/**/**/ requires at least one intermediate directory",
            "+    # So we keep the pattern as-is",
            " ",
            "-    if pat.startswith(b\"**/\"):",
            "-        # Leading **/",
            "-        pat = pat[2:]",
            "+    # Handle patterns with no slashes (match at any level)",
            "+    if b\"/\" not in pat[:-1]:  # No slash except possibly at end",
            "         res += b\"(.*/)?\"",
            " ",
            "-    if pat.startswith(b\"/\"):",
            "-        pat = pat[1:]",
            "+    # Handle leading patterns",
            "+    pat, prefix_added = _handle_leading_patterns(pat, res)",
            "+    if prefix_added:",
            "+        res += prefix_added",
            "+",
            "+    # Process the rest of the pattern",
            "+    if pat == b\"**\":",
            "+        res += b\".*\"",
            "+    else:",
            "+        segments = pat.split(b\"/\")",
            "+        i = 0",
            "+        while i < len(segments):",
            "+            segment = segments[i]",
            "+",
            "+            # Add slash separator (except for first segment)",
            "+            if i > 0 and segments[i - 1] != b\"**\":",
            "+                res += re.escape(b\"/\")",
            "+",
            "+            if segment == b\"**\":",
            "+                regex_part, skip_next = _handle_double_asterisk(segments, i)",
            "+                res += regex_part",
            "+                if regex_part == b\".*\":  # End of pattern",
            "+                    break",
            "+                if skip_next:",
            "+                    i += 1",
            "+            else:",
            "+                res += _translate_segment(segment)",
            " ",
            "-    for i, segment in enumerate(pat.split(b\"/\")):",
            "-        if segment == b\"**\":",
            "-            res += b\"(/.*)?\"",
            "-            continue",
            "-        else:",
            "-            res += (re.escape(b\"/\") if i > 0 else b\"\") + _translate_segment(segment)",
            "+            i += 1",
            " ",
            "+    # Add optional trailing slash for files",
            "     if not pat.endswith(b\"/\"):",
            "         res += b\"/?\"",
            " ",
            "     return res + b\"\\\\Z\"",
            " ",
            " ",
            " def read_ignore_patterns(f: BinaryIO) -> Iterable[bytes]:",
            "@@ -149,21 +297,32 @@",
            " ",
            " class Pattern:",
            "     \"\"\"A single ignore pattern.\"\"\"",
            " ",
            "     def __init__(self, pattern: bytes, ignorecase: bool = False) -> None:",
            "         self.pattern = pattern",
            "         self.ignorecase = ignorecase",
            "-        if pattern[0:1] == b\"!\":",
            "+",
            "+        # Handle negation",
            "+        if pattern.startswith(b\"!\"):",
            "             self.is_exclude = False",
            "             pattern = pattern[1:]",
            "         else:",
            "-            if pattern[0:1] == b\"\\\\\":",
            "+            # Handle escaping of ! and # at start only",
            "+            if (",
            "+                pattern.startswith(b\"\\\\\")",
            "+                and len(pattern) > 1",
            "+                and pattern[1:2] in (b\"!\", b\"#\")",
            "+            ):",
            "                 pattern = pattern[1:]",
            "             self.is_exclude = True",
            "+",
            "+        # Check if this is a directory-only pattern",
            "+        self.is_directory_only = pattern.endswith(b\"/\")",
            "+",
            "         flags = 0",
            "         if self.ignorecase:",
            "             flags = re.IGNORECASE",
            "         self._re = re.compile(translate(pattern), flags)",
            " ",
            "     def __bytes__(self) -> bytes:",
            "         return self.pattern",
            "@@ -184,20 +343,41 @@",
            "     def match(self, path: bytes) -> bool:",
            "         \"\"\"Try to match a path against this ignore pattern.",
            " ",
            "         Args:",
            "           path: Path to match (relative to ignore location)",
            "         Returns: boolean",
            "         \"\"\"",
            "-        return bool(self._re.match(path))",
            "+        if self._re.match(path):",
            "+            return True",
            "+",
            "+        # Special handling for directory patterns that exclude files under them",
            "+        if self.is_directory_only and self.is_exclude:",
            "+            # For exclusion directory patterns, also match files under the directory",
            "+            if not path.endswith(b\"/\"):",
            "+                # This is a file - check if it's under any directory that matches the pattern",
            "+                path_dir = path.rsplit(b\"/\", 1)[0] + b\"/\"",
            "+                if len(path.split(b\"/\")) > 1 and self._re.match(path_dir):",
            "+                    return True",
            "+",
            "+        return False",
            " ",
            " ",
            " class IgnoreFilter:",
            "+    \"\"\"Filter to apply gitignore patterns.",
            "+",
            "+    Important: When checking if directories are ignored, include a trailing slash.",
            "+    For example, use is_ignored(\"dir/\") instead of is_ignored(\"dir\").",
            "+    \"\"\"",
            "+",
            "     def __init__(",
            "-        self, patterns: Iterable[bytes], ignorecase: bool = False, path=None",
            "+        self,",
            "+        patterns: Iterable[bytes],",
            "+        ignorecase: bool = False,",
            "+        path: Optional[str] = None,",
            "     ) -> None:",
            "         self._patterns: list[Pattern] = []",
            "         self._ignorecase = ignorecase",
            "         self._path = path",
            "         for pattern in patterns:",
            "             self.append_pattern(pattern)",
            " ",
            "@@ -215,44 +395,66 @@",
            "         \"\"\"",
            "         if not isinstance(path, bytes):",
            "             path = os.fsencode(path)",
            "         for pattern in self._patterns:",
            "             if pattern.match(path):",
            "                 yield pattern",
            " ",
            "-    def is_ignored(self, path: bytes) -> Optional[bool]:",
            "-        \"\"\"Check whether a path is ignored.",
            "+    def is_ignored(self, path: Union[bytes, str]) -> Optional[bool]:",
            "+        \"\"\"Check whether a path is ignored using Git-compliant logic.",
            " ",
            "         For directories, include a trailing slash.",
            " ",
            "         Returns: status is None if file is not mentioned, True if it is",
            "             included, False if it is explicitly excluded.",
            "         \"\"\"",
            "-        status = None",
            "-        for pattern in self.find_matching(path):",
            "-            status = pattern.is_exclude",
            "-        return status",
            "+        matching_patterns = list(self.find_matching(path))",
            "+        if not matching_patterns:",
            "+            return None",
            "+",
            "+        # Basic rule: last matching pattern wins",
            "+        last_pattern = matching_patterns[-1]",
            "+        result = last_pattern.is_exclude",
            "+",
            "+        # Apply Git's parent directory exclusion rule for negations",
            "+        if not result:  # Only applies to inclusions (negations)",
            "+            result = self._apply_parent_exclusion_rule(",
            "+                path.decode() if isinstance(path, bytes) else path, matching_patterns",
            "+            )",
            "+",
            "+        return result",
            "+",
            "+    def _apply_parent_exclusion_rule(",
            "+        self, path: str, matching_patterns: list[Pattern]",
            "+    ) -> bool:",
            "+        \"\"\"Apply Git's parent directory exclusion rule.",
            "+",
            "+        \"It is not possible to re-include a file if a parent directory of that file is excluded.\"",
            "+        \"\"\"",
            "+        return _check_parent_exclusion(path, matching_patterns)",
            " ",
            "     @classmethod",
            "-    def from_path(cls, path, ignorecase: bool = False) -> \"IgnoreFilter\":",
            "+    def from_path(",
            "+        cls, path: Union[str, os.PathLike], ignorecase: bool = False",
            "+    ) -> \"IgnoreFilter\":",
            "         with open(path, \"rb\") as f:",
            "-            return cls(read_ignore_patterns(f), ignorecase, path=path)",
            "+            return cls(read_ignore_patterns(f), ignorecase, path=str(path))",
            " ",
            "     def __repr__(self) -> str:",
            "         path = getattr(self, \"_path\", None)",
            "         if path is not None:",
            "             return f\"{type(self).__name__}.from_path({path!r})\"",
            "         else:",
            "             return f\"<{type(self).__name__}>\"",
            " ",
            " ",
            " class IgnoreFilterStack:",
            "     \"\"\"Check for ignore status in multiple filters.\"\"\"",
            " ",
            "-    def __init__(self, filters) -> None:",
            "+    def __init__(self, filters: list[IgnoreFilter]) -> None:",
            "         self._filters = filters",
            " ",
            "     def is_ignored(self, path: str) -> Optional[bool]:",
            "         \"\"\"Check whether a path is explicitly included or excluded in ignores.",
            " ",
            "         Args:",
            "           path: Path to check",
            "@@ -283,15 +485,19 @@",
            "     except KeyError:",
            "         pass",
            " ",
            "     return get_xdg_config_home_path(\"git\", \"ignore\")",
            " ",
            " ",
            " class IgnoreFilterManager:",
            "-    \"\"\"Ignore file manager.\"\"\"",
            "+    \"\"\"Ignore file manager with Git-compliant behavior.",
            "+",
            "+    Important: When checking if directories are ignored, include a trailing slash.",
            "+    For example, use is_ignored(\"dir/\") instead of is_ignored(\"dir\").",
            "+    \"\"\"",
            " ",
            "     def __init__(",
            "         self,",
            "         top_path: str,",
            "         global_filters: list[IgnoreFilter],",
            "         ignorecase: bool,",
            "     ) -> None:",
            "@@ -308,16 +514,23 @@",
            "             return self._path_filters[path]",
            "         except KeyError:",
            "             pass",
            " ",
            "         p = os.path.join(self._top_path, path, \".gitignore\")",
            "         try:",
            "             self._path_filters[path] = IgnoreFilter.from_path(p, self._ignorecase)",
            "-        except OSError:",
            "+        except (FileNotFoundError, NotADirectoryError):",
            "             self._path_filters[path] = None",
            "+        except OSError as e:",
            "+            # On Windows, opening a path that contains a symlink can fail with",
            "+            # errno 22 (Invalid argument) when the symlink points outside the repo",
            "+            if e.errno == 22:",
            "+                self._path_filters[path] = None",
            "+            else:",
            "+                raise",
            "         return self._path_filters[path]",
            " ",
            "     def find_matching(self, path: str) -> Iterable[Pattern]:",
            "         \"\"\"Find matching patterns for path.",
            " ",
            "         Args:",
            "           path: Path to check",
            "@@ -345,23 +558,64 @@",
            "                 filters.insert(0, (i, ignore_filter))",
            "         return iter(matches)",
            " ",
            "     def is_ignored(self, path: str) -> Optional[bool]:",
            "         \"\"\"Check whether a path is explicitly included or excluded in ignores.",
            " ",
            "         Args:",
            "-          path: Path to check",
            "+          path: Path to check. For directories, the path should end with '/'.",
            "+",
            "         Returns:",
            "           None if the file is not mentioned, True if it is included,",
            "           False if it is explicitly excluded.",
            "         \"\"\"",
            "         matches = list(self.find_matching(path))",
            "-        if matches:",
            "-            return matches[-1].is_exclude",
            "-        return None",
            "+        if not matches:",
            "+            return None",
            "+",
            "+        # Standard behavior - last matching pattern wins",
            "+        result = matches[-1].is_exclude",
            "+",
            "+        # Apply Git's parent directory exclusion rule for negations",
            "+        if not result:  # Only check if we would include due to negation",
            "+            result = _check_parent_exclusion(path, matches)",
            "+",
            "+        # Apply special case for issue #1203: directory traversal with ** patterns",
            "+        if result and path.endswith(\"/\"):",
            "+            result = self._apply_directory_traversal_rule(path, matches)",
            "+",
            "+        return result",
            "+",
            "+    def _apply_directory_traversal_rule(self, path: str, matches: list) -> bool:",
            "+        \"\"\"Apply directory traversal rule for issue #1203.",
            "+",
            "+        If a directory would be ignored by a ** pattern, but there are negation",
            "+        patterns for its subdirectories, then the directory itself should not",
            "+        be ignored (to allow traversal).",
            "+        \"\"\"",
            "+        # Get the last pattern that determined the result",
            "+        last_excluding_pattern = None",
            "+        for match in matches:",
            "+            if match.is_exclude:",
            "+                last_excluding_pattern = match",
            "+",
            "+        if last_excluding_pattern and (",
            "+            last_excluding_pattern.pattern.endswith(b\"**\")",
            "+            or b\"**\" in last_excluding_pattern.pattern",
            "+        ):",
            "+            # Check if subdirectories would be unignored",
            "+            test_subdir = path + \"test/\"",
            "+            test_matches = list(self.find_matching(test_subdir))",
            "+            if test_matches:",
            "+                # Use standard logic for test case - last matching pattern wins",
            "+                test_result = test_matches[-1].is_exclude",
            "+                if test_result is False:",
            "+                    return False",
            "+",
            "+        return True  # Keep original result",
            " ",
            "     @classmethod",
            "     def from_repo(cls, repo: \"Repo\") -> \"IgnoreFilterManager\":",
            "         \"\"\"Create a IgnoreFilterManager from a repository.",
            " ",
            "         Args:",
            "           repo: Repository object"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/index.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/index.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/index.py",
            "@@ -17,29 +17,39 @@",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Parser for the git index file format.\"\"\"",
            " ",
            "+import errno",
            " import os",
            "+import shutil",
            " import stat",
            " import struct",
            " import sys",
            "-from collections.abc import Iterable, Iterator",
            "+import types",
            "+from collections.abc import Generator, Iterable, Iterator",
            " from dataclasses import dataclass",
            " from enum import Enum",
            " from typing import (",
            "+    TYPE_CHECKING,",
            "     Any,",
            "     BinaryIO,",
            "     Callable,",
            "     Optional,",
            "     Union,",
            "+    cast,",
            " )",
            " ",
            "+if TYPE_CHECKING:",
            "+    from .file import _GitFile",
            "+    from .line_ending import BlobNormalizer",
            "+    from .repo import Repo",
            "+",
            " from .file import GitFile",
            " from .object_store import iter_tree_contents",
            " from .objects import (",
            "     S_IFGITLINK,",
            "     S_ISGITLINK,",
            "     Blob,",
            "     ObjectID,",
            "@@ -64,14 +74,190 @@",
            " EXTENDED_FLAG_SKIP_WORKTREE = 0x4000",
            " ",
            " # used by \"git add -N\"",
            " EXTENDED_FLAG_INTEND_TO_ADD = 0x2000",
            " ",
            " DEFAULT_VERSION = 2",
            " ",
            "+# Index extension signatures",
            "+TREE_EXTENSION = b\"TREE\"",
            "+REUC_EXTENSION = b\"REUC\"",
            "+UNTR_EXTENSION = b\"UNTR\"",
            "+EOIE_EXTENSION = b\"EOIE\"",
            "+IEOT_EXTENSION = b\"IEOT\"",
            "+",
            "+",
            "+def _encode_varint(value: int) -> bytes:",
            "+    \"\"\"Encode an integer using variable-width encoding.",
            "+",
            "+    Same format as used for OFS_DELTA pack entries and index v4 path compression.",
            "+    Uses 7 bits per byte, with the high bit indicating continuation.",
            "+",
            "+    Args:",
            "+      value: Integer to encode",
            "+    Returns:",
            "+      Encoded bytes",
            "+    \"\"\"",
            "+    if value == 0:",
            "+        return b\"\\x00\"",
            "+",
            "+    result = []",
            "+    while value > 0:",
            "+        byte = value & 0x7F  # Take lower 7 bits",
            "+        value >>= 7",
            "+        if value > 0:",
            "+            byte |= 0x80  # Set continuation bit",
            "+        result.append(byte)",
            "+",
            "+    return bytes(result)",
            "+",
            "+",
            "+def _decode_varint(data: bytes, offset: int = 0) -> tuple[int, int]:",
            "+    \"\"\"Decode a variable-width encoded integer.",
            "+",
            "+    Args:",
            "+      data: Bytes to decode from",
            "+      offset: Starting offset in data",
            "+    Returns:",
            "+      tuple of (decoded_value, new_offset)",
            "+    \"\"\"",
            "+    value = 0",
            "+    shift = 0",
            "+    pos = offset",
            "+",
            "+    while pos < len(data):",
            "+        byte = data[pos]",
            "+        pos += 1",
            "+        value |= (byte & 0x7F) << shift",
            "+        shift += 7",
            "+        if not (byte & 0x80):  # No continuation bit",
            "+            break",
            "+",
            "+    return value, pos",
            "+",
            "+",
            "+def _compress_path(path: bytes, previous_path: bytes) -> bytes:",
            "+    \"\"\"Compress a path relative to the previous path for index version 4.",
            "+",
            "+    Args:",
            "+      path: Path to compress",
            "+      previous_path: Previous path for comparison",
            "+    Returns:",
            "+      Compressed path data (varint prefix_len + suffix)",
            "+    \"\"\"",
            "+    # Find the common prefix length",
            "+    common_len = 0",
            "+    min_len = min(len(path), len(previous_path))",
            "+",
            "+    for i in range(min_len):",
            "+        if path[i] == previous_path[i]:",
            "+            common_len += 1",
            "+        else:",
            "+            break",
            "+",
            "+    # The number of bytes to remove from the end of previous_path",
            "+    # to get the common prefix",
            "+    remove_len = len(previous_path) - common_len",
            "+",
            "+    # The suffix to append",
            "+    suffix = path[common_len:]",
            "+",
            "+    # Encode: varint(remove_len) + suffix + NUL",
            "+    return _encode_varint(remove_len) + suffix + b\"\\x00\"",
            "+",
            "+",
            "+def _decompress_path(",
            "+    data: bytes, offset: int, previous_path: bytes",
            "+) -> tuple[bytes, int]:",
            "+    \"\"\"Decompress a path from index version 4 compressed format.",
            "+",
            "+    Args:",
            "+      data: Raw data containing compressed path",
            "+      offset: Starting offset in data",
            "+      previous_path: Previous path for decompression",
            "+    Returns:",
            "+      tuple of (decompressed_path, new_offset)",
            "+    \"\"\"",
            "+    # Decode the number of bytes to remove from previous path",
            "+    remove_len, new_offset = _decode_varint(data, offset)",
            "+",
            "+    # Find the NUL terminator for the suffix",
            "+    suffix_start = new_offset",
            "+    suffix_end = suffix_start",
            "+    while suffix_end < len(data) and data[suffix_end] != 0:",
            "+        suffix_end += 1",
            "+",
            "+    if suffix_end >= len(data):",
            "+        raise ValueError(\"Unterminated path suffix in compressed entry\")",
            "+",
            "+    suffix = data[suffix_start:suffix_end]",
            "+    new_offset = suffix_end + 1  # Skip the NUL terminator",
            "+",
            "+    # Reconstruct the path",
            "+    if remove_len > len(previous_path):",
            "+        raise ValueError(",
            "+            f\"Invalid path compression: trying to remove {remove_len} bytes from {len(previous_path)}-byte path\"",
            "+        )",
            "+",
            "+    prefix = previous_path[:-remove_len] if remove_len > 0 else previous_path",
            "+    path = prefix + suffix",
            "+",
            "+    return path, new_offset",
            "+",
            "+",
            "+def _decompress_path_from_stream(",
            "+    f: BinaryIO, previous_path: bytes",
            "+) -> tuple[bytes, int]:",
            "+    \"\"\"Decompress a path from index version 4 compressed format, reading from stream.",
            "+",
            "+    Args:",
            "+      f: File-like object to read from",
            "+      previous_path: Previous path for decompression",
            "+    Returns:",
            "+      tuple of (decompressed_path, bytes_consumed)",
            "+    \"\"\"",
            "+    # Decode the varint for remove_len by reading byte by byte",
            "+    remove_len = 0",
            "+    shift = 0",
            "+    bytes_consumed = 0",
            "+",
            "+    while True:",
            "+        byte_data = f.read(1)",
            "+        if not byte_data:",
            "+            raise ValueError(\"Unexpected end of file while reading varint\")",
            "+        byte = byte_data[0]",
            "+        bytes_consumed += 1",
            "+        remove_len |= (byte & 0x7F) << shift",
            "+        shift += 7",
            "+        if not (byte & 0x80):  # No continuation bit",
            "+            break",
            "+",
            "+    # Read the suffix until NUL terminator",
            "+    suffix = b\"\"",
            "+    while True:",
            "+        byte_data = f.read(1)",
            "+        if not byte_data:",
            "+            raise ValueError(\"Unexpected end of file while reading path suffix\")",
            "+        byte = byte_data[0]",
            "+        bytes_consumed += 1",
            "+        if byte == 0:  # NUL terminator",
            "+            break",
            "+        suffix += bytes([byte])",
            "+",
            "+    # Reconstruct the path",
            "+    if remove_len > len(previous_path):",
            "+        raise ValueError(",
            "+            f\"Invalid path compression: trying to remove {remove_len} bytes from {len(previous_path)}-byte path\"",
            "+        )",
            "+",
            "+    prefix = previous_path[:-remove_len] if remove_len > 0 else previous_path",
            "+    path = prefix + suffix",
            "+",
            "+    return path, bytes_consumed",
            "+",
            " ",
            " class Stage(Enum):",
            "     NORMAL = 0",
            "     MERGE_CONFLICT_ANCESTOR = 1",
            "     MERGE_CONFLICT_THIS = 2",
            "     MERGE_CONFLICT_OTHER = 3",
            " ",
            "@@ -92,55 +278,163 @@",
            "     extended_flags: int",
            " ",
            "     def stage(self) -> Stage:",
            "         return Stage((self.flags & FLAG_STAGEMASK) >> FLAG_STAGESHIFT)",
            " ",
            " ",
            " @dataclass",
            "+class IndexExtension:",
            "+    \"\"\"Base class for index extensions.\"\"\"",
            "+",
            "+    signature: bytes",
            "+    data: bytes",
            "+",
            "+    @classmethod",
            "+    def from_raw(cls, signature: bytes, data: bytes) -> \"IndexExtension\":",
            "+        \"\"\"Create an extension from raw data.",
            "+",
            "+        Args:",
            "+          signature: 4-byte extension signature",
            "+          data: Extension data",
            "+        Returns:",
            "+          Parsed extension object",
            "+        \"\"\"",
            "+        if signature == TREE_EXTENSION:",
            "+            return TreeExtension.from_bytes(data)",
            "+        elif signature == REUC_EXTENSION:",
            "+            return ResolveUndoExtension.from_bytes(data)",
            "+        elif signature == UNTR_EXTENSION:",
            "+            return UntrackedExtension.from_bytes(data)",
            "+        else:",
            "+            # Unknown extension - just store raw data",
            "+            return cls(signature, data)",
            "+",
            "+    def to_bytes(self) -> bytes:",
            "+        \"\"\"Serialize extension to bytes.\"\"\"",
            "+        return self.data",
            "+",
            "+",
            "+class TreeExtension(IndexExtension):",
            "+    \"\"\"Tree cache extension.\"\"\"",
            "+",
            "+    def __init__(self, entries: list[tuple[bytes, bytes, int]]) -> None:",
            "+        self.entries = entries",
            "+        super().__init__(TREE_EXTENSION, b\"\")",
            "+",
            "+    @classmethod",
            "+    def from_bytes(cls, data: bytes) -> \"TreeExtension\":",
            "+        # TODO: Implement tree cache parsing",
            "+        return cls([])",
            "+",
            "+    def to_bytes(self) -> bytes:",
            "+        # TODO: Implement tree cache serialization",
            "+        return b\"\"",
            "+",
            "+",
            "+class ResolveUndoExtension(IndexExtension):",
            "+    \"\"\"Resolve undo extension for recording merge conflicts.\"\"\"",
            "+",
            "+    def __init__(self, entries: list[tuple[bytes, list[tuple[int, bytes]]]]) -> None:",
            "+        self.entries = entries",
            "+        super().__init__(REUC_EXTENSION, b\"\")",
            "+",
            "+    @classmethod",
            "+    def from_bytes(cls, data: bytes) -> \"ResolveUndoExtension\":",
            "+        # TODO: Implement resolve undo parsing",
            "+        return cls([])",
            "+",
            "+    def to_bytes(self) -> bytes:",
            "+        # TODO: Implement resolve undo serialization",
            "+        return b\"\"",
            "+",
            "+",
            "+class UntrackedExtension(IndexExtension):",
            "+    \"\"\"Untracked cache extension.\"\"\"",
            "+",
            "+    def __init__(self, data: bytes) -> None:",
            "+        super().__init__(UNTR_EXTENSION, data)",
            "+",
            "+    @classmethod",
            "+    def from_bytes(cls, data: bytes) -> \"UntrackedExtension\":",
            "+        return cls(data)",
            "+",
            "+",
            "+@dataclass",
            " class IndexEntry:",
            "     ctime: Union[int, float, tuple[int, int]]",
            "     mtime: Union[int, float, tuple[int, int]]",
            "     dev: int",
            "     ino: int",
            "     mode: int",
            "     uid: int",
            "     gid: int",
            "     size: int",
            "     sha: bytes",
            "+    flags: int = 0",
            "+    extended_flags: int = 0",
            " ",
            "     @classmethod",
            "     def from_serialized(cls, serialized: SerializedIndexEntry) -> \"IndexEntry\":",
            "         return cls(",
            "             ctime=serialized.ctime,",
            "             mtime=serialized.mtime,",
            "             dev=serialized.dev,",
            "             ino=serialized.ino,",
            "             mode=serialized.mode,",
            "             uid=serialized.uid,",
            "             gid=serialized.gid,",
            "             size=serialized.size,",
            "             sha=serialized.sha,",
            "+            flags=serialized.flags,",
            "+            extended_flags=serialized.extended_flags,",
            "         )",
            " ",
            "     def serialize(self, name: bytes, stage: Stage) -> SerializedIndexEntry:",
            "+        # Clear out any existing stage bits, then set them from the Stage.",
            "+        new_flags = self.flags & ~FLAG_STAGEMASK",
            "+        new_flags |= stage.value << FLAG_STAGESHIFT",
            "         return SerializedIndexEntry(",
            "             name=name,",
            "             ctime=self.ctime,",
            "             mtime=self.mtime,",
            "             dev=self.dev,",
            "             ino=self.ino,",
            "             mode=self.mode,",
            "             uid=self.uid,",
            "             gid=self.gid,",
            "             size=self.size,",
            "             sha=self.sha,",
            "-            flags=stage.value << FLAG_STAGESHIFT,",
            "-            extended_flags=0,",
            "+            flags=new_flags,",
            "+            extended_flags=self.extended_flags,",
            "         )",
            " ",
            "+    def stage(self) -> Stage:",
            "+        return Stage((self.flags & FLAG_STAGEMASK) >> FLAG_STAGESHIFT)",
            "+",
            "+    @property",
            "+    def skip_worktree(self) -> bool:",
            "+        \"\"\"Return True if the skip-worktree bit is set in extended_flags.\"\"\"",
            "+        return bool(self.extended_flags & EXTENDED_FLAG_SKIP_WORKTREE)",
            "+",
            "+    def set_skip_worktree(self, skip: bool = True) -> None:",
            "+        \"\"\"Helper method to set or clear the skip-worktree bit in extended_flags.",
            "+        Also sets FLAG_EXTENDED in self.flags if needed.",
            "+        \"\"\"",
            "+        if skip:",
            "+            # Turn on the skip-worktree bit",
            "+            self.extended_flags |= EXTENDED_FLAG_SKIP_WORKTREE",
            "+            # Also ensure the main 'extended' bit is set in flags",
            "+            self.flags |= FLAG_EXTENDED",
            "+        else:",
            "+            # Turn off the skip-worktree bit",
            "+            self.extended_flags &= ~EXTENDED_FLAG_SKIP_WORKTREE",
            "+            # Optionally unset the main extended bit if no extended flags remain",
            "+            if self.extended_flags == 0:",
            "+                self.flags &= ~FLAG_EXTENDED",
            "+",
            " ",
            " class ConflictedIndexEntry:",
            "     \"\"\"Index entry that represents a conflict.\"\"\"",
            " ",
            "     ancestor: Optional[IndexEntry]",
            "     this: Optional[IndexEntry]",
            "     other: Optional[IndexEntry]",
            "@@ -173,31 +467,31 @@",
            "         (dirname, basename) = path.rsplit(b\"/\", 1)",
            "     except ValueError:",
            "         return (b\"\", path)",
            "     else:",
            "         return (dirname, basename)",
            " ",
            " ",
            "-def pathjoin(*args):",
            "+def pathjoin(*args: bytes) -> bytes:",
            "     \"\"\"Join a /-delimited path.\"\"\"",
            "     return b\"/\".join([p for p in args if p])",
            " ",
            " ",
            "-def read_cache_time(f):",
            "+def read_cache_time(f: BinaryIO) -> tuple[int, int]:",
            "     \"\"\"Read a cache time.",
            " ",
            "     Args:",
            "       f: File-like object to read from",
            "     Returns:",
            "       Tuple with seconds and nanoseconds",
            "     \"\"\"",
            "     return struct.unpack(\">LL\", f.read(8))",
            " ",
            " ",
            "-def write_cache_time(f, t) -> None:",
            "+def write_cache_time(f: BinaryIO, t: Union[int, float, tuple[int, int]]) -> None:",
            "     \"\"\"Write a cache time.",
            " ",
            "     Args:",
            "       f: File-like object to write to",
            "       t: Time to write (as int, float or tuple with secs and nsecs)",
            "     \"\"\"",
            "     if isinstance(t, int):",
            "@@ -206,19 +500,23 @@",
            "         (secs, nsecs) = divmod(t, 1.0)",
            "         t = (int(secs), int(nsecs * 1000000000))",
            "     elif not isinstance(t, tuple):",
            "         raise TypeError(t)",
            "     f.write(struct.pack(\">LL\", *t))",
            " ",
            " ",
            "-def read_cache_entry(f, version: int) -> SerializedIndexEntry:",
            "+def read_cache_entry(",
            "+    f: BinaryIO, version: int, previous_path: bytes = b\"\"",
            "+) -> SerializedIndexEntry:",
            "     \"\"\"Read an entry from a cache file.",
            " ",
            "     Args:",
            "       f: File-like object to read from",
            "+      version: Index version",
            "+      previous_path: Previous entry's path (for version 4 compression)",
            "     \"\"\"",
            "     beginoffset = f.tell()",
            "     ctime = read_cache_time(f)",
            "     mtime = read_cache_time(f)",
            "     (",
            "         dev,",
            "         ino,",
            "@@ -231,19 +529,27 @@",
            "     ) = struct.unpack(\">LLLLLL20sH\", f.read(20 + 4 * 6 + 2))",
            "     if flags & FLAG_EXTENDED:",
            "         if version < 3:",
            "             raise AssertionError(\"extended flag set in index with version < 3\")",
            "         (extended_flags,) = struct.unpack(\">H\", f.read(2))",
            "     else:",
            "         extended_flags = 0",
            "-    name = f.read(flags & FLAG_NAMEMASK)",
            "+",
            "+    if version >= 4:",
            "+        # Version 4: paths are always compressed (name_len should be 0)",
            "+        name, consumed = _decompress_path_from_stream(f, previous_path)",
            "+    else:",
            "+        # Versions < 4: regular name reading",
            "+        name = f.read(flags & FLAG_NAMEMASK)",
            "+",
            "     # Padding:",
            "     if version < 4:",
            "         real_size = (f.tell() - beginoffset + 8) & ~7",
            "         f.read((beginoffset + real_size) - f.tell())",
            "+",
            "     return SerializedIndexEntry(",
            "         name,",
            "         ctime,",
            "         mtime,",
            "         dev,",
            "         ino,",
            "         mode,",
            "@@ -252,29 +558,43 @@",
            "         size,",
            "         sha_to_hex(sha),",
            "         flags & ~FLAG_NAMEMASK,",
            "         extended_flags,",
            "     )",
            " ",
            " ",
            "-def write_cache_entry(f, entry: SerializedIndexEntry, version: int) -> None:",
            "+def write_cache_entry(",
            "+    f: BinaryIO, entry: SerializedIndexEntry, version: int, previous_path: bytes = b\"\"",
            "+) -> None:",
            "     \"\"\"Write an index entry to a file.",
            " ",
            "     Args:",
            "       f: File object",
            "-      entry: IndexEntry to write, tuple with:",
            "+      entry: IndexEntry to write",
            "+      version: Index format version",
            "+      previous_path: Previous entry's path (for version 4 compression)",
            "     \"\"\"",
            "     beginoffset = f.tell()",
            "     write_cache_time(f, entry.ctime)",
            "     write_cache_time(f, entry.mtime)",
            "-    flags = len(entry.name) | (entry.flags & ~FLAG_NAMEMASK)",
            "+",
            "+    if version >= 4:",
            "+        # Version 4: use compression but set name_len to actual filename length",
            "+        # This matches how C Git implements index v4 flags",
            "+        compressed_path = _compress_path(entry.name, previous_path)",
            "+        flags = len(entry.name) | (entry.flags & ~FLAG_NAMEMASK)",
            "+    else:",
            "+        # Versions < 4: include actual name length",
            "+        flags = len(entry.name) | (entry.flags & ~FLAG_NAMEMASK)",
            "+",
            "     if entry.extended_flags:",
            "         flags |= FLAG_EXTENDED",
            "     if flags & FLAG_EXTENDED and version is not None and version < 3:",
            "         raise AssertionError(\"unable to use extended flags in version < 3\")",
            "+",
            "     f.write(",
            "         struct.pack(",
            "             b\">LLLLLL20sH\",",
            "             entry.dev & 0xFFFFFFFF,",
            "             entry.ino & 0xFFFFFFFF,",
            "             entry.mode,",
            "             entry.uid,",
            "@@ -282,40 +602,144 @@",
            "             entry.size,",
            "             hex_to_sha(entry.sha),",
            "             flags,",
            "         )",
            "     )",
            "     if flags & FLAG_EXTENDED:",
            "         f.write(struct.pack(b\">H\", entry.extended_flags))",
            "-    f.write(entry.name)",
            "-    if version < 4:",
            "+",
            "+    if version >= 4:",
            "+        # Version 4: always write compressed path",
            "+        f.write(compressed_path)",
            "+    else:",
            "+        # Versions < 4: write regular path and padding",
            "+        f.write(entry.name)",
            "         real_size = (f.tell() - beginoffset + 8) & ~7",
            "         f.write(b\"\\0\" * ((beginoffset + real_size) - f.tell()))",
            " ",
            " ",
            " class UnsupportedIndexFormat(Exception):",
            "     \"\"\"An unsupported index format was encountered.\"\"\"",
            " ",
            "-    def __init__(self, version) -> None:",
            "+    def __init__(self, version: int) -> None:",
            "         self.index_format_version = version",
            " ",
            " ",
            "-def read_index(f: BinaryIO) -> Iterator[SerializedIndexEntry]:",
            "-    \"\"\"Read an index file, yielding the individual entries.\"\"\"",
            "+def read_index_header(f: BinaryIO) -> tuple[int, int]:",
            "+    \"\"\"Read an index header from a file.",
            "+",
            "+    Returns:",
            "+      tuple of (version, num_entries)",
            "+    \"\"\"",
            "     header = f.read(4)",
            "     if header != b\"DIRC\":",
            "         raise AssertionError(f\"Invalid index file header: {header!r}\")",
            "     (version, num_entries) = struct.unpack(b\">LL\", f.read(4 * 2))",
            "-    if version not in (1, 2, 3):",
            "+    if version not in (1, 2, 3, 4):",
            "         raise UnsupportedIndexFormat(version)",
            "+    return version, num_entries",
            "+",
            "+",
            "+def write_index_extension(f: BinaryIO, extension: IndexExtension) -> None:",
            "+    \"\"\"Write an index extension.",
            "+",
            "+    Args:",
            "+      f: File-like object to write to",
            "+      extension: Extension to write",
            "+    \"\"\"",
            "+    data = extension.to_bytes()",
            "+    f.write(extension.signature)",
            "+    f.write(struct.pack(\">I\", len(data)))",
            "+    f.write(data)",
            "+",
            "+",
            "+def read_index(f: BinaryIO) -> Iterator[SerializedIndexEntry]:",
            "+    \"\"\"Read an index file, yielding the individual entries.\"\"\"",
            "+    version, num_entries = read_index_header(f)",
            "+    previous_path = b\"\"",
            "     for i in range(num_entries):",
            "-        yield read_cache_entry(f, version)",
            "+        entry = read_cache_entry(f, version, previous_path)",
            "+        previous_path = entry.name",
            "+        yield entry",
            "+",
            "+",
            "+def read_index_dict_with_version(",
            "+    f: BinaryIO,",
            "+) -> tuple[",
            "+    dict[bytes, Union[IndexEntry, ConflictedIndexEntry]], int, list[IndexExtension]",
            "+]:",
            "+    \"\"\"Read an index file and return it as a dictionary along with the version.",
            " ",
            "+    Returns:",
            "+      tuple of (entries_dict, version, extensions)",
            "+    \"\"\"",
            "+    version, num_entries = read_index_header(f)",
            " ",
            "-def read_index_dict(f) -> dict[bytes, Union[IndexEntry, ConflictedIndexEntry]]:",
            "+    ret: dict[bytes, Union[IndexEntry, ConflictedIndexEntry]] = {}",
            "+    previous_path = b\"\"",
            "+    for i in range(num_entries):",
            "+        entry = read_cache_entry(f, version, previous_path)",
            "+        previous_path = entry.name",
            "+        stage = entry.stage()",
            "+        if stage == Stage.NORMAL:",
            "+            ret[entry.name] = IndexEntry.from_serialized(entry)",
            "+        else:",
            "+            existing = ret.setdefault(entry.name, ConflictedIndexEntry())",
            "+            if isinstance(existing, IndexEntry):",
            "+                raise AssertionError(f\"Non-conflicted entry for {entry.name!r} exists\")",
            "+            if stage == Stage.MERGE_CONFLICT_ANCESTOR:",
            "+                existing.ancestor = IndexEntry.from_serialized(entry)",
            "+            elif stage == Stage.MERGE_CONFLICT_THIS:",
            "+                existing.this = IndexEntry.from_serialized(entry)",
            "+            elif stage == Stage.MERGE_CONFLICT_OTHER:",
            "+                existing.other = IndexEntry.from_serialized(entry)",
            "+",
            "+    # Read extensions",
            "+    extensions = []",
            "+    while True:",
            "+        # Check if we're at the end (20 bytes before EOF for SHA checksum)",
            "+        current_pos = f.tell()",
            "+        f.seek(0, 2)  # EOF",
            "+        eof_pos = f.tell()",
            "+        f.seek(current_pos)",
            "+",
            "+        if current_pos >= eof_pos - 20:",
            "+            break",
            "+",
            "+        # Try to read extension signature",
            "+        signature = f.read(4)",
            "+        if len(signature) < 4:",
            "+            break",
            "+",
            "+        # Check if it's a valid extension signature (4 uppercase letters)",
            "+        if not all(65 <= b <= 90 for b in signature):",
            "+            # Not an extension, seek back",
            "+            f.seek(-4, 1)",
            "+            break",
            "+",
            "+        # Read extension size",
            "+        size_data = f.read(4)",
            "+        if len(size_data) < 4:",
            "+            break",
            "+        size = struct.unpack(\">I\", size_data)[0]",
            "+",
            "+        # Read extension data",
            "+        data = f.read(size)",
            "+        if len(data) < size:",
            "+            break",
            "+",
            "+        extension = IndexExtension.from_raw(signature, data)",
            "+        extensions.append(extension)",
            "+",
            "+    return ret, version, extensions",
            "+",
            "+",
            "+def read_index_dict(",
            "+    f: BinaryIO,",
            "+) -> dict[bytes, Union[IndexEntry, ConflictedIndexEntry]]:",
            "     \"\"\"Read an index file and return it as a dictionary.",
            "        Dict Key is tuple of path and stage number, as",
            "             path alone is not unique",
            "     Args:",
            "       f: File object to read fromls.",
            "     \"\"\"",
            "     ret: dict[bytes, Union[IndexEntry, ConflictedIndexEntry]] = {}",
            "@@ -333,35 +757,59 @@",
            "                 existing.this = IndexEntry.from_serialized(entry)",
            "             elif stage == Stage.MERGE_CONFLICT_OTHER:",
            "                 existing.other = IndexEntry.from_serialized(entry)",
            "     return ret",
            " ",
            " ",
            " def write_index(",
            "-    f: BinaryIO, entries: list[SerializedIndexEntry], version: Optional[int] = None",
            "+    f: BinaryIO,",
            "+    entries: list[SerializedIndexEntry],",
            "+    version: Optional[int] = None,",
            "+    extensions: Optional[list[IndexExtension]] = None,",
            " ) -> None:",
            "     \"\"\"Write an index file.",
            " ",
            "     Args:",
            "       f: File-like object to write to",
            "       version: Version number to write",
            "       entries: Iterable over the entries to write",
            "+      extensions: Optional list of extensions to write",
            "     \"\"\"",
            "     if version is None:",
            "         version = DEFAULT_VERSION",
            "+    # STEP 1: check if any extended_flags are set",
            "+    uses_extended_flags = any(e.extended_flags != 0 for e in entries)",
            "+    if uses_extended_flags and version < 3:",
            "+        # Force or bump the version to 3",
            "+        version = 3",
            "+    # The rest is unchanged, but you might insert a final check:",
            "+    if version < 3:",
            "+        # Double-check no extended flags appear",
            "+        for e in entries:",
            "+            if e.extended_flags != 0:",
            "+                raise AssertionError(\"Attempt to use extended flags in index < v3\")",
            "+    # Proceed with the existing code to write the header and entries.",
            "     f.write(b\"DIRC\")",
            "     f.write(struct.pack(b\">LL\", version, len(entries)))",
            "+    previous_path = b\"\"",
            "     for entry in entries:",
            "-        write_cache_entry(f, entry, version)",
            "+        write_cache_entry(f, entry, version=version, previous_path=previous_path)",
            "+        previous_path = entry.name",
            "+",
            "+    # Write extensions",
            "+    if extensions:",
            "+        for extension in extensions:",
            "+            write_index_extension(f, extension)",
            " ",
            " ",
            " def write_index_dict(",
            "     f: BinaryIO,",
            "     entries: dict[bytes, Union[IndexEntry, ConflictedIndexEntry]],",
            "     version: Optional[int] = None,",
            "+    extensions: Optional[list[IndexExtension]] = None,",
            " ) -> None:",
            "     \"\"\"Write an index file based on the contents of a dictionary.",
            "     being careful to sort by path and then by stage.",
            "     \"\"\"",
            "     entries_list = []",
            "     for key in sorted(entries):",
            "         value = entries[key]",
            "@@ -376,15 +824,16 @@",
            "                 )",
            "             if value.other is not None:",
            "                 entries_list.append(",
            "                     value.other.serialize(key, Stage.MERGE_CONFLICT_OTHER)",
            "                 )",
            "         else:",
            "             entries_list.append(value.serialize(key, Stage.NORMAL))",
            "-    write_index(f, entries_list, version=version)",
            "+",
            "+    write_index(f, entries_list, version=version, extensions=extensions)",
            " ",
            " ",
            " def cleanup_mode(mode: int) -> int:",
            "     \"\"\"Cleanup a mode value.",
            " ",
            "     This will return a mode that can be stored in a tree object.",
            " ",
            "@@ -407,55 +856,98 @@",
            " ",
            " ",
            " class Index:",
            "     \"\"\"A Git Index file.\"\"\"",
            " ",
            "     _byname: dict[bytes, Union[IndexEntry, ConflictedIndexEntry]]",
            " ",
            "-    def __init__(self, filename: Union[bytes, str], read=True) -> None:",
            "+    def __init__(",
            "+        self,",
            "+        filename: Union[bytes, str, os.PathLike],",
            "+        read: bool = True,",
            "+        skip_hash: bool = False,",
            "+        version: Optional[int] = None,",
            "+    ) -> None:",
            "         \"\"\"Create an index object associated with the given filename.",
            " ",
            "         Args:",
            "           filename: Path to the index file",
            "           read: Whether to initialize the index from the given file, should it exist.",
            "+          skip_hash: Whether to skip SHA1 hash when writing (for manyfiles feature)",
            "+          version: Index format version to use (None = auto-detect from file or use default)",
            "         \"\"\"",
            "-        self._filename = filename",
            "+        self._filename = os.fspath(filename)",
            "         # TODO(jelmer): Store the version returned by read_index",
            "-        self._version = None",
            "+        self._version = version",
            "+        self._skip_hash = skip_hash",
            "+        self._extensions: list[IndexExtension] = []",
            "         self.clear()",
            "         if read:",
            "             self.read()",
            " ",
            "     @property",
            "-    def path(self):",
            "+    def path(self) -> Union[bytes, str]:",
            "         return self._filename",
            " ",
            "     def __repr__(self) -> str:",
            "         return f\"{self.__class__.__name__}({self._filename!r})\"",
            " ",
            "     def write(self) -> None:",
            "         \"\"\"Write current contents of index to disk.\"\"\"",
            "+        from typing import BinaryIO, cast",
            "+",
            "         f = GitFile(self._filename, \"wb\")",
            "         try:",
            "-            f = SHA1Writer(f)",
            "-            write_index_dict(f, self._byname, version=self._version)",
            "-        finally:",
            "+            # Filter out extensions with no meaningful data",
            "+            meaningful_extensions = []",
            "+            for ext in self._extensions:",
            "+                # Skip extensions that have empty data",
            "+                ext_data = ext.to_bytes()",
            "+                if ext_data:",
            "+                    meaningful_extensions.append(ext)",
            "+",
            "+            if self._skip_hash:",
            "+                # When skipHash is enabled, write the index without computing SHA1",
            "+                write_index_dict(",
            "+                    cast(BinaryIO, f),",
            "+                    self._byname,",
            "+                    version=self._version,",
            "+                    extensions=meaningful_extensions,",
            "+                )",
            "+                # Write 20 zero bytes instead of SHA1",
            "+                f.write(b\"\\x00\" * 20)",
            "+                f.close()",
            "+            else:",
            "+                sha1_writer = SHA1Writer(cast(BinaryIO, f))",
            "+                write_index_dict(",
            "+                    cast(BinaryIO, sha1_writer),",
            "+                    self._byname,",
            "+                    version=self._version,",
            "+                    extensions=meaningful_extensions,",
            "+                )",
            "+                sha1_writer.close()",
            "+        except:",
            "             f.close()",
            "+            raise",
            " ",
            "     def read(self) -> None:",
            "         \"\"\"Read current contents of index from disk.\"\"\"",
            "         if not os.path.exists(self._filename):",
            "             return",
            "         f = GitFile(self._filename, \"rb\")",
            "         try:",
            "-            f = SHA1Reader(f)",
            "-            self.update(read_index_dict(f))",
            "-            # FIXME: Additional data?",
            "-            f.read(os.path.getsize(self._filename) - f.tell() - 20)",
            "-            f.check_sha()",
            "+            sha1_reader = SHA1Reader(f)",
            "+            entries, version, extensions = read_index_dict_with_version(",
            "+                cast(BinaryIO, sha1_reader)",
            "+            )",
            "+            self._version = version",
            "+            self._extensions = extensions",
            "+            self.update(entries)",
            "+            # Extensions have already been read by read_index_dict_with_version",
            "+            sha1_reader.check_sha(allow_empty=True)",
            "         finally:",
            "             f.close()",
            " ",
            "     def __len__(self) -> int:",
            "         \"\"\"Number of entries in this index file.\"\"\"",
            "         return len(self._byname)",
            " ",
            "@@ -467,15 +959,15 @@",
            "         \"\"\"",
            "         return self._byname[key]",
            " ",
            "     def __iter__(self) -> Iterator[bytes]:",
            "         \"\"\"Iterate over the paths and stages in this index.\"\"\"",
            "         return iter(self._byname)",
            " ",
            "-    def __contains__(self, key) -> bool:",
            "+    def __contains__(self, key: bytes) -> bool:",
            "         return key in self._byname",
            " ",
            "     def get_sha1(self, path: bytes) -> bytes:",
            "         \"\"\"Return the (git object) SHA1 for the object at a path.\"\"\"",
            "         value = self[path]",
            "         if isinstance(value, ConflictedIndexEntry):",
            "             raise UnmergedEntries",
            "@@ -525,43 +1017,58 @@",
            " ",
            "     def update(",
            "         self, entries: dict[bytes, Union[IndexEntry, ConflictedIndexEntry]]",
            "     ) -> None:",
            "         for key, value in entries.items():",
            "             self[key] = value",
            " ",
            "-    def paths(self):",
            "+    def paths(self) -> Generator[bytes, None, None]:",
            "         yield from self._byname.keys()",
            " ",
            "     def changes_from_tree(",
            "-        self, object_store, tree: ObjectID, want_unchanged: bool = False",
            "-    ):",
            "+        self,",
            "+        object_store: ObjectContainer,",
            "+        tree: ObjectID,",
            "+        want_unchanged: bool = False,",
            "+    ) -> Generator[",
            "+        tuple[",
            "+            tuple[Optional[bytes], Optional[bytes]],",
            "+            tuple[Optional[int], Optional[int]],",
            "+            tuple[Optional[bytes], Optional[bytes]],",
            "+        ],",
            "+        None,",
            "+        None,",
            "+    ]:",
            "         \"\"\"Find the differences between the contents of this index and a tree.",
            " ",
            "         Args:",
            "           object_store: Object store to use for retrieving tree contents",
            "           tree: SHA1 of the root tree",
            "           want_unchanged: Whether unchanged files should be reported",
            "         Returns: Iterator over tuples with (oldpath, newpath), (oldmode,",
            "             newmode), (oldsha, newsha)",
            "         \"\"\"",
            " ",
            "-        def lookup_entry(path):",
            "+        def lookup_entry(path: bytes) -> tuple[bytes, int]:",
            "             entry = self[path]",
            "-            return entry.sha, cleanup_mode(entry.mode)",
            "+            if hasattr(entry, \"sha\") and hasattr(entry, \"mode\"):",
            "+                return entry.sha, cleanup_mode(entry.mode)",
            "+            else:",
            "+                # Handle ConflictedIndexEntry case",
            "+                return b\"\", 0",
            " ",
            "         yield from changes_from_tree(",
            "             self.paths(),",
            "             lookup_entry,",
            "             object_store,",
            "             tree,",
            "             want_unchanged=want_unchanged,",
            "         )",
            " ",
            "-    def commit(self, object_store):",
            "+    def commit(self, object_store: ObjectContainer) -> bytes:",
            "         \"\"\"Create a new tree from an index.",
            " ",
            "         Args:",
            "           object_store: Object store to save the tree in",
            "         Returns:",
            "           Root tree SHA",
            "         \"\"\"",
            "@@ -577,31 +1084,31 @@",
            "       object_store: Object store to add trees to",
            "       blobs: Iterable over blob path, sha, mode entries",
            "     Returns:",
            "       SHA1 of the created tree.",
            "     \"\"\"",
            "     trees: dict[bytes, Any] = {b\"\": {}}",
            " ",
            "-    def add_tree(path):",
            "+    def add_tree(path: bytes) -> dict[bytes, Any]:",
            "         if path in trees:",
            "             return trees[path]",
            "         dirname, basename = pathsplit(path)",
            "         t = add_tree(dirname)",
            "         assert isinstance(basename, bytes)",
            "-        newtree = {}",
            "+        newtree: dict[bytes, Any] = {}",
            "         t[basename] = newtree",
            "         trees[path] = newtree",
            "         return newtree",
            " ",
            "     for path, sha, mode in blobs:",
            "         tree_path, basename = pathsplit(path)",
            "         tree = add_tree(tree_path)",
            "         tree[basename] = (mode, sha)",
            " ",
            "-    def build_tree(path):",
            "+    def build_tree(path: bytes) -> bytes:",
            "         tree = Tree()",
            "         for basename, entry in trees[path].items():",
            "             if isinstance(entry, dict):",
            "                 mode = stat.S_IFDIR",
            "                 sha = build_tree(pathjoin(path, basename))",
            "             else:",
            "                 (mode, sha) = entry",
            "@@ -625,15 +1132,15 @@",
            " ",
            " ",
            " def changes_from_tree(",
            "     names: Iterable[bytes],",
            "     lookup_entry: Callable[[bytes], tuple[bytes, int]],",
            "     object_store: ObjectContainer,",
            "     tree: Optional[bytes],",
            "-    want_unchanged=False,",
            "+    want_unchanged: bool = False,",
            " ) -> Iterable[",
            "     tuple[",
            "         tuple[Optional[bytes], Optional[bytes]],",
            "         tuple[Optional[int], Optional[int]],",
            "         tuple[Optional[bytes], Optional[bytes]],",
            "     ]",
            " ]:",
            "@@ -671,100 +1178,111 @@",
            "         except KeyError:",
            "             pass",
            "         else:",
            "             yield ((None, name), (None, other_mode), (None, other_sha))",
            " ",
            " ",
            " def index_entry_from_stat(",
            "-    stat_val,",
            "+    stat_val: os.stat_result,",
            "     hex_sha: bytes,",
            "     mode: Optional[int] = None,",
            "-):",
            "+) -> IndexEntry:",
            "     \"\"\"Create a new index entry from a stat value.",
            " ",
            "     Args:",
            "       stat_val: POSIX stat_result instance",
            "       hex_sha: Hex sha of the object",
            "     \"\"\"",
            "     if mode is None:",
            "         mode = cleanup_mode(stat_val.st_mode)",
            " ",
            "     return IndexEntry(",
            "-        stat_val.st_ctime,",
            "-        stat_val.st_mtime,",
            "-        stat_val.st_dev,",
            "-        stat_val.st_ino,",
            "-        mode,",
            "-        stat_val.st_uid,",
            "-        stat_val.st_gid,",
            "-        stat_val.st_size,",
            "-        hex_sha,",
            "+        ctime=stat_val.st_ctime,",
            "+        mtime=stat_val.st_mtime,",
            "+        dev=stat_val.st_dev,",
            "+        ino=stat_val.st_ino,",
            "+        mode=mode,",
            "+        uid=stat_val.st_uid,",
            "+        gid=stat_val.st_gid,",
            "+        size=stat_val.st_size,",
            "+        sha=hex_sha,",
            "+        flags=0,",
            "+        extended_flags=0,",
            "     )",
            " ",
            " ",
            " if sys.platform == \"win32\":",
            "     # On Windows, creating symlinks either requires administrator privileges",
            "     # or developer mode. Raise a more helpful error when we're unable to",
            "     # create symlinks",
            " ",
            "     # https://github.com/jelmer/dulwich/issues/1005",
            " ",
            "     class WindowsSymlinkPermissionError(PermissionError):",
            "-        def __init__(self, errno, msg, filename) -> None:",
            "+        def __init__(self, errno: int, msg: str, filename: Optional[str]) -> None:",
            "             super(PermissionError, self).__init__(",
            "                 errno,",
            "-                \"Unable to create symlink; \"",
            "-                f\"do you have developer mode enabled? {msg}\",",
            "+                f\"Unable to create symlink; do you have developer mode enabled? {msg}\",",
            "                 filename,",
            "             )",
            " ",
            "-    def symlink(src, dst, target_is_directory=False, *, dir_fd=None):",
            "+    def symlink(",
            "+        src: Union[str, bytes],",
            "+        dst: Union[str, bytes],",
            "+        target_is_directory: bool = False,",
            "+        *,",
            "+        dir_fd: Optional[int] = None,",
            "+    ) -> None:",
            "         try:",
            "             return os.symlink(",
            "                 src, dst, target_is_directory=target_is_directory, dir_fd=dir_fd",
            "             )",
            "         except PermissionError as e:",
            "-            raise WindowsSymlinkPermissionError(e.errno, e.strerror, e.filename) from e",
            "+            raise WindowsSymlinkPermissionError(",
            "+                e.errno or 0, e.strerror or \"\", e.filename",
            "+            ) from e",
            " else:",
            "     symlink = os.symlink",
            " ",
            " ",
            " def build_file_from_blob(",
            "     blob: Blob,",
            "     mode: int,",
            "     target_path: bytes,",
            "     *,",
            "-    honor_filemode=True,",
            "-    tree_encoding=\"utf-8\",",
            "-    symlink_fn=None,",
            "-):",
            "+    honor_filemode: bool = True,",
            "+    tree_encoding: str = \"utf-8\",",
            "+    symlink_fn: Optional[Callable] = None,",
            "+) -> os.stat_result:",
            "     \"\"\"Build a file or symlink on disk based on a Git object.",
            " ",
            "     Args:",
            "       blob: The git object",
            "       mode: File mode",
            "       target_path: Path to write to",
            "       honor_filemode: An optional flag to honor core.filemode setting in",
            "         config file, default is core.filemode=True, change executable bit",
            "-      symlink: Function to use for creating symlinks",
            "+      symlink_fn: Function to use for creating symlinks",
            "     Returns: stat object for the file",
            "     \"\"\"",
            "     try:",
            "         oldstat = os.lstat(target_path)",
            "     except FileNotFoundError:",
            "         oldstat = None",
            "     contents = blob.as_raw_string()",
            "     if stat.S_ISLNK(mode):",
            "         if oldstat:",
            "-            os.unlink(target_path)",
            "+            _remove_file_with_readonly_handling(target_path)",
            "         if sys.platform == \"win32\":",
            "             # os.readlink on Python3 on Windows requires a unicode string.",
            "-            contents = contents.decode(tree_encoding)  # type: ignore",
            "-            target_path = target_path.decode(tree_encoding)  # type: ignore",
            "-        (symlink_fn or symlink)(contents, target_path)",
            "+            contents_str = contents.decode(tree_encoding)",
            "+            target_path_str = target_path.decode(tree_encoding)",
            "+            (symlink_fn or symlink)(contents_str, target_path_str)",
            "+        else:",
            "+            (symlink_fn or symlink)(contents, target_path)",
            "     else:",
            "         if oldstat is not None and oldstat.st_size == len(contents):",
            "             with open(target_path, \"rb\") as f:",
            "                 if f.read() == contents:",
            "                     return oldstat",
            " ",
            "         with open(target_path, \"wb\") as f:",
            "@@ -789,15 +1307,72 @@",
            "     if stripped in INVALID_DOTNAMES:",
            "         return False",
            "     if stripped == b\"git~1\":",
            "         return False",
            "     return True",
            " ",
            " ",
            "-def validate_path(path: bytes, element_validator=validate_path_element_default) -> bool:",
            "+# HFS+ ignorable Unicode codepoints (from Git's utf8.c)",
            "+HFS_IGNORABLE_CHARS = {",
            "+    0x200C,  # ZERO WIDTH NON-JOINER",
            "+    0x200D,  # ZERO WIDTH JOINER",
            "+    0x200E,  # LEFT-TO-RIGHT MARK",
            "+    0x200F,  # RIGHT-TO-LEFT MARK",
            "+    0x202A,  # LEFT-TO-RIGHT EMBEDDING",
            "+    0x202B,  # RIGHT-TO-LEFT EMBEDDING",
            "+    0x202C,  # POP DIRECTIONAL FORMATTING",
            "+    0x202D,  # LEFT-TO-RIGHT OVERRIDE",
            "+    0x202E,  # RIGHT-TO-LEFT OVERRIDE",
            "+    0x206A,  # INHIBIT SYMMETRIC SWAPPING",
            "+    0x206B,  # ACTIVATE SYMMETRIC SWAPPING",
            "+    0x206C,  # INHIBIT ARABIC FORM SHAPING",
            "+    0x206D,  # ACTIVATE ARABIC FORM SHAPING",
            "+    0x206E,  # NATIONAL DIGIT SHAPES",
            "+    0x206F,  # NOMINAL DIGIT SHAPES",
            "+    0xFEFF,  # ZERO WIDTH NO-BREAK SPACE",
            "+}",
            "+",
            "+",
            "+def validate_path_element_hfs(element: bytes) -> bool:",
            "+    \"\"\"Validate path element for HFS+ filesystem.",
            "+",
            "+    Equivalent to Git's is_hfs_dotgit and related checks.",
            "+    Uses NFD normalization and ignores HFS+ ignorable characters.",
            "+    \"\"\"",
            "+    import unicodedata",
            "+",
            "+    try:",
            "+        # Decode to Unicode",
            "+        element_str = element.decode(\"utf-8\", errors=\"strict\")",
            "+    except UnicodeDecodeError:",
            "+        # Malformed UTF-8 - be conservative and reject",
            "+        return False",
            "+",
            "+    # Remove HFS+ ignorable characters (like Git's next_hfs_char)",
            "+    filtered = \"\".join(c for c in element_str if ord(c) not in HFS_IGNORABLE_CHARS)",
            "+",
            "+    # Normalize to NFD (HFS+ uses a variant of NFD)",
            "+    normalized = unicodedata.normalize(\"NFD\", filtered)",
            "+",
            "+    # Check against invalid names (case-insensitive)",
            "+    normalized_bytes = normalized.encode(\"utf-8\", errors=\"strict\")",
            "+    if normalized_bytes.lower() in INVALID_DOTNAMES:",
            "+        return False",
            "+",
            "+    # Also check for 8.3 short name",
            "+    if normalized_bytes.lower() == b\"git~1\":",
            "+        return False",
            "+",
            "+    return True",
            "+",
            "+",
            "+def validate_path(",
            "+    path: bytes,",
            "+    element_validator: Callable[[bytes], bool] = validate_path_element_default,",
            "+) -> bool:",
            "     \"\"\"Default path validator that just checks for .git/.\"\"\"",
            "     parts = path.split(b\"/\")",
            "     for p in parts:",
            "         if not element_validator(p):",
            "             return False",
            "     else:",
            "         return True",
            "@@ -805,28 +1380,31 @@",
            " ",
            " def build_index_from_tree(",
            "     root_path: Union[str, bytes],",
            "     index_path: Union[str, bytes],",
            "     object_store: ObjectContainer,",
            "     tree_id: bytes,",
            "     honor_filemode: bool = True,",
            "-    validate_path_element=validate_path_element_default,",
            "-    symlink_fn=None,",
            "+    validate_path_element: Callable[[bytes], bool] = validate_path_element_default,",
            "+    symlink_fn: Optional[Callable] = None,",
            "+    blob_normalizer: Optional[\"BlobNormalizer\"] = None,",
            " ) -> None:",
            "     \"\"\"Generate and materialize index from a tree.",
            " ",
            "     Args:",
            "       tree_id: Tree to materialize",
            "       root_path: Target dir for materialized index files",
            "       index_path: Target path for generated index",
            "       object_store: Non-empty object store holding tree contents",
            "       honor_filemode: An optional flag to honor core.filemode setting in",
            "         config file, default is core.filemode=True, change executable bit",
            "       validate_path_element: Function to validate path elements to check",
            "         out; default just refuses .git and .. directories.",
            "+      blob_normalizer: An optional BlobNormalizer to use for converting line",
            "+        endings when writing blobs to the working directory.",
            " ",
            "     Note: existing index is wiped and contents are not merged",
            "         in a working dir. Suitable only for fresh clones.",
            "     \"\"\"",
            "     index = Index(index_path, read=False)",
            "     if not isinstance(root_path, bytes):",
            "         root_path = os.fsencode(root_path)",
            "@@ -844,14 +1422,17 @@",
            "             if not os.path.isdir(full_path):",
            "                 os.mkdir(full_path)",
            "             st = os.lstat(full_path)",
            "             # TODO(jelmer): record and return submodule paths",
            "         else:",
            "             obj = object_store[entry.sha]",
            "             assert isinstance(obj, Blob)",
            "+            # Apply blob normalization for checkout if normalizer is provided",
            "+            if blob_normalizer is not None:",
            "+                obj = blob_normalizer.checkout_normalize(obj, entry.path)",
            "             st = build_file_from_blob(",
            "                 obj,",
            "                 entry.mode,",
            "                 full_path,",
            "                 honor_filemode=honor_filemode,",
            "                 symlink_fn=symlink_fn,",
            "             )",
            "@@ -877,15 +1458,17 @@",
            "             # default to a stage 0 index entry (normal)",
            "             # when reading from the filesystem",
            "         index[entry.path] = index_entry_from_stat(st, entry.sha)",
            " ",
            "     index.write()",
            " ",
            " ",
            "-def blob_from_path_and_mode(fs_path: bytes, mode: int, tree_encoding=\"utf-8\"):",
            "+def blob_from_path_and_mode(",
            "+    fs_path: bytes, mode: int, tree_encoding: str = \"utf-8\"",
            "+) -> Blob:",
            "     \"\"\"Create a blob from a path and a stat object.",
            " ",
            "     Args:",
            "       fs_path: Full file system path to file",
            "       mode: File mode",
            "     Returns: A `Blob` object",
            "     \"\"\"",
            "@@ -899,15 +1482,17 @@",
            "             blob.data = os.readlink(fs_path)",
            "     else:",
            "         with open(fs_path, \"rb\") as f:",
            "             blob.data = f.read()",
            "     return blob",
            " ",
            " ",
            "-def blob_from_path_and_stat(fs_path: bytes, st, tree_encoding=\"utf-8\"):",
            "+def blob_from_path_and_stat(",
            "+    fs_path: bytes, st: os.stat_result, tree_encoding: str = \"utf-8\"",
            "+) -> Blob:",
            "     \"\"\"Create a blob from a path and a stat object.",
            " ",
            "     Args:",
            "       fs_path: Full file system path to file",
            "       st: A stat object",
            "     Returns: A `Blob` object",
            "     \"\"\"",
            "@@ -934,15 +1519,15 @@",
            "         return None",
            "     try:",
            "         return repo.head()",
            "     except KeyError:",
            "         return None",
            " ",
            " ",
            "-def _has_directory_changed(tree_path: bytes, entry) -> bool:",
            "+def _has_directory_changed(tree_path: bytes, entry: IndexEntry) -> bool:",
            "     \"\"\"Check if a directory has changed after getting an error.",
            " ",
            "     When handling an error trying to create a blob from a path, call this",
            "     function. It will check if the path is a directory. If it's a directory",
            "     and a submodule, check the submodule head to see if it's has changed. If",
            "     not, consider the file as changed as Git tracked a file and not a",
            "     directory.",
            "@@ -959,17 +1544,470 @@",
            "     else:",
            "         # The file was changed to a directory, so consider it removed.",
            "         return True",
            " ",
            "     return False",
            " ",
            " ",
            "-def get_unstaged_changes(",
            "-    index: Index, root_path: Union[str, bytes], filter_blob_callback=None",
            "+os_sep_bytes = os.sep.encode(\"ascii\")",
            "+",
            "+",
            "+def _ensure_parent_dir_exists(full_path: bytes) -> None:",
            "+    \"\"\"Ensure parent directory exists, checking no parent is a file.\"\"\"",
            "+    parent_dir = os.path.dirname(full_path)",
            "+    if parent_dir and not os.path.exists(parent_dir):",
            "+        # Check if any parent in the path is a file",
            "+        parts = parent_dir.split(os_sep_bytes)",
            "+        for i in range(len(parts)):",
            "+            partial_path = os_sep_bytes.join(parts[: i + 1])",
            "+            if (",
            "+                partial_path",
            "+                and os.path.exists(partial_path)",
            "+                and not os.path.isdir(partial_path)",
            "+            ):",
            "+                # Parent path is a file, this is an error",
            "+                raise OSError(",
            "+                    f\"Cannot create directory, parent path is a file: {partial_path!r}\"",
            "+                )",
            "+        os.makedirs(parent_dir)",
            "+",
            "+",
            "+def _remove_file_with_readonly_handling(path: bytes) -> None:",
            "+    \"\"\"Remove a file, handling read-only files on Windows.",
            "+",
            "+    Args:",
            "+      path: Path to the file to remove",
            "+    \"\"\"",
            "+    try:",
            "+        os.unlink(path)",
            "+    except PermissionError:",
            "+        # On Windows, remove read-only attribute and retry",
            "+        if sys.platform == \"win32\":",
            "+            os.chmod(path, stat.S_IWRITE | stat.S_IREAD)",
            "+            os.unlink(path)",
            "+        else:",
            "+            raise",
            "+",
            "+",
            "+def _remove_empty_parents(path: bytes, stop_at: bytes) -> None:",
            "+    \"\"\"Remove empty parent directories up to stop_at.\"\"\"",
            "+    parent = os.path.dirname(path)",
            "+    while parent and parent != stop_at:",
            "+        try:",
            "+            os.rmdir(parent)",
            "+            parent = os.path.dirname(parent)",
            "+        except FileNotFoundError:",
            "+            # Directory doesn't exist - stop trying",
            "+            break",
            "+        except OSError as e:",
            "+            if e.errno == errno.ENOTEMPTY:",
            "+                # Directory not empty - stop trying",
            "+                break",
            "+            raise",
            "+",
            "+",
            "+def _check_symlink_matches(",
            "+    full_path: bytes, repo_object_store, entry_sha: bytes",
            "+) -> bool:",
            "+    \"\"\"Check if symlink target matches expected target.",
            "+",
            "+    Returns True if symlink needs to be written, False if it matches.",
            "+    \"\"\"",
            "+    try:",
            "+        current_target = os.readlink(full_path)",
            "+        blob_obj = repo_object_store[entry_sha]",
            "+        expected_target = blob_obj.as_raw_string()",
            "+        if isinstance(current_target, str):",
            "+            current_target = current_target.encode()",
            "+        return current_target != expected_target",
            "+    except FileNotFoundError:",
            "+        # Symlink doesn't exist",
            "+        return True",
            "+    except OSError as e:",
            "+        if e.errno == errno.EINVAL:",
            "+            # Not a symlink",
            "+            return True",
            "+        raise",
            "+",
            "+",
            "+def _check_file_matches(",
            "+    repo_object_store,",
            "+    full_path: bytes,",
            "+    entry_sha: bytes,",
            "+    entry_mode: int,",
            "+    current_stat: os.stat_result,",
            "+    honor_filemode: bool,",
            "+    blob_normalizer: Optional[\"BlobNormalizer\"] = None,",
            "+    tree_path: Optional[bytes] = None,",
            "+) -> bool:",
            "+    \"\"\"Check if a file on disk matches the expected git object.",
            "+",
            "+    Returns True if file needs to be written, False if it matches.",
            "+    \"\"\"",
            "+    # Check mode first (if honor_filemode is True)",
            "+    if honor_filemode:",
            "+        current_mode = stat.S_IMODE(current_stat.st_mode)",
            "+        expected_mode = stat.S_IMODE(entry_mode)",
            "+        if current_mode != expected_mode:",
            "+            return True",
            "+",
            "+    # If mode matches (or we don't care), check content via size first",
            "+    blob_obj = repo_object_store[entry_sha]",
            "+    if current_stat.st_size != blob_obj.raw_length():",
            "+        return True",
            "+",
            "+    # Size matches, check actual content",
            "+    try:",
            "+        with open(full_path, \"rb\") as f:",
            "+            current_content = f.read()",
            "+            expected_content = blob_obj.as_raw_string()",
            "+            if blob_normalizer and tree_path is not None:",
            "+                normalized_blob = blob_normalizer.checkout_normalize(",
            "+                    blob_obj, tree_path",
            "+                )",
            "+                expected_content = normalized_blob.as_raw_string()",
            "+            return current_content != expected_content",
            "+    except (FileNotFoundError, PermissionError, IsADirectoryError):",
            "+        return True",
            "+",
            "+",
            "+def _transition_to_submodule(repo, path, full_path, current_stat, entry, index):",
            "+    \"\"\"Transition any type to submodule.\"\"\"",
            "+    from .submodule import ensure_submodule_placeholder",
            "+",
            "+    if current_stat is not None and stat.S_ISDIR(current_stat.st_mode):",
            "+        # Already a directory, just ensure .git file exists",
            "+        ensure_submodule_placeholder(repo, path)",
            "+    else:",
            "+        # Remove whatever is there and create submodule",
            "+        if current_stat is not None:",
            "+            _remove_file_with_readonly_handling(full_path)",
            "+        ensure_submodule_placeholder(repo, path)",
            "+",
            "+    st = os.lstat(full_path)",
            "+    index[path] = index_entry_from_stat(st, entry.sha)",
            "+",
            "+",
            "+def _transition_to_file(",
            "+    object_store,",
            "+    path,",
            "+    full_path,",
            "+    current_stat,",
            "+    entry,",
            "+    index,",
            "+    honor_filemode,",
            "+    symlink_fn,",
            "+    blob_normalizer,",
            " ):",
            "+    \"\"\"Transition any type to regular file or symlink.\"\"\"",
            "+    # Check if we need to update",
            "+    if (",
            "+        current_stat is not None",
            "+        and stat.S_ISREG(current_stat.st_mode)",
            "+        and not stat.S_ISLNK(entry.mode)",
            "+    ):",
            "+        # File to file - check if update needed",
            "+        needs_update = _check_file_matches(",
            "+            object_store,",
            "+            full_path,",
            "+            entry.sha,",
            "+            entry.mode,",
            "+            current_stat,",
            "+            honor_filemode,",
            "+            blob_normalizer,",
            "+            path,",
            "+        )",
            "+    elif (",
            "+        current_stat is not None",
            "+        and stat.S_ISLNK(current_stat.st_mode)",
            "+        and stat.S_ISLNK(entry.mode)",
            "+    ):",
            "+        # Symlink to symlink - check if update needed",
            "+        needs_update = _check_symlink_matches(full_path, object_store, entry.sha)",
            "+    else:",
            "+        needs_update = True",
            "+",
            "+    if not needs_update:",
            "+        # Just update index - current_stat should always be valid here since we're not updating",
            "+        index[path] = index_entry_from_stat(current_stat, entry.sha)",
            "+        return",
            "+",
            "+    # Remove existing entry if needed",
            "+    if current_stat is not None and stat.S_ISDIR(current_stat.st_mode):",
            "+        # Remove directory",
            "+        dir_contents = set(os.listdir(full_path))",
            "+        git_file_name = b\".git\" if isinstance(full_path, bytes) else \".git\"",
            "+",
            "+        if git_file_name in dir_contents:",
            "+            if dir_contents != {git_file_name}:",
            "+                raise IsADirectoryError(",
            "+                    f\"Cannot replace submodule with untracked files: {full_path!r}\"",
            "+                )",
            "+            shutil.rmtree(full_path)",
            "+        else:",
            "+            try:",
            "+                os.rmdir(full_path)",
            "+            except OSError as e:",
            "+                if e.errno == errno.ENOTEMPTY:",
            "+                    raise IsADirectoryError(",
            "+                        f\"Cannot replace non-empty directory with file: {full_path!r}\"",
            "+                    )",
            "+                raise",
            "+    elif current_stat is not None:",
            "+        _remove_file_with_readonly_handling(full_path)",
            "+",
            "+    # Ensure parent directory exists",
            "+    _ensure_parent_dir_exists(full_path)",
            "+",
            "+    # Write the file",
            "+    blob_obj = object_store[entry.sha]",
            "+    assert isinstance(blob_obj, Blob)",
            "+    if blob_normalizer:",
            "+        blob_obj = blob_normalizer.checkout_normalize(blob_obj, path)",
            "+    st = build_file_from_blob(",
            "+        blob_obj,",
            "+        entry.mode,",
            "+        full_path,",
            "+        honor_filemode=honor_filemode,",
            "+        symlink_fn=symlink_fn,",
            "+    )",
            "+    index[path] = index_entry_from_stat(st, entry.sha)",
            "+",
            "+",
            "+def _transition_to_absent(repo, path, full_path, current_stat, index):",
            "+    \"\"\"Remove any type of entry.\"\"\"",
            "+    if current_stat is None:",
            "+        return",
            "+",
            "+    if stat.S_ISDIR(current_stat.st_mode):",
            "+        # Check if it's a submodule directory",
            "+        dir_contents = set(os.listdir(full_path))",
            "+        git_file_name = b\".git\" if isinstance(full_path, bytes) else \".git\"",
            "+",
            "+        if git_file_name in dir_contents and dir_contents == {git_file_name}:",
            "+            shutil.rmtree(full_path)",
            "+        else:",
            "+            try:",
            "+                os.rmdir(full_path)",
            "+            except OSError as e:",
            "+                if e.errno not in (errno.ENOTEMPTY, errno.EEXIST):",
            "+                    raise",
            "+    else:",
            "+        _remove_file_with_readonly_handling(full_path)",
            "+",
            "+    try:",
            "+        del index[path]",
            "+    except KeyError:",
            "+        pass",
            "+",
            "+    # Try to remove empty parent directories",
            "+    _remove_empty_parents(",
            "+        full_path, repo.path if isinstance(repo.path, bytes) else repo.path.encode()",
            "+    )",
            "+",
            "+",
            "+def update_working_tree(",
            "+    repo: \"Repo\",",
            "+    old_tree_id: Optional[bytes],",
            "+    new_tree_id: bytes,",
            "+    honor_filemode: bool = True,",
            "+    validate_path_element: Optional[Callable[[bytes], bool]] = None,",
            "+    symlink_fn: Optional[Callable] = None,",
            "+    force_remove_untracked: bool = False,",
            "+    blob_normalizer: Optional[\"BlobNormalizer\"] = None,",
            "+) -> None:",
            "+    \"\"\"Update the working tree and index to match a new tree.",
            "+",
            "+    This function handles:",
            "+    - Adding new files",
            "+    - Updating modified files",
            "+    - Removing deleted files",
            "+    - Cleaning up empty directories",
            "+",
            "+    Args:",
            "+      repo: Repository object",
            "+      old_tree_id: SHA of the tree before the update",
            "+      new_tree_id: SHA of the tree to update to",
            "+      honor_filemode: An optional flag to honor core.filemode setting",
            "+      validate_path_element: Function to validate path elements to check out",
            "+      symlink_fn: Function to use for creating symlinks",
            "+      force_remove_untracked: If True, remove files that exist in working",
            "+        directory but not in target tree, even if old_tree_id is None",
            "+      blob_normalizer: An optional BlobNormalizer to use for converting line",
            "+        endings when writing blobs to the working directory.",
            "+    \"\"\"",
            "+    if validate_path_element is None:",
            "+        validate_path_element = validate_path_element_default",
            "+",
            "+    repo_path = repo.path if isinstance(repo.path, bytes) else repo.path.encode()",
            "+    index = repo.open_index()",
            "+",
            "+    # Build sets of paths for efficient lookup",
            "+    new_paths = {}",
            "+    for entry in iter_tree_contents(repo.object_store, new_tree_id):",
            "+        if entry.path.startswith(b\".git\") or not validate_path(",
            "+            entry.path, validate_path_element",
            "+        ):",
            "+            continue",
            "+        new_paths[entry.path] = entry",
            "+",
            "+    old_paths = {}",
            "+    if old_tree_id:",
            "+        for entry in iter_tree_contents(repo.object_store, old_tree_id):",
            "+            if not entry.path.startswith(b\".git\"):",
            "+                old_paths[entry.path] = entry",
            "+",
            "+    # Process all paths",
            "+    all_paths = set(new_paths.keys()) | set(old_paths.keys())",
            "+",
            "+    # Check for paths that need to become directories",
            "+    paths_needing_dir = set()",
            "+    for path in new_paths:",
            "+        parts = path.split(b\"/\")",
            "+        for i in range(1, len(parts)):",
            "+            parent = b\"/\".join(parts[:i])",
            "+            if parent in old_paths and parent not in new_paths:",
            "+                paths_needing_dir.add(parent)",
            "+",
            "+    # Check if any path that needs to become a directory has been modified",
            "+    current_stat: Optional[os.stat_result]",
            "+    stat_cache: dict[bytes, Optional[os.stat_result]] = {}",
            "+    for path in paths_needing_dir:",
            "+        full_path = _tree_to_fs_path(repo_path, path)",
            "+        try:",
            "+            current_stat = os.lstat(full_path)",
            "+        except FileNotFoundError:",
            "+            # File doesn't exist, proceed",
            "+            stat_cache[full_path] = None",
            "+        except PermissionError:",
            "+            # Can't read file, proceed",
            "+            pass",
            "+        else:",
            "+            stat_cache[full_path] = current_stat",
            "+            if stat.S_ISREG(current_stat.st_mode):",
            "+                # Check if file has been modified",
            "+                old_entry = old_paths[path]",
            "+                if _check_file_matches(",
            "+                    repo.object_store,",
            "+                    full_path,",
            "+                    old_entry.sha,",
            "+                    old_entry.mode,",
            "+                    current_stat,",
            "+                    honor_filemode,",
            "+                    blob_normalizer,",
            "+                    path,",
            "+                ):",
            "+                    # File has been modified, can't replace with directory",
            "+                    raise OSError(",
            "+                        f\"Cannot replace modified file with directory: {path!r}\"",
            "+                    )",
            "+",
            "+    # Process in two passes: deletions first, then additions/updates",
            "+    # This handles case-only renames on case-insensitive filesystems correctly",
            "+    paths_to_remove = []",
            "+    paths_to_update = []",
            "+",
            "+    for path in sorted(all_paths):",
            "+        if path in new_paths:",
            "+            paths_to_update.append(path)",
            "+        else:",
            "+            paths_to_remove.append(path)",
            "+",
            "+    # First process removals",
            "+    for path in paths_to_remove:",
            "+        full_path = _tree_to_fs_path(repo_path, path)",
            "+",
            "+        # Determine current state - use cache if available",
            "+        try:",
            "+            current_stat = stat_cache[full_path]",
            "+        except KeyError:",
            "+            try:",
            "+                current_stat = os.lstat(full_path)",
            "+            except FileNotFoundError:",
            "+                current_stat = None",
            "+",
            "+        _transition_to_absent(repo, path, full_path, current_stat, index)",
            "+",
            "+    # Then process additions/updates",
            "+    for path in paths_to_update:",
            "+        full_path = _tree_to_fs_path(repo_path, path)",
            "+",
            "+        # Determine current state - use cache if available",
            "+        try:",
            "+            current_stat = stat_cache[full_path]",
            "+        except KeyError:",
            "+            try:",
            "+                current_stat = os.lstat(full_path)",
            "+            except FileNotFoundError:",
            "+                current_stat = None",
            "+",
            "+        new_entry = new_paths[path]",
            "+",
            "+        # Path should exist",
            "+        if S_ISGITLINK(new_entry.mode):",
            "+            _transition_to_submodule(",
            "+                repo, path, full_path, current_stat, new_entry, index",
            "+            )",
            "+        else:",
            "+            _transition_to_file(",
            "+                repo.object_store,",
            "+                path,",
            "+                full_path,",
            "+                current_stat,",
            "+                new_entry,",
            "+                index,",
            "+                honor_filemode,",
            "+                symlink_fn,",
            "+                blob_normalizer,",
            "+            )",
            "+",
            "+    # Handle force_remove_untracked",
            "+    if force_remove_untracked:",
            "+        for root, dirs, files in os.walk(repo_path):",
            "+            if b\".git\" in os.fsencode(root):",
            "+                continue",
            "+            root_bytes = os.fsencode(root)",
            "+            for file in files:",
            "+                full_path = os.path.join(root_bytes, os.fsencode(file))",
            "+                tree_path = os.path.relpath(full_path, repo_path)",
            "+                if os.sep != \"/\":",
            "+                    tree_path = tree_path.replace(os.sep.encode(), b\"/\")",
            "+",
            "+                if tree_path not in new_paths:",
            "+                    _remove_file_with_readonly_handling(full_path)",
            "+                    if tree_path in index:",
            "+                        del index[tree_path]",
            "+",
            "+        # Clean up empty directories",
            "+        for root, dirs, files in os.walk(repo_path, topdown=False):",
            "+            root_bytes = os.fsencode(root)",
            "+            if (",
            "+                b\".git\" not in root_bytes",
            "+                and root_bytes != repo_path",
            "+                and not files",
            "+                and not dirs",
            "+            ):",
            "+                try:",
            "+                    os.rmdir(root)",
            "+                except FileNotFoundError:",
            "+                    # Directory was already removed",
            "+                    pass",
            "+                except OSError as e:",
            "+                    if e.errno != errno.ENOTEMPTY:",
            "+                        # Only ignore \"directory not empty\" errors",
            "+                        raise",
            "+",
            "+    index.write()",
            "+",
            "+",
            "+def get_unstaged_changes(",
            "+    index: Index,",
            "+    root_path: Union[str, bytes],",
            "+    filter_blob_callback: Optional[Callable] = None,",
            "+) -> Generator[bytes, None, None]:",
            "     \"\"\"Walk through an index and check for differences against working tree.",
            " ",
            "     Args:",
            "       index: index to check",
            "       root_path: path in which to find files",
            "     Returns: iterator over paths with unstaged changes",
            "     \"\"\"",
            "@@ -1003,18 +2041,15 @@",
            "             # different from whatever file used to exist.",
            "             yield tree_path",
            "         else:",
            "             if blob.id != entry.sha:",
            "                 yield tree_path",
            " ",
            " ",
            "-os_sep_bytes = os.sep.encode(\"ascii\")",
            "-",
            "-",
            "-def _tree_to_fs_path(root_path: bytes, tree_path: bytes):",
            "+def _tree_to_fs_path(root_path: bytes, tree_path: bytes) -> bytes:",
            "     \"\"\"Convert a git tree path to a file system path.",
            " ",
            "     Args:",
            "       root_path: Root filesystem path",
            "       tree_path: Git tree path as bytes",
            " ",
            "     Returns: File system path.",
            "@@ -1042,15 +2077,15 @@",
            "     if os_sep_bytes != b\"/\":",
            "         tree_path = fs_path_bytes.replace(os_sep_bytes, b\"/\")",
            "     else:",
            "         tree_path = fs_path_bytes",
            "     return tree_path",
            " ",
            " ",
            "-def index_entry_from_directory(st, path: bytes) -> Optional[IndexEntry]:",
            "+def index_entry_from_directory(st: os.stat_result, path: bytes) -> Optional[IndexEntry]:",
            "     if os.path.exists(os.path.join(path, b\".git\")):",
            "         head = read_submodule_head(path)",
            "         if head is None:",
            "             return None",
            "         return index_entry_from_stat(st, head, mode=S_IFGITLINK)",
            "     return None",
            " ",
            "@@ -1103,15 +2138,18 @@",
            "             entry = index_entry_from_path(p, object_store=object_store)",
            "         except (FileNotFoundError, IsADirectoryError):",
            "             entry = None",
            "         yield path, entry",
            " ",
            " ",
            " def iter_fresh_objects(",
            "-    paths: Iterable[bytes], root_path: bytes, include_deleted=False, object_store=None",
            "+    paths: Iterable[bytes],",
            "+    root_path: bytes,",
            "+    include_deleted: bool = False,",
            "+    object_store: Optional[ObjectContainer] = None,",
            " ) -> Iterator[tuple[bytes, Optional[bytes], Optional[int]]]:",
            "     \"\"\"Iterate over versions of objects on disk referenced by index.",
            " ",
            "     Args:",
            "       root_path: Root path to access from",
            "       include_deleted: Include deleted entries with sha and",
            "         mode set to None",
            "@@ -1142,26 +2180,35 @@",
            " ",
            " class locked_index:",
            "     \"\"\"Lock the index while making modifications.",
            " ",
            "     Works as a context manager.",
            "     \"\"\"",
            " ",
            "+    _file: \"_GitFile\"",
            "+",
            "     def __init__(self, path: Union[bytes, str]) -> None:",
            "         self._path = path",
            " ",
            "-    def __enter__(self):",
            "+    def __enter__(self) -> Index:",
            "         self._file = GitFile(self._path, \"wb\")",
            "         self._index = Index(self._path)",
            "         return self._index",
            " ",
            "-    def __exit__(self, exc_type, exc_value, traceback):",
            "+    def __exit__(",
            "+        self,",
            "+        exc_type: Optional[type],",
            "+        exc_value: Optional[BaseException],",
            "+        traceback: Optional[types.TracebackType],",
            "+    ) -> None:",
            "         if exc_type is not None:",
            "             self._file.abort()",
            "             return",
            "         try:",
            "-            f = SHA1Writer(self._file)",
            "-            write_index_dict(f, self._index._byname)",
            "+            from typing import BinaryIO, cast",
            "+",
            "+            f = SHA1Writer(cast(BinaryIO, self._file))",
            "+            write_index_dict(cast(BinaryIO, f), self._index._byname)",
            "         except BaseException:",
            "             self._file.abort()",
            "         else:",
            "             f.close()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/lfs.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/lfs.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/lfs.py",
            "@@ -18,48 +18,53 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " import hashlib",
            " import os",
            " import tempfile",
            "+from collections.abc import Iterable",
            "+from typing import TYPE_CHECKING, BinaryIO, Optional",
            "+",
            "+if TYPE_CHECKING:",
            "+    from .repo import Repo",
            " ",
            " ",
            " class LFSStore:",
            "     \"\"\"Stores objects on disk, indexed by SHA256.\"\"\"",
            " ",
            "-    def __init__(self, path) -> None:",
            "+    def __init__(self, path: str) -> None:",
            "         self.path = path",
            " ",
            "     @classmethod",
            "-    def create(cls, lfs_dir):",
            "+    def create(cls, lfs_dir: str) -> \"LFSStore\":",
            "         if not os.path.isdir(lfs_dir):",
            "             os.mkdir(lfs_dir)",
            "         os.mkdir(os.path.join(lfs_dir, \"tmp\"))",
            "         os.mkdir(os.path.join(lfs_dir, \"objects\"))",
            "         return cls(lfs_dir)",
            " ",
            "     @classmethod",
            "-    def from_repo(cls, repo, create=False):",
            "-        lfs_dir = os.path.join(repo.controldir, \"lfs\")",
            "+    def from_repo(cls, repo: \"Repo\", create: bool = False) -> \"LFSStore\":",
            "+        lfs_dir = os.path.join(repo.controldir(), \"lfs\")",
            "         if create:",
            "             return cls.create(lfs_dir)",
            "         return cls(lfs_dir)",
            " ",
            "-    def _sha_path(self, sha):",
            "+    def _sha_path(self, sha: str) -> str:",
            "         return os.path.join(self.path, \"objects\", sha[0:2], sha[2:4], sha)",
            " ",
            "-    def open_object(self, sha):",
            "+    def open_object(self, sha: str) -> BinaryIO:",
            "         \"\"\"Open an object by sha.\"\"\"",
            "         try:",
            "             return open(self._sha_path(sha), \"rb\")",
            "         except FileNotFoundError as exc:",
            "             raise KeyError(sha) from exc",
            " ",
            "-    def write_object(self, chunks):",
            "+    def write_object(self, chunks: Iterable[bytes]) -> str:",
            "         \"\"\"Write an object.",
            " ",
            "         Returns: object SHA",
            "         \"\"\"",
            "         sha = hashlib.sha256()",
            "         tmpdir = os.path.join(self.path, \"tmp\")",
            "         with tempfile.NamedTemporaryFile(dir=tmpdir, mode=\"wb\", delete=False) as f:",
            "@@ -69,7 +74,112 @@",
            "             f.flush()",
            "             tmppath = f.name",
            "         path = self._sha_path(sha.hexdigest())",
            "         if not os.path.exists(os.path.dirname(path)):",
            "             os.makedirs(os.path.dirname(path))",
            "         os.rename(tmppath, path)",
            "         return sha.hexdigest()",
            "+",
            "+",
            "+class LFSPointer:",
            "+    \"\"\"Represents an LFS pointer file.\"\"\"",
            "+",
            "+    def __init__(self, oid: str, size: int) -> None:",
            "+        self.oid = oid",
            "+        self.size = size",
            "+",
            "+    @classmethod",
            "+    def from_bytes(cls, data: bytes) -> Optional[\"LFSPointer\"]:",
            "+        \"\"\"Parse LFS pointer from bytes.",
            "+",
            "+        Returns None if data is not a valid LFS pointer.",
            "+        \"\"\"",
            "+        try:",
            "+            text = data.decode(\"utf-8\")",
            "+        except UnicodeDecodeError:",
            "+            return None",
            "+",
            "+        # LFS pointer files have a specific format",
            "+        lines = text.strip().split(\"\\n\")",
            "+        if len(lines) < 3:",
            "+            return None",
            "+",
            "+        # Must start with version",
            "+        if not lines[0].startswith(\"version https://git-lfs.github.com/spec/v1\"):",
            "+            return None",
            "+",
            "+        oid = None",
            "+        size = None",
            "+",
            "+        for line in lines[1:]:",
            "+            if line.startswith(\"oid sha256:\"):",
            "+                oid = line[11:].strip()",
            "+            elif line.startswith(\"size \"):",
            "+                try:",
            "+                    size = int(line[5:].strip())",
            "+                except ValueError:",
            "+                    return None",
            "+",
            "+        if oid is None or size is None:",
            "+            return None",
            "+",
            "+        return cls(oid, size)",
            "+",
            "+    def to_bytes(self) -> bytes:",
            "+        \"\"\"Convert LFS pointer to bytes.\"\"\"",
            "+        return (",
            "+            f\"version https://git-lfs.github.com/spec/v1\\n\"",
            "+            f\"oid sha256:{self.oid}\\n\"",
            "+            f\"size {self.size}\\n\"",
            "+        ).encode()",
            "+",
            "+    def is_valid_oid(self) -> bool:",
            "+        \"\"\"Check if the OID is valid SHA256.\"\"\"",
            "+        if len(self.oid) != 64:",
            "+            return False",
            "+        try:",
            "+            int(self.oid, 16)",
            "+            return True",
            "+        except ValueError:",
            "+            return False",
            "+",
            "+",
            "+class LFSFilterDriver:",
            "+    \"\"\"LFS filter driver implementation.\"\"\"",
            "+",
            "+    def __init__(self, lfs_store: \"LFSStore\") -> None:",
            "+        self.lfs_store = lfs_store",
            "+",
            "+    def clean(self, data: bytes) -> bytes:",
            "+        \"\"\"Convert file content to LFS pointer (clean filter).\"\"\"",
            "+        # Check if data is already an LFS pointer",
            "+        pointer = LFSPointer.from_bytes(data)",
            "+        if pointer is not None:",
            "+            return data",
            "+",
            "+        # Store the file content in LFS",
            "+        sha = self.lfs_store.write_object([data])",
            "+",
            "+        # Create and return LFS pointer",
            "+        pointer = LFSPointer(sha, len(data))",
            "+        return pointer.to_bytes()",
            "+",
            "+    def smudge(self, data: bytes) -> bytes:",
            "+        \"\"\"Convert LFS pointer to file content (smudge filter).\"\"\"",
            "+        # Try to parse as LFS pointer",
            "+        pointer = LFSPointer.from_bytes(data)",
            "+        if pointer is None:",
            "+            # Not an LFS pointer, return as-is",
            "+            return data",
            "+",
            "+        # Validate the pointer",
            "+        if not pointer.is_valid_oid():",
            "+            return data",
            "+",
            "+        try:",
            "+            # Read the actual content from LFS store",
            "+            with self.lfs_store.open_object(pointer.oid) as f:",
            "+                return f.read()",
            "+        except KeyError:",
            "+            # Object not found in LFS store, return pointer as-is",
            "+            # This matches Git LFS behavior when object is missing",
            "+            return data"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/line_ending.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/line_ending.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/line_ending.py",
            "@@ -133,76 +133,114 @@",
            " Sources:",
            " - https://git-scm.com/docs/git-config#git-config-coreeol",
            " - https://git-scm.com/docs/git-config#git-config-coreautocrlf",
            " - https://git-scm.com/docs/gitattributes#_checking_out_and_checking_in",
            " - https://adaptivepatchwork.com/2012/03/01/mind-the-end-of-your-line/",
            " \"\"\"",
            " ",
            "+from typing import TYPE_CHECKING, Any, Callable, Optional, Union",
            "+",
            "+if TYPE_CHECKING:",
            "+    from .config import StackedConfig",
            "+    from .object_store import BaseObjectStore",
            "+",
            " from .object_store import iter_tree_contents",
            "-from .objects import Blob",
            "+from .objects import Blob, ObjectID",
            " from .patch import is_binary",
            " ",
            " CRLF = b\"\\r\\n\"",
            " LF = b\"\\n\"",
            " ",
            " ",
            "-def convert_crlf_to_lf(text_hunk):",
            "+def convert_crlf_to_lf(text_hunk: bytes) -> bytes:",
            "     \"\"\"Convert CRLF in text hunk into LF.",
            " ",
            "     Args:",
            "       text_hunk: A bytes string representing a text hunk",
            "     Returns: The text hunk with the same type, with CRLF replaced into LF",
            "     \"\"\"",
            "     return text_hunk.replace(CRLF, LF)",
            " ",
            " ",
            "-def convert_lf_to_crlf(text_hunk):",
            "+def convert_lf_to_crlf(text_hunk: bytes) -> bytes:",
            "     \"\"\"Convert LF in text hunk into CRLF.",
            " ",
            "     Args:",
            "       text_hunk: A bytes string representing a text hunk",
            "     Returns: The text hunk with the same type, with LF replaced into CRLF",
            "     \"\"\"",
            "-    # TODO find a more efficient way of doing it",
            "-    intermediary = text_hunk.replace(CRLF, LF)",
            "-    return intermediary.replace(LF, CRLF)",
            "+    # Single-pass conversion: split on LF and join with CRLF",
            "+    # This avoids the double replacement issue",
            "+    parts = text_hunk.split(LF)",
            "+    # Remove any trailing CR to avoid CRCRLF",
            "+    cleaned_parts = []",
            "+    for i, part in enumerate(parts):",
            "+        if i < len(parts) - 1 and part.endswith(b\"\\r\"):",
            "+            cleaned_parts.append(part[:-1])",
            "+        else:",
            "+            cleaned_parts.append(part)",
            "+    return CRLF.join(cleaned_parts)",
            " ",
            " ",
            "-def get_checkout_filter(core_eol, core_autocrlf, git_attributes):",
            "+def get_checkout_filter(",
            "+    core_eol: str, core_autocrlf: Union[bool, str], git_attributes: dict[str, Any]",
            "+) -> Optional[Callable[[bytes], bytes]]:",
            "     \"\"\"Returns the correct checkout filter based on the passed arguments.\"\"\"",
            "     # TODO this function should process the git_attributes for the path and if",
            "     # the text attribute is not defined, fallback on the",
            "     # get_checkout_filter_autocrlf function with the autocrlf value",
            "-    return get_checkout_filter_autocrlf(core_autocrlf)",
            "+    if isinstance(core_autocrlf, bool):",
            "+        autocrlf_bytes = b\"true\" if core_autocrlf else b\"false\"",
            "+    else:",
            "+        autocrlf_bytes = (",
            "+            core_autocrlf.encode(\"ascii\")",
            "+            if isinstance(core_autocrlf, str)",
            "+            else core_autocrlf",
            "+        )",
            "+    return get_checkout_filter_autocrlf(autocrlf_bytes)",
            " ",
            " ",
            "-def get_checkin_filter(core_eol, core_autocrlf, git_attributes):",
            "+def get_checkin_filter(",
            "+    core_eol: str, core_autocrlf: Union[bool, str], git_attributes: dict[str, Any]",
            "+) -> Optional[Callable[[bytes], bytes]]:",
            "     \"\"\"Returns the correct checkin filter based on the passed arguments.\"\"\"",
            "     # TODO this function should process the git_attributes for the path and if",
            "     # the text attribute is not defined, fallback on the",
            "     # get_checkin_filter_autocrlf function with the autocrlf value",
            "-    return get_checkin_filter_autocrlf(core_autocrlf)",
            "+    if isinstance(core_autocrlf, bool):",
            "+        autocrlf_bytes = b\"true\" if core_autocrlf else b\"false\"",
            "+    else:",
            "+        autocrlf_bytes = (",
            "+            core_autocrlf.encode(\"ascii\")",
            "+            if isinstance(core_autocrlf, str)",
            "+            else core_autocrlf",
            "+        )",
            "+    return get_checkin_filter_autocrlf(autocrlf_bytes)",
            " ",
            " ",
            "-def get_checkout_filter_autocrlf(core_autocrlf):",
            "+def get_checkout_filter_autocrlf(",
            "+    core_autocrlf: bytes,",
            "+) -> Optional[Callable[[bytes], bytes]]:",
            "     \"\"\"Returns the correct checkout filter base on autocrlf value.",
            " ",
            "     Args:",
            "       core_autocrlf: The bytes configuration value of core.autocrlf.",
            "         Valid values are: b'true', b'false' or b'input'.",
            "     Returns: Either None if no filter has to be applied or a function",
            "         accepting a single argument, a binary text hunk",
            "     \"\"\"",
            "     if core_autocrlf == b\"true\":",
            "         return convert_lf_to_crlf",
            " ",
            "     return None",
            " ",
            " ",
            "-def get_checkin_filter_autocrlf(core_autocrlf):",
            "+def get_checkin_filter_autocrlf(",
            "+    core_autocrlf: bytes,",
            "+) -> Optional[Callable[[bytes], bytes]]:",
            "     \"\"\"Returns the correct checkin filter base on autocrlf value.",
            " ",
            "     Args:",
            "       core_autocrlf: The bytes configuration value of core.autocrlf.",
            "         Valid values are: b'true', b'false' or b'input'.",
            "     Returns: Either None if no filter has to be applied or a function",
            "         accepting a single argument, a binary text hunk",
            "@@ -215,56 +253,71 @@",
            " ",
            " ",
            " class BlobNormalizer:",
            "     \"\"\"An object to store computation result of which filter to apply based",
            "     on configuration, gitattributes, path and operation (checkin or checkout).",
            "     \"\"\"",
            " ",
            "-    def __init__(self, config_stack, gitattributes) -> None:",
            "+    def __init__(",
            "+        self, config_stack: \"StackedConfig\", gitattributes: dict[str, Any]",
            "+    ) -> None:",
            "         self.config_stack = config_stack",
            "         self.gitattributes = gitattributes",
            " ",
            "         # Compute which filters we needs based on parameters",
            "         try:",
            "-            core_eol = config_stack.get(\"core\", \"eol\")",
            "+            core_eol_raw = config_stack.get(\"core\", \"eol\")",
            "+            core_eol: str = (",
            "+                core_eol_raw.decode(\"ascii\")",
            "+                if isinstance(core_eol_raw, bytes)",
            "+                else core_eol_raw",
            "+            )",
            "         except KeyError:",
            "             core_eol = \"native\"",
            " ",
            "         try:",
            "-            core_autocrlf = config_stack.get(\"core\", \"autocrlf\").lower()",
            "+            core_autocrlf_raw = config_stack.get(\"core\", \"autocrlf\")",
            "+            if isinstance(core_autocrlf_raw, bytes):",
            "+                core_autocrlf: Union[bool, str] = core_autocrlf_raw.decode(",
            "+                    \"ascii\"",
            "+                ).lower()",
            "+            else:",
            "+                core_autocrlf = core_autocrlf_raw.lower()",
            "         except KeyError:",
            "             core_autocrlf = False",
            " ",
            "         self.fallback_read_filter = get_checkout_filter(",
            "             core_eol, core_autocrlf, self.gitattributes",
            "         )",
            "         self.fallback_write_filter = get_checkin_filter(",
            "             core_eol, core_autocrlf, self.gitattributes",
            "         )",
            " ",
            "-    def checkin_normalize(self, blob, tree_path):",
            "+    def checkin_normalize(self, blob: Blob, tree_path: bytes) -> Blob:",
            "         \"\"\"Normalize a blob during a checkin operation.\"\"\"",
            "         if self.fallback_write_filter is not None:",
            "             return normalize_blob(",
            "                 blob, self.fallback_write_filter, binary_detection=True",
            "             )",
            " ",
            "         return blob",
            " ",
            "-    def checkout_normalize(self, blob, tree_path):",
            "+    def checkout_normalize(self, blob: Blob, tree_path: bytes) -> Blob:",
            "         \"\"\"Normalize a blob during a checkout operation.\"\"\"",
            "         if self.fallback_read_filter is not None:",
            "             return normalize_blob(",
            "                 blob, self.fallback_read_filter, binary_detection=True",
            "             )",
            " ",
            "         return blob",
            " ",
            " ",
            "-def normalize_blob(blob, conversion, binary_detection):",
            "+def normalize_blob(",
            "+    blob: Blob, conversion: Callable[[bytes], bytes], binary_detection: bool",
            "+) -> Blob:",
            "     \"\"\"Takes a blob as input returns either the original blob if",
            "     binary_detection is True and the blob content looks like binary, else",
            "     return a new blob with converted data.",
            "     \"\"\"",
            "     # Read the original blob",
            "     data = blob.data",
            " ",
            "@@ -281,24 +334,30 @@",
            "     new_blob = Blob()",
            "     new_blob.data = converted_data",
            " ",
            "     return new_blob",
            " ",
            " ",
            " class TreeBlobNormalizer(BlobNormalizer):",
            "-    def __init__(self, config_stack, git_attributes, object_store, tree=None) -> None:",
            "+    def __init__(",
            "+        self,",
            "+        config_stack: \"StackedConfig\",",
            "+        git_attributes: dict[str, Any],",
            "+        object_store: \"BaseObjectStore\",",
            "+        tree: Optional[ObjectID] = None,",
            "+    ) -> None:",
            "         super().__init__(config_stack, git_attributes)",
            "         if tree:",
            "             self.existing_paths = {",
            "                 name for name, _, _ in iter_tree_contents(object_store, tree)",
            "             }",
            "         else:",
            "             self.existing_paths = set()",
            " ",
            "-    def checkin_normalize(self, blob, tree_path):",
            "+    def checkin_normalize(self, blob: Blob, tree_path: bytes) -> Blob:",
            "         # Existing files should only be normalized on checkin if it was",
            "         # previously normalized on checkout",
            "         if (",
            "             self.fallback_read_filter is not None",
            "             or tree_path not in self.existing_paths",
            "         ):",
            "             return super().checkin_normalize(blob, tree_path)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/log_utils.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/log_utils.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/log_utils.py",
            "@@ -41,15 +41,15 @@",
            " ",
            " getLogger = logging.getLogger",
            " ",
            " ",
            " class _NullHandler(logging.Handler):",
            "     \"\"\"No-op logging handler to avoid unexpected logging warnings.\"\"\"",
            " ",
            "-    def emit(self, record) -> None:",
            "+    def emit(self, record: logging.LogRecord) -> None:",
            "         pass",
            " ",
            " ",
            " _NULL_HANDLER = _NullHandler()",
            " _DULWICH_LOGGER = getLogger(\"dulwich\")",
            " _DULWICH_LOGGER.addHandler(_NULL_HANDLER)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/lru_cache.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/lru_cache.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/lru_cache.py",
            "@@ -19,35 +19,37 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"A simple least-recently-used (LRU) cache.\"\"\"",
            " ",
            " from collections.abc import Iterable, Iterator",
            "-from typing import Callable, Generic, Optional, TypeVar",
            "+from typing import Callable, Generic, Optional, TypeVar, Union, cast",
            " ",
            " _null_key = object()",
            " ",
            " ",
            " K = TypeVar(\"K\")",
            " V = TypeVar(\"V\")",
            " ",
            " ",
            " class _LRUNode(Generic[K, V]):",
            "     \"\"\"This maintains the linked-list which is the lru internals.\"\"\"",
            " ",
            "     __slots__ = (\"cleanup\", \"key\", \"next_key\", \"prev\", \"size\", \"value\")",
            " ",
            "     prev: Optional[\"_LRUNode[K, V]\"]",
            "-    next_key: K",
            "+    next_key: Union[K, object]",
            "     size: Optional[int]",
            " ",
            "-    def __init__(self, key: K, value: V, cleanup=None) -> None:",
            "+    def __init__(",
            "+        self, key: K, value: V, cleanup: Optional[Callable[[K, V], None]] = None",
            "+    ) -> None:",
            "         self.prev = None",
            "-        self.next_key = _null_key  # type: ignore",
            "+        self.next_key = _null_key",
            "         self.key = key",
            "         self.value = value",
            "         self.cleanup = cleanup",
            "         # TODO: We could compute this 'on-the-fly' like we used to, and remove",
            "         #       one pointer from this object, we just need to decide if it",
            "         #       actually costs us much of anything in normal usage",
            "         self.size = None",
            "@@ -103,15 +105,15 @@",
            "         # benchmarking shows that the lookup of _null_key in globals is faster",
            "         # than the attribute lookup for (node is self._least_recently_used)",
            "         if next_key is _null_key:",
            "             # 'node' is the _least_recently_used, because it doesn't have a",
            "             # 'next' item. So move the current lru to the previous node.",
            "             self._least_recently_used = node_prev",
            "         else:",
            "-            node_next = cache[next_key]",
            "+            node_next = cache[cast(K, next_key)]",
            "             node_next.prev = node_prev",
            "         assert node_prev",
            "         assert mru",
            "         node_prev.next_key = next_key",
            "         # Insert this node at the front of the list",
            "         node.next_key = mru.key",
            "         mru.prev = node",
            "@@ -132,33 +134,33 @@",
            "                     \" supposed to have a previous entry\"",
            "                     f\" {node}\"",
            "                 )",
            "         while node is not None:",
            "             if node.next_key is _null_key:",
            "                 if node is not self._least_recently_used:",
            "                     raise AssertionError(",
            "-                        \"only the last node should have\" f\" no next value: {node}\"",
            "+                        f\"only the last node should have no next value: {node}\"",
            "                     )",
            "                 node_next = None",
            "             else:",
            "-                node_next = self._cache[node.next_key]",
            "+                node_next = self._cache[cast(K, node.next_key)]",
            "                 if node_next.prev is not node:",
            "                     raise AssertionError(",
            "-                        \"inconsistency found, node.next.prev\" f\" != node: {node}\"",
            "+                        f\"inconsistency found, node.next.prev != node: {node}\"",
            "                     )",
            "             if node.prev is None:",
            "                 if node is not self._most_recently_used:",
            "                     raise AssertionError(",
            "                         \"only the _most_recently_used should\"",
            "                         f\" not have a previous node: {node}\"",
            "                     )",
            "             else:",
            "                 if node.prev.next_key != node.key:",
            "                     raise AssertionError(",
            "-                        \"inconsistency found, node.prev.next\" f\" != node: {node}\"",
            "+                        f\"inconsistency found, node.prev.next != node: {node}\"",
            "                     )",
            "             yield node",
            "             node = node_next",
            " ",
            "     def add(",
            "         self, key: K, value: V, cleanup: Optional[Callable[[K, V], None]] = None",
            "     ) -> None:",
            "@@ -243,15 +245,15 @@",
            "         # at the front",
            "         # REMOVE",
            "         if node is self._least_recently_used:",
            "             self._least_recently_used = node.prev",
            "         if node.prev is not None:",
            "             node.prev.next_key = node.next_key",
            "         if node.next_key is not _null_key:",
            "-            node_next = self._cache[node.next_key]",
            "+            node_next = self._cache[cast(K, node.next_key)]",
            "             node_next.prev = node.prev",
            "         # INSERT",
            "         node.next_key = self._most_recently_used.key",
            "         self._most_recently_used.prev = node",
            "         self._most_recently_used = node",
            "         node.prev = None",
            " ",
            "@@ -263,19 +265,19 @@",
            "         if self._least_recently_used is None:",
            "             self._most_recently_used = None",
            "         node.run_cleanup()",
            "         # Now remove this node from the linked list",
            "         if node.prev is not None:",
            "             node.prev.next_key = node.next_key",
            "         if node.next_key is not _null_key:",
            "-            node_next = self._cache[node.next_key]",
            "+            node_next = self._cache[cast(K, node.next_key)]",
            "             node_next.prev = node.prev",
            "         # And remove this node's pointers",
            "         node.prev = None",
            "-        node.next_key = _null_key  # type: ignore",
            "+        node.next_key = _null_key",
            " ",
            "     def _remove_lru(self) -> None:",
            "         \"\"\"Remove one entry from the lru, and handle consequences.",
            " ",
            "         If there are no more references to the lru, then this entry should be",
            "         removed from the cache.",
            "         \"\"\"",
            "@@ -288,15 +290,17 @@",
            "         while self._cache:",
            "             self._remove_lru()",
            " ",
            "     def resize(self, max_cache: int, after_cleanup_count: Optional[int] = None) -> None:",
            "         \"\"\"Change the number of entries that will be cached.\"\"\"",
            "         self._update_max_cache(max_cache, after_cleanup_count=after_cleanup_count)",
            " ",
            "-    def _update_max_cache(self, max_cache, after_cleanup_count=None) -> None:",
            "+    def _update_max_cache(",
            "+        self, max_cache: int, after_cleanup_count: Optional[int] = None",
            "+    ) -> None:",
            "         self._max_cache = max_cache",
            "         if after_cleanup_count is None:",
            "             self._after_cleanup_count = self._max_cache * 8 / 10",
            "         else:",
            "             self._after_cleanup_count = min(after_cleanup_count, self._max_cache)",
            "         self.cleanup()",
            " ",
            "@@ -331,15 +335,15 @@",
            "             using simple strings, or a more complex function if you are using",
            "             something like a list of strings, or even a custom object.",
            "             The function should take the form \"compute_size(value) => integer\".",
            "             If not supplied, it defaults to 'len()'",
            "         \"\"\"",
            "         self._value_size = 0",
            "         if compute_size is None:",
            "-            self._compute_size = len  # type: ignore",
            "+            self._compute_size = cast(Callable[[V], int], len)",
            "         else:",
            "             self._compute_size = compute_size",
            "         self._update_max_size(max_size, after_cleanup_size=after_cleanup_size)",
            "         LRUCache.__init__(self, max_cache=max(int(max_size / 512), 1))",
            " ",
            "     def add(",
            "         self, key: K, value: V, cleanup: Optional[Callable[[K, V], None]] = None"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/mailmap.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/mailmap.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/mailmap.py",
            "@@ -17,31 +17,37 @@",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Mailmap file reader.\"\"\"",
            " ",
            "-from typing import Optional",
            "+from collections.abc import Iterator",
            "+from typing import IO, Optional, Union",
            " ",
            " ",
            "-def parse_identity(text):",
            "+def parse_identity(text: bytes) -> tuple[Optional[bytes], Optional[bytes]]:",
            "     # TODO(jelmer): Integrate this with dulwich.fastexport.split_email and",
            "     # dulwich.repo.check_user_identity",
            "-    (name, email) = text.rsplit(b\"<\", 1)",
            "-    name = name.strip()",
            "-    email = email.rstrip(b\">\").strip()",
            "-    if not name:",
            "-        name = None",
            "-    if not email:",
            "-        email = None",
            "+    (name_str, email_str) = text.rsplit(b\"<\", 1)",
            "+    name_str = name_str.strip()",
            "+    email_str = email_str.rstrip(b\">\").strip()",
            "+    name: Optional[bytes] = name_str if name_str else None",
            "+    email: Optional[bytes] = email_str if email_str else None",
            "     return (name, email)",
            " ",
            " ",
            "-def read_mailmap(f):",
            "+def read_mailmap(",
            "+    f: IO[bytes],",
            "+) -> Iterator[",
            "+    tuple[",
            "+        tuple[Optional[bytes], Optional[bytes]],",
            "+        Optional[tuple[Optional[bytes], Optional[bytes]]],",
            "+    ]",
            "+]:",
            "     \"\"\"Read a mailmap.",
            " ",
            "     Args:",
            "       f: File-like object to read from",
            "     Returns: Iterator over",
            "         ((canonical_name, canonical_email), (from_name, from_email)) tuples",
            "     \"\"\"",
            "@@ -60,21 +66,38 @@",
            "         parsed_canonical_identity = parse_identity(canonical_identity)",
            "         yield parsed_canonical_identity, parsed_from_identity",
            " ",
            " ",
            " class Mailmap:",
            "     \"\"\"Class for accessing a mailmap file.\"\"\"",
            " ",
            "-    def __init__(self, map=None) -> None:",
            "-        self._table: dict[tuple[Optional[str], Optional[str]], tuple[str, str]] = {}",
            "+    def __init__(",
            "+        self,",
            "+        map: Optional[",
            "+            Iterator[",
            "+                tuple[",
            "+                    tuple[Optional[bytes], Optional[bytes]],",
            "+                    Optional[tuple[Optional[bytes], Optional[bytes]]],",
            "+                ]",
            "+            ]",
            "+        ] = None,",
            "+    ) -> None:",
            "+        self._table: dict[",
            "+            tuple[Optional[bytes], Optional[bytes]],",
            "+            tuple[Optional[bytes], Optional[bytes]],",
            "+        ] = {}",
            "         if map:",
            "             for canonical_identity, from_identity in map:",
            "                 self.add_entry(canonical_identity, from_identity)",
            " ",
            "-    def add_entry(self, canonical_identity, from_identity=None) -> None:",
            "+    def add_entry(",
            "+        self,",
            "+        canonical_identity: tuple[Optional[bytes], Optional[bytes]],",
            "+        from_identity: Optional[tuple[Optional[bytes], Optional[bytes]]] = None,",
            "+    ) -> None:",
            "         \"\"\"Add an entry to the mail mail.",
            " ",
            "         Any of the fields can be None, but at least one of them needs to be",
            "         set.",
            " ",
            "         Args:",
            "           canonical_identity: The canonical identity (tuple)",
            "@@ -87,15 +110,17 @@",
            "         (canonical_name, canonical_email) = canonical_identity",
            "         if from_name is None and from_email is None:",
            "             self._table[canonical_name, None] = canonical_identity",
            "             self._table[None, canonical_email] = canonical_identity",
            "         else:",
            "             self._table[from_name, from_email] = canonical_identity",
            " ",
            "-    def lookup(self, identity):",
            "+    def lookup(",
            "+        self, identity: Union[bytes, tuple[Optional[bytes], Optional[bytes]]]",
            "+    ) -> Union[bytes, tuple[Optional[bytes], Optional[bytes]]]:",
            "         \"\"\"Lookup an identity in this mailmail.\"\"\"",
            "         if not isinstance(identity, tuple):",
            "             was_tuple = False",
            "             identity = parse_identity(identity)",
            "         else:",
            "             was_tuple = True",
            "         for query in [identity, (None, identity[1]), (identity[0], None)]:",
            "@@ -105,13 +130,18 @@",
            "                     canonical_identity[0] or identity[0],",
            "                     canonical_identity[1] or identity[1],",
            "                 )",
            "                 break",
            "         if was_tuple:",
            "             return identity",
            "         else:",
            "-            return identity[0] + b\" <\" + identity[1] + b\">\"",
            "+            name, email = identity",
            "+            if name is None:",
            "+                name = b\"\"",
            "+            if email is None:",
            "+                email = b\"\"",
            "+            return name + b\" <\" + email + b\">\"",
            " ",
            "     @classmethod",
            "-    def from_path(cls, path):",
            "+    def from_path(cls, path: str) -> \"Mailmap\":",
            "         with open(path, \"rb\") as f:",
            "             return cls(read_mailmap(f))"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/object_store.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/object_store.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/object_store.py",
            "@@ -23,22 +23,24 @@",
            " ",
            " \"\"\"Git object store interfaces and implementation.\"\"\"",
            " ",
            " import binascii",
            " import os",
            " import stat",
            " import sys",
            "+import time",
            " import warnings",
            " from collections.abc import Iterable, Iterator, Sequence",
            " from contextlib import suppress",
            " from io import BytesIO",
            " from typing import (",
            "     Callable,",
            "     Optional,",
            "     Protocol,",
            "+    Union,",
            "     cast,",
            " )",
            " ",
            " from .errors import NotTreeError",
            " from .file import GitFile",
            " from .objects import (",
            "     S_ISGITLINK,",
            "@@ -84,14 +86,95 @@",
            " PACKDIR = \"pack\"",
            " ",
            " # use permissions consistent with Git; just readable by everyone",
            " # TODO: should packs also be non-writable on Windows? if so, that",
            " # would requite some rather significant adjustments to the test suite",
            " PACK_MODE = 0o444 if sys.platform != \"win32\" else 0o644",
            " ",
            "+# Grace period for cleaning up temporary pack files (in seconds)",
            "+# Matches git's default of 2 weeks",
            "+DEFAULT_TEMPFILE_GRACE_PERIOD = 14 * 24 * 60 * 60  # 2 weeks",
            "+",
            "+",
            "+def find_shallow(store, heads, depth):",
            "+    \"\"\"Find shallow commits according to a given depth.",
            "+",
            "+    Args:",
            "+      store: An ObjectStore for looking up objects.",
            "+      heads: Iterable of head SHAs to start walking from.",
            "+      depth: The depth of ancestors to include. A depth of one includes",
            "+        only the heads themselves.",
            "+    Returns: A tuple of (shallow, not_shallow), sets of SHAs that should be",
            "+        considered shallow and unshallow according to the arguments. Note that",
            "+        these sets may overlap if a commit is reachable along multiple paths.",
            "+    \"\"\"",
            "+    parents = {}",
            "+",
            "+    def get_parents(sha):",
            "+        result = parents.get(sha, None)",
            "+        if not result:",
            "+            result = store[sha].parents",
            "+            parents[sha] = result",
            "+        return result",
            "+",
            "+    todo = []  # stack of (sha, depth)",
            "+    for head_sha in heads:",
            "+        obj = store[head_sha]",
            "+        # Peel tags if necessary",
            "+        while isinstance(obj, Tag):",
            "+            _, sha = obj.object",
            "+            obj = store[sha]",
            "+        if isinstance(obj, Commit):",
            "+            todo.append((obj.id, 1))",
            "+",
            "+    not_shallow = set()",
            "+    shallow = set()",
            "+    while todo:",
            "+        sha, cur_depth = todo.pop()",
            "+        if cur_depth < depth:",
            "+            not_shallow.add(sha)",
            "+            new_depth = cur_depth + 1",
            "+            todo.extend((p, new_depth) for p in get_parents(sha))",
            "+        else:",
            "+            shallow.add(sha)",
            "+",
            "+    return shallow, not_shallow",
            "+",
            "+",
            "+def get_depth(",
            "+    store,",
            "+    head,",
            "+    get_parents=lambda commit: commit.parents,",
            "+    max_depth=None,",
            "+):",
            "+    \"\"\"Return the current available depth for the given head.",
            "+    For commits with multiple parents, the largest possible depth will be",
            "+    returned.",
            "+",
            "+    Args:",
            "+        head: commit to start from",
            "+        get_parents: optional function for getting the parents of a commit",
            "+        max_depth: maximum depth to search",
            "+    \"\"\"",
            "+    if head not in store:",
            "+        return 0",
            "+    current_depth = 1",
            "+    queue = [(head, current_depth)]",
            "+    while queue and (max_depth is None or current_depth < max_depth):",
            "+        e, depth = queue.pop(0)",
            "+        current_depth = max(current_depth, depth)",
            "+        cmt = store[e]",
            "+        if isinstance(cmt, Tag):",
            "+            _cls, sha = cmt.object",
            "+            cmt = store[sha]",
            "+        queue.extend(",
            "+            (parent, depth + 1) for parent in get_parents(cmt) if parent in store",
            "+        )",
            "+    return current_depth",
            "+",
            " ",
            " class PackContainer(Protocol):",
            "     def add_pack(self) -> tuple[BytesIO, Callable[[], None], Callable[[], None]]:",
            "         \"\"\"Add a new pack.\"\"\"",
            " ",
            " ",
            " class BaseObjectStore:",
            "@@ -329,63 +412,99 @@",
            "         returned.",
            " ",
            "         Args:",
            "             head: commit to start from",
            "             get_parents: optional function for getting the parents of a commit",
            "             max_depth: maximum depth to search",
            "         \"\"\"",
            "-        if head not in self:",
            "-            return 0",
            "-        current_depth = 1",
            "-        queue = [(head, current_depth)]",
            "-        while queue and (max_depth is None or current_depth < max_depth):",
            "-            e, depth = queue.pop(0)",
            "-            current_depth = max(current_depth, depth)",
            "-            cmt = self[e]",
            "-            if isinstance(cmt, Tag):",
            "-                _cls, sha = cmt.object",
            "-                cmt = self[sha]",
            "-            queue.extend(",
            "-                (parent, depth + 1) for parent in get_parents(cmt) if parent in self",
            "-            )",
            "-        return current_depth",
            "+        return get_depth(self, head, get_parents=get_parents, max_depth=max_depth)",
            " ",
            "     def close(self) -> None:",
            "         \"\"\"Close any files opened by this object store.\"\"\"",
            "         # Default implementation is a NO-OP",
            " ",
            "+    def prune(self, grace_period: Optional[int] = None) -> None:",
            "+        \"\"\"Prune/clean up this object store.",
            "+",
            "+        This includes removing orphaned temporary files and other",
            "+        housekeeping tasks. Default implementation is a NO-OP.",
            "+",
            "+        Args:",
            "+          grace_period: Grace period in seconds for removing temporary files.",
            "+                       If None, uses the default grace period.",
            "+        \"\"\"",
            "+        # Default implementation is a NO-OP",
            "+",
            "     def iter_prefix(self, prefix: bytes) -> Iterator[ObjectID]:",
            "         \"\"\"Iterate over all SHA1s that start with a given prefix.",
            " ",
            "         The default implementation is a naive iteration over all objects.",
            "         However, subclasses may override this method with more efficient",
            "         implementations.",
            "         \"\"\"",
            "         for sha in self:",
            "             if sha.startswith(prefix):",
            "                 yield sha",
            " ",
            "+    def get_commit_graph(self):",
            "+        \"\"\"Get the commit graph for this object store.",
            "+",
            "+        Returns:",
            "+          CommitGraph object if available, None otherwise",
            "+        \"\"\"",
            "+        return None",
            "+",
            "+    def write_commit_graph(self, refs=None, reachable=True) -> None:",
            "+        \"\"\"Write a commit graph file for this object store.",
            "+",
            "+        Args:",
            "+            refs: List of refs to include. If None, includes all refs from object store.",
            "+            reachable: If True, includes all commits reachable from refs.",
            "+                      If False, only includes the direct ref targets.",
            "+",
            "+        Note:",
            "+            Default implementation does nothing. Subclasses should override",
            "+            this method to provide commit graph writing functionality.",
            "+        \"\"\"",
            "+        raise NotImplementedError(self.write_commit_graph)",
            "+",
            "+    def get_object_mtime(self, sha):",
            "+        \"\"\"Get the modification time of an object.",
            "+",
            "+        Args:",
            "+          sha: SHA1 of the object",
            "+",
            "+        Returns:",
            "+          Modification time as seconds since epoch",
            "+",
            "+        Raises:",
            "+          KeyError: if the object is not found",
            "+        \"\"\"",
            "+        # Default implementation raises KeyError",
            "+        # Subclasses should override to provide actual mtime",
            "+        raise KeyError(sha)",
            "+",
            " ",
            "-class PackBasedObjectStore(BaseObjectStore):",
            "-    def __init__(self, pack_compression_level=-1) -> None:",
            "+class PackBasedObjectStore(BaseObjectStore, PackedObjectContainer):",
            "+    def __init__(self, pack_compression_level=-1, pack_index_version=None) -> None:",
            "         self._pack_cache: dict[str, Pack] = {}",
            "         self.pack_compression_level = pack_compression_level",
            "+        self.pack_index_version = pack_index_version",
            " ",
            "     def add_pack(self) -> tuple[BytesIO, Callable[[], None], Callable[[], None]]:",
            "         \"\"\"Add a new pack to this object store.\"\"\"",
            "         raise NotImplementedError(self.add_pack)",
            " ",
            "     def add_pack_data(",
            "         self, count: int, unpacked_objects: Iterator[UnpackedObject], progress=None",
            "     ) -> None:",
            "         \"\"\"Add pack data to this object store.",
            " ",
            "         Args:",
            "           count: Number of items to add",
            "-          pack_data: Iterator over pack data tuples",
            "         \"\"\"",
            "         if count == 0:",
            "             # Don't bother writing an empty pack file",
            "             return",
            "         f, commit, abort = self.add_pack()",
            "         try:",
            "             write_pack_data(",
            "@@ -480,68 +599,105 @@",
            "         self._clear_cached_packs()",
            " ",
            "     @property",
            "     def packs(self):",
            "         \"\"\"List with pack objects.\"\"\"",
            "         return list(self._iter_cached_packs()) + list(self._update_pack_cache())",
            " ",
            "+    def count_pack_files(self) -> int:",
            "+        \"\"\"Count the number of pack files.",
            "+",
            "+        Returns:",
            "+            Number of pack files (excluding those with .keep files)",
            "+        \"\"\"",
            "+        count = 0",
            "+        for pack in self.packs:",
            "+            # Check if there's a .keep file for this pack",
            "+            keep_path = pack._basename + \".keep\"",
            "+            if not os.path.exists(keep_path):",
            "+                count += 1",
            "+        return count",
            "+",
            "     def _iter_alternate_objects(self):",
            "         \"\"\"Iterate over the SHAs of all the objects in alternate stores.\"\"\"",
            "         for alternate in self.alternates:",
            "             yield from alternate",
            " ",
            "     def _iter_loose_objects(self):",
            "         \"\"\"Iterate over the SHAs of all loose objects.\"\"\"",
            "         raise NotImplementedError(self._iter_loose_objects)",
            " ",
            "     def _get_loose_object(self, sha) -> Optional[ShaFile]:",
            "         raise NotImplementedError(self._get_loose_object)",
            " ",
            "-    def _remove_loose_object(self, sha) -> None:",
            "-        raise NotImplementedError(self._remove_loose_object)",
            "+    def delete_loose_object(self, sha) -> None:",
            "+        \"\"\"Delete a loose object.",
            "+",
            "+        This method only handles loose objects. For packed objects,",
            "+        use repack(exclude=...) to exclude them during repacking.",
            "+        \"\"\"",
            "+        raise NotImplementedError(self.delete_loose_object)",
            " ",
            "     def _remove_pack(self, name) -> None:",
            "         raise NotImplementedError(self._remove_pack)",
            " ",
            "     def pack_loose_objects(self):",
            "         \"\"\"Pack loose objects.",
            " ",
            "         Returns: Number of objects packed",
            "         \"\"\"",
            "         objects = set()",
            "         for sha in self._iter_loose_objects():",
            "             objects.add((self._get_loose_object(sha), None))",
            "         self.add_objects(list(objects))",
            "         for obj, path in objects:",
            "-            self._remove_loose_object(obj.id)",
            "+            self.delete_loose_object(obj.id)",
            "         return len(objects)",
            " ",
            "-    def repack(self):",
            "+    def repack(self, exclude=None):",
            "         \"\"\"Repack the packs in this repository.",
            " ",
            "         Note that this implementation is fairly naive and currently keeps all",
            "         objects in memory while it repacks.",
            "+",
            "+        Args:",
            "+          exclude: Optional set of object SHAs to exclude from repacking",
            "         \"\"\"",
            "+        if exclude is None:",
            "+            exclude = set()",
            "+",
            "         loose_objects = set()",
            "+        excluded_loose_objects = set()",
            "         for sha in self._iter_loose_objects():",
            "-            loose_objects.add(self._get_loose_object(sha))",
            "+            if sha not in exclude:",
            "+                loose_objects.add(self._get_loose_object(sha))",
            "+            else:",
            "+                excluded_loose_objects.add(sha)",
            "+",
            "         objects = {(obj, None) for obj in loose_objects}",
            "         old_packs = {p.name(): p for p in self.packs}",
            "         for name, pack in old_packs.items():",
            "-            objects.update((obj, None) for obj in pack.iterobjects())",
            "-",
            "-        # The name of the consolidated pack might match the name of a",
            "-        # pre-existing pack. Take care not to remove the newly created",
            "-        # consolidated pack.",
            "+            objects.update(",
            "+                (obj, None) for obj in pack.iterobjects() if obj.id not in exclude",
            "+            )",
            " ",
            "-        consolidated = self.add_objects(objects)",
            "-        old_packs.pop(consolidated.name(), None)",
            "+        # Only create a new pack if there are objects to pack",
            "+        if objects:",
            "+            # The name of the consolidated pack might match the name of a",
            "+            # pre-existing pack. Take care not to remove the newly created",
            "+            # consolidated pack.",
            "+            consolidated = self.add_objects(objects)",
            "+            old_packs.pop(consolidated.name(), None)",
            " ",
            "+        # Delete loose objects that were packed",
            "         for obj in loose_objects:",
            "-            self._remove_loose_object(obj.id)",
            "+            self.delete_loose_object(obj.id)",
            "+        # Delete excluded loose objects",
            "+        for sha in excluded_loose_objects:",
            "+            self.delete_loose_object(sha)",
            "         for name, pack in old_packs.items():",
            "             self._remove_pack(pack)",
            "         self._update_pack_cache()",
            "         return len(objects)",
            " ",
            "     def __iter__(self):",
            "         \"\"\"Iterate over the SHAs that are present in this store.\"\"\"",
            "@@ -600,20 +756,19 @@",
            "                 return alternate.get_raw(hexsha)",
            "             except KeyError:",
            "                 pass",
            "         raise KeyError(hexsha)",
            " ",
            "     def iter_unpacked_subset(",
            "         self,",
            "-        shas,",
            "-        *,",
            "-        include_comp=False,",
            "+        shas: set[bytes],",
            "+        include_comp: bool = False,",
            "         allow_missing: bool = False,",
            "         convert_ofs_delta: bool = True,",
            "-    ) -> Iterator[ShaFile]:",
            "+    ) -> Iterator[UnpackedObject]:",
            "         todo: set[bytes] = set(shas)",
            "         for p in self._iter_cached_packs():",
            "             for unpacked in p.iter_unpacked_subset(",
            "                 todo,",
            "                 include_comp=include_comp,",
            "                 allow_missing=True,",
            "                 convert_ofs_delta=convert_ofs_delta,",
            "@@ -724,36 +879,51 @@",
            "         record_iter = (full_unpacked_object(o) for (o, p) in objects)",
            "         return self.add_pack_data(count, record_iter, progress=progress)",
            " ",
            " ",
            " class DiskObjectStore(PackBasedObjectStore):",
            "     \"\"\"Git-style object store that exists on disk.\"\"\"",
            " ",
            "+    path: Union[str, os.PathLike]",
            "+    pack_dir: Union[str, os.PathLike]",
            "+",
            "     def __init__(",
            "-        self, path, loose_compression_level=-1, pack_compression_level=-1",
            "+        self,",
            "+        path: Union[str, os.PathLike],",
            "+        loose_compression_level=-1,",
            "+        pack_compression_level=-1,",
            "+        pack_index_version=None,",
            "     ) -> None:",
            "         \"\"\"Open an object store.",
            " ",
            "         Args:",
            "           path: Path of the object store.",
            "           loose_compression_level: zlib compression level for loose objects",
            "           pack_compression_level: zlib compression level for pack objects",
            "+          pack_index_version: pack index version to use (1, 2, or 3)",
            "         \"\"\"",
            "-        super().__init__(pack_compression_level=pack_compression_level)",
            "+        super().__init__(",
            "+            pack_compression_level=pack_compression_level,",
            "+            pack_index_version=pack_index_version,",
            "+        )",
            "         self.path = path",
            "         self.pack_dir = os.path.join(self.path, PACKDIR)",
            "         self._alternates = None",
            "         self.loose_compression_level = loose_compression_level",
            "         self.pack_compression_level = pack_compression_level",
            "+        self.pack_index_version = pack_index_version",
            "+",
            "+        # Commit graph support - lazy loaded",
            "+        self._commit_graph = None",
            " ",
            "     def __repr__(self) -> str:",
            "         return f\"<{self.__class__.__name__}({self.path!r})>\"",
            " ",
            "     @classmethod",
            "-    def from_config(cls, path, config):",
            "+    def from_config(cls, path: Union[str, os.PathLike], config):",
            "         try:",
            "             default_compression_level = int(",
            "                 config.get((b\"core\",), b\"compression\").decode()",
            "             )",
            "         except KeyError:",
            "             default_compression_level = -1",
            "         try:",
            "@@ -764,15 +934,21 @@",
            "             loose_compression_level = default_compression_level",
            "         try:",
            "             pack_compression_level = int(",
            "                 config.get((b\"core\",), \"packCompression\").decode()",
            "             )",
            "         except KeyError:",
            "             pack_compression_level = default_compression_level",
            "-        return cls(path, loose_compression_level, pack_compression_level)",
            "+        try:",
            "+            pack_index_version = int(config.get((b\"pack\",), b\"indexVersion\").decode())",
            "+        except KeyError:",
            "+            pack_index_version = None",
            "+        return cls(",
            "+            path, loose_compression_level, pack_compression_level, pack_index_version",
            "+        )",
            " ",
            "     @property",
            "     def alternates(self):",
            "         if self._alternates is not None:",
            "             return self._alternates",
            "         self._alternates = []",
            "         for path in self._read_alternate_paths():",
            "@@ -854,24 +1030,85 @@",
            "                 continue",
            "             for rest in os.listdir(os.path.join(self.path, base)):",
            "                 sha = os.fsencode(base + rest)",
            "                 if not valid_hexsha(sha):",
            "                     continue",
            "                 yield sha",
            " ",
            "+    def count_loose_objects(self) -> int:",
            "+        \"\"\"Count the number of loose objects in the object store.",
            "+",
            "+        Returns:",
            "+            Number of loose objects",
            "+        \"\"\"",
            "+        count = 0",
            "+        if not os.path.exists(self.path):",
            "+            return 0",
            "+",
            "+        for i in range(256):",
            "+            subdir = os.path.join(self.path, f\"{i:02x}\")",
            "+            try:",
            "+                count += len(",
            "+                    [",
            "+                        name",
            "+                        for name in os.listdir(subdir)",
            "+                        if len(name) == 38  # 40 - 2 for the prefix",
            "+                    ]",
            "+                )",
            "+            except FileNotFoundError:",
            "+                # Directory may have been removed or is inaccessible",
            "+                continue",
            "+",
            "+        return count",
            "+",
            "     def _get_loose_object(self, sha):",
            "         path = self._get_shafile_path(sha)",
            "         try:",
            "             return ShaFile.from_path(path)",
            "         except FileNotFoundError:",
            "             return None",
            " ",
            "-    def _remove_loose_object(self, sha) -> None:",
            "+    def delete_loose_object(self, sha) -> None:",
            "         os.remove(self._get_shafile_path(sha))",
            " ",
            "+    def get_object_mtime(self, sha):",
            "+        \"\"\"Get the modification time of an object.",
            "+",
            "+        Args:",
            "+          sha: SHA1 of the object",
            "+",
            "+        Returns:",
            "+          Modification time as seconds since epoch",
            "+",
            "+        Raises:",
            "+          KeyError: if the object is not found",
            "+        \"\"\"",
            "+        # First check if it's a loose object",
            "+        if self.contains_loose(sha):",
            "+            path = self._get_shafile_path(sha)",
            "+            try:",
            "+                return os.path.getmtime(path)",
            "+            except FileNotFoundError:",
            "+                pass",
            "+",
            "+        # Check if it's in a pack file",
            "+        for pack in self.packs:",
            "+            try:",
            "+                if sha in pack:",
            "+                    # Use the pack file's mtime for packed objects",
            "+                    pack_path = pack._data_path",
            "+                    try:",
            "+                        return os.path.getmtime(pack_path)",
            "+                    except (FileNotFoundError, AttributeError):",
            "+                        pass",
            "+            except PackFileDisappeared:",
            "+                pass",
            "+",
            "+        raise KeyError(sha)",
            "+",
            "     def _remove_pack(self, pack) -> None:",
            "         try:",
            "             del self._pack_cache[os.path.basename(pack._basename)]",
            "         except KeyError:",
            "             pass",
            "         pack.close()",
            "         os.remove(pack.data.path)",
            "@@ -933,15 +1170,17 @@",
            "             # removal, silently passing if the target does not exist.",
            "             with suppress(FileNotFoundError):",
            "                 os.remove(target_pack_path)",
            "         os.rename(path, target_pack_path)",
            " ",
            "         # Write the index.",
            "         with GitFile(target_index_path, \"wb\", mask=PACK_MODE) as index_file:",
            "-            write_pack_index(index_file, entries, pack_sha)",
            "+            write_pack_index(",
            "+                index_file, entries, pack_sha, version=self.pack_index_version",
            "+            )",
            " ",
            "         # Add the pack to the store and return it.",
            "         final_pack = Pack(pack_base_name)",
            "         final_pack.check_length_and_checksum()",
            "         self._add_cached_pack(pack_base_name, final_pack)",
            "         return final_pack",
            " ",
            "@@ -1018,15 +1257,15 @@",
            "             return  # Already there, no need to write again",
            "         with GitFile(path, \"wb\", mask=PACK_MODE) as f:",
            "             f.write(",
            "                 obj.as_legacy_object(compression_level=self.loose_compression_level)",
            "             )",
            " ",
            "     @classmethod",
            "-    def init(cls, path):",
            "+    def init(cls, path: Union[str, os.PathLike]):",
            "         try:",
            "             os.mkdir(path)",
            "         except FileExistsError:",
            "             pass",
            "         os.mkdir(os.path.join(path, \"info\"))",
            "         os.mkdir(os.path.join(path, PACKDIR))",
            "         return cls(path)",
            "@@ -1061,14 +1300,146 @@",
            "                     yield sha",
            "         for alternate in self.alternates:",
            "             for sha in alternate.iter_prefix(prefix):",
            "                 if sha not in seen:",
            "                     seen.add(sha)",
            "                     yield sha",
            " ",
            "+    def get_commit_graph(self):",
            "+        \"\"\"Get the commit graph for this object store.",
            "+",
            "+        Returns:",
            "+          CommitGraph object if available, None otherwise",
            "+        \"\"\"",
            "+        if self._commit_graph is None:",
            "+            from .commit_graph import read_commit_graph",
            "+",
            "+            # Look for commit graph in our objects directory",
            "+            graph_file = os.path.join(self.path, \"info\", \"commit-graph\")",
            "+            if os.path.exists(graph_file):",
            "+                self._commit_graph = read_commit_graph(graph_file)",
            "+        return self._commit_graph",
            "+",
            "+    def write_commit_graph(self, refs=None, reachable=True) -> None:",
            "+        \"\"\"Write a commit graph file for this object store.",
            "+",
            "+        Args:",
            "+            refs: List of refs to include. If None, includes all refs from object store.",
            "+            reachable: If True, includes all commits reachable from refs.",
            "+                      If False, only includes the direct ref targets.",
            "+        \"\"\"",
            "+        from .commit_graph import get_reachable_commits",
            "+",
            "+        if refs is None:",
            "+            # Get all commit objects from the object store",
            "+            all_refs = []",
            "+            # Iterate through all objects to find commits",
            "+            for sha in self:",
            "+                try:",
            "+                    obj = self[sha]",
            "+                    if obj.type_name == b\"commit\":",
            "+                        all_refs.append(sha)",
            "+                except KeyError:",
            "+                    continue",
            "+        else:",
            "+            # Use provided refs",
            "+            all_refs = refs",
            "+",
            "+        if not all_refs:",
            "+            return  # No commits to include",
            "+",
            "+        if reachable:",
            "+            # Get all reachable commits",
            "+            commit_ids = get_reachable_commits(self, all_refs)",
            "+        else:",
            "+            # Just use the direct ref targets - ensure they're hex ObjectIDs",
            "+            commit_ids = []",
            "+            for ref in all_refs:",
            "+                if isinstance(ref, bytes) and len(ref) == 40:",
            "+                    # Already hex ObjectID",
            "+                    commit_ids.append(ref)",
            "+                elif isinstance(ref, bytes) and len(ref) == 20:",
            "+                    # Binary SHA, convert to hex ObjectID",
            "+                    from .objects import sha_to_hex",
            "+",
            "+                    commit_ids.append(sha_to_hex(ref))",
            "+                else:",
            "+                    # Assume it's already correct format",
            "+                    commit_ids.append(ref)",
            "+",
            "+        if commit_ids:",
            "+            # Write commit graph directly to our object store path",
            "+            # Generate the commit graph",
            "+            from .commit_graph import generate_commit_graph",
            "+",
            "+            graph = generate_commit_graph(self, commit_ids)",
            "+",
            "+            if graph.entries:",
            "+                # Ensure the info directory exists",
            "+                info_dir = os.path.join(self.path, \"info\")",
            "+                os.makedirs(info_dir, exist_ok=True)",
            "+",
            "+                # Write using GitFile for atomic operation",
            "+                graph_path = os.path.join(info_dir, \"commit-graph\")",
            "+                with GitFile(graph_path, \"wb\") as f:",
            "+                    graph.write_to_file(f)",
            "+",
            "+            # Clear cached commit graph so it gets reloaded",
            "+            self._commit_graph = None",
            "+",
            "+    def prune(self, grace_period: Optional[int] = None) -> None:",
            "+        \"\"\"Prune/clean up this object store.",
            "+",
            "+        This removes temporary files that were left behind by interrupted",
            "+        pack operations. These are files that start with ``tmp_pack_`` in the",
            "+        repository directory or files with .pack extension but no corresponding",
            "+        .idx file in the pack directory.",
            "+",
            "+        Args:",
            "+          grace_period: Grace period in seconds for removing temporary files.",
            "+                       If None, uses DEFAULT_TEMPFILE_GRACE_PERIOD.",
            "+        \"\"\"",
            "+        import glob",
            "+",
            "+        if grace_period is None:",
            "+            grace_period = DEFAULT_TEMPFILE_GRACE_PERIOD",
            "+",
            "+        # Clean up tmp_pack_* files in the repository directory",
            "+        for tmp_file in glob.glob(os.path.join(self.path, \"tmp_pack_*\")):",
            "+            # Check if file is old enough (more than grace period)",
            "+            mtime = os.path.getmtime(tmp_file)",
            "+            if time.time() - mtime > grace_period:",
            "+                os.remove(tmp_file)",
            "+",
            "+        # Clean up orphaned .pack files without corresponding .idx files",
            "+        try:",
            "+            pack_dir_contents = os.listdir(self.pack_dir)",
            "+        except FileNotFoundError:",
            "+            return",
            "+",
            "+        pack_files = {}",
            "+        idx_files = set()",
            "+",
            "+        for name in pack_dir_contents:",
            "+            if name.endswith(\".pack\"):",
            "+                base_name = name[:-5]  # Remove .pack extension",
            "+                pack_files[base_name] = name",
            "+            elif name.endswith(\".idx\"):",
            "+                base_name = name[:-4]  # Remove .idx extension",
            "+                idx_files.add(base_name)",
            "+",
            "+        # Remove .pack files without corresponding .idx files",
            "+        for base_name, pack_name in pack_files.items():",
            "+            if base_name not in idx_files:",
            "+                pack_path = os.path.join(self.pack_dir, pack_name)",
            "+                # Check if file is old enough (more than grace period)",
            "+                mtime = os.path.getmtime(pack_path)",
            "+                if time.time() - mtime > grace_period:",
            "+                    os.remove(pack_path)",
            "+",
            " ",
            " class MemoryObjectStore(BaseObjectStore):",
            "     \"\"\"Object store that keeps all objects in memory.\"\"\"",
            " ",
            "     def __init__(self) -> None:",
            "         super().__init__()",
            "         self._data: dict[str, ShaFile] = {}",
            "@@ -1146,14 +1517,15 @@",
            "             size = f.tell()",
            "             if size > 0:",
            "                 f.seek(0)",
            "                 p = PackData.from_file(f, size)",
            "                 for obj in PackInflater.for_pack_data(p, self.get_raw):",
            "                     self.add_object(obj)",
            "                 p.close()",
            "+                f.close()",
            "             else:",
            "                 f.close()",
            " ",
            "         def abort() -> None:",
            "             f.close()",
            " ",
            "         return f, commit, abort",
            "@@ -1161,18 +1533,34 @@",
            "     def add_pack_data(",
            "         self, count: int, unpacked_objects: Iterator[UnpackedObject], progress=None",
            "     ) -> None:",
            "         \"\"\"Add pack data to this object store.",
            " ",
            "         Args:",
            "           count: Number of items to add",
            "-          pack_data: Iterator over pack data tuples",
            "         \"\"\"",
            "-        for unpacked_object in unpacked_objects:",
            "-            self.add_object(unpacked_object.sha_file())",
            "+        if count == 0:",
            "+            return",
            "+",
            "+        # Since MemoryObjectStore doesn't support pack files, we need to",
            "+        # extract individual objects. To handle deltas properly, we write",
            "+        # to a temporary pack and then use PackInflater to resolve them.",
            "+        f, commit, abort = self.add_pack()",
            "+        try:",
            "+            write_pack_data(",
            "+                f.write,",
            "+                unpacked_objects,",
            "+                num_records=count,",
            "+                progress=progress,",
            "+            )",
            "+        except BaseException:",
            "+            abort()",
            "+            raise",
            "+        else:",
            "+            commit()",
            " ",
            "     def add_thin_pack(self, read_all, read_some, progress=None) -> None:",
            "         \"\"\"Add a new thin pack to this object store.",
            " ",
            "         Thin packs are packs that contain deltas with parents that exist",
            "         outside the pack. Because this object store doesn't support packs, we",
            "         extract and add the individual objects.",
            "@@ -1286,15 +1674,14 @@",
            "         sent",
            "       haves: SHA1s of commits not to send (already present in target)",
            "       wants: SHA1s of commits to send",
            "       progress: Optional function to report progress to.",
            "       get_tagged: Function that returns a dict of pointed-to sha -> tag",
            "         sha for including tags.",
            "       get_parents: Optional function for getting the parents of a commit.",
            "-      tagged: dict of pointed-to sha -> tag sha for including tags",
            "     \"\"\"",
            " ",
            "     def __init__(",
            "         self,",
            "         object_store,",
            "         haves,",
            "         wants,",
            "@@ -1415,39 +1802,49 @@",
            "         return (sha, pack_hint)",
            " ",
            "     def __iter__(self):",
            "         return self",
            " ",
            " ",
            " class ObjectStoreGraphWalker:",
            "-    \"\"\"Graph walker that finds what commits are missing from an object store.",
            "+    \"\"\"Graph walker that finds what commits are missing from an object store.\"\"\"",
            " ",
            "-    Attributes:",
            "-      heads: Revisions without descendants in the local repo",
            "-      get_parents: Function to retrieve parents in the local repo",
            "-    \"\"\"",
            "+    heads: set[ObjectID]",
            "+    \"\"\"Revisions without descendants in the local repo.\"\"\"",
            "+",
            "+    get_parents: Callable[[ObjectID], ObjectID]",
            "+    \"\"\"Function to retrieve parents in the local repo.\"\"\"",
            " ",
            "-    def __init__(self, local_heads, get_parents, shallow=None) -> None:",
            "+    shallow: set[ObjectID]",
            "+",
            "+    def __init__(",
            "+        self,",
            "+        local_heads: Iterable[ObjectID],",
            "+        get_parents,",
            "+        shallow: Optional[set[ObjectID]] = None,",
            "+        update_shallow=None,",
            "+    ) -> None:",
            "         \"\"\"Create a new instance.",
            " ",
            "         Args:",
            "           local_heads: Heads to start search with",
            "           get_parents: Function for finding the parents of a SHA1.",
            "         \"\"\"",
            "         self.heads = set(local_heads)",
            "         self.get_parents = get_parents",
            "         self.parents: dict[ObjectID, Optional[list[ObjectID]]] = {}",
            "         if shallow is None:",
            "             shallow = set()",
            "         self.shallow = shallow",
            "+        self.update_shallow = update_shallow",
            " ",
            "     def nak(self) -> None:",
            "         \"\"\"Nothing in common was found.\"\"\"",
            " ",
            "-    def ack(self, sha) -> None:",
            "+    def ack(self, sha: ObjectID) -> None:",
            "         \"\"\"Ack that a revision and its ancestors are present in the source.\"\"\"",
            "         if len(sha) != 40:",
            "             raise ValueError(f\"unexpected sha {sha!r} received\")",
            "         ancestors = {sha}",
            " ",
            "         # stop if we run out of heads to remove",
            "         while self.heads:",
            "@@ -1565,20 +1962,28 @@",
            "                     yield o_id",
            "                     done.add(o_id)",
            " ",
            "     def iterobjects_subset(",
            "         self, shas: Iterable[bytes], *, allow_missing: bool = False",
            "     ) -> Iterator[ShaFile]:",
            "         todo = set(shas)",
            "+        found: set[bytes] = set()",
            "+",
            "         for b in self.bases:",
            "-            for o in b.iterobjects_subset(todo, allow_missing=True):",
            "+            # Create a copy of todo for each base to avoid modifying",
            "+            # the set while iterating through it",
            "+            current_todo = todo - found",
            "+            for o in b.iterobjects_subset(current_todo, allow_missing=True):",
            "                 yield o",
            "-                todo.remove(o.id)",
            "-        if todo and not allow_missing:",
            "-            raise KeyError(o.id)",
            "+                found.add(o.id)",
            "+",
            "+        # Check for any remaining objects not found",
            "+        missing = todo - found",
            "+        if missing and not allow_missing:",
            "+            raise KeyError(next(iter(missing)))",
            " ",
            "     def iter_unpacked_subset(",
            "         self,",
            "         shas: Iterable[bytes],",
            "         *,",
            "         include_comp=False,",
            "         allow_missing: bool = False,",
            "@@ -1635,15 +2040,15 @@",
            "     def _iter_loose_objects(self):",
            "         \"\"\"Iterate over the SHAs of all loose objects.\"\"\"",
            "         return iter([])",
            " ",
            "     def _get_loose_object(self, sha) -> None:",
            "         return None",
            " ",
            "-    def _remove_loose_object(self, sha) -> None:",
            "+    def delete_loose_object(self, sha) -> None:",
            "         # Doesn't exist..",
            "         pass",
            " ",
            "     def _remove_pack(self, name) -> None:",
            "         raise NotImplementedError(self._remove_pack)",
            " ",
            "     def _iter_pack_names(self) -> Iterator[str]:",
            "@@ -1692,27 +2097,31 @@",
            "             p = PackData(pf.name, pf)",
            "             entries = p.sorted_entries()",
            "             basename = iter_sha1(entry[0] for entry in entries).decode(\"ascii\")",
            "             idxf = tempfile.SpooledTemporaryFile(",
            "                 max_size=PACK_SPOOL_FILE_MAX_SIZE, prefix=\"incoming-\"",
            "             )",
            "             checksum = p.get_stored_checksum()",
            "-            write_pack_index(idxf, entries, checksum)",
            "+            write_pack_index(idxf, entries, checksum, version=self.pack_index_version)",
            "             idxf.seek(0)",
            "             idx = load_pack_index_file(basename + \".idx\", idxf)",
            "             for pack in self.packs:",
            "                 if pack.get_stored_checksum() == p.get_stored_checksum():",
            "                     p.close()",
            "                     idx.close()",
            "+                    pf.close()",
            "+                    idxf.close()",
            "                     return pack",
            "             pf.seek(0)",
            "             idxf.seek(0)",
            "             self._upload_pack(basename, pf, idxf)",
            "             final_pack = Pack.from_objects(p, idx)",
            "             self._add_cached_pack(basename, final_pack)",
            "+            pf.close()",
            "+            idxf.close()",
            "             return final_pack",
            " ",
            "         return pf, commit, pf.close",
            " ",
            " ",
            " def _collect_ancestors(",
            "     store: ObjectContainer,"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/objects.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/objects.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/objects.py",
            "@@ -22,41 +22,48 @@",
            " ",
            " \"\"\"Access to base git objects.\"\"\"",
            " ",
            " import binascii",
            " import os",
            " import posixpath",
            " import stat",
            "-import warnings",
            " import zlib",
            " from collections import namedtuple",
            " from collections.abc import Callable, Iterable, Iterator",
            " from hashlib import sha1",
            "-from io import BytesIO",
            "+from io import BufferedIOBase, BytesIO",
            " from typing import (",
            "+    IO,",
            "     TYPE_CHECKING,",
            "-    BinaryIO,",
            "     Optional,",
            "     Union,",
            " )",
            " ",
            "+try:",
            "+    from typing import TypeGuard  # type: ignore",
            "+except ImportError:",
            "+    from typing_extensions import TypeGuard",
            "+",
            "+from . import replace_me",
            " from .errors import (",
            "     ChecksumMismatch,",
            "     FileFormatException,",
            "     NotBlobError,",
            "     NotCommitError,",
            "     NotTagError,",
            "     NotTreeError,",
            "     ObjectFormatException,",
            " )",
            " from .file import GitFile",
            " ",
            " if TYPE_CHECKING:",
            "     from _hashlib import HASH",
            " ",
            "+    from .file import _GitFile",
            "+",
            " ZERO_SHA = b\"0\" * 40",
            " ",
            " # Header fields for commits",
            " _TREE_HEADER = b\"tree\"",
            " _PARENT_HEADER = b\"parent\"",
            " _AUTHOR_HEADER = b\"author\"",
            " _COMMITTER_HEADER = b\"committer\"",
            "@@ -82,102 +89,104 @@",
            " ObjectID = bytes",
            " ",
            " ",
            " class EmptyFileException(FileFormatException):",
            "     \"\"\"An unexpectedly empty file was encountered.\"\"\"",
            " ",
            " ",
            "-def S_ISGITLINK(m):",
            "+def S_ISGITLINK(m: int) -> bool:",
            "     \"\"\"Check if a mode indicates a submodule.",
            " ",
            "     Args:",
            "       m: Mode to check",
            "     Returns: a ``boolean``",
            "     \"\"\"",
            "     return stat.S_IFMT(m) == S_IFGITLINK",
            " ",
            " ",
            "-def _decompress(string):",
            "+def _decompress(string: bytes) -> bytes:",
            "     dcomp = zlib.decompressobj()",
            "     dcomped = dcomp.decompress(string)",
            "     dcomped += dcomp.flush()",
            "     return dcomped",
            " ",
            " ",
            "-def sha_to_hex(sha):",
            "+def sha_to_hex(sha: ObjectID) -> bytes:",
            "     \"\"\"Takes a string and returns the hex of the sha within.\"\"\"",
            "     hexsha = binascii.hexlify(sha)",
            "     assert len(hexsha) == 40, f\"Incorrect length of sha1 string: {hexsha!r}\"",
            "     return hexsha",
            " ",
            " ",
            "-def hex_to_sha(hex):",
            "+def hex_to_sha(hex: Union[bytes, str]) -> bytes:",
            "     \"\"\"Takes a hex sha and returns a binary sha.\"\"\"",
            "-    assert len(hex) == 40, f\"Incorrect length of hexsha: {hex}\"",
            "+    assert len(hex) == 40, f\"Incorrect length of hexsha: {hex!r}\"",
            "     try:",
            "         return binascii.unhexlify(hex)",
            "     except TypeError as exc:",
            "         if not isinstance(hex, bytes):",
            "             raise",
            "         raise ValueError(exc.args[0]) from exc",
            " ",
            " ",
            "-def valid_hexsha(hex) -> bool:",
            "+def valid_hexsha(hex: Union[bytes, str]) -> bool:",
            "     if len(hex) != 40:",
            "         return False",
            "     try:",
            "         binascii.unhexlify(hex)",
            "     except (TypeError, binascii.Error):",
            "         return False",
            "     else:",
            "         return True",
            " ",
            " ",
            "-def hex_to_filename(path, hex):",
            "+def hex_to_filename(",
            "+    path: Union[str, bytes], hex: Union[str, bytes]",
            "+) -> Union[str, bytes]:",
            "     \"\"\"Takes a hex sha and returns its filename relative to the given path.\"\"\"",
            "     # os.path.join accepts bytes or unicode, but all args must be of the same",
            "     # type. Make sure that hex which is expected to be bytes, is the same type",
            "     # as path.",
            "-    if type(path) is not type(hex) and getattr(path, \"encode\", None) is not None:",
            "-        hex = hex.decode(\"ascii\")",
            "+    if type(path) is not type(hex) and isinstance(path, str):",
            "+        hex = hex.decode(\"ascii\")  # type: ignore",
            "     dir = hex[:2]",
            "     file = hex[2:]",
            "     # Check from object dir",
            "-    return os.path.join(path, dir, file)",
            "+    return os.path.join(path, dir, file)  # type: ignore",
            " ",
            " ",
            "-def filename_to_hex(filename):",
            "+def filename_to_hex(filename: Union[str, bytes]) -> str:",
            "     \"\"\"Takes an object filename and returns its corresponding hex sha.\"\"\"",
            "     # grab the last (up to) two path components",
            "-    names = filename.rsplit(os.path.sep, 2)[-2:]",
            "-    errmsg = f\"Invalid object filename: {filename}\"",
            "+    names = filename.rsplit(os.path.sep, 2)[-2:]  # type: ignore",
            "+    errmsg = f\"Invalid object filename: {filename!r}\"",
            "     assert len(names) == 2, errmsg",
            "     base, rest = names",
            "     assert len(base) == 2 and len(rest) == 38, errmsg",
            "-    hex = (base + rest).encode(\"ascii\")",
            "-    hex_to_sha(hex)",
            "-    return hex",
            "+    hex_bytes = (base + rest).encode(\"ascii\")  # type: ignore",
            "+    hex_to_sha(hex_bytes)",
            "+    return hex_bytes.decode(\"ascii\")",
            " ",
            " ",
            " def object_header(num_type: int, length: int) -> bytes:",
            "     \"\"\"Return an object header for the given numeric type and text length.\"\"\"",
            "     cls = object_class(num_type)",
            "     if cls is None:",
            "         raise AssertionError(f\"unsupported class type num: {num_type}\")",
            "     return cls.type_name + b\" \" + str(length).encode(\"ascii\") + b\"\\0\"",
            " ",
            " ",
            "-def serializable_property(name: str, docstring: Optional[str] = None):",
            "+def serializable_property(name: str, docstring: Optional[str] = None) -> property:",
            "     \"\"\"A property that helps tracking whether serialization is necessary.\"\"\"",
            " ",
            "-    def set(obj, value) -> None:",
            "+    def set(obj: \"ShaFile\", value: object) -> None:",
            "         setattr(obj, \"_\" + name, value)",
            "         obj._needs_serialization = True",
            " ",
            "-    def get(obj):",
            "+    def get(obj: \"ShaFile\") -> object:",
            "         return getattr(obj, \"_\" + name)",
            " ",
            "     return property(get, set, doc=docstring)",
            " ",
            " ",
            " def object_class(type: Union[bytes, int]) -> Optional[type[\"ShaFile\"]]:",
            "     \"\"\"Get the object class corresponding to the given type.",
            "@@ -186,25 +195,25 @@",
            "       type: Either a type name string or a numeric type.",
            "     Returns: The ShaFile subclass corresponding to the given type, or None if",
            "         type is not a valid type name/number.",
            "     \"\"\"",
            "     return _TYPE_MAP.get(type, None)",
            " ",
            " ",
            "-def check_hexsha(hex, error_msg) -> None:",
            "+def check_hexsha(hex: Union[str, bytes], error_msg: str) -> None:",
            "     \"\"\"Check if a string is a valid hex sha string.",
            " ",
            "     Args:",
            "       hex: Hex string to check",
            "       error_msg: Error message to use in exception",
            "     Raises:",
            "       ObjectFormatException: Raised when the string is not valid",
            "     \"\"\"",
            "     if not valid_hexsha(hex):",
            "-        raise ObjectFormatException(f\"{error_msg} {hex}\")",
            "+        raise ObjectFormatException(f\"{error_msg} {hex!r}\")",
            " ",
            " ",
            " def check_identity(identity: Optional[bytes], error_msg: str) -> None:",
            "     \"\"\"Check if the specified identity is valid.",
            " ",
            "     This will raise an exception if the identity is not valid.",
            " ",
            "@@ -225,68 +234,107 @@",
            "             b\"\\0\" not in identity,",
            "             b\"\\n\" not in identity,",
            "         ]",
            "     ):",
            "         raise ObjectFormatException(error_msg)",
            " ",
            " ",
            "-def check_time(time_seconds) -> None:",
            "+def check_time(time_seconds: int) -> None:",
            "     \"\"\"Check if the specified time is not prone to overflow error.",
            " ",
            "     This will raise an exception if the time is not valid.",
            " ",
            "     Args:",
            "       time_seconds: time in seconds",
            " ",
            "     \"\"\"",
            "     # Prevent overflow error",
            "     if time_seconds > MAX_TIME:",
            "         raise ObjectFormatException(f\"Date field should not exceed {MAX_TIME}\")",
            " ",
            " ",
            "-def git_line(*items):",
            "+def git_line(*items: bytes) -> bytes:",
            "     \"\"\"Formats items into a space separated line.\"\"\"",
            "     return b\" \".join(items) + b\"\\n\"",
            " ",
            " ",
            " class FixedSha:",
            "     \"\"\"SHA object that behaves like hashlib's but is given a fixed value.\"\"\"",
            " ",
            "     __slots__ = (\"_hexsha\", \"_sha\")",
            " ",
            "-    def __init__(self, hexsha) -> None:",
            "-        if getattr(hexsha, \"encode\", None) is not None:",
            "-            hexsha = hexsha.encode(\"ascii\")",
            "+    def __init__(self, hexsha: Union[str, bytes]) -> None:",
            "+        if isinstance(hexsha, str):",
            "+            hexsha = hexsha.encode(\"ascii\")  # type: ignore",
            "         if not isinstance(hexsha, bytes):",
            "             raise TypeError(f\"Expected bytes for hexsha, got {hexsha!r}\")",
            "         self._hexsha = hexsha",
            "         self._sha = hex_to_sha(hexsha)",
            " ",
            "     def digest(self) -> bytes:",
            "         \"\"\"Return the raw SHA digest.\"\"\"",
            "         return self._sha",
            " ",
            "     def hexdigest(self) -> str:",
            "         \"\"\"Return the hex SHA digest.\"\"\"",
            "         return self._hexsha.decode(\"ascii\")",
            " ",
            " ",
            "+# Type guard functions for runtime type narrowing",
            "+if TYPE_CHECKING:",
            "+",
            "+    def is_commit(obj: \"ShaFile\") -> TypeGuard[\"Commit\"]:",
            "+        \"\"\"Check if a ShaFile is a Commit.\"\"\"",
            "+        return obj.type_name == b\"commit\"",
            "+",
            "+    def is_tree(obj: \"ShaFile\") -> TypeGuard[\"Tree\"]:",
            "+        \"\"\"Check if a ShaFile is a Tree.\"\"\"",
            "+        return obj.type_name == b\"tree\"",
            "+",
            "+    def is_blob(obj: \"ShaFile\") -> TypeGuard[\"Blob\"]:",
            "+        \"\"\"Check if a ShaFile is a Blob.\"\"\"",
            "+        return obj.type_name == b\"blob\"",
            "+",
            "+    def is_tag(obj: \"ShaFile\") -> TypeGuard[\"Tag\"]:",
            "+        \"\"\"Check if a ShaFile is a Tag.\"\"\"",
            "+        return obj.type_name == b\"tag\"",
            "+else:",
            "+    # Runtime versions without type narrowing",
            "+    def is_commit(obj: \"ShaFile\") -> bool:",
            "+        \"\"\"Check if a ShaFile is a Commit.\"\"\"",
            "+        return obj.type_name == b\"commit\"",
            "+",
            "+    def is_tree(obj: \"ShaFile\") -> bool:",
            "+        \"\"\"Check if a ShaFile is a Tree.\"\"\"",
            "+        return obj.type_name == b\"tree\"",
            "+",
            "+    def is_blob(obj: \"ShaFile\") -> bool:",
            "+        \"\"\"Check if a ShaFile is a Blob.\"\"\"",
            "+        return obj.type_name == b\"blob\"",
            "+",
            "+    def is_tag(obj: \"ShaFile\") -> bool:",
            "+        \"\"\"Check if a ShaFile is a Tag.\"\"\"",
            "+        return obj.type_name == b\"tag\"",
            "+",
            "+",
            " class ShaFile:",
            "     \"\"\"A git SHA file.\"\"\"",
            " ",
            "     __slots__ = (\"_chunked_text\", \"_needs_serialization\", \"_sha\")",
            " ",
            "     _needs_serialization: bool",
            "     type_name: bytes",
            "     type_num: int",
            "     _chunked_text: Optional[list[bytes]]",
            "     _sha: Union[FixedSha, None, \"HASH\"]",
            " ",
            "     @staticmethod",
            "-    def _parse_legacy_object_header(magic, f: BinaryIO) -> \"ShaFile\":",
            "+    def _parse_legacy_object_header(",
            "+        magic: bytes, f: Union[BufferedIOBase, IO[bytes], \"_GitFile\"]",
            "+    ) -> \"ShaFile\":",
            "         \"\"\"Parse a legacy object, creating it but not reading the file.\"\"\"",
            "         bufsize = 1024",
            "         decomp = zlib.decompressobj()",
            "         header = decomp.decompress(magic)",
            "         start = 0",
            "         end = -1",
            "         while end < 0:",
            "@@ -304,15 +352,15 @@",
            "         obj_class = object_class(type_name)",
            "         if not obj_class:",
            "             raise ObjectFormatException(",
            "                 \"Not a known type: {}\".format(type_name.decode(\"ascii\"))",
            "             )",
            "         return obj_class()",
            " ",
            "-    def _parse_legacy_object(self, map) -> None:",
            "+    def _parse_legacy_object(self, map: bytes) -> None:",
            "         \"\"\"Parse a legacy object, setting the raw string.\"\"\"",
            "         text = _decompress(map)",
            "         header_end = text.find(b\"\\0\")",
            "         if header_end < 0:",
            "             raise ObjectFormatException(\"Invalid object header, no \\\\0\")",
            "         self.set_raw_string(text[header_end + 1 :])",
            " ",
            "@@ -378,23 +426,25 @@",
            "         if sha is None:",
            "             self._sha = None",
            "         else:",
            "             self._sha = FixedSha(sha)  # type: ignore",
            "         self._needs_serialization = False",
            " ",
            "     @staticmethod",
            "-    def _parse_object_header(magic, f):",
            "+    def _parse_object_header(",
            "+        magic: bytes, f: Union[BufferedIOBase, IO[bytes], \"_GitFile\"]",
            "+    ) -> \"ShaFile\":",
            "         \"\"\"Parse a new style object, creating it but not reading the file.\"\"\"",
            "         num_type = (ord(magic[0:1]) >> 4) & 7",
            "         obj_class = object_class(num_type)",
            "         if not obj_class:",
            "             raise ObjectFormatException(f\"Not a known type {num_type}\")",
            "         return obj_class()",
            " ",
            "-    def _parse_object(self, map) -> None:",
            "+    def _parse_object(self, map: bytes) -> None:",
            "         \"\"\"Parse a new style object, setting self._text.\"\"\"",
            "         # skip type and size; type must have already been determined, and",
            "         # we trust zlib to fail if it's otherwise corrupted",
            "         byte = ord(map[0:1])",
            "         used = 1",
            "         while (byte & 0x80) != 0:",
            "             byte = ord(map[used : used + 1])",
            "@@ -406,15 +456,15 @@",
            "     def _is_legacy_object(cls, magic: bytes) -> bool:",
            "         b0 = ord(magic[0:1])",
            "         b1 = ord(magic[1:2])",
            "         word = (b0 << 8) + b1",
            "         return (b0 & 0x8F) == 0x08 and (word % 31) == 0",
            " ",
            "     @classmethod",
            "-    def _parse_file(cls, f):",
            "+    def _parse_file(cls, f: Union[BufferedIOBase, IO[bytes], \"_GitFile\"]) -> \"ShaFile\":",
            "         map = f.read()",
            "         if not map:",
            "             raise EmptyFileException(\"Corrupted empty file detected\")",
            " ",
            "         if cls._is_legacy_object(map):",
            "             obj = cls._parse_legacy_object_header(map, f)",
            "             obj._parse_legacy_object(map)",
            "@@ -432,32 +482,32 @@",
            "     def _deserialize(self, chunks: list[bytes]) -> None:",
            "         raise NotImplementedError(self._deserialize)",
            " ",
            "     def _serialize(self) -> list[bytes]:",
            "         raise NotImplementedError(self._serialize)",
            " ",
            "     @classmethod",
            "-    def from_path(cls, path):",
            "+    def from_path(cls, path: Union[str, bytes]) -> \"ShaFile\":",
            "         \"\"\"Open a SHA file from disk.\"\"\"",
            "         with GitFile(path, \"rb\") as f:",
            "             return cls.from_file(f)",
            " ",
            "     @classmethod",
            "-    def from_file(cls, f):",
            "+    def from_file(cls, f: Union[BufferedIOBase, IO[bytes], \"_GitFile\"]) -> \"ShaFile\":",
            "         \"\"\"Get the contents of a SHA file on disk.\"\"\"",
            "         try:",
            "             obj = cls._parse_file(f)",
            "             obj._sha = None",
            "             return obj",
            "         except (IndexError, ValueError) as exc:",
            "             raise ObjectFormatException(\"invalid object header\") from exc",
            " ",
            "     @staticmethod",
            "     def from_raw_string(",
            "-        type_num, string: bytes, sha: Optional[ObjectID] = None",
            "+        type_num: int, string: bytes, sha: Optional[ObjectID] = None",
            "     ) -> \"ShaFile\":",
            "         \"\"\"Creates an object of the indicated type from the raw string given.",
            " ",
            "         Args:",
            "           type_num: The numeric type of the object.",
            "           string: The raw uncompressed contents.",
            "           sha: Optional known sha for the object",
            "@@ -468,15 +518,15 @@",
            "         obj = cls()",
            "         obj.set_raw_string(string, sha)",
            "         return obj",
            " ",
            "     @staticmethod",
            "     def from_raw_chunks(",
            "         type_num: int, chunks: list[bytes], sha: Optional[ObjectID] = None",
            "-    ):",
            "+    ) -> \"ShaFile\":",
            "         \"\"\"Creates an object of the indicated type from the raw chunks given.",
            " ",
            "         Args:",
            "           type_num: The numeric type of the object.",
            "           chunks: An iterable of the raw uncompressed contents.",
            "           sha: Optional known sha for the object",
            "         \"\"\"",
            "@@ -484,21 +534,21 @@",
            "         if cls is None:",
            "             raise AssertionError(f\"unsupported class type num: {type_num}\")",
            "         obj = cls()",
            "         obj.set_raw_chunks(chunks, sha)",
            "         return obj",
            " ",
            "     @classmethod",
            "-    def from_string(cls, string):",
            "+    def from_string(cls, string: bytes) -> \"ShaFile\":",
            "         \"\"\"Create a ShaFile from a string.\"\"\"",
            "         obj = cls()",
            "         obj.set_raw_string(string)",
            "         return obj",
            " ",
            "-    def _check_has_member(self, member, error_msg) -> None:",
            "+    def _check_has_member(self, member: str, error_msg: str) -> None:",
            "         \"\"\"Check that the object has a given member variable.",
            " ",
            "         Args:",
            "           member: the member variable to check for",
            "           error_msg: the message for an error if the member is missing",
            "         Raises:",
            "           ObjectFormatException: with the given error_msg if member is",
            "@@ -525,22 +575,22 @@",
            "             self._sha = None",
            "             new_sha = self.id",
            "         except Exception as exc:",
            "             raise ObjectFormatException(exc) from exc",
            "         if old_sha != new_sha:",
            "             raise ChecksumMismatch(new_sha, old_sha)",
            " ",
            "-    def _header(self):",
            "+    def _header(self) -> bytes:",
            "         return object_header(self.type_num, self.raw_length())",
            " ",
            "     def raw_length(self) -> int:",
            "         \"\"\"Returns the length of the raw string of this object.\"\"\"",
            "         return sum(map(len, self.as_raw_chunks()))",
            " ",
            "-    def sha(self) -> \"HASH\":",
            "+    def sha(self) -> Union[FixedSha, \"HASH\"]:",
            "         \"\"\"The SHA1 object that is the name of this object.\"\"\"",
            "         if self._sha is None or self._needs_serialization:",
            "             # this is a local because as_raw_chunks() overwrites self._sha",
            "             new_sha = sha1()",
            "             new_sha.update(self._header())",
            "             for chunk in self.as_raw_chunks():",
            "                 new_sha.update(chunk)",
            "@@ -551,36 +601,36 @@",
            "         \"\"\"Create a new copy of this SHA1 object from its raw string.\"\"\"",
            "         obj_class = object_class(self.type_num)",
            "         if obj_class is None:",
            "             raise AssertionError(f\"invalid type num {self.type_num}\")",
            "         return obj_class.from_raw_string(self.type_num, self.as_raw_string(), self.id)",
            " ",
            "     @property",
            "-    def id(self):",
            "+    def id(self) -> bytes:",
            "         \"\"\"The hex SHA of this object.\"\"\"",
            "         return self.sha().hexdigest().encode(\"ascii\")",
            " ",
            "     def __repr__(self) -> str:",
            "-        return f\"<{self.__class__.__name__} {self.id}>\"",
            "+        return f\"<{self.__class__.__name__} {self.id!r}>\"",
            " ",
            "-    def __ne__(self, other) -> bool:",
            "+    def __ne__(self, other: object) -> bool:",
            "         \"\"\"Check whether this object does not match the other.\"\"\"",
            "         return not isinstance(other, ShaFile) or self.id != other.id",
            " ",
            "-    def __eq__(self, other) -> bool:",
            "+    def __eq__(self, other: object) -> bool:",
            "         \"\"\"Return True if the SHAs of the two objects match.\"\"\"",
            "         return isinstance(other, ShaFile) and self.id == other.id",
            " ",
            "-    def __lt__(self, other) -> bool:",
            "+    def __lt__(self, other: object) -> bool:",
            "         \"\"\"Return whether SHA of this object is less than the other.\"\"\"",
            "         if not isinstance(other, ShaFile):",
            "             raise TypeError",
            "         return self.id < other.id",
            " ",
            "-    def __le__(self, other) -> bool:",
            "+    def __le__(self, other: object) -> bool:",
            "         \"\"\"Check whether SHA of this object is less than or equal to the other.\"\"\"",
            "         if not isinstance(other, ShaFile):",
            "             raise TypeError",
            "         return self.id <= other.id",
            " ",
            " ",
            " class Blob(ShaFile):",
            "@@ -594,44 +644,44 @@",
            "     _chunked_text: list[bytes]",
            " ",
            "     def __init__(self) -> None:",
            "         super().__init__()",
            "         self._chunked_text = []",
            "         self._needs_serialization = False",
            " ",
            "-    def _get_data(self):",
            "+    def _get_data(self) -> bytes:",
            "         return self.as_raw_string()",
            " ",
            "-    def _set_data(self, data) -> None:",
            "+    def _set_data(self, data: bytes) -> None:",
            "         self.set_raw_string(data)",
            " ",
            "     data = property(",
            "         _get_data, _set_data, doc=\"The text contained within the blob object.\"",
            "     )",
            " ",
            "-    def _get_chunked(self):",
            "+    def _get_chunked(self) -> list[bytes]:",
            "         return self._chunked_text",
            " ",
            "     def _set_chunked(self, chunks: list[bytes]) -> None:",
            "         self._chunked_text = chunks",
            " ",
            "-    def _serialize(self):",
            "+    def _serialize(self) -> list[bytes]:",
            "         return self._chunked_text",
            " ",
            "-    def _deserialize(self, chunks) -> None:",
            "+    def _deserialize(self, chunks: list[bytes]) -> None:",
            "         self._chunked_text = chunks",
            " ",
            "     chunked = property(",
            "         _get_chunked,",
            "         _set_chunked,",
            "         doc=\"The text in the blob object, as chunks (not necessarily lines)\",",
            "     )",
            " ",
            "     @classmethod",
            "-    def from_path(cls, path):",
            "+    def from_path(cls, path: Union[str, bytes]) -> \"Blob\":",
            "         blob = ShaFile.from_path(path)",
            "         if not isinstance(blob, cls):",
            "             raise NotBlobError(path)",
            "         return blob",
            " ",
            "     def check(self) -> None:",
            "         \"\"\"Check this object for internal consistency.",
            "@@ -681,15 +731,15 @@",
            "         field named None for the freeform tag/commit text.",
            "     \"\"\"",
            "     f = BytesIO(b\"\".join(chunks))",
            "     k = None",
            "     v = b\"\"",
            "     eof = False",
            " ",
            "-    def _strip_last_newline(value):",
            "+    def _strip_last_newline(value: bytes) -> bytes:",
            "         \"\"\"Strip the last newline from value.\"\"\"",
            "         if value and value.endswith(b\"\\n\"):",
            "             return value[:-1]",
            "         return value",
            " ",
            "     # Parse the headers",
            "     #",
            "@@ -721,15 +771,17 @@",
            "         # We didn't reach the end of file while parsing headers. We can return",
            "         # the rest of the file as a message.",
            "         yield (None, f.read())",
            " ",
            "     f.close()",
            " ",
            " ",
            "-def _format_message(headers, body):",
            "+def _format_message(",
            "+    headers: list[tuple[bytes, bytes]], body: Optional[bytes]",
            "+) -> Iterator[bytes]:",
            "     for field, value in headers:",
            "         lines = value.split(b\"\\n\")",
            "         yield git_line(field, lines[0])",
            "         for line in lines[1:]:",
            "             yield b\" \" + line + b\"\\n\"",
            "     yield b\"\\n\"  # There must be a new line after the headers",
            "     if body:",
            "@@ -750,26 +802,34 @@",
            "         \"_signature\",",
            "         \"_tag_time\",",
            "         \"_tag_timezone\",",
            "         \"_tag_timezone_neg_utc\",",
            "         \"_tagger\",",
            "     )",
            " ",
            "+    _message: Optional[bytes]",
            "+    _name: Optional[bytes]",
            "+    _object_class: Optional[type[\"ShaFile\"]]",
            "+    _object_sha: Optional[bytes]",
            "+    _signature: Optional[bytes]",
            "+    _tag_time: Optional[int]",
            "+    _tag_timezone: Optional[int]",
            "+    _tag_timezone_neg_utc: Optional[bool]",
            "     _tagger: Optional[bytes]",
            " ",
            "     def __init__(self) -> None:",
            "         super().__init__()",
            "         self._tagger = None",
            "         self._tag_time = None",
            "         self._tag_timezone = None",
            "         self._tag_timezone_neg_utc = False",
            "         self._signature: Optional[bytes] = None",
            " ",
            "     @classmethod",
            "-    def from_path(cls, filename):",
            "+    def from_path(cls, filename: Union[str, bytes]) -> \"Tag\":",
            "         tag = ShaFile.from_path(filename)",
            "         if not isinstance(tag, cls):",
            "             raise NotTagError(filename)",
            "         return tag",
            " ",
            "     def check(self) -> None:",
            "         \"\"\"Check this object for internal consistency.",
            "@@ -782,43 +842,55 @@",
            "         self._check_has_member(\"_object_sha\", \"missing object sha\")",
            "         self._check_has_member(\"_object_class\", \"missing object type\")",
            "         self._check_has_member(\"_name\", \"missing tag name\")",
            " ",
            "         if not self._name:",
            "             raise ObjectFormatException(\"empty tag name\")",
            " ",
            "+        if self._object_sha is None:",
            "+            raise ObjectFormatException(\"missing object sha\")",
            "         check_hexsha(self._object_sha, \"invalid object sha\")",
            " ",
            "         if self._tagger is not None:",
            "             check_identity(self._tagger, \"invalid tagger\")",
            " ",
            "         self._check_has_member(\"_tag_time\", \"missing tag time\")",
            "+        if self._tag_time is None:",
            "+            raise ObjectFormatException(\"missing tag time\")",
            "         check_time(self._tag_time)",
            " ",
            "         last = None",
            "         for field, _ in _parse_message(self._chunked_text):",
            "             if field == _OBJECT_HEADER and last is not None:",
            "                 raise ObjectFormatException(\"unexpected object\")",
            "             elif field == _TYPE_HEADER and last != _OBJECT_HEADER:",
            "                 raise ObjectFormatException(\"unexpected type\")",
            "             elif field == _TAG_HEADER and last != _TYPE_HEADER:",
            "                 raise ObjectFormatException(\"unexpected tag name\")",
            "             elif field == _TAGGER_HEADER and last != _TAG_HEADER:",
            "                 raise ObjectFormatException(\"unexpected tagger\")",
            "             last = field",
            " ",
            "-    def _serialize(self):",
            "+    def _serialize(self) -> list[bytes]:",
            "         headers = []",
            "+        if self._object_sha is None:",
            "+            raise ObjectFormatException(\"missing object sha\")",
            "         headers.append((_OBJECT_HEADER, self._object_sha))",
            "+        if self._object_class is None:",
            "+            raise ObjectFormatException(\"missing object class\")",
            "         headers.append((_TYPE_HEADER, self._object_class.type_name))",
            "+        if self._name is None:",
            "+            raise ObjectFormatException(\"missing tag name\")",
            "         headers.append((_TAG_HEADER, self._name))",
            "         if self._tagger:",
            "             if self._tag_time is None:",
            "                 headers.append((_TAGGER_HEADER, self._tagger))",
            "             else:",
            "+                if self._tag_timezone is None or self._tag_timezone_neg_utc is None:",
            "+                    raise ObjectFormatException(\"missing timezone info\")",
            "                 headers.append(",
            "                     (",
            "                         _TAGGER_HEADER,",
            "                         format_time_entry(",
            "                             self._tagger,",
            "                             self._tag_time,",
            "                             (self._tag_timezone, self._tag_timezone_neg_utc),",
            "@@ -828,15 +900,15 @@",
            " ",
            "         if self.message is None and self._signature is None:",
            "             body = None",
            "         else:",
            "             body = (self.message or b\"\") + (self._signature or b\"\")",
            "         return list(_format_message(headers, body))",
            " ",
            "-    def _deserialize(self, chunks) -> None:",
            "+    def _deserialize(self, chunks: list[bytes]) -> None:",
            "         \"\"\"Grab the metadata attached to the tag.\"\"\"",
            "         self._tagger = None",
            "         self._tag_time = None",
            "         self._tag_timezone = None",
            "         self._tag_timezone_neg_utc = False",
            "         for field, value in _parse_message(chunks):",
            "             if field == _OBJECT_HEADER:",
            "@@ -846,14 +918,16 @@",
            "                 obj_class = object_class(value)",
            "                 if not obj_class:",
            "                     raise ObjectFormatException(f\"Not a known type: {value!r}\")",
            "                 self._object_class = obj_class",
            "             elif field == _TAG_HEADER:",
            "                 self._name = value",
            "             elif field == _TAGGER_HEADER:",
            "+                if value is None:",
            "+                    raise ObjectFormatException(\"missing tagger value\")",
            "                 (",
            "                     self._tagger,",
            "                     self._tag_time,",
            "                     (self._tag_timezone, self._tag_timezone_neg_utc),",
            "                 ) = parse_time_entry(value)",
            "             elif field is None:",
            "                 if value is None:",
            "@@ -869,35 +943,36 @@",
            "                         self._message = value[:sig_idx]",
            "                         self._signature = value[sig_idx:]",
            "             else:",
            "                 raise ObjectFormatException(",
            "                     f\"Unknown field {field.decode('ascii', 'replace')}\"",
            "                 )",
            " ",
            "-    def _get_object(self):",
            "+    def _get_object(self) -> tuple[type[ShaFile], bytes]:",
            "         \"\"\"Get the object pointed to by this tag.",
            " ",
            "         Returns: tuple of (object class, sha).",
            "         \"\"\"",
            "+        if self._object_class is None or self._object_sha is None:",
            "+            raise ValueError(\"Tag object is not properly initialized\")",
            "         return (self._object_class, self._object_sha)",
            " ",
            "-    def _set_object(self, value) -> None:",
            "+    def _set_object(self, value: tuple[type[ShaFile], bytes]) -> None:",
            "         (self._object_class, self._object_sha) = value",
            "         self._needs_serialization = True",
            " ",
            "     object = property(_get_object, _set_object)",
            " ",
            "     name = serializable_property(\"name\", \"The name of this tag\")",
            "     tagger = serializable_property(",
            "         \"tagger\", \"Returns the name of the person who created this tag\"",
            "     )",
            "     tag_time = serializable_property(",
            "         \"tag_time\",",
            "-        \"The creation timestamp of the tag.  As the number of seconds \"",
            "-        \"since the epoch\",",
            "+        \"The creation timestamp of the tag.  As the number of seconds since the epoch\",",
            "     )",
            "     tag_timezone = serializable_property(",
            "         \"tag_timezone\", \"The timezone that tag_time is in.\"",
            "     )",
            "     message = serializable_property(\"message\", \"the message attached to this tag\")",
            " ",
            "     signature = serializable_property(\"signature\", \"Optional detached GPG signature\")",
            "@@ -961,22 +1036,22 @@",
            "                                 return",
            "                 raise gpg.errors.MissingSignatures(result, keys, results=(data, result))",
            " ",
            " ",
            " class TreeEntry(namedtuple(\"TreeEntry\", [\"path\", \"mode\", \"sha\"])):",
            "     \"\"\"Named tuple encapsulating a single tree entry.\"\"\"",
            " ",
            "-    def in_path(self, path: bytes):",
            "+    def in_path(self, path: bytes) -> \"TreeEntry\":",
            "         \"\"\"Return a copy of this entry with the given path prepended.\"\"\"",
            "         if not isinstance(self.path, bytes):",
            "             raise TypeError(f\"Expected bytes for path, got {path!r}\")",
            "         return TreeEntry(posixpath.join(path, self.path), self.mode, self.sha)",
            " ",
            " ",
            "-def parse_tree(text, strict=False):",
            "+def parse_tree(text: bytes, strict: bool = False) -> Iterator[tuple[bytes, int, bytes]]:",
            "     \"\"\"Parse a tree text.",
            " ",
            "     Args:",
            "       text: Serialized text to parse",
            "     Returns: iterator of tuples of (name, mode, sha)",
            " ",
            "     Raises:",
            "@@ -984,43 +1059,45 @@",
            "     \"\"\"",
            "     count = 0",
            "     length = len(text)",
            "     while count < length:",
            "         mode_end = text.index(b\" \", count)",
            "         mode_text = text[count:mode_end]",
            "         if strict and mode_text.startswith(b\"0\"):",
            "-            raise ObjectFormatException(f\"Invalid mode '{mode_text}'\")",
            "+            raise ObjectFormatException(f\"Invalid mode {mode_text!r}\")",
            "         try:",
            "             mode = int(mode_text, 8)",
            "         except ValueError as exc:",
            "-            raise ObjectFormatException(f\"Invalid mode '{mode_text}'\") from exc",
            "+            raise ObjectFormatException(f\"Invalid mode {mode_text!r}\") from exc",
            "         name_end = text.index(b\"\\0\", mode_end)",
            "         name = text[mode_end + 1 : name_end]",
            "         count = name_end + 21",
            "         sha = text[name_end + 1 : count]",
            "         if len(sha) != 20:",
            "             raise ObjectFormatException(\"Sha has invalid length\")",
            "         hexsha = sha_to_hex(sha)",
            "         yield (name, mode, hexsha)",
            " ",
            " ",
            "-def serialize_tree(items):",
            "+def serialize_tree(items: Iterable[tuple[bytes, int, bytes]]) -> Iterator[bytes]:",
            "     \"\"\"Serialize the items in a tree to a text.",
            " ",
            "     Args:",
            "       items: Sorted iterable over (name, mode, sha) tuples",
            "     Returns: Serialized tree text as chunks",
            "     \"\"\"",
            "     for name, mode, hexsha in items:",
            "         yield (",
            "             (f\"{mode:04o}\").encode(\"ascii\") + b\" \" + name + b\"\\0\" + hex_to_sha(hexsha)",
            "         )",
            " ",
            " ",
            "-def sorted_tree_items(entries, name_order: bool):",
            "+def sorted_tree_items(",
            "+    entries: dict[bytes, tuple[int, bytes]], name_order: bool",
            "+) -> Iterator[TreeEntry]:",
            "     \"\"\"Iterate over a tree entries dictionary.",
            " ",
            "     Args:",
            "       name_order: If True, iterate entries in order of their name. If",
            "         False, iterate entries in tree order, that is, treat subtree entries as",
            "         having '/' appended.",
            "       entries: Dictionary mapping names to (mode, sha) tuples",
            "@@ -1052,15 +1129,17 @@",
            " ",
            " ",
            " def key_entry_name_order(entry: tuple[bytes, tuple[int, ObjectID]]) -> bytes:",
            "     \"\"\"Sort key for tree entry in name order.\"\"\"",
            "     return entry[0]",
            " ",
            " ",
            "-def pretty_format_tree_entry(name, mode, hexsha, encoding=\"utf-8\") -> str:",
            "+def pretty_format_tree_entry(",
            "+    name: bytes, mode: int, hexsha: bytes, encoding: str = \"utf-8\"",
            "+) -> str:",
            "     \"\"\"Pretty format tree entry.",
            " ",
            "     Args:",
            "       name: Name of the directory entry",
            "       mode: Mode of entry",
            "       hexsha: Hexsha of the referenced object",
            "     Returns: string describing the tree entry",
            "@@ -1076,15 +1155,15 @@",
            "         name.decode(encoding, \"replace\"),",
            "     )",
            " ",
            " ",
            " class SubmoduleEncountered(Exception):",
            "     \"\"\"A submodule was encountered while resolving a path.\"\"\"",
            " ",
            "-    def __init__(self, path, sha) -> None:",
            "+    def __init__(self, path: bytes, sha: ObjectID) -> None:",
            "         self.path = path",
            "         self.sha = sha",
            " ",
            " ",
            " class Tree(ShaFile):",
            "     \"\"\"A Git tree object.\"\"\"",
            " ",
            "@@ -1094,62 +1173,62 @@",
            "     __slots__ = \"_entries\"",
            " ",
            "     def __init__(self) -> None:",
            "         super().__init__()",
            "         self._entries: dict[bytes, tuple[int, bytes]] = {}",
            " ",
            "     @classmethod",
            "-    def from_path(cls, filename):",
            "+    def from_path(cls, filename: Union[str, bytes]) -> \"Tree\":",
            "         tree = ShaFile.from_path(filename)",
            "         if not isinstance(tree, cls):",
            "             raise NotTreeError(filename)",
            "         return tree",
            " ",
            "-    def __contains__(self, name) -> bool:",
            "+    def __contains__(self, name: bytes) -> bool:",
            "         return name in self._entries",
            " ",
            "-    def __getitem__(self, name):",
            "+    def __getitem__(self, name: bytes) -> tuple[int, ObjectID]:",
            "         return self._entries[name]",
            " ",
            "-    def __setitem__(self, name, value) -> None:",
            "+    def __setitem__(self, name: bytes, value: tuple[int, ObjectID]) -> None:",
            "         \"\"\"Set a tree entry by name.",
            " ",
            "         Args:",
            "           name: The name of the entry, as a string.",
            "           value: A tuple of (mode, hexsha), where mode is the mode of the",
            "             entry as an integral type and hexsha is the hex SHA of the entry as",
            "             a string.",
            "         \"\"\"",
            "         mode, hexsha = value",
            "         self._entries[name] = (mode, hexsha)",
            "         self._needs_serialization = True",
            " ",
            "-    def __delitem__(self, name) -> None:",
            "+    def __delitem__(self, name: bytes) -> None:",
            "         del self._entries[name]",
            "         self._needs_serialization = True",
            " ",
            "     def __len__(self) -> int:",
            "         return len(self._entries)",
            " ",
            "-    def __iter__(self):",
            "+    def __iter__(self) -> Iterator[bytes]:",
            "         return iter(self._entries)",
            " ",
            "-    def add(self, name, mode, hexsha) -> None:",
            "+    def add(self, name: bytes, mode: int, hexsha: bytes) -> None:",
            "         \"\"\"Add an entry to the tree.",
            " ",
            "         Args:",
            "           mode: The mode of the entry as an integral type. Not all",
            "             possible modes are supported by git; see check() for details.",
            "           name: The name of the entry, as a string.",
            "           hexsha: The hex SHA of the entry as a string.",
            "         \"\"\"",
            "         self._entries[name] = mode, hexsha",
            "         self._needs_serialization = True",
            " ",
            "-    def iteritems(self, name_order=False) -> Iterator[TreeEntry]:",
            "+    def iteritems(self, name_order: bool = False) -> Iterator[TreeEntry]:",
            "         \"\"\"Iterate over entries.",
            " ",
            "         Args:",
            "           name_order: If True, iterate in name order instead of tree",
            "             order.",
            "         Returns: Iterator over (name, mode, sha) tuples",
            "         \"\"\"",
            "@@ -1158,15 +1237,15 @@",
            "     def items(self) -> list[TreeEntry]:",
            "         \"\"\"Return the sorted entries in this tree.",
            " ",
            "         Returns: List with (name, mode, sha) tuples",
            "         \"\"\"",
            "         return list(self.iteritems())",
            " ",
            "-    def _deserialize(self, chunks) -> None:",
            "+    def _deserialize(self, chunks: list[bytes]) -> None:",
            "         \"\"\"Grab the entries in the tree.\"\"\"",
            "         try:",
            "             parsed_entries = parse_tree(b\"\".join(chunks))",
            "         except ValueError as exc:",
            "             raise ObjectFormatException(exc) from exc",
            "         # TODO: list comprehension is for efficiency in the common (small)",
            "         # case; if memory efficiency in the large case is a concern, use a",
            "@@ -1188,64 +1267,68 @@",
            "             stat.S_IFLNK,",
            "             stat.S_IFDIR,",
            "             S_IFGITLINK,",
            "             # TODO: optionally exclude as in git fsck --strict",
            "             stat.S_IFREG | 0o664,",
            "         )",
            "         for name, mode, sha in parse_tree(b\"\".join(self._chunked_text), True):",
            "-            check_hexsha(sha, f\"invalid sha {sha}\")",
            "+            check_hexsha(sha, f\"invalid sha {sha!r}\")",
            "             if b\"/\" in name or name in (b\"\", b\".\", b\"..\", b\".git\"):",
            "                 raise ObjectFormatException(",
            "                     \"invalid name {}\".format(name.decode(\"utf-8\", \"replace\"))",
            "                 )",
            " ",
            "             if mode not in allowed_modes:",
            "                 raise ObjectFormatException(f\"invalid mode {mode:06o}\")",
            " ",
            "             entry = (name, (mode, sha))",
            "             if last:",
            "                 if key_entry(last) > key_entry(entry):",
            "                     raise ObjectFormatException(\"entries not sorted\")",
            "                 if name == last[0]:",
            "-                    raise ObjectFormatException(f\"duplicate entry {name}\")",
            "+                    raise ObjectFormatException(f\"duplicate entry {name!r}\")",
            "             last = entry",
            " ",
            "-    def _serialize(self):",
            "+    def _serialize(self) -> list[bytes]:",
            "         return list(serialize_tree(self.iteritems()))",
            " ",
            "     def as_pretty_string(self) -> str:",
            "         text: list[str] = []",
            "         for name, mode, hexsha in self.iteritems():",
            "             text.append(pretty_format_tree_entry(name, mode, hexsha))",
            "         return \"\".join(text)",
            " ",
            "-    def lookup_path(self, lookup_obj: Callable[[ObjectID], ShaFile], path: bytes):",
            "+    def lookup_path(",
            "+        self, lookup_obj: Callable[[ObjectID], ShaFile], path: bytes",
            "+    ) -> tuple[int, ObjectID]:",
            "         \"\"\"Look up an object in a Git tree.",
            " ",
            "         Args:",
            "           lookup_obj: Callback for retrieving object by SHA1",
            "           path: Path to lookup",
            "         Returns: A tuple of (mode, SHA) of the resulting path.",
            "         \"\"\"",
            "         parts = path.split(b\"/\")",
            "         sha = self.id",
            "-        mode = None",
            "+        mode: Optional[int] = None",
            "         for i, p in enumerate(parts):",
            "             if not p:",
            "                 continue",
            "             if mode is not None and S_ISGITLINK(mode):",
            "                 raise SubmoduleEncountered(b\"/\".join(parts[:i]), sha)",
            "             obj = lookup_obj(sha)",
            "             if not isinstance(obj, Tree):",
            "                 raise NotTreeError(sha)",
            "             mode, sha = obj[p]",
            "+        if mode is None:",
            "+            raise ValueError(\"No valid path found\")",
            "         return mode, sha",
            " ",
            " ",
            "-def parse_timezone(text):",
            "+def parse_timezone(text: bytes) -> tuple[int, bool]:",
            "     \"\"\"Parse a timezone text fragment (e.g. '+0100').",
            " ",
            "     Args:",
            "       text: Text to parse.",
            "     Returns: Tuple with timezone as seconds difference to UTC",
            "         and a boolean indicating whether this was a UTC timezone",
            "         prefixed with a negative sign (-0000).",
            "@@ -1266,15 +1349,15 @@",
            "     minutes = offset % 100",
            "     return (",
            "         signum * (hours * 3600 + minutes * 60),",
            "         unnecessary_negative_timezone,",
            "     )",
            " ",
            " ",
            "-def format_timezone(offset, unnecessary_negative_timezone=False):",
            "+def format_timezone(offset: int, unnecessary_negative_timezone: bool = False) -> bytes:",
            "     \"\"\"Format a timezone for Git serialization.",
            " ",
            "     Args:",
            "       offset: Timezone offset as seconds difference to UTC",
            "       unnecessary_negative_timezone: Whether to use a minus sign for",
            "         UTC or positive timezones (-0000 and --700 rather than +0000 / +0700).",
            "     \"\"\"",
            "@@ -1284,15 +1367,17 @@",
            "         sign = \"-\"",
            "         offset = -offset",
            "     else:",
            "         sign = \"+\"",
            "     return (\"%c%02d%02d\" % (sign, offset / 3600, (offset / 60) % 60)).encode(\"ascii\")  # noqa: UP031",
            " ",
            " ",
            "-def parse_time_entry(value):",
            "+def parse_time_entry(",
            "+    value: bytes,",
            "+) -> tuple[bytes, Optional[int], tuple[Optional[int], bool]]:",
            "     \"\"\"Parse event.",
            " ",
            "     Args:",
            "       value: Bytes representing a git commit/tag line",
            "     Raises:",
            "       ObjectFormatException in case of parsing error (malformed",
            "       field date)",
            "@@ -1309,60 +1394,90 @@",
            "         time = int(timetext)",
            "         timezone, timezone_neg_utc = parse_timezone(timezonetext)",
            "     except ValueError as exc:",
            "         raise ObjectFormatException(exc) from exc",
            "     return person, time, (timezone, timezone_neg_utc)",
            " ",
            " ",
            "-def format_time_entry(person, time, timezone_info):",
            "+def format_time_entry(",
            "+    person: bytes, time: int, timezone_info: tuple[int, bool]",
            "+) -> bytes:",
            "     \"\"\"Format an event.\"\"\"",
            "     (timezone, timezone_neg_utc) = timezone_info",
            "     return b\" \".join(",
            "         [person, str(time).encode(\"ascii\"), format_timezone(timezone, timezone_neg_utc)]",
            "     )",
            " ",
            " ",
            "-def parse_commit(chunks):",
            "+@replace_me(since=\"0.21.0\", remove_in=\"0.24.0\")",
            "+def parse_commit(",
            "+    chunks: Iterable[bytes],",
            "+) -> tuple[",
            "+    Optional[bytes],",
            "+    list[bytes],",
            "+    tuple[Optional[bytes], Optional[int], tuple[Optional[int], Optional[bool]]],",
            "+    tuple[Optional[bytes], Optional[int], tuple[Optional[int], Optional[bool]]],",
            "+    Optional[bytes],",
            "+    list[Tag],",
            "+    Optional[bytes],",
            "+    Optional[bytes],",
            "+    list[tuple[bytes, bytes]],",
            "+]:",
            "     \"\"\"Parse a commit object from chunks.",
            " ",
            "     Args:",
            "       chunks: Chunks to parse",
            "     Returns: Tuple of (tree, parents, author_info, commit_info,",
            "         encoding, mergetag, gpgsig, message, extra)",
            "     \"\"\"",
            "-    warnings.warn(\"parse_commit will be removed in 0.22\", DeprecationWarning)",
            "     parents = []",
            "     extra = []",
            "     tree = None",
            "-    author_info = (None, None, (None, None))",
            "-    commit_info = (None, None, (None, None))",
            "+    author_info: tuple[",
            "+        Optional[bytes], Optional[int], tuple[Optional[int], Optional[bool]]",
            "+    ] = (None, None, (None, None))",
            "+    commit_info: tuple[",
            "+        Optional[bytes], Optional[int], tuple[Optional[int], Optional[bool]]",
            "+    ] = (None, None, (None, None))",
            "     encoding = None",
            "     mergetag = []",
            "     message = None",
            "     gpgsig = None",
            " ",
            "     for field, value in _parse_message(chunks):",
            "         # TODO(jelmer): Enforce ordering",
            "         if field == _TREE_HEADER:",
            "             tree = value",
            "         elif field == _PARENT_HEADER:",
            "+            if value is None:",
            "+                raise ObjectFormatException(\"missing parent value\")",
            "             parents.append(value)",
            "         elif field == _AUTHOR_HEADER:",
            "+            if value is None:",
            "+                raise ObjectFormatException(\"missing author value\")",
            "             author_info = parse_time_entry(value)",
            "         elif field == _COMMITTER_HEADER:",
            "+            if value is None:",
            "+                raise ObjectFormatException(\"missing committer value\")",
            "             commit_info = parse_time_entry(value)",
            "         elif field == _ENCODING_HEADER:",
            "             encoding = value",
            "         elif field == _MERGETAG_HEADER:",
            "-            mergetag.append(Tag.from_string(value + b\"\\n\"))",
            "+            if value is None:",
            "+                raise ObjectFormatException(\"missing mergetag value\")",
            "+            tag = Tag.from_string(value + b\"\\n\")",
            "+            assert isinstance(tag, Tag)",
            "+            mergetag.append(tag)",
            "         elif field == _GPGSIG_HEADER:",
            "             gpgsig = value",
            "         elif field is None:",
            "             message = value",
            "         else:",
            "+            if value is None:",
            "+                raise ObjectFormatException(f\"missing value for field {field!r}\")",
            "             extra.append((field, value))",
            "     return (",
            "         tree,",
            "         parents,",
            "         author_info,",
            "         commit_info,",
            "         encoding,",
            "@@ -1404,47 +1519,57 @@",
            "         self._mergetag: list[Tag] = []",
            "         self._gpgsig: Optional[bytes] = None",
            "         self._extra: list[tuple[bytes, Optional[bytes]]] = []",
            "         self._author_timezone_neg_utc: Optional[bool] = False",
            "         self._commit_timezone_neg_utc: Optional[bool] = False",
            " ",
            "     @classmethod",
            "-    def from_path(cls, path):",
            "+    def from_path(cls, path: Union[str, bytes]) -> \"Commit\":",
            "         commit = ShaFile.from_path(path)",
            "         if not isinstance(commit, cls):",
            "             raise NotCommitError(path)",
            "         return commit",
            " ",
            "-    def _deserialize(self, chunks) -> None:",
            "+    def _deserialize(self, chunks: list[bytes]) -> None:",
            "         self._parents = []",
            "         self._extra = []",
            "         self._tree = None",
            "-        author_info = (None, None, (None, None))",
            "-        commit_info = (None, None, (None, None))",
            "+        author_info: tuple[",
            "+            Optional[bytes], Optional[int], tuple[Optional[int], Optional[bool]]",
            "+        ] = (None, None, (None, None))",
            "+        commit_info: tuple[",
            "+            Optional[bytes], Optional[int], tuple[Optional[int], Optional[bool]]",
            "+        ] = (None, None, (None, None))",
            "         self._encoding = None",
            "         self._mergetag = []",
            "         self._message = None",
            "         self._gpgsig = None",
            " ",
            "         for field, value in _parse_message(chunks):",
            "             # TODO(jelmer): Enforce ordering",
            "             if field == _TREE_HEADER:",
            "                 self._tree = value",
            "             elif field == _PARENT_HEADER:",
            "                 assert value is not None",
            "                 self._parents.append(value)",
            "             elif field == _AUTHOR_HEADER:",
            "+                if value is None:",
            "+                    raise ObjectFormatException(\"missing author value\")",
            "                 author_info = parse_time_entry(value)",
            "             elif field == _COMMITTER_HEADER:",
            "+                if value is None:",
            "+                    raise ObjectFormatException(\"missing committer value\")",
            "                 commit_info = parse_time_entry(value)",
            "             elif field == _ENCODING_HEADER:",
            "                 self._encoding = value",
            "             elif field == _MERGETAG_HEADER:",
            "                 assert value is not None",
            "-                self._mergetag.append(Tag.from_string(value + b\"\\n\"))",
            "+                tag = Tag.from_string(value + b\"\\n\")",
            "+                assert isinstance(tag, Tag)",
            "+                self._mergetag.append(tag)",
            "             elif field == _GPGSIG_HEADER:",
            "                 self._gpgsig = value",
            "             elif field is None:",
            "                 self._message = value",
            "             else:",
            "                 self._extra.append((field, value))",
            " ",
            "@@ -1471,19 +1596,24 @@",
            "         self._check_has_member(\"_author\", \"missing author\")",
            "         self._check_has_member(\"_committer\", \"missing committer\")",
            "         self._check_has_member(\"_author_time\", \"missing author time\")",
            "         self._check_has_member(\"_commit_time\", \"missing commit time\")",
            " ",
            "         for parent in self._parents:",
            "             check_hexsha(parent, \"invalid parent sha\")",
            "+        assert self._tree is not None  # checked by _check_has_member above",
            "         check_hexsha(self._tree, \"invalid tree sha\")",
            " ",
            "+        assert self._author is not None  # checked by _check_has_member above",
            "+        assert self._committer is not None  # checked by _check_has_member above",
            "         check_identity(self._author, \"invalid author\")",
            "         check_identity(self._committer, \"invalid committer\")",
            " ",
            "+        assert self._author_time is not None  # checked by _check_has_member above",
            "+        assert self._commit_time is not None  # checked by _check_has_member above",
            "         check_time(self._author_time)",
            "         check_time(self._commit_time)",
            " ",
            "         last = None",
            "         for field, _ in _parse_message(self._chunked_text):",
            "             if field == _TREE_HEADER and last is not None:",
            "                 raise ObjectFormatException(\"unexpected tree\")",
            "@@ -1561,73 +1691,80 @@",
            "                 for key in keys:",
            "                     for subkey in keys:",
            "                         for sig in result.signatures:",
            "                             if subkey.can_sign and subkey.fpr == sig.fpr:",
            "                                 return",
            "                 raise gpg.errors.MissingSignatures(result, keys, results=(data, result))",
            " ",
            "-    def _serialize(self):",
            "+    def _serialize(self) -> list[bytes]:",
            "         headers = []",
            "+        assert self._tree is not None",
            "         tree_bytes = self._tree.id if isinstance(self._tree, Tree) else self._tree",
            "         headers.append((_TREE_HEADER, tree_bytes))",
            "         for p in self._parents:",
            "             headers.append((_PARENT_HEADER, p))",
            "+        assert self._author is not None",
            "+        assert self._author_time is not None",
            "+        assert self._author_timezone is not None",
            "+        assert self._author_timezone_neg_utc is not None",
            "         headers.append(",
            "             (",
            "                 _AUTHOR_HEADER,",
            "                 format_time_entry(",
            "                     self._author,",
            "                     self._author_time,",
            "                     (self._author_timezone, self._author_timezone_neg_utc),",
            "                 ),",
            "             )",
            "         )",
            "+        assert self._committer is not None",
            "+        assert self._commit_time is not None",
            "+        assert self._commit_timezone is not None",
            "+        assert self._commit_timezone_neg_utc is not None",
            "         headers.append(",
            "             (",
            "                 _COMMITTER_HEADER,",
            "                 format_time_entry(",
            "                     self._committer,",
            "                     self._commit_time,",
            "                     (self._commit_timezone, self._commit_timezone_neg_utc),",
            "                 ),",
            "             )",
            "         )",
            "         if self.encoding:",
            "             headers.append((_ENCODING_HEADER, self.encoding))",
            "         for mergetag in self.mergetag:",
            "             headers.append((_MERGETAG_HEADER, mergetag.as_raw_string()[:-1]))",
            "-        headers.extend(self._extra)",
            "+        headers.extend(",
            "+            (field, value) for field, value in self._extra if value is not None",
            "+        )",
            "         if self.gpgsig:",
            "             headers.append((_GPGSIG_HEADER, self.gpgsig))",
            "         return list(_format_message(headers, self._message))",
            " ",
            "     tree = serializable_property(\"tree\", \"Tree that is the state of this commit\")",
            " ",
            "-    def _get_parents(self):",
            "+    def _get_parents(self) -> list[bytes]:",
            "         \"\"\"Return a list of parents of this commit.\"\"\"",
            "         return self._parents",
            " ",
            "-    def _set_parents(self, value) -> None:",
            "+    def _set_parents(self, value: list[bytes]) -> None:",
            "         \"\"\"Set a list of parents of this commit.\"\"\"",
            "         self._needs_serialization = True",
            "         self._parents = value",
            " ",
            "     parents = property(",
            "         _get_parents,",
            "         _set_parents,",
            "         doc=\"Parents of this commit, by their SHA1.\",",
            "     )",
            " ",
            "-    def _get_extra(self):",
            "+    @replace_me(since=\"0.21.0\", remove_in=\"0.24.0\")",
            "+    def _get_extra(self) -> list[tuple[bytes, Optional[bytes]]]:",
            "         \"\"\"Return extra settings of this commit.\"\"\"",
            "-        warnings.warn(",
            "-            \"Commit.extra is deprecated. Use Commit._extra instead.\",",
            "-            DeprecationWarning,",
            "-            stacklevel=2,",
            "-        )",
            "         return self._extra",
            " ",
            "     extra = property(",
            "         _get_extra,",
            "         doc=\"Extra header fields not understood (presumably added in a \"",
            "         \"newer version of git). Kept verbatim so the object can \"",
            "         \"be correctly reserialized. For private commit metadata, use \"",
            "@@ -1640,15 +1777,15 @@",
            "         \"committer\", \"The name of the committer of the commit\"",
            "     )",
            " ",
            "     message = serializable_property(\"message\", \"The commit message\")",
            " ",
            "     commit_time = serializable_property(",
            "         \"commit_time\",",
            "-        \"The timestamp of the commit. As the number of seconds since the \" \"epoch.\",",
            "+        \"The timestamp of the commit. As the number of seconds since the epoch.\",",
            "     )",
            " ",
            "     commit_timezone = serializable_property(",
            "         \"commit_timezone\", \"The zone the commit time is in\"",
            "     )",
            " ",
            "     author_time = serializable_property("
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/objectspec.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/objectspec.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/objectspec.py",
            "@@ -20,15 +20,15 @@",
            " #",
            " ",
            " \"\"\"Object specification.\"\"\"",
            " ",
            " from collections.abc import Iterator",
            " from typing import TYPE_CHECKING, Optional, Union",
            " ",
            "-from .objects import Commit, ShaFile, Tree",
            "+from .objects import Commit, ShaFile, Tag, Tree",
            " ",
            " if TYPE_CHECKING:",
            "     from .refs import Ref, RefsContainer",
            "     from .repo import Repo",
            " ",
            " ",
            " def to_bytes(text: Union[str, bytes]) -> bytes:",
            "@@ -62,15 +62,23 @@",
            "       KeyError: If the object can not be found",
            "     \"\"\"",
            "     treeish = to_bytes(treeish)",
            "     try:",
            "         treeish = parse_ref(repo, treeish)",
            "     except KeyError:  # treeish is commit sha",
            "         pass",
            "-    o = repo[treeish]",
            "+    try:",
            "+        o = repo[treeish]",
            "+    except KeyError:",
            "+        # Try parsing as commit (handles short hashes)",
            "+        try:",
            "+            commit = parse_commit(repo, treeish)",
            "+            return repo[commit.tree]",
            "+        except KeyError:",
            "+            raise KeyError(treeish)",
            "     if o.type_name == b\"commit\":",
            "         return repo[o.tree]",
            "     return o",
            " ",
            " ",
            " def parse_ref(",
            "     container: Union[\"Repo\", \"RefsContainer\"], refspec: Union[str, bytes]",
            "@@ -233,30 +241,50 @@",
            "       repo: A` Repo` object",
            "       committish: A string referring to a single commit.",
            "     Returns: A Commit object",
            "     Raises:",
            "       KeyError: When the reference commits can not be found",
            "       ValueError: If the range can not be parsed",
            "     \"\"\"",
            "+",
            "+    def dereference_tag(obj):",
            "+        \"\"\"Follow tag references until we reach a non-tag object.\"\"\"",
            "+        while isinstance(obj, Tag):",
            "+            obj_type, obj_sha = obj.object",
            "+            try:",
            "+                obj = repo.object_store[obj_sha]",
            "+            except KeyError:",
            "+                # Tag points to a missing object",
            "+                raise KeyError(obj_sha)",
            "+        if not isinstance(obj, Commit):",
            "+            raise ValueError(f\"Expected commit, got {obj.type_name}\")",
            "+        return obj",
            "+",
            "     committish = to_bytes(committish)",
            "     try:",
            "-        return repo[committish]",
            "+        obj = repo[committish]",
            "     except KeyError:",
            "         pass",
            "+    else:",
            "+        return dereference_tag(obj)",
            "     try:",
            "-        return repo[parse_ref(repo, committish)]",
            "+        obj = repo[parse_ref(repo, committish)]",
            "     except KeyError:",
            "         pass",
            "+    else:",
            "+        return dereference_tag(obj)",
            "     if len(committish) >= 4 and len(committish) < 40:",
            "         try:",
            "             int(committish, 16)",
            "         except ValueError:",
            "             pass",
            "         else:",
            "             try:",
            "-                return scan_for_short_id(repo.object_store, committish, Commit)",
            "+                obj = scan_for_short_id(repo.object_store, committish, Commit)",
            "             except KeyError:",
            "                 pass",
            "+            else:",
            "+                return dereference_tag(obj)",
            "     raise KeyError(committish)",
            " ",
            " ",
            " # TODO: parse_path_in_tree(), which handles e.g. v1.0:Documentation"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/pack.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/pack.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/pack.py",
            "@@ -70,14 +70,15 @@",
            " else:",
            "     has_mmap = True",
            " ",
            " # For some reason the above try, except fails to set has_mmap = False for plan9",
            " if sys.platform == \"Plan9\":",
            "     has_mmap = False",
            " ",
            "+from . import replace_me",
            " from .errors import ApplyDeltaError, ChecksumMismatch",
            " from .file import GitFile",
            " from .lru_cache import LRUSizeCache",
            " from .objects import ObjectID, ShaFile, hex_to_sha, object_header, sha_to_hex",
            " ",
            " OFS_DELTA = 6",
            " REF_DELTA = 7",
            "@@ -86,14 +87,17 @@",
            " ",
            " ",
            " DEFAULT_PACK_DELTA_WINDOW_SIZE = 10",
            " ",
            " # Keep pack files under 16Mb in memory, otherwise write them out to disk",
            " PACK_SPOOL_FILE_MAX_SIZE = 16 * 1024 * 1024",
            " ",
            "+# Default pack index version to use when none is specified",
            "+DEFAULT_PACK_INDEX_VERSION = 2",
            "+",
            " ",
            " OldUnpackedObject = Union[tuple[Union[bytes, int], list[bytes]], list[bytes]]",
            " ResolveExtRefFn = Callable[[bytes], tuple[int, OldUnpackedObject]]",
            " ProgressFn = Callable[[int, str], None]",
            " PackHint = tuple[int, Optional[bytes]]",
            " ",
            " ",
            "@@ -274,15 +278,15 @@",
            "         return not (self == other)",
            " ",
            "     def __repr__(self) -> str:",
            "         data = [f\"{s}={getattr(self, s)!r}\" for s in self.__slots__]",
            "         return \"{}({})\".format(self.__class__.__name__, \", \".join(data))",
            " ",
            " ",
            "-_ZLIB_BUFSIZE = 4096",
            "+_ZLIB_BUFSIZE = 65536  # 64KB buffer for better I/O performance",
            " ",
            " ",
            " def read_zlib_chunks(",
            "     read_some: Callable[[int], bytes],",
            "     unpacked: UnpackedObject,",
            "     include_comp: bool = False,",
            "     buffer_size: int = _ZLIB_BUFSIZE,",
            "@@ -358,15 +362,15 @@",
            "     \"\"\"",
            "     sha = sha1()",
            "     for name in iter:",
            "         sha.update(name)",
            "     return sha.hexdigest().encode(\"ascii\")",
            " ",
            " ",
            "-def load_pack_index(path):",
            "+def load_pack_index(path: Union[str, os.PathLike]):",
            "     \"\"\"Load an index file by path.",
            " ",
            "     Args:",
            "       path: Path to the index file",
            "     Returns: A PackIndex loaded from the given path",
            "     \"\"\"",
            "     with GitFile(path, \"rb\") as f:",
            "@@ -381,37 +385,39 @@",
            "     # Attempt to use mmap if possible",
            "     if fd is not None:",
            "         if size is None:",
            "             size = os.fstat(fd).st_size",
            "         if has_mmap:",
            "             try:",
            "                 contents = mmap.mmap(fd, size, access=mmap.ACCESS_READ)",
            "-            except OSError:",
            "-                # Perhaps a socket?",
            "+            except (OSError, ValueError):",
            "+                # Can't mmap - perhaps a socket or invalid file descriptor",
            "                 pass",
            "             else:",
            "                 return contents, size",
            "     contents = f.read()",
            "     size = len(contents)",
            "     return contents, size",
            " ",
            " ",
            "-def load_pack_index_file(path, f):",
            "+def load_pack_index_file(path: Union[str, os.PathLike], f):",
            "     \"\"\"Load an index file from a file-like object.",
            " ",
            "     Args:",
            "       path: Path for the index file",
            "       f: File-like object",
            "     Returns: A PackIndex loaded from the given file",
            "     \"\"\"",
            "     contents, size = _load_file_contents(f)",
            "     if contents[:4] == b\"\\377tOc\":",
            "         version = struct.unpack(b\">L\", contents[4:8])[0]",
            "         if version == 2:",
            "             return PackIndex2(path, file=f, contents=contents, size=size)",
            "+        elif version == 3:",
            "+            return PackIndex3(path, file=f, contents=contents, size=size)",
            "         else:",
            "             raise KeyError(f\"Unknown pack index format {version}\")",
            "     else:",
            "         return PackIndex1(path, file=f, contents=contents, size=size)",
            " ",
            " ",
            " def bisect_find_sha(start, end, sha, unpack_name):",
            "@@ -443,14 +449,18 @@",
            " class PackIndex:",
            "     \"\"\"An index in to a packfile.",
            " ",
            "     Given a sha id of an object a pack index can tell you the location in the",
            "     packfile of that object if it has it.",
            "     \"\"\"",
            " ",
            "+    # Default to SHA-1 for backward compatibility",
            "+    hash_algorithm = 1",
            "+    hash_size = 20",
            "+",
            "     def __eq__(self, other):",
            "         if not isinstance(other, PackIndex):",
            "             return False",
            " ",
            "         for (name1, _, _), (name2, _, _) in zip(",
            "             self.iterentries(), other.iterentries()",
            "         ):",
            "@@ -480,18 +490,16 @@",
            "     def get_pack_checksum(self) -> bytes:",
            "         \"\"\"Return the SHA1 checksum stored for the corresponding packfile.",
            " ",
            "         Returns: 20-byte binary digest",
            "         \"\"\"",
            "         raise NotImplementedError(self.get_pack_checksum)",
            " ",
            "+    @replace_me(since=\"0.21.0\", remove_in=\"0.23.0\")",
            "     def object_index(self, sha: bytes) -> int:",
            "-        warnings.warn(",
            "-            \"Please use object_offset instead\", DeprecationWarning, stacklevel=2",
            "-        )",
            "         return self.object_offset(sha)",
            " ",
            "     def object_offset(self, sha: bytes) -> int:",
            "         \"\"\"Return the offset in to the corresponding packfile for the object.",
            " ",
            "         Given the name of an object it will return the offset that object",
            "         lives at within the corresponding pack file. If the pack file doesn't",
            "@@ -762,15 +770,17 @@",
            "             elif started:",
            "                 break",
            " ",
            " ",
            " class PackIndex1(FilePackIndex):",
            "     \"\"\"Version 1 Pack Index file.\"\"\"",
            " ",
            "-    def __init__(self, filename: str, file=None, contents=None, size=None) -> None:",
            "+    def __init__(",
            "+        self, filename: Union[str, os.PathLike], file=None, contents=None, size=None",
            "+    ) -> None:",
            "         super().__init__(filename, file, contents, size)",
            "         self.version = 1",
            "         self._fan_out_table = self._read_fan_out_table(0)",
            " ",
            "     def _unpack_entry(self, i):",
            "         (offset, name) = unpack_from(\">L20s\", self._contents, (0x100 * 4) + (i * 24))",
            "         return (name, offset, None)",
            "@@ -787,15 +797,17 @@",
            "         # Not stored in v1 index files",
            "         return None",
            " ",
            " ",
            " class PackIndex2(FilePackIndex):",
            "     \"\"\"Version 2 Pack Index file.\"\"\"",
            " ",
            "-    def __init__(self, filename: str, file=None, contents=None, size=None) -> None:",
            "+    def __init__(",
            "+        self, filename: Union[str, os.PathLike], file=None, contents=None, size=None",
            "+    ) -> None:",
            "         super().__init__(filename, file, contents, size)",
            "         if self._contents[:4] != b\"\\377tOc\":",
            "             raise AssertionError(\"Not a v2 pack index file\")",
            "         (self.version,) = unpack_from(b\">L\", self._contents, 4)",
            "         if self.version != 2:",
            "             raise AssertionError(f\"Version was {self.version}\")",
            "         self._fan_out_table = self._read_fan_out_table(8)",
            "@@ -825,14 +837,76 @@",
            "             offset = unpack_from(\">Q\", self._contents, offset)[0]",
            "         return offset",
            " ",
            "     def _unpack_crc32_checksum(self, i):",
            "         return unpack_from(\">L\", self._contents, self._crc32_table_offset + i * 4)[0]",
            " ",
            " ",
            "+class PackIndex3(FilePackIndex):",
            "+    \"\"\"Version 3 Pack Index file.",
            "+",
            "+    Supports variable hash sizes for SHA-1 (20 bytes) and SHA-256 (32 bytes).",
            "+    \"\"\"",
            "+",
            "+    def __init__(",
            "+        self, filename: Union[str, os.PathLike], file=None, contents=None, size=None",
            "+    ) -> None:",
            "+        super().__init__(filename, file, contents, size)",
            "+        if self._contents[:4] != b\"\\377tOc\":",
            "+            raise AssertionError(\"Not a v3 pack index file\")",
            "+        (self.version,) = unpack_from(b\">L\", self._contents, 4)",
            "+        if self.version != 3:",
            "+            raise AssertionError(f\"Version was {self.version}\")",
            "+",
            "+        # Read hash algorithm identifier (1 = SHA-1, 2 = SHA-256)",
            "+        (self.hash_algorithm,) = unpack_from(b\">L\", self._contents, 8)",
            "+        if self.hash_algorithm == 1:",
            "+            self.hash_size = 20  # SHA-1",
            "+        elif self.hash_algorithm == 2:",
            "+            self.hash_size = 32  # SHA-256",
            "+        else:",
            "+            raise AssertionError(f\"Unknown hash algorithm {self.hash_algorithm}\")",
            "+",
            "+        # Read length of shortened object names",
            "+        (self.shortened_oid_len,) = unpack_from(b\">L\", self._contents, 12)",
            "+",
            "+        # Calculate offsets based on variable hash size",
            "+        self._fan_out_table = self._read_fan_out_table(",
            "+            16",
            "+        )  # After header (4 + 4 + 4 + 4)",
            "+        self._name_table_offset = 16 + 0x100 * 4",
            "+        self._crc32_table_offset = self._name_table_offset + self.hash_size * len(self)",
            "+        self._pack_offset_table_offset = self._crc32_table_offset + 4 * len(self)",
            "+        self._pack_offset_largetable_offset = self._pack_offset_table_offset + 4 * len(",
            "+            self",
            "+        )",
            "+",
            "+    def _unpack_entry(self, i):",
            "+        return (",
            "+            self._unpack_name(i),",
            "+            self._unpack_offset(i),",
            "+            self._unpack_crc32_checksum(i),",
            "+        )",
            "+",
            "+    def _unpack_name(self, i):",
            "+        offset = self._name_table_offset + i * self.hash_size",
            "+        return self._contents[offset : offset + self.hash_size]",
            "+",
            "+    def _unpack_offset(self, i):",
            "+        offset = self._pack_offset_table_offset + i * 4",
            "+        offset = unpack_from(\">L\", self._contents, offset)[0]",
            "+        if offset & (2**31):",
            "+            offset = self._pack_offset_largetable_offset + (offset & (2**31 - 1)) * 8",
            "+            offset = unpack_from(\">Q\", self._contents, offset)[0]",
            "+        return offset",
            "+",
            "+    def _unpack_crc32_checksum(self, i):",
            "+        return unpack_from(\">L\", self._contents, self._crc32_table_offset + i * 4)[0]",
            "+",
            "+",
            " def read_pack_header(read) -> tuple[int, int]:",
            "     \"\"\"Read the header of a pack file.",
            " ",
            "     Args:",
            "       read: Read function",
            "     Returns: Tuple of (pack version, number of objects). If no data is",
            "         available to read, returns (None, None).",
            "@@ -1188,15 +1262,15 @@",
            "     get mmap sorted out it will have to do.",
            " ",
            "     Currently there are no integrity checks done. Also no attempt is made to",
            "     try and detect the delta case, or a request for an object at the wrong",
            "     position.  It will all just throw a zlib or KeyError.",
            "     \"\"\"",
            " ",
            "-    def __init__(self, filename, file=None, size=None) -> None:",
            "+    def __init__(self, filename: Union[str, os.PathLike], file=None, size=None) -> None:",
            "         \"\"\"Create a PackData object representing the pack in the given filename.",
            " ",
            "         The file must exist and stay readable until the object is disposed of.",
            "         It must also stay the same size. It will be mapped whenever needed.",
            " ",
            "         Currently there is a restriction on the size of the pack as the python",
            "         mmap implementation is flawed.",
            "@@ -1222,15 +1296,15 @@",
            "         return self._filename",
            " ",
            "     @classmethod",
            "     def from_file(cls, file, size=None):",
            "         return cls(str(file), file=file, size=size)",
            " ",
            "     @classmethod",
            "-    def from_path(cls, path):",
            "+    def from_path(cls, path: Union[str, os.PathLike]):",
            "         return cls(filename=path)",
            " ",
            "     def close(self) -> None:",
            "         self._file.close()",
            " ",
            "     def __enter__(self):",
            "         return self",
            "@@ -1336,30 +1410,62 @@",
            "         \"\"\"",
            "         entries = self.sorted_entries(",
            "             progress=progress, resolve_ext_ref=resolve_ext_ref",
            "         )",
            "         with GitFile(filename, \"wb\") as f:",
            "             return write_pack_index_v2(f, entries, self.calculate_checksum())",
            " ",
            "-    def create_index(self, filename, progress=None, version=2, resolve_ext_ref=None):",
            "+    def create_index_v3(",
            "+        self, filename, progress=None, resolve_ext_ref=None, hash_algorithm=1",
            "+    ):",
            "+        \"\"\"Create a version 3 index file for this data file.",
            "+",
            "+        Args:",
            "+          filename: Index filename.",
            "+          progress: Progress report function",
            "+          resolve_ext_ref: Function to resolve external references",
            "+          hash_algorithm: Hash algorithm identifier (1 = SHA-1, 2 = SHA-256)",
            "+        Returns: Checksum of index file",
            "+        \"\"\"",
            "+        entries = self.sorted_entries(",
            "+            progress=progress, resolve_ext_ref=resolve_ext_ref",
            "+        )",
            "+        with GitFile(filename, \"wb\") as f:",
            "+            return write_pack_index_v3(",
            "+                f, entries, self.calculate_checksum(), hash_algorithm",
            "+            )",
            "+",
            "+    def create_index(",
            "+        self, filename, progress=None, version=2, resolve_ext_ref=None, hash_algorithm=1",
            "+    ):",
            "         \"\"\"Create an  index file for this data file.",
            " ",
            "         Args:",
            "           filename: Index filename.",
            "           progress: Progress report function",
            "+          version: Index version (1, 2, or 3)",
            "+          resolve_ext_ref: Function to resolve external references",
            "+          hash_algorithm: Hash algorithm identifier for v3 (1 = SHA-1, 2 = SHA-256)",
            "         Returns: Checksum of index file",
            "         \"\"\"",
            "         if version == 1:",
            "             return self.create_index_v1(",
            "                 filename, progress, resolve_ext_ref=resolve_ext_ref",
            "             )",
            "         elif version == 2:",
            "             return self.create_index_v2(",
            "                 filename, progress, resolve_ext_ref=resolve_ext_ref",
            "             )",
            "+        elif version == 3:",
            "+            return self.create_index_v3(",
            "+                filename,",
            "+                progress,",
            "+                resolve_ext_ref=resolve_ext_ref,",
            "+                hash_algorithm=hash_algorithm,",
            "+            )",
            "         else:",
            "             raise ValueError(f\"unknown index format {version}\")",
            " ",
            "     def get_stored_checksum(self):",
            "         \"\"\"Return the expected checksum stored in this pack.\"\"\"",
            "         self._file.seek(-20, SEEK_END)",
            "         return self._file.read(20)",
            "@@ -1589,50 +1695,108 @@",
            " class PackInflater(DeltaChainIterator[ShaFile]):",
            "     \"\"\"Delta chain iterator that yields ShaFile objects.\"\"\"",
            " ",
            "     def _result(self, unpacked):",
            "         return unpacked.sha_file()",
            " ",
            " ",
            "-class SHA1Reader:",
            "+class SHA1Reader(BinaryIO):",
            "     \"\"\"Wrapper for file-like object that remembers the SHA1 of its data.\"\"\"",
            " ",
            "     def __init__(self, f) -> None:",
            "         self.f = f",
            "         self.sha1 = sha1(b\"\")",
            " ",
            "-    def read(self, num=None):",
            "-        data = self.f.read(num)",
            "+    def read(self, size: int = -1) -> bytes:",
            "+        data = self.f.read(size)",
            "         self.sha1.update(data)",
            "         return data",
            " ",
            "-    def check_sha(self) -> None:",
            "+    def check_sha(self, allow_empty: bool = False) -> None:",
            "         stored = self.f.read(20)",
            "-        if stored != self.sha1.digest():",
            "+        # If git option index.skipHash is set the index will be empty",
            "+        if stored != self.sha1.digest() and (",
            "+            not allow_empty",
            "+            or sha_to_hex(stored) != b\"0000000000000000000000000000000000000000\"",
            "+        ):",
            "             raise ChecksumMismatch(self.sha1.hexdigest(), sha_to_hex(stored))",
            " ",
            "     def close(self):",
            "         return self.f.close()",
            " ",
            "-    def tell(self):",
            "+    def tell(self) -> int:",
            "         return self.f.tell()",
            " ",
            "+    # BinaryIO abstract methods",
            "+    def readable(self) -> bool:",
            "+        return True",
            "+",
            "+    def writable(self) -> bool:",
            "+        return False",
            "+",
            "+    def seekable(self) -> bool:",
            "+        return getattr(self.f, \"seekable\", lambda: False)()",
            "+",
            "+    def seek(self, offset: int, whence: int = 0) -> int:",
            "+        return self.f.seek(offset, whence)",
            "+",
            "+    def flush(self) -> None:",
            "+        if hasattr(self.f, \"flush\"):",
            "+            self.f.flush()",
            "+",
            "+    def readline(self, size: int = -1) -> bytes:",
            "+        return self.f.readline(size)",
            "+",
            "+    def readlines(self, hint: int = -1) -> list[bytes]:",
            "+        return self.f.readlines(hint)",
            "+",
            "+    def writelines(self, lines) -> None:",
            "+        raise UnsupportedOperation(\"writelines\")",
            "+",
            "+    def write(self, data) -> int:",
            "+        raise UnsupportedOperation(\"write\")",
            "+",
            "+    def __enter__(self):",
            "+        return self",
            "+",
            "+    def __exit__(self, type, value, traceback):",
            "+        self.close()",
            "+",
            "+    def __iter__(self):",
            "+        return self",
            "+",
            "+    def __next__(self) -> bytes:",
            "+        line = self.readline()",
            "+        if not line:",
            "+            raise StopIteration",
            "+        return line",
            "+",
            "+    def fileno(self) -> int:",
            "+        return self.f.fileno()",
            " ",
            "-class SHA1Writer:",
            "+    def isatty(self) -> bool:",
            "+        return getattr(self.f, \"isatty\", lambda: False)()",
            "+",
            "+    def truncate(self, size: Optional[int] = None) -> int:",
            "+        raise UnsupportedOperation(\"truncate\")",
            "+",
            "+",
            "+class SHA1Writer(BinaryIO):",
            "     \"\"\"Wrapper for file-like object that remembers the SHA1 of its data.\"\"\"",
            " ",
            "     def __init__(self, f) -> None:",
            "         self.f = f",
            "         self.length = 0",
            "         self.sha1 = sha1(b\"\")",
            " ",
            "-    def write(self, data) -> None:",
            "+    def write(self, data) -> int:",
            "         self.sha1.update(data)",
            "         self.f.write(data)",
            "         self.length += len(data)",
            "+        return len(data)",
            " ",
            "     def write_sha(self):",
            "         sha = self.sha1.digest()",
            "         assert len(sha) == 20",
            "         self.f.write(sha)",
            "         self.length += len(sha)",
            "         return sha",
            "@@ -1641,17 +1805,68 @@",
            "         sha = self.write_sha()",
            "         self.f.close()",
            "         return sha",
            " ",
            "     def offset(self):",
            "         return self.length",
            " ",
            "-    def tell(self):",
            "+    def tell(self) -> int:",
            "         return self.f.tell()",
            " ",
            "+    # BinaryIO abstract methods",
            "+    def readable(self) -> bool:",
            "+        return False",
            "+",
            "+    def writable(self) -> bool:",
            "+        return True",
            "+",
            "+    def seekable(self) -> bool:",
            "+        return getattr(self.f, \"seekable\", lambda: False)()",
            "+",
            "+    def seek(self, offset: int, whence: int = 0) -> int:",
            "+        return self.f.seek(offset, whence)",
            "+",
            "+    def flush(self) -> None:",
            "+        if hasattr(self.f, \"flush\"):",
            "+            self.f.flush()",
            "+",
            "+    def readline(self, size: int = -1) -> bytes:",
            "+        raise UnsupportedOperation(\"readline\")",
            "+",
            "+    def readlines(self, hint: int = -1) -> list[bytes]:",
            "+        raise UnsupportedOperation(\"readlines\")",
            "+",
            "+    def writelines(self, lines) -> None:",
            "+        for line in lines:",
            "+            self.write(line)",
            "+",
            "+    def read(self, size: int = -1) -> bytes:",
            "+        raise UnsupportedOperation(\"read\")",
            "+",
            "+    def __enter__(self):",
            "+        return self",
            "+",
            "+    def __exit__(self, type, value, traceback):",
            "+        self.close()",
            "+",
            "+    def __iter__(self):",
            "+        return self",
            "+",
            "+    def __next__(self) -> bytes:",
            "+        raise UnsupportedOperation(\"__next__\")",
            "+",
            "+    def fileno(self) -> int:",
            "+        return self.f.fileno()",
            "+",
            "+    def isatty(self) -> bool:",
            "+        return getattr(self.f, \"isatty\", lambda: False)()",
            "+",
            "+    def truncate(self, size: Optional[int] = None) -> int:",
            "+        raise UnsupportedOperation(\"truncate\")",
            "+",
            " ",
            " def pack_object_header(type_num, delta_base, size):",
            "     \"\"\"Create a pack object header for the given object info.",
            " ",
            "     Args:",
            "       type_num: Numeric type of the object.",
            "       delta_base: Delta base offset or ref, or None for whole objects.",
            "@@ -1729,16 +1944,14 @@",
            "     delta_window_size: Optional[int] = None,",
            "     compression_level: int = -1,",
            " ):",
            "     \"\"\"Write a new pack data file.",
            " ",
            "     Args:",
            "       filename: Path to the new pack file (without .pack extension)",
            "-      container: PackedObjectContainer",
            "-      entries: Sequence of (object_id, path) tuples to write",
            "       delta_window_size: Delta window size",
            "       deltify: Whether to deltify pack objects",
            "       compression_level: the zlib compression level",
            "     Returns: Tuple with checksum of pack file and index file",
            "     \"\"\"",
            "     with GitFile(filename + \".pack\", \"wb\") as f:",
            "         entries, data_sum = write_pack_objects(",
            "@@ -1746,15 +1959,15 @@",
            "             objects,",
            "             delta_window_size=delta_window_size,",
            "             deltify=deltify,",
            "             compression_level=compression_level,",
            "         )",
            "     entries = sorted([(k, v[0], v[1]) for (k, v) in entries.items()])",
            "     with GitFile(filename + \".idx\", \"wb\") as f:",
            "-        return data_sum, write_pack_index_v2(f, entries, data_sum)",
            "+        return data_sum, write_pack_index(f, entries, data_sum)",
            " ",
            " ",
            " def pack_header_chunks(num_objects):",
            "     \"\"\"Yield chunks for a pack header.\"\"\"",
            "     yield b\"PACK\"  # Pack header",
            "     yield struct.pack(b\">L\", 2)  # Pack version",
            "     yield struct.pack(b\">L\", num_objects)  # Number of objects in pack",
            "@@ -1787,15 +2000,15 @@",
            "         container.iter_unpacked_subset(",
            "             object_ids, allow_missing=True, convert_ofs_delta=True",
            "         )",
            "     ):",
            "         if progress is not None and i % 1000 == 0:",
            "             progress(f\"checking for reusable deltas: {i}/{len(object_ids)}\\r\".encode())",
            "         if unpacked.pack_type_num == REF_DELTA:",
            "-            hexsha = sha_to_hex(unpacked.delta_base)",
            "+            hexsha = sha_to_hex(unpacked.delta_base)  # type: ignore",
            "             if hexsha in object_ids or hexsha in other_haves:",
            "                 yield unpacked",
            "                 reused += 1",
            "     if progress is not None:",
            "         progress((f\"found {reused} deltas to reuse\\n\").encode())",
            " ",
            " ",
            "@@ -1939,16 +2152,14 @@",
            "     reuse_deltas: bool = True,",
            "     ofs_delta: bool = True,",
            "     other_haves: Optional[set[bytes]] = None,",
            "     progress=None,",
            " ) -> Iterator[UnpackedObject]:",
            "     \"\"\"Create pack data from objects.",
            " ",
            "-    Args:",
            "-      objects: Pack objects",
            "     Returns: Tuples with (type_num, hexdigest, delta base, object chunks)",
            "     \"\"\"",
            "     todo = dict(object_ids)",
            "     if reuse_deltas:",
            "         for unpack in find_reusable_deltas(",
            "             container, set(todo), other_haves=other_haves, progress=progress",
            "         ):",
            "@@ -1993,15 +2204,14 @@",
            "     other_haves: Optional[set[bytes]] = None,",
            " ):",
            "     \"\"\"Write a new pack data file.",
            " ",
            "     Args:",
            "       write: write function to use",
            "       container: PackedObjectContainer",
            "-      entries: Sequence of (object_id, path) tuples to write",
            "       delta_window_size: Sliding window size for searching for deltas;",
            "                          Set to None for default window size.",
            "       deltify: Whether to deltify objects",
            "       compression_level: the zlib compression level to use",
            "     Returns: Dict mapping id -> (offset, crc32 checksum), pack checksum",
            "     \"\"\"",
            "     pack_contents_count = len(object_ids)",
            "@@ -2302,15 +2512,18 @@",
            "             i += 7",
            "             if not cmd & 0x80:",
            "                 break",
            "         return size, index",
            " ",
            "     src_size, index = get_delta_header_size(delta, index)",
            "     dest_size, index = get_delta_header_size(delta, index)",
            "-    assert src_size == len(src_buf), f\"{src_size} vs {len(src_buf)}\"",
            "+    if src_size != len(src_buf):",
            "+        raise ApplyDeltaError(",
            "+            f\"Unexpected source buffer size: {src_size} vs {len(src_buf)}\"",
            "+        )",
            "     while index < delta_length:",
            "         cmd = ord(delta[index : index + 1])",
            "         index += 1",
            "         if cmd & 0x80:",
            "             cp_off = 0",
            "             for i in range(4):",
            "                 if cmd & (1 << i):",
            "@@ -2384,15 +2597,117 @@",
            "     for offset in largetable:",
            "         f.write(struct.pack(b\">Q\", offset))",
            "     assert len(pack_checksum) == 20",
            "     f.write(pack_checksum)",
            "     return f.write_sha()",
            " ",
            " ",
            "-write_pack_index = write_pack_index_v2",
            "+def write_pack_index_v3(",
            "+    f, entries: Iterable[PackIndexEntry], pack_checksum: bytes, hash_algorithm: int = 1",
            "+) -> bytes:",
            "+    \"\"\"Write a new pack index file in v3 format.",
            "+",
            "+    Args:",
            "+      f: File-like object to write to",
            "+      entries: List of tuples with object name (sha), offset_in_pack, and",
            "+        crc32_checksum.",
            "+      pack_checksum: Checksum of the pack file.",
            "+      hash_algorithm: Hash algorithm identifier (1 = SHA-1, 2 = SHA-256)",
            "+    Returns: The SHA of the index file written",
            "+    \"\"\"",
            "+    if hash_algorithm == 1:",
            "+        hash_size = 20  # SHA-1",
            "+        writer_cls = SHA1Writer",
            "+    elif hash_algorithm == 2:",
            "+        hash_size = 32  # SHA-256",
            "+        # TODO: Add SHA256Writer when SHA-256 support is implemented",
            "+        raise NotImplementedError(\"SHA-256 support not yet implemented\")",
            "+    else:",
            "+        raise ValueError(f\"Unknown hash algorithm {hash_algorithm}\")",
            "+",
            "+    # Convert entries to list to allow multiple iterations",
            "+    entries_list = list(entries)",
            "+",
            "+    # Calculate shortest unambiguous prefix length for object names",
            "+    # For now, use full hash size (this could be optimized)",
            "+    shortened_oid_len = hash_size",
            "+",
            "+    f = writer_cls(f)",
            "+    f.write(b\"\\377tOc\")  # Magic!",
            "+    f.write(struct.pack(\">L\", 3))  # Version 3",
            "+    f.write(struct.pack(\">L\", hash_algorithm))  # Hash algorithm",
            "+    f.write(struct.pack(\">L\", shortened_oid_len))  # Shortened OID length",
            "+",
            "+    fan_out_table: dict[int, int] = defaultdict(lambda: 0)",
            "+    for name, offset, entry_checksum in entries_list:",
            "+        if len(name) != hash_size:",
            "+            raise ValueError(",
            "+                f\"Object name has wrong length: expected {hash_size}, got {len(name)}\"",
            "+            )",
            "+        fan_out_table[ord(name[:1])] += 1",
            "+",
            "+    # Fan-out table",
            "+    largetable: list[int] = []",
            "+    for i in range(0x100):",
            "+        f.write(struct.pack(b\">L\", fan_out_table[i]))",
            "+        fan_out_table[i + 1] += fan_out_table[i]",
            "+",
            "+    # Object names table",
            "+    for name, offset, entry_checksum in entries_list:",
            "+        f.write(name)",
            "+",
            "+    # CRC32 checksums table",
            "+    for name, offset, entry_checksum in entries_list:",
            "+        f.write(struct.pack(b\">L\", entry_checksum))",
            "+",
            "+    # Offset table",
            "+    for name, offset, entry_checksum in entries_list:",
            "+        if offset < 2**31:",
            "+            f.write(struct.pack(b\">L\", offset))",
            "+        else:",
            "+            f.write(struct.pack(b\">L\", 2**31 + len(largetable)))",
            "+            largetable.append(offset)",
            "+",
            "+    # Large offset table",
            "+    for offset in largetable:",
            "+        f.write(struct.pack(b\">Q\", offset))",
            "+",
            "+    assert len(pack_checksum) == hash_size, (",
            "+        f\"Pack checksum has wrong length: expected {hash_size}, got {len(pack_checksum)}\"",
            "+    )",
            "+    f.write(pack_checksum)",
            "+    return f.write_sha()",
            "+",
            "+",
            "+def write_pack_index(",
            "+    index_filename, entries, pack_checksum, progress=None, version=None",
            "+):",
            "+    \"\"\"Write a pack index file.",
            "+",
            "+    Args:",
            "+      index_filename: Index filename.",
            "+      entries: List of (checksum, offset, crc32) tuples",
            "+      pack_checksum: Checksum of the pack file.",
            "+      progress: Progress function (not currently used)",
            "+      version: Pack index version to use (1, 2, or 3). If None, defaults to DEFAULT_PACK_INDEX_VERSION.",
            "+",
            "+    Returns:",
            "+      SHA of the written index file",
            "+    \"\"\"",
            "+    if version is None:",
            "+        version = DEFAULT_PACK_INDEX_VERSION",
            "+",
            "+    if version == 1:",
            "+        return write_pack_index_v1(index_filename, entries, pack_checksum)",
            "+    elif version == 2:",
            "+        return write_pack_index_v2(index_filename, entries, pack_checksum)",
            "+    elif version == 3:",
            "+        return write_pack_index_v3(index_filename, entries, pack_checksum)",
            "+    else:",
            "+        raise ValueError(f\"Unsupported pack index version: {version}\")",
            " ",
            " ",
            " class Pack:",
            "     \"\"\"A Git pack object.\"\"\"",
            " ",
            "     _data_load: Optional[Callable[[], PackData]]",
            "     _idx_load: Optional[Callable[[], PackIndex]]",
            "@@ -2481,17 +2796,17 @@",
            " ",
            "     def __iter__(self):",
            "         \"\"\"Iterate over all the sha1s of the objects in this pack.\"\"\"",
            "         return iter(self.index)",
            " ",
            "     def check_length_and_checksum(self) -> None:",
            "         \"\"\"Sanity check the length and checksum of the pack index and data.\"\"\"",
            "-        assert len(self.index) == len(",
            "-            self.data",
            "-        ), f\"Length mismatch: {len(self.index)} (index) != {len(self.data)} (data)\"",
            "+        assert len(self.index) == len(self.data), (",
            "+            f\"Length mismatch: {len(self.index)} (index) != {len(self.data)} (data)\"",
            "+        )",
            "         idx_stored_checksum = self.index.get_pack_checksum()",
            "         data_stored_checksum = self.data.get_stored_checksum()",
            "         if idx_stored_checksum != data_stored_checksum:",
            "             raise ChecksumMismatch(",
            "                 sha_to_hex(idx_stored_checksum),",
            "                 sha_to_hex(data_stored_checksum),",
            "             )"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/patch.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/patch.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/patch.py",
            "@@ -23,36 +23,54 @@",
            " ",
            " These patches are basically unified diffs with some extra metadata tacked",
            " on.",
            " \"\"\"",
            " ",
            " import email.parser",
            " import time",
            "+from collections.abc import Generator",
            " from difflib import SequenceMatcher",
            "-from typing import BinaryIO, Optional, TextIO, Union",
            "+from typing import (",
            "+    TYPE_CHECKING,",
            "+    BinaryIO,",
            "+    Optional,",
            "+    TextIO,",
            "+    Union,",
            "+)",
            "+",
            "+if TYPE_CHECKING:",
            "+    import email.message",
            "+",
            "+    from .object_store import BaseObjectStore",
            " ",
            " from .objects import S_ISGITLINK, Blob, Commit",
            "-from .pack import ObjectContainer",
            " ",
            " FIRST_FEW_BYTES = 8000",
            " ",
            " ",
            " def write_commit_patch(",
            "-    f, commit, contents, progress, version=None, encoding=None",
            "+    f: BinaryIO,",
            "+    commit: \"Commit\",",
            "+    contents: Union[str, bytes],",
            "+    progress: tuple[int, int],",
            "+    version: Optional[str] = None,",
            "+    encoding: Optional[str] = None,",
            " ) -> None:",
            "     \"\"\"Write a individual file patch.",
            " ",
            "     Args:",
            "       commit: Commit object",
            "-      progress: Tuple with current patch number and total.",
            "+      progress: tuple with current patch number and total.",
            " ",
            "     Returns:",
            "       tuple with filename and contents",
            "     \"\"\"",
            "     encoding = encoding or getattr(f, \"encoding\", \"ascii\")",
            "+    if encoding is None:",
            "+        encoding = \"ascii\"",
            "     if isinstance(contents, str):",
            "         contents = contents.encode(encoding)",
            "     (num, total) = progress",
            "     f.write(",
            "         b\"From \"",
            "         + commit.id",
            "         + b\" \"",
            "@@ -83,60 +101,62 @@",
            "     f.write(contents)",
            "     f.write(b\"-- \\n\")",
            "     if version is None:",
            "         from dulwich import __version__ as dulwich_version",
            " ",
            "         f.write(b\"Dulwich %d.%d.%d\\n\" % dulwich_version)",
            "     else:",
            "+        if encoding is None:",
            "+            encoding = \"ascii\"",
            "         f.write(version.encode(encoding) + b\"\\n\")",
            " ",
            " ",
            "-def get_summary(commit):",
            "+def get_summary(commit: \"Commit\") -> str:",
            "     \"\"\"Determine the summary line for use in a filename.",
            " ",
            "     Args:",
            "       commit: Commit",
            "     Returns: Summary string",
            "     \"\"\"",
            "     decoded = commit.message.decode(errors=\"replace\")",
            "     return decoded.splitlines()[0].replace(\" \", \"-\")",
            " ",
            " ",
            " #  Unified Diff",
            "-def _format_range_unified(start, stop) -> str:",
            "+def _format_range_unified(start: int, stop: int) -> str:",
            "     \"\"\"Convert range to the \"ed\" format.\"\"\"",
            "     # Per the diff spec at http://www.unix.org/single_unix_specification/",
            "     beginning = start + 1  # lines start numbering with one",
            "     length = stop - start",
            "     if length == 1:",
            "         return f\"{beginning}\"",
            "     if not length:",
            "         beginning -= 1  # empty ranges begin at line just before the range",
            "     return f\"{beginning},{length}\"",
            " ",
            " ",
            " def unified_diff(",
            "-    a,",
            "-    b,",
            "-    fromfile=\"\",",
            "-    tofile=\"\",",
            "-    fromfiledate=\"\",",
            "-    tofiledate=\"\",",
            "-    n=3,",
            "-    lineterm=\"\\n\",",
            "-    tree_encoding=\"utf-8\",",
            "-    output_encoding=\"utf-8\",",
            "-):",
            "+    a: list[bytes],",
            "+    b: list[bytes],",
            "+    fromfile: bytes = b\"\",",
            "+    tofile: bytes = b\"\",",
            "+    fromfiledate: str = \"\",",
            "+    tofiledate: str = \"\",",
            "+    n: int = 3,",
            "+    lineterm: str = \"\\n\",",
            "+    tree_encoding: str = \"utf-8\",",
            "+    output_encoding: str = \"utf-8\",",
            "+) -> Generator[bytes, None, None]:",
            "     \"\"\"difflib.unified_diff that can detect \"No newline at end of file\" as",
            "     original \"git diff\" does.",
            " ",
            "     Based on the same function in Python2.7 difflib.py",
            "     \"\"\"",
            "     started = False",
            "-    for group in SequenceMatcher(None, a, b).get_grouped_opcodes(n):",
            "+    for group in SequenceMatcher(a=a, b=b).get_grouped_opcodes(n):",
            "         if not started:",
            "             started = True",
            "             fromdate = f\"\\t{fromfiledate}\" if fromfiledate else \"\"",
            "             todate = f\"\\t{tofiledate}\" if tofiledate else \"\"",
            "             yield f\"--- {fromfile.decode(tree_encoding)}{fromdate}{lineterm}\".encode(",
            "                 output_encoding",
            "             )",
            "@@ -162,39 +182,43 @@",
            "             if tag in (\"replace\", \"insert\"):",
            "                 for line in b[j1:j2]:",
            "                     if not line[-1:] == b\"\\n\":",
            "                         line += b\"\\n\\\\ No newline at end of file\\n\"",
            "                     yield b\"+\" + line",
            " ",
            " ",
            "-def is_binary(content):",
            "+def is_binary(content: bytes) -> bool:",
            "     \"\"\"See if the first few bytes contain any null characters.",
            " ",
            "     Args:",
            "       content: Bytestring to check for binary content",
            "     \"\"\"",
            "     return b\"\\0\" in content[:FIRST_FEW_BYTES]",
            " ",
            " ",
            "-def shortid(hexsha):",
            "+def shortid(hexsha: Optional[bytes]) -> bytes:",
            "     if hexsha is None:",
            "         return b\"0\" * 7",
            "     else:",
            "         return hexsha[:7]",
            " ",
            " ",
            "-def patch_filename(p, root):",
            "+def patch_filename(p: Optional[bytes], root: bytes) -> bytes:",
            "     if p is None:",
            "         return b\"/dev/null\"",
            "     else:",
            "         return root + b\"/\" + p",
            " ",
            " ",
            " def write_object_diff(",
            "-    f, store: ObjectContainer, old_file, new_file, diff_binary=False",
            "+    f: BinaryIO,",
            "+    store: \"BaseObjectStore\",",
            "+    old_file: tuple[Optional[bytes], Optional[int], Optional[bytes]],",
            "+    new_file: tuple[Optional[bytes], Optional[int], Optional[bytes]],",
            "+    diff_binary: bool = False,",
            " ) -> None:",
            "     \"\"\"Write the diff for an object.",
            " ",
            "     Args:",
            "       f: File-like object to write to",
            "       store: Store to retrieve objects from, if necessary",
            "       old_file: (path, mode, hexsha) tuple",
            "@@ -205,23 +229,30 @@",
            "     Note: the tuple elements should be None for nonexistent files",
            "     \"\"\"",
            "     (old_path, old_mode, old_id) = old_file",
            "     (new_path, new_mode, new_id) = new_file",
            "     patched_old_path = patch_filename(old_path, b\"a\")",
            "     patched_new_path = patch_filename(new_path, b\"b\")",
            " ",
            "-    def content(mode, hexsha):",
            "+    def content(mode: Optional[int], hexsha: Optional[bytes]) -> Blob:",
            "+        from typing import cast",
            "+",
            "         if hexsha is None:",
            "-            return Blob.from_string(b\"\")",
            "-        elif S_ISGITLINK(mode):",
            "-            return Blob.from_string(b\"Subproject commit \" + hexsha + b\"\\n\")",
            "+            return cast(Blob, Blob.from_string(b\"\"))",
            "+        elif mode is not None and S_ISGITLINK(mode):",
            "+            return cast(Blob, Blob.from_string(b\"Subproject commit \" + hexsha + b\"\\n\"))",
            "         else:",
            "-            return store[hexsha]",
            "+            obj = store[hexsha]",
            "+            if isinstance(obj, Blob):",
            "+                return obj",
            "+            else:",
            "+                # Fallback for non-blob objects",
            "+                return cast(Blob, Blob.from_string(obj.as_raw_string()))",
            " ",
            "-    def lines(content):",
            "+    def lines(content: \"Blob\") -> list[bytes]:",
            "         if not content:",
            "             return []",
            "         else:",
            "             return content.splitlines()",
            " ",
            "     f.writelines(",
            "         gen_diff_header((old_path, new_path), (old_mode, new_mode), (old_id, new_id))",
            "@@ -245,15 +276,19 @@",
            "                 patched_old_path,",
            "                 patched_new_path,",
            "             )",
            "         )",
            " ",
            " ",
            " # TODO(jelmer): Support writing unicode, rather than bytes.",
            "-def gen_diff_header(paths, modes, shas):",
            "+def gen_diff_header(",
            "+    paths: tuple[Optional[bytes], Optional[bytes]],",
            "+    modes: tuple[Optional[int], Optional[int]],",
            "+    shas: tuple[Optional[bytes], Optional[bytes]],",
            "+) -> Generator[bytes, None, None]:",
            "     \"\"\"Write a blob diff header.",
            " ",
            "     Args:",
            "       paths: Tuple with old and new path",
            "       modes: Tuple with old and new modes",
            "       shas: Tuple with old and new shas",
            "     \"\"\"",
            "@@ -278,30 +313,34 @@",
            "     yield b\"index \" + shortid(old_sha) + b\"..\" + shortid(new_sha)",
            "     if new_mode is not None and old_mode is not None:",
            "         yield (f\" {new_mode:o}\").encode(\"ascii\")",
            "     yield b\"\\n\"",
            " ",
            " ",
            " # TODO(jelmer): Support writing unicode, rather than bytes.",
            "-def write_blob_diff(f, old_file, new_file) -> None:",
            "+def write_blob_diff(",
            "+    f: BinaryIO,",
            "+    old_file: tuple[Optional[bytes], Optional[int], Optional[\"Blob\"]],",
            "+    new_file: tuple[Optional[bytes], Optional[int], Optional[\"Blob\"]],",
            "+) -> None:",
            "     \"\"\"Write blob diff.",
            " ",
            "     Args:",
            "       f: File-like object to write to",
            "       old_file: (path, mode, hexsha) tuple (None if nonexisting)",
            "       new_file: (path, mode, hexsha) tuple (None if nonexisting)",
            " ",
            "     Note: The use of write_object_diff is recommended over this function.",
            "     \"\"\"",
            "     (old_path, old_mode, old_blob) = old_file",
            "     (new_path, new_mode, new_blob) = new_file",
            "     patched_old_path = patch_filename(old_path, b\"a\")",
            "     patched_new_path = patch_filename(new_path, b\"b\")",
            " ",
            "-    def lines(blob):",
            "+    def lines(blob: Optional[\"Blob\"]) -> list[bytes]:",
            "         if blob is not None:",
            "             return blob.splitlines()",
            "         else:",
            "             return []",
            " ",
            "     f.writelines(",
            "         gen_diff_header(",
            "@@ -313,15 +352,21 @@",
            "     old_contents = lines(old_blob)",
            "     new_contents = lines(new_blob)",
            "     f.writelines(",
            "         unified_diff(old_contents, new_contents, patched_old_path, patched_new_path)",
            "     )",
            " ",
            " ",
            "-def write_tree_diff(f, store, old_tree, new_tree, diff_binary=False) -> None:",
            "+def write_tree_diff(",
            "+    f: BinaryIO,",
            "+    store: \"BaseObjectStore\",",
            "+    old_tree: Optional[bytes],",
            "+    new_tree: Optional[bytes],",
            "+    diff_binary: bool = False,",
            "+) -> None:",
            "     \"\"\"Write tree diff.",
            " ",
            "     Args:",
            "       f: File-like object to write to.",
            "       old_tree: Old tree id",
            "       new_tree: New tree id",
            "       diff_binary: Whether to diff files even if they",
            "@@ -334,15 +379,17 @@",
            "             store,",
            "             (oldpath, oldmode, oldsha),",
            "             (newpath, newmode, newsha),",
            "             diff_binary=diff_binary,",
            "         )",
            " ",
            " ",
            "-def git_am_patch_split(f: Union[TextIO, BinaryIO], encoding: Optional[str] = None):",
            "+def git_am_patch_split(",
            "+    f: Union[TextIO, BinaryIO], encoding: Optional[str] = None",
            "+) -> tuple[\"Commit\", bytes, Optional[bytes]]:",
            "     \"\"\"Parse a git-am-style patch and split it up into bits.",
            " ",
            "     Args:",
            "       f: File-like object to parse",
            "       encoding: Encoding to use when creating Git objects",
            "     Returns: Tuple with commit object, diff contents and git version",
            "     \"\"\"",
            "@@ -354,37 +401,47 @@",
            "         msg = bparser.parsebytes(contents)",
            "     else:",
            "         uparser = email.parser.Parser()",
            "         msg = uparser.parsestr(contents)",
            "     return parse_patch_message(msg, encoding)",
            " ",
            " ",
            "-def parse_patch_message(msg, encoding=None):",
            "+def parse_patch_message(",
            "+    msg: \"email.message.Message\", encoding: Optional[str] = None",
            "+) -> tuple[\"Commit\", bytes, Optional[bytes]]:",
            "     \"\"\"Extract a Commit object and patch from an e-mail message.",
            " ",
            "     Args:",
            "       msg: An email message (email.message.Message)",
            "       encoding: Encoding to use to encode Git commits",
            "     Returns: Tuple with commit object, diff contents and git version",
            "     \"\"\"",
            "     c = Commit()",
            "+    if encoding is None:",
            "+        encoding = \"ascii\"",
            "     c.author = msg[\"from\"].encode(encoding)",
            "     c.committer = msg[\"from\"].encode(encoding)",
            "     try:",
            "         patch_tag_start = msg[\"subject\"].index(\"[PATCH\")",
            "     except ValueError:",
            "         subject = msg[\"subject\"]",
            "     else:",
            "         close = msg[\"subject\"].index(\"] \", patch_tag_start)",
            "         subject = msg[\"subject\"][close + 2 :]",
            "     c.message = (subject.replace(\"\\n\", \"\") + \"\\n\").encode(encoding)",
            "     first = True",
            " ",
            "     body = msg.get_payload(decode=True)",
            "-    lines = body.splitlines(True)",
            "+    if isinstance(body, str):",
            "+        body = body.encode(encoding)",
            "+    if isinstance(body, bytes):",
            "+        lines = body.splitlines(True)",
            "+    else:",
            "+        # Handle other types by converting to string first",
            "+        lines = str(body).encode(encoding).splitlines(True)",
            "     line_iter = iter(lines)",
            " ",
            "     for line in line_iter:",
            "         if line == b\"---\\n\":",
            "             break",
            "         if first:",
            "             if line.startswith(b\"From: \"):"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/porcelain.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/porcelain.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/porcelain.py",
            "@@ -65,4387 +65,9416 @@",
            " 00000400: 230a 0a22 2222 5369 6d70 6c65 2077 7261  #..\"\"\"Simple wra",
            " 00000410: 7070 6572 2074 6861 7420 7072 6f76 6964  pper that provid",
            " 00000420: 6573 2070 6f72 6365 6c61 696e 2d6c 696b  es porcelain-lik",
            " 00000430: 6520 6675 6e63 7469 6f6e 7320 6f6e 2074  e functions on t",
            " 00000440: 6f70 206f 6620 4475 6c77 6963 682e 0a0a  op of Dulwich...",
            " 00000450: 4375 7272 656e 746c 7920 696d 706c 656d  Currently implem",
            " 00000460: 656e 7465 643a 0a20 2a20 6172 6368 6976  ented:. * archiv",
            "-00000470: 650a 202a 2061 6464 0a20 2a20 6272 616e  e. * add. * bran",
            "-00000480: 6368 7b5f 6372 6561 7465 2c5f 6465 6c65  ch{_create,_dele",
            "-00000490: 7465 2c5f 6c69 7374 7d0a 202a 2063 6865  te,_list}. * che",
            "-000004a0: 636b 2d69 676e 6f72 650a 202a 2063 6865  ck-ignore. * che",
            "-000004b0: 636b 6f75 745f 6272 616e 6368 0a20 2a20  ckout_branch. * ",
            "-000004c0: 636c 6f6e 650a 202a 2063 6f6d 6d69 740a  clone. * commit.",
            "-000004d0: 202a 2063 6f6d 6d69 742d 7472 6565 0a20   * commit-tree. ",
            "-000004e0: 2a20 6461 656d 6f6e 0a20 2a20 6465 7363  * daemon. * desc",
            "-000004f0: 7269 6265 0a20 2a20 6469 6666 2d74 7265  ribe. * diff-tre",
            "-00000500: 650a 202a 2066 6574 6368 0a20 2a20 666f  e. * fetch. * fo",
            "-00000510: 722d 6561 6368 2d72 6566 0a20 2a20 696e  r-each-ref. * in",
            "-00000520: 6974 0a20 2a20 6c73 2d66 696c 6573 0a20  it. * ls-files. ",
            "-00000530: 2a20 6c73 2d72 656d 6f74 650a 202a 206c  * ls-remote. * l",
            "-00000540: 732d 7472 6565 0a20 2a20 7075 6c6c 0a20  s-tree. * pull. ",
            "-00000550: 2a20 7075 7368 0a20 2a20 726d 0a20 2a20  * push. * rm. * ",
            "-00000560: 7265 6d6f 7465 7b5f 6164 647d 0a20 2a20  remote{_add}. * ",
            "-00000570: 7265 6365 6976 652d 7061 636b 0a20 2a20  receive-pack. * ",
            "-00000580: 7265 7365 740a 202a 2073 7562 6d6f 6475  reset. * submodu",
            "-00000590: 6c65 5f61 6464 0a20 2a20 7375 626d 6f64  le_add. * submod",
            "-000005a0: 756c 655f 696e 6974 0a20 2a20 7375 626d  ule_init. * subm",
            "-000005b0: 6f64 756c 655f 6c69 7374 0a20 2a20 7265  odule_list. * re",
            "-000005c0: 762d 6c69 7374 0a20 2a20 7461 677b 5f63  v-list. * tag{_c",
            "-000005d0: 7265 6174 652c 5f64 656c 6574 652c 5f6c  reate,_delete,_l",
            "-000005e0: 6973 747d 0a20 2a20 7570 6c6f 6164 2d70  ist}. * upload-p",
            "-000005f0: 6163 6b0a 202a 2075 7064 6174 652d 7365  ack. * update-se",
            "-00000600: 7276 6572 2d69 6e66 6f0a 202a 2073 7461  rver-info. * sta",
            "-00000610: 7475 730a 202a 2073 796d 626f 6c69 632d  tus. * symbolic-",
            "-00000620: 7265 660a 0a54 6865 7365 2066 756e 6374  ref..These funct",
            "-00000630: 696f 6e73 2061 7265 206d 6561 6e74 2074  ions are meant t",
            "-00000640: 6f20 6265 6861 7665 2073 696d 696c 6172  o behave similar",
            "-00000650: 6c79 2074 6f20 7468 6520 6769 7420 7375  ly to the git su",
            "-00000660: 6263 6f6d 6d61 6e64 732e 0a44 6966 6665  bcommands..Diffe",
            "-00000670: 7265 6e63 6573 2069 6e20 6265 6861 7669  rences in behavi",
            "-00000680: 6f75 7220 6172 6520 636f 6e73 6964 6572  our are consider",
            "-00000690: 6564 2062 7567 732e 0a0a 4e6f 7465 3a20  ed bugs...Note: ",
            "-000006a0: 6f6e 6520 6f66 2074 6865 2063 6f6e 7365  one of the conse",
            "-000006b0: 7175 656e 6365 7320 6f66 2074 6869 7320  quences of this ",
            "-000006c0: 6973 2074 6861 7420 7061 7468 7320 7465  is that paths te",
            "-000006d0: 6e64 2074 6f20 6265 0a69 6e74 6572 7072  nd to be.interpr",
            "-000006e0: 6574 6564 2072 656c 6174 6976 6520 746f  eted relative to",
            "-000006f0: 2074 6865 2063 7572 7265 6e74 2077 6f72   the current wor",
            "-00000700: 6b69 6e67 2064 6972 6563 746f 7279 2072  king directory r",
            "-00000710: 6174 6865 7220 7468 616e 2072 656c 6174  ather than relat",
            "-00000720: 6976 650a 746f 2074 6865 2072 6570 6f73  ive.to the repos",
            "-00000730: 6974 6f72 7920 726f 6f74 2e0a 0a46 756e  itory root...Fun",
            "-00000740: 6374 696f 6e73 2073 686f 756c 6420 6765  ctions should ge",
            "-00000750: 6e65 7261 6c6c 7920 6163 6365 7074 2062  nerally accept b",
            "-00000760: 6f74 6820 756e 6963 6f64 6520 7374 7269  oth unicode stri",
            "-00000770: 6e67 7320 616e 6420 6279 7465 7374 7269  ngs and bytestri",
            "-00000780: 6e67 730a 2222 220a 0a69 6d70 6f72 7420  ngs.\"\"\"..import ",
            "-00000790: 6461 7465 7469 6d65 0a69 6d70 6f72 7420  datetime.import ",
            "-000007a0: 666e 6d61 7463 680a 696d 706f 7274 206f  fnmatch.import o",
            "-000007b0: 730a 696d 706f 7274 2070 6f73 6978 7061  s.import posixpa",
            "-000007c0: 7468 0a69 6d70 6f72 7420 7374 6174 0a69  th.import stat.i",
            "-000007d0: 6d70 6f72 7420 7379 730a 696d 706f 7274  mport sys.import",
            "-000007e0: 2074 696d 650a 6672 6f6d 2063 6f6c 6c65   time.from colle",
            "-000007f0: 6374 696f 6e73 2069 6d70 6f72 7420 6e61  ctions import na",
            "-00000800: 6d65 6474 7570 6c65 0a66 726f 6d20 636f  medtuple.from co",
            "-00000810: 6e74 6578 746c 6962 2069 6d70 6f72 7420  ntextlib import ",
            "-00000820: 636c 6f73 696e 672c 2063 6f6e 7465 7874  closing, context",
            "-00000830: 6d61 6e61 6765 720a 6672 6f6d 2069 6f20  manager.from io ",
            "-00000840: 696d 706f 7274 2042 7974 6573 494f 2c20  import BytesIO, ",
            "-00000850: 5261 7749 4f42 6173 650a 6672 6f6d 2070  RawIOBase.from p",
            "-00000860: 6174 686c 6962 2069 6d70 6f72 7420 5061  athlib import Pa",
            "-00000870: 7468 0a66 726f 6d20 7479 7069 6e67 2069  th.from typing i",
            "-00000880: 6d70 6f72 7420 4f70 7469 6f6e 616c 2c20  mport Optional, ",
            "-00000890: 556e 696f 6e0a 0a66 726f 6d20 2e61 7263  Union..from .arc",
            "-000008a0: 6869 7665 2069 6d70 6f72 7420 7461 725f  hive import tar_",
            "-000008b0: 7374 7265 616d 0a66 726f 6d20 2e63 6c69  stream.from .cli",
            "-000008c0: 656e 7420 696d 706f 7274 2067 6574 5f74  ent import get_t",
            "-000008d0: 7261 6e73 706f 7274 5f61 6e64 5f70 6174  ransport_and_pat",
            "-000008e0: 680a 6672 6f6d 202e 636f 6e66 6967 2069  h.from .config i",
            "-000008f0: 6d70 6f72 7420 436f 6e66 6967 2c20 436f  mport Config, Co",
            "-00000900: 6e66 6967 4669 6c65 2c20 5374 6163 6b65  nfigFile, Stacke",
            "-00000910: 6443 6f6e 6669 672c 2072 6561 645f 7375  dConfig, read_su",
            "-00000920: 626d 6f64 756c 6573 0a66 726f 6d20 2e64  bmodules.from .d",
            "-00000930: 6966 665f 7472 6565 2069 6d70 6f72 7420  iff_tree import ",
            "-00000940: 280a 2020 2020 4348 414e 4745 5f41 4444  (.    CHANGE_ADD",
            "-00000950: 2c0a 2020 2020 4348 414e 4745 5f43 4f50  ,.    CHANGE_COP",
            "-00000960: 592c 0a20 2020 2043 4841 4e47 455f 4445  Y,.    CHANGE_DE",
            "-00000970: 4c45 5445 2c0a 2020 2020 4348 414e 4745  LETE,.    CHANGE",
            "-00000980: 5f4d 4f44 4946 592c 0a20 2020 2043 4841  _MODIFY,.    CHA",
            "-00000990: 4e47 455f 5245 4e41 4d45 2c0a 2020 2020  NGE_RENAME,.    ",
            "-000009a0: 5245 4e41 4d45 5f43 4841 4e47 455f 5459  RENAME_CHANGE_TY",
            "-000009b0: 5045 532c 0a29 0a66 726f 6d20 2e65 7272  PES,.).from .err",
            "-000009c0: 6f72 7320 696d 706f 7274 2053 656e 6450  ors import SendP",
            "-000009d0: 6163 6b45 7272 6f72 0a66 726f 6d20 2e66  ackError.from .f",
            "-000009e0: 696c 6520 696d 706f 7274 2065 6e73 7572  ile import ensur",
            "-000009f0: 655f 6469 725f 6578 6973 7473 0a66 726f  e_dir_exists.fro",
            "-00000a00: 6d20 2e67 7261 7068 2069 6d70 6f72 7420  m .graph import ",
            "-00000a10: 6361 6e5f 6661 7374 5f66 6f72 7761 7264  can_fast_forward",
            "-00000a20: 0a66 726f 6d20 2e69 676e 6f72 6520 696d  .from .ignore im",
            "-00000a30: 706f 7274 2049 676e 6f72 6546 696c 7465  port IgnoreFilte",
            "-00000a40: 724d 616e 6167 6572 0a66 726f 6d20 2e69  rManager.from .i",
            "-00000a50: 6e64 6578 2069 6d70 6f72 7420 280a 2020  ndex import (.  ",
            "-00000a60: 2020 5f66 735f 746f 5f74 7265 655f 7061    _fs_to_tree_pa",
            "-00000a70: 7468 2c0a 2020 2020 626c 6f62 5f66 726f  th,.    blob_fro",
            "-00000a80: 6d5f 7061 7468 5f61 6e64 5f73 7461 742c  m_path_and_stat,",
            "-00000a90: 0a20 2020 2062 7569 6c64 5f66 696c 655f  .    build_file_",
            "-00000aa0: 6672 6f6d 5f62 6c6f 622c 0a20 2020 2067  from_blob,.    g",
            "-00000ab0: 6574 5f75 6e73 7461 6765 645f 6368 616e  et_unstaged_chan",
            "-00000ac0: 6765 732c 0a20 2020 2069 6e64 6578 5f65  ges,.    index_e",
            "-00000ad0: 6e74 7279 5f66 726f 6d5f 7374 6174 2c0a  ntry_from_stat,.",
            "-00000ae0: 290a 6672 6f6d 202e 6f62 6a65 6374 5f73  ).from .object_s",
            "-00000af0: 746f 7265 2069 6d70 6f72 7420 6974 6572  tore import iter",
            "-00000b00: 5f74 7265 655f 636f 6e74 656e 7473 2c20  _tree_contents, ",
            "-00000b10: 7472 6565 5f6c 6f6f 6b75 705f 7061 7468  tree_lookup_path",
            "-00000b20: 0a66 726f 6d20 2e6f 626a 6563 7473 2069  .from .objects i",
            "-00000b30: 6d70 6f72 7420 280a 2020 2020 436f 6d6d  mport (.    Comm",
            "-00000b40: 6974 2c0a 2020 2020 5461 672c 0a20 2020  it,.    Tag,.   ",
            "-00000b50: 2066 6f72 6d61 745f 7469 6d65 7a6f 6e65   format_timezone",
            "-00000b60: 2c0a 2020 2020 7061 7273 655f 7469 6d65  ,.    parse_time",
            "-00000b70: 7a6f 6e65 2c0a 2020 2020 7072 6574 7479  zone,.    pretty",
            "-00000b80: 5f66 6f72 6d61 745f 7472 6565 5f65 6e74  _format_tree_ent",
            "-00000b90: 7279 2c0a 290a 6672 6f6d 202e 6f62 6a65  ry,.).from .obje",
            "-00000ba0: 6374 7370 6563 2069 6d70 6f72 7420 280a  ctspec import (.",
            "-00000bb0: 2020 2020 7061 7273 655f 636f 6d6d 6974      parse_commit",
            "-00000bc0: 2c0a 2020 2020 7061 7273 655f 6f62 6a65  ,.    parse_obje",
            "-00000bd0: 6374 2c0a 2020 2020 7061 7273 655f 7265  ct,.    parse_re",
            "-00000be0: 662c 0a20 2020 2070 6172 7365 5f72 6566  f,.    parse_ref",
            "-00000bf0: 7475 706c 6573 2c0a 2020 2020 7061 7273  tuples,.    pars",
            "-00000c00: 655f 7472 6565 2c0a 2020 2020 746f 5f62  e_tree,.    to_b",
            "-00000c10: 7974 6573 2c0a 290a 6672 6f6d 202e 7061  ytes,.).from .pa",
            "-00000c20: 636b 2069 6d70 6f72 7420 7772 6974 655f  ck import write_",
            "-00000c30: 7061 636b 5f66 726f 6d5f 636f 6e74 6169  pack_from_contai",
            "-00000c40: 6e65 722c 2077 7269 7465 5f70 6163 6b5f  ner, write_pack_",
            "-00000c50: 696e 6465 780a 6672 6f6d 202e 7061 7463  index.from .patc",
            "-00000c60: 6820 696d 706f 7274 2077 7269 7465 5f74  h import write_t",
            "-00000c70: 7265 655f 6469 6666 0a66 726f 6d20 2e70  ree_diff.from .p",
            "-00000c80: 726f 746f 636f 6c20 696d 706f 7274 205a  rotocol import Z",
            "-00000c90: 4552 4f5f 5348 412c 2050 726f 746f 636f  ERO_SHA, Protoco",
            "-00000ca0: 6c0a 6672 6f6d 202e 7265 6673 2069 6d70  l.from .refs imp",
            "-00000cb0: 6f72 7420 280a 2020 2020 4c4f 4341 4c5f  ort (.    LOCAL_",
            "-00000cc0: 4252 414e 4348 5f50 5245 4649 582c 0a20  BRANCH_PREFIX,. ",
            "-00000cd0: 2020 204c 4f43 414c 5f52 454d 4f54 455f     LOCAL_REMOTE_",
            "-00000ce0: 5052 4546 4958 2c0a 2020 2020 4c4f 4341  PREFIX,.    LOCA",
            "-00000cf0: 4c5f 5441 475f 5052 4546 4958 2c0a 2020  L_TAG_PREFIX,.  ",
            "-00000d00: 2020 5f69 6d70 6f72 745f 7265 6d6f 7465    _import_remote",
            "-00000d10: 5f72 6566 732c 0a29 0a66 726f 6d20 2e72  _refs,.).from .r",
            "-00000d20: 6570 6f20 696d 706f 7274 2042 6173 6552  epo import BaseR",
            "-00000d30: 6570 6f2c 2052 6570 6f2c 2067 6574 5f75  epo, Repo, get_u",
            "-00000d40: 7365 725f 6964 656e 7469 7479 0a66 726f  ser_identity.fro",
            "-00000d50: 6d20 2e73 6572 7665 7220 696d 706f 7274  m .server import",
            "-00000d60: 2028 0a20 2020 2046 696c 6553 7973 7465   (.    FileSyste",
            "-00000d70: 6d42 6163 6b65 6e64 2c0a 2020 2020 5265  mBackend,.    Re",
            "-00000d80: 6365 6976 6550 6163 6b48 616e 646c 6572  ceivePackHandler",
            "-00000d90: 2c0a 2020 2020 5443 5047 6974 5365 7276  ,.    TCPGitServ",
            "-00000da0: 6572 2c0a 2020 2020 5570 6c6f 6164 5061  er,.    UploadPa",
            "-00000db0: 636b 4861 6e64 6c65 722c 0a29 0a66 726f  ckHandler,.).fro",
            "-00000dc0: 6d20 2e73 6572 7665 7220 696d 706f 7274  m .server import",
            "-00000dd0: 2075 7064 6174 655f 7365 7276 6572 5f69   update_server_i",
            "-00000de0: 6e66 6f20 6173 2073 6572 7665 725f 7570  nfo as server_up",
            "-00000df0: 6461 7465 5f73 6572 7665 725f 696e 666f  date_server_info",
            "-00000e00: 0a0a 2320 4d6f 6475 6c65 206c 6576 656c  ..# Module level",
            "-00000e10: 2074 7570 6c65 2064 6566 696e 6974 696f   tuple definitio",
            "-00000e20: 6e20 666f 7220 7374 6174 7573 206f 7574  n for status out",
            "-00000e30: 7075 740a 4769 7453 7461 7475 7320 3d20  put.GitStatus = ",
            "-00000e40: 6e61 6d65 6474 7570 6c65 2822 4769 7453  namedtuple(\"GitS",
            "-00000e50: 7461 7475 7322 2c20 2273 7461 6765 6420  tatus\", \"staged ",
            "-00000e60: 756e 7374 6167 6564 2075 6e74 7261 636b  unstaged untrack",
            "-00000e70: 6564 2229 0a0a 0a63 6c61 7373 204e 6f6e  ed\")...class Non",
            "-00000e80: 6553 7472 6561 6d28 5261 7749 4f42 6173  eStream(RawIOBas",
            "-00000e90: 6529 3a0a 2020 2020 2222 2246 616c 6c62  e):.    \"\"\"Fallb",
            "-00000ea0: 6163 6b20 6966 2073 7464 6f75 7420 6f72  ack if stdout or",
            "-00000eb0: 2073 7464 6572 7220 6172 6520 756e 6176   stderr are unav",
            "-00000ec0: 6169 6c61 626c 652c 2064 6f65 7320 6e6f  ailable, does no",
            "-00000ed0: 7468 696e 672e 2222 220a 0a20 2020 2064  thing.\"\"\"..    d",
            "-00000ee0: 6566 2072 6561 6428 7365 6c66 2c20 7369  ef read(self, si",
            "-00000ef0: 7a65 3d2d 3129 202d 3e20 4e6f 6e65 3a0a  ze=-1) -> None:.",
            "-00000f00: 2020 2020 2020 2020 7265 7475 726e 204e          return N",
            "-00000f10: 6f6e 650a 0a20 2020 2064 6566 2072 6561  one..    def rea",
            "-00000f20: 6461 6c6c 2873 656c 6629 202d 3e20 6279  dall(self) -> by",
            "-00000f30: 7465 733a 0a20 2020 2020 2020 2072 6574  tes:.        ret",
            "-00000f40: 7572 6e20 6222 220a 0a20 2020 2064 6566  urn b\"\"..    def",
            "-00000f50: 2072 6561 6469 6e74 6f28 7365 6c66 2c20   readinto(self, ",
            "-00000f60: 6229 202d 3e20 4e6f 6e65 3a0a 2020 2020  b) -> None:.    ",
            "-00000f70: 2020 2020 7265 7475 726e 204e 6f6e 650a      return None.",
            "-00000f80: 0a20 2020 2064 6566 2077 7269 7465 2873  .    def write(s",
            "-00000f90: 656c 662c 2062 2920 2d3e 204e 6f6e 653a  elf, b) -> None:",
            "-00000fa0: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return ",
            "-00000fb0: 4e6f 6e65 0a0a 0a64 6566 6175 6c74 5f62  None...default_b",
            "-00000fc0: 7974 6573 5f6f 7574 5f73 7472 6561 6d20  ytes_out_stream ",
            "-00000fd0: 3d20 6765 7461 7474 7228 7379 732e 7374  = getattr(sys.st",
            "-00000fe0: 646f 7574 2c20 2262 7566 6665 7222 2c20  dout, \"buffer\", ",
            "-00000ff0: 4e6f 6e65 2920 6f72 204e 6f6e 6553 7472  None) or NoneStr",
            "-00001000: 6561 6d28 290a 6465 6661 756c 745f 6279  eam().default_by",
            "-00001010: 7465 735f 6572 725f 7374 7265 616d 203d  tes_err_stream =",
            "-00001020: 2067 6574 6174 7472 2873 7973 2e73 7464   getattr(sys.std",
            "-00001030: 6572 722c 2022 6275 6666 6572 222c 204e  err, \"buffer\", N",
            "-00001040: 6f6e 6529 206f 7220 4e6f 6e65 5374 7265  one) or NoneStre",
            "-00001050: 616d 2829 0a0a 0a44 4546 4155 4c54 5f45  am()...DEFAULT_E",
            "-00001060: 4e43 4f44 494e 4720 3d20 2275 7466 2d38  NCODING = \"utf-8",
            "-00001070: 220a 0a0a 636c 6173 7320 4572 726f 7228  \"...class Error(",
            "-00001080: 4578 6365 7074 696f 6e29 3a0a 2020 2020  Exception):.    ",
            "-00001090: 2222 2250 6f72 6365 6c61 696e 2d62 6173  \"\"\"Porcelain-bas",
            "-000010a0: 6564 2065 7272 6f72 2e22 2222 0a0a 2020  ed error.\"\"\"..  ",
            "-000010b0: 2020 6465 6620 5f5f 696e 6974 5f5f 2873    def __init__(s",
            "-000010c0: 656c 662c 206d 7367 2920 2d3e 204e 6f6e  elf, msg) -> Non",
            "-000010d0: 653a 0a20 2020 2020 2020 2073 7570 6572  e:.        super",
            "-000010e0: 2829 2e5f 5f69 6e69 745f 5f28 6d73 6729  ().__init__(msg)",
            "-000010f0: 0a0a 0a63 6c61 7373 2052 656d 6f74 6545  ...class RemoteE",
            "-00001100: 7869 7374 7328 4572 726f 7229 3a0a 2020  xists(Error):.  ",
            "-00001110: 2020 2222 2252 6169 7365 6420 7768 656e    \"\"\"Raised when",
            "-00001120: 2074 6865 2072 656d 6f74 6520 616c 7265   the remote alre",
            "-00001130: 6164 7920 6578 6973 7473 2e22 2222 0a0a  ady exists.\"\"\"..",
            "-00001140: 0a63 6c61 7373 2054 696d 657a 6f6e 6546  .class TimezoneF",
            "-00001150: 6f72 6d61 7445 7272 6f72 2845 7272 6f72  ormatError(Error",
            "-00001160: 293a 0a20 2020 2022 2222 5261 6973 6564  ):.    \"\"\"Raised",
            "-00001170: 2077 6865 6e20 7468 6520 7469 6d65 7a6f   when the timezo",
            "-00001180: 6e65 2063 616e 6e6f 7420 6265 2064 6574  ne cannot be det",
            "-00001190: 6572 6d69 6e65 6420 6672 6f6d 2061 2067  ermined from a g",
            "-000011a0: 6976 656e 2073 7472 696e 672e 2222 220a  iven string.\"\"\".",
            "-000011b0: 0a0a 636c 6173 7320 4368 6563 6b6f 7574  ..class Checkout",
            "-000011c0: 4572 726f 7228 4572 726f 7229 3a0a 2020  Error(Error):.  ",
            "-000011d0: 2020 2222 2249 6e64 6963 6174 6573 2074    \"\"\"Indicates t",
            "-000011e0: 6861 7420 6120 6368 6563 6b6f 7574 2063  hat a checkout c",
            "-000011f0: 616e 6e6f 7420 6265 2070 6572 666f 726d  annot be perform",
            "-00001200: 6564 2e22 2222 0a0a 0a64 6566 2070 6172  ed.\"\"\"...def par",
            "-00001210: 7365 5f74 696d 657a 6f6e 655f 666f 726d  se_timezone_form",
            "-00001220: 6174 2874 7a5f 7374 7229 3a0a 2020 2020  at(tz_str):.    ",
            "-00001230: 2222 2250 6172 7365 2067 6976 656e 2073  \"\"\"Parse given s",
            "-00001240: 7472 696e 6720 616e 6420 6174 7465 6d70  tring and attemp",
            "-00001250: 7420 746f 2072 6574 7572 6e20 6120 7469  t to return a ti",
            "-00001260: 6d65 7a6f 6e65 206f 6666 7365 742e 0a0a  mezone offset...",
            "-00001270: 2020 2020 4469 6666 6572 656e 7420 666f      Different fo",
            "-00001280: 726d 6174 7320 6172 6520 636f 6e73 6964  rmats are consid",
            "-00001290: 6572 6564 2069 6e20 7468 6520 666f 6c6c  ered in the foll",
            "-000012a0: 6f77 696e 6720 6f72 6465 723a 0a0a 2020  owing order:..  ",
            "-000012b0: 2020 202d 2047 6974 2069 6e74 6572 6e61     - Git interna",
            "-000012c0: 6c20 666f 726d 6174 3a20 3c75 6e69 7820  l format: <unix ",
            "-000012d0: 7469 6d65 7374 616d 703e 203c 7469 6d65  timestamp> <time",
            "-000012e0: 7a6f 6e65 206f 6666 7365 743e 0a20 2020  zone offset>.   ",
            "-000012f0: 2020 2d20 5246 4320 3238 3232 3a20 652e    - RFC 2822: e.",
            "-00001300: 672e 204d 6f6e 2c20 3230 204e 6f76 2031  g. Mon, 20 Nov 1",
            "-00001310: 3939 3520 3139 3a31 323a 3038 202d 3035  995 19:12:08 -05",
            "-00001320: 3030 0a20 2020 2020 2d20 4953 4f20 3836  00.     - ISO 86",
            "-00001330: 3031 3a20 652e 672e 2031 3939 352d 3131  01: e.g. 1995-11",
            "-00001340: 2d32 3054 3139 3a31 323a 3038 2d30 3530  -20T19:12:08-050",
            "-00001350: 300a 0a20 2020 2041 7267 733a 0a20 2020  0..    Args:.   ",
            "-00001360: 2020 2074 7a5f 7374 723a 2064 6174 6574     tz_str: datet",
            "-00001370: 696d 6520 7374 7269 6e67 0a20 2020 2052  ime string.    R",
            "-00001380: 6574 7572 6e73 3a20 5469 6d65 7a6f 6e65  eturns: Timezone",
            "-00001390: 206f 6666 7365 7420 6173 2069 6e74 6567   offset as integ",
            "-000013a0: 6572 0a20 2020 2052 6169 7365 733a 0a20  er.    Raises:. ",
            "-000013b0: 2020 2020 2054 696d 657a 6f6e 6546 6f72       TimezoneFor",
            "-000013c0: 6d61 7445 7272 6f72 3a20 6966 2074 696d  matError: if tim",
            "-000013d0: 657a 6f6e 6520 696e 666f 726d 6174 696f  ezone informatio",
            "-000013e0: 6e20 6361 6e6e 6f74 2062 6520 6578 7472  n cannot be extr",
            "-000013f0: 6163 7465 640a 2020 2020 2222 220a 2020  acted.    \"\"\".  ",
            "-00001400: 2020 696d 706f 7274 2072 650a 0a20 2020    import re..   ",
            "-00001410: 2023 2047 6974 2069 6e74 6572 6e61 6c20   # Git internal ",
            "-00001420: 666f 726d 6174 0a20 2020 2069 6e74 6572  format.    inter",
            "-00001430: 6e61 6c5f 666f 726d 6174 5f70 6174 7465  nal_format_patte",
            "-00001440: 726e 203d 2072 652e 636f 6d70 696c 6528  rn = re.compile(",
            "-00001450: 225e 5b30 2d39 5d2b 205b 2b2d 5d5b 302d  \"^[0-9]+ [+-][0-",
            "-00001460: 395d 7b2c 347d 2422 290a 2020 2020 6966  9]{,4}$\").    if",
            "-00001470: 2072 652e 6d61 7463 6828 696e 7465 726e   re.match(intern",
            "-00001480: 616c 5f66 6f72 6d61 745f 7061 7474 6572  al_format_patter",
            "-00001490: 6e2c 2074 7a5f 7374 7229 3a0a 2020 2020  n, tz_str):.    ",
            "-000014a0: 2020 2020 7472 793a 0a20 2020 2020 2020      try:.       ",
            "-000014b0: 2020 2020 2074 7a5f 696e 7465 726e 616c       tz_internal",
            "-000014c0: 203d 2070 6172 7365 5f74 696d 657a 6f6e   = parse_timezon",
            "-000014d0: 6528 747a 5f73 7472 2e73 706c 6974 2822  e(tz_str.split(\"",
            "-000014e0: 2022 295b 315d 2e65 6e63 6f64 6528 4445   \")[1].encode(DE",
            "-000014f0: 4641 554c 545f 454e 434f 4449 4e47 2929  FAULT_ENCODING))",
            "-00001500: 0a20 2020 2020 2020 2020 2020 2072 6574  .            ret",
            "-00001510: 7572 6e20 747a 5f69 6e74 6572 6e61 6c5b  urn tz_internal[",
            "-00001520: 305d 0a20 2020 2020 2020 2065 7863 6570  0].        excep",
            "-00001530: 7420 5661 6c75 6545 7272 6f72 3a0a 2020  t ValueError:.  ",
            "-00001540: 2020 2020 2020 2020 2020 7061 7373 0a0a            pass..",
            "-00001550: 2020 2020 2320 5246 4320 3238 3232 0a20      # RFC 2822. ",
            "-00001560: 2020 2069 6d70 6f72 7420 656d 6169 6c2e     import email.",
            "-00001570: 7574 696c 730a 0a20 2020 2072 6663 5f32  utils..    rfc_2",
            "-00001580: 3832 3220 3d20 656d 6169 6c2e 7574 696c  822 = email.util",
            "-00001590: 732e 7061 7273 6564 6174 655f 747a 2874  s.parsedate_tz(t",
            "-000015a0: 7a5f 7374 7229 0a20 2020 2069 6620 7266  z_str).    if rf",
            "-000015b0: 635f 3238 3232 3a0a 2020 2020 2020 2020  c_2822:.        ",
            "-000015c0: 7265 7475 726e 2072 6663 5f32 3832 325b  return rfc_2822[",
            "-000015d0: 395d 0a0a 2020 2020 2320 4953 4f20 3836  9]..    # ISO 86",
            "-000015e0: 3031 0a0a 2020 2020 2320 5375 7070 6f72  01..    # Suppor",
            "-000015f0: 7465 6420 6f66 6673 6574 733a 0a20 2020  ted offsets:.   ",
            "-00001600: 2023 2073 4848 4d4d 2c20 7348 483a 4d4d   # sHHMM, sHH:MM",
            "-00001610: 2c20 7348 480a 2020 2020 6973 6f5f 3836  , sHH.    iso_86",
            "-00001620: 3031 5f70 6174 7465 726e 203d 2072 652e  01_pattern = re.",
            "-00001630: 636f 6d70 696c 6528 0a20 2020 2020 2020  compile(.       ",
            "-00001640: 2022 5b30 2d39 5d20 3f28 5b2b 2d5d 2928   \"[0-9] ?([+-])(",
            "-00001650: 5b30 2d39 5d7b 327d 2928 3f3a 3a28 3f3d  [0-9]{2})(?::(?=",
            "-00001660: 5b30 2d39 5d7b 327d 2929 3f28 5b30 2d39  [0-9]{2}))?([0-9",
            "-00001670: 5d7b 327d 293f 2422 0a20 2020 2029 0a20  ]{2})?$\".    ). ",
            "-00001680: 2020 206d 6174 6368 203d 2072 652e 7365     match = re.se",
            "-00001690: 6172 6368 2869 736f 5f38 3630 315f 7061  arch(iso_8601_pa",
            "-000016a0: 7474 6572 6e2c 2074 7a5f 7374 7229 0a20  ttern, tz_str). ",
            "-000016b0: 2020 2074 6f74 616c 5f73 6563 7320 3d20     total_secs = ",
            "-000016c0: 300a 2020 2020 6966 206d 6174 6368 3a0a  0.    if match:.",
            "-000016d0: 2020 2020 2020 2020 7369 676e 2c20 686f          sign, ho",
            "-000016e0: 7572 732c 206d 696e 7574 6573 203d 206d  urs, minutes = m",
            "-000016f0: 6174 6368 2e67 726f 7570 7328 290a 2020  atch.groups().  ",
            "-00001700: 2020 2020 2020 746f 7461 6c5f 7365 6373        total_secs",
            "-00001710: 202b 3d20 696e 7428 686f 7572 7329 202a   += int(hours) *",
            "-00001720: 2033 3630 300a 2020 2020 2020 2020 6966   3600.        if",
            "-00001730: 206d 696e 7574 6573 3a0a 2020 2020 2020   minutes:.      ",
            "-00001740: 2020 2020 2020 746f 7461 6c5f 7365 6373        total_secs",
            "-00001750: 202b 3d20 696e 7428 6d69 6e75 7465 7329   += int(minutes)",
            "-00001760: 202a 2036 300a 2020 2020 2020 2020 746f   * 60.        to",
            "-00001770: 7461 6c5f 7365 6373 203d 202d 746f 7461  tal_secs = -tota",
            "-00001780: 6c5f 7365 6373 2069 6620 7369 676e 203d  l_secs if sign =",
            "-00001790: 3d20 222d 2220 656c 7365 2074 6f74 616c  = \"-\" else total",
            "-000017a0: 5f73 6563 730a 2020 2020 2020 2020 7265  _secs.        re",
            "-000017b0: 7475 726e 2074 6f74 616c 5f73 6563 730a  turn total_secs.",
            "-000017c0: 0a20 2020 2023 2059 5959 592e 4d4d 2e44  .    # YYYY.MM.D",
            "-000017d0: 442c 204d 4d2f 4444 2f59 5959 592c 2044  D, MM/DD/YYYY, D",
            "-000017e0: 442e 4d4d 2e59 5959 5920 636f 6e74 6169  D.MM.YYYY contai",
            "-000017f0: 6e20 6e6f 2074 696d 657a 6f6e 6520 696e  n no timezone in",
            "-00001800: 666f 726d 6174 696f 6e0a 0a20 2020 2072  formation..    r",
            "-00001810: 6169 7365 2054 696d 657a 6f6e 6546 6f72  aise TimezoneFor",
            "-00001820: 6d61 7445 7272 6f72 2874 7a5f 7374 7229  matError(tz_str)",
            "-00001830: 0a0a 0a64 6566 2067 6574 5f75 7365 725f  ...def get_user_",
            "-00001840: 7469 6d65 7a6f 6e65 7328 293a 0a20 2020  timezones():.   ",
            "-00001850: 2022 2222 5265 7472 6965 7665 206c 6f63   \"\"\"Retrieve loc",
            "-00001860: 616c 2074 696d 657a 6f6e 6520 6173 2064  al timezone as d",
            "-00001870: 6573 6372 6962 6564 2069 6e0a 2020 2020  escribed in.    ",
            "-00001880: 6874 7470 733a 2f2f 7261 772e 6769 7468  https://raw.gith",
            "-00001890: 7562 7573 6572 636f 6e74 656e 742e 636f  ubusercontent.co",
            "-000018a0: 6d2f 6769 742f 6769 742f 7632 2e33 2e30  m/git/git/v2.3.0",
            "-000018b0: 2f44 6f63 756d 656e 7461 7469 6f6e 2f64  /Documentation/d",
            "-000018c0: 6174 652d 666f 726d 6174 732e 7478 740a  ate-formats.txt.",
            "-000018d0: 2020 2020 5265 7475 726e 733a 2041 2074      Returns: A t",
            "-000018e0: 7570 6c65 2063 6f6e 7461 696e 696e 6720  uple containing ",
            "-000018f0: 6175 7468 6f72 2074 696d 657a 6f6e 652c  author timezone,",
            "-00001900: 2063 6f6d 6d69 7474 6572 2074 696d 657a   committer timez",
            "-00001910: 6f6e 652e 0a20 2020 2022 2222 0a20 2020  one..    \"\"\".   ",
            "-00001920: 206c 6f63 616c 5f74 696d 657a 6f6e 6520   local_timezone ",
            "-00001930: 3d20 7469 6d65 2e6c 6f63 616c 7469 6d65  = time.localtime",
            "-00001940: 2829 2e74 6d5f 676d 746f 6666 0a0a 2020  ().tm_gmtoff..  ",
            "-00001950: 2020 6966 206f 732e 656e 7669 726f 6e2e    if os.environ.",
            "-00001960: 6765 7428 2247 4954 5f41 5554 484f 525f  get(\"GIT_AUTHOR_",
            "-00001970: 4441 5445 2229 3a0a 2020 2020 2020 2020  DATE\"):.        ",
            "-00001980: 6175 7468 6f72 5f74 696d 657a 6f6e 6520  author_timezone ",
            "-00001990: 3d20 7061 7273 655f 7469 6d65 7a6f 6e65  = parse_timezone",
            "-000019a0: 5f66 6f72 6d61 7428 6f73 2e65 6e76 6972  _format(os.envir",
            "-000019b0: 6f6e 5b22 4749 545f 4155 5448 4f52 5f44  on[\"GIT_AUTHOR_D",
            "-000019c0: 4154 4522 5d29 0a20 2020 2065 6c73 653a  ATE\"]).    else:",
            "-000019d0: 0a20 2020 2020 2020 2061 7574 686f 725f  .        author_",
            "-000019e0: 7469 6d65 7a6f 6e65 203d 206c 6f63 616c  timezone = local",
            "-000019f0: 5f74 696d 657a 6f6e 650a 2020 2020 6966  _timezone.    if",
            "-00001a00: 206f 732e 656e 7669 726f 6e2e 6765 7428   os.environ.get(",
            "-00001a10: 2247 4954 5f43 4f4d 4d49 5454 4552 5f44  \"GIT_COMMITTER_D",
            "-00001a20: 4154 4522 293a 0a20 2020 2020 2020 2063  ATE\"):.        c",
            "-00001a30: 6f6d 6d69 745f 7469 6d65 7a6f 6e65 203d  ommit_timezone =",
            "-00001a40: 2070 6172 7365 5f74 696d 657a 6f6e 655f   parse_timezone_",
            "-00001a50: 666f 726d 6174 286f 732e 656e 7669 726f  format(os.enviro",
            "-00001a60: 6e5b 2247 4954 5f43 4f4d 4d49 5454 4552  n[\"GIT_COMMITTER",
            "-00001a70: 5f44 4154 4522 5d29 0a20 2020 2065 6c73  _DATE\"]).    els",
            "-00001a80: 653a 0a20 2020 2020 2020 2063 6f6d 6d69  e:.        commi",
            "-00001a90: 745f 7469 6d65 7a6f 6e65 203d 206c 6f63  t_timezone = loc",
            "-00001aa0: 616c 5f74 696d 657a 6f6e 650a 0a20 2020  al_timezone..   ",
            "-00001ab0: 2072 6574 7572 6e20 6175 7468 6f72 5f74   return author_t",
            "-00001ac0: 696d 657a 6f6e 652c 2063 6f6d 6d69 745f  imezone, commit_",
            "-00001ad0: 7469 6d65 7a6f 6e65 0a0a 0a64 6566 206f  timezone...def o",
            "-00001ae0: 7065 6e5f 7265 706f 2870 6174 685f 6f72  pen_repo(path_or",
            "-00001af0: 5f72 6570 6f29 3a0a 2020 2020 2222 224f  _repo):.    \"\"\"O",
            "-00001b00: 7065 6e20 616e 2061 7267 756d 656e 7420  pen an argument ",
            "-00001b10: 7468 6174 2063 616e 2062 6520 6120 7265  that can be a re",
            "-00001b20: 706f 7369 746f 7279 206f 7220 6120 7061  pository or a pa",
            "-00001b30: 7468 2066 6f72 2061 2072 6570 6f73 6974  th for a reposit",
            "-00001b40: 6f72 792e 2222 220a 2020 2020 6966 2069  ory.\"\"\".    if i",
            "-00001b50: 7369 6e73 7461 6e63 6528 7061 7468 5f6f  sinstance(path_o",
            "-00001b60: 725f 7265 706f 2c20 4261 7365 5265 706f  r_repo, BaseRepo",
            "-00001b70: 293a 0a20 2020 2020 2020 2072 6574 7572  ):.        retur",
            "-00001b80: 6e20 7061 7468 5f6f 725f 7265 706f 0a20  n path_or_repo. ",
            "-00001b90: 2020 2072 6574 7572 6e20 5265 706f 2870     return Repo(p",
            "-00001ba0: 6174 685f 6f72 5f72 6570 6f29 0a0a 0a40  ath_or_repo)...@",
            "-00001bb0: 636f 6e74 6578 746d 616e 6167 6572 0a64  contextmanager.d",
            "-00001bc0: 6566 205f 6e6f 6f70 5f63 6f6e 7465 7874  ef _noop_context",
            "-00001bd0: 5f6d 616e 6167 6572 286f 626a 293a 0a20  _manager(obj):. ",
            "-00001be0: 2020 2022 2222 436f 6e74 6578 7420 6d61     \"\"\"Context ma",
            "-00001bf0: 6e61 6765 7220 7468 6174 2068 6173 2074  nager that has t",
            "-00001c00: 6865 2073 616d 6520 6170 6920 6173 2063  he same api as c",
            "-00001c10: 6c6f 7369 6e67 2062 7574 2064 6f65 7320  losing but does ",
            "-00001c20: 6e6f 7468 696e 672e 2222 220a 2020 2020  nothing.\"\"\".    ",
            "-00001c30: 7969 656c 6420 6f62 6a0a 0a0a 6465 6620  yield obj...def ",
            "-00001c40: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "-00001c50: 6728 7061 7468 5f6f 725f 7265 706f 293a  g(path_or_repo):",
            "-00001c60: 0a20 2020 2022 2222 4f70 656e 2061 6e20  .    \"\"\"Open an ",
            "-00001c70: 6172 6775 6d65 6e74 2074 6861 7420 6361  argument that ca",
            "-00001c80: 6e20 6265 2061 2072 6570 6f73 6974 6f72  n be a repositor",
            "-00001c90: 7920 6f72 2061 2070 6174 6820 666f 7220  y or a path for ",
            "-00001ca0: 6120 7265 706f 7369 746f 7279 2e0a 2020  a repository..  ",
            "-00001cb0: 2020 7265 7475 726e 7320 6120 636f 6e74    returns a cont",
            "-00001cc0: 6578 7420 6d61 6e61 6765 7220 7468 6174  ext manager that",
            "-00001cd0: 2077 696c 6c20 636c 6f73 6520 7468 6520   will close the ",
            "-00001ce0: 7265 706f 206f 6e20 6578 6974 2069 6620  repo on exit if ",
            "-00001cf0: 7468 6520 6172 6775 6d65 6e74 0a20 2020  the argument.   ",
            "-00001d00: 2069 7320 6120 7061 7468 2c20 656c 7365   is a path, else",
            "-00001d10: 2064 6f65 7320 6e6f 7468 696e 6720 6966   does nothing if",
            "-00001d20: 2074 6865 2061 7267 756d 656e 7420 6973   the argument is",
            "-00001d30: 2061 2072 6570 6f2e 0a20 2020 2022 2222   a repo..    \"\"\"",
            "-00001d40: 0a20 2020 2069 6620 6973 696e 7374 616e  .    if isinstan",
            "-00001d50: 6365 2870 6174 685f 6f72 5f72 6570 6f2c  ce(path_or_repo,",
            "-00001d60: 2042 6173 6552 6570 6f29 3a0a 2020 2020   BaseRepo):.    ",
            "-00001d70: 2020 2020 7265 7475 726e 205f 6e6f 6f70      return _noop",
            "-00001d80: 5f63 6f6e 7465 7874 5f6d 616e 6167 6572  _context_manager",
            "-00001d90: 2870 6174 685f 6f72 5f72 6570 6f29 0a20  (path_or_repo). ",
            "-00001da0: 2020 2072 6574 7572 6e20 636c 6f73 696e     return closin",
            "-00001db0: 6728 5265 706f 2870 6174 685f 6f72 5f72  g(Repo(path_or_r",
            "-00001dc0: 6570 6f29 290a 0a0a 6465 6620 7061 7468  epo))...def path",
            "-00001dd0: 5f74 6f5f 7472 6565 5f70 6174 6828 7265  _to_tree_path(re",
            "-00001de0: 706f 7061 7468 2c20 7061 7468 2c20 7472  popath, path, tr",
            "-00001df0: 6565 5f65 6e63 6f64 696e 673d 4445 4641  ee_encoding=DEFA",
            "-00001e00: 554c 545f 454e 434f 4449 4e47 293a 0a20  ULT_ENCODING):. ",
            "-00001e10: 2020 2022 2222 436f 6e76 6572 7420 6120     \"\"\"Convert a ",
            "-00001e20: 7061 7468 2074 6f20 6120 7061 7468 2075  path to a path u",
            "-00001e30: 7361 626c 6520 696e 2061 6e20 696e 6465  sable in an inde",
            "-00001e40: 782c 2065 2e67 2e20 6279 7465 7320 616e  x, e.g. bytes an",
            "-00001e50: 6420 7265 6c61 7469 7665 2074 6f0a 2020  d relative to.  ",
            "-00001e60: 2020 7468 6520 7265 706f 7369 746f 7279    the repository",
            "-00001e70: 2072 6f6f 742e 0a0a 2020 2020 4172 6773   root...    Args",
            "-00001e80: 3a0a 2020 2020 2020 7265 706f 7061 7468  :.      repopath",
            "-00001e90: 3a20 5265 706f 7369 746f 7279 2070 6174  : Repository pat",
            "-00001ea0: 682c 2061 6273 6f6c 7574 6520 6f72 2072  h, absolute or r",
            "-00001eb0: 656c 6174 6976 6520 746f 2074 6865 2063  elative to the c",
            "-00001ec0: 7764 0a20 2020 2020 2070 6174 683a 2041  wd.      path: A",
            "-00001ed0: 2070 6174 682c 2061 6273 6f6c 7574 6520   path, absolute ",
            "-00001ee0: 6f72 2072 656c 6174 6976 6520 746f 2074  or relative to t",
            "-00001ef0: 6865 2063 7764 0a20 2020 2052 6574 7572  he cwd.    Retur",
            "-00001f00: 6e73 3a20 4120 7061 7468 2066 6f72 6d61  ns: A path forma",
            "-00001f10: 7474 6564 2066 6f72 2075 7365 2069 6e20  tted for use in ",
            "-00001f20: 652e 672e 2061 6e20 696e 6465 780a 2020  e.g. an index.  ",
            "-00001f30: 2020 2222 220a 2020 2020 2320 5265 736f    \"\"\".    # Reso",
            "-00001f40: 6c76 6520 6d69 6768 7420 7265 7475 726e  lve might return",
            "-00001f50: 7320 6120 7265 6c61 7469 7665 2070 6174  s a relative pat",
            "-00001f60: 6820 6f6e 2057 696e 646f 7773 0a20 2020  h on Windows.   ",
            "-00001f70: 2023 2068 7474 7073 3a2f 2f62 7567 732e   # https://bugs.",
            "-00001f80: 7079 7468 6f6e 2e6f 7267 2f69 7373 7565  python.org/issue",
            "-00001f90: 3338 3637 310a 2020 2020 6966 2073 7973  38671.    if sys",
            "-00001fa0: 2e70 6c61 7466 6f72 6d20 3d3d 2022 7769  .platform == \"wi",
            "-00001fb0: 6e33 3222 3a0a 2020 2020 2020 2020 7061  n32\":.        pa",
            "-00001fc0: 7468 203d 206f 732e 7061 7468 2e61 6273  th = os.path.abs",
            "-00001fd0: 7061 7468 2870 6174 6829 0a0a 2020 2020  path(path)..    ",
            "-00001fe0: 7061 7468 203d 2050 6174 6828 7061 7468  path = Path(path",
            "-00001ff0: 290a 2020 2020 7265 736f 6c76 6564 5f70  ).    resolved_p",
            "-00002000: 6174 6820 3d20 7061 7468 2e72 6573 6f6c  ath = path.resol",
            "-00002010: 7665 2829 0a0a 2020 2020 2320 5265 736f  ve()..    # Reso",
            "-00002020: 6c76 6520 616e 6420 6162 7370 6174 6820  lve and abspath ",
            "-00002030: 7365 656d 7320 746f 2062 6568 6176 6520  seems to behave ",
            "-00002040: 6469 6666 6572 656e 746c 7920 7265 6761  differently rega",
            "-00002050: 7264 696e 6720 7379 6d6c 696e 6b73 2c0a  rding symlinks,.",
            "-00002060: 2020 2020 2320 6173 2077 6520 6172 6520      # as we are ",
            "-00002070: 646f 696e 6720 6162 7370 6174 6820 6f6e  doing abspath on",
            "-00002080: 2074 6865 2066 696c 6520 7061 7468 2c20   the file path, ",
            "-00002090: 7765 206e 6565 6420 746f 2064 6f20 7468  we need to do th",
            "-000020a0: 6520 7361 6d65 206f 6e0a 2020 2020 2320  e same on.    # ",
            "-000020b0: 7468 6520 7265 706f 2070 6174 6820 6f72  the repo path or",
            "-000020c0: 2074 6865 7920 6d69 6768 7420 6e6f 7420   they might not ",
            "-000020d0: 6d61 7463 680a 2020 2020 6966 2073 7973  match.    if sys",
            "-000020e0: 2e70 6c61 7466 6f72 6d20 3d3d 2022 7769  .platform == \"wi",
            "-000020f0: 6e33 3222 3a0a 2020 2020 2020 2020 7265  n32\":.        re",
            "-00002100: 706f 7061 7468 203d 206f 732e 7061 7468  popath = os.path",
            "-00002110: 2e61 6273 7061 7468 2872 6570 6f70 6174  .abspath(repopat",
            "-00002120: 6829 0a0a 2020 2020 7265 706f 7061 7468  h)..    repopath",
            "-00002130: 203d 2050 6174 6828 7265 706f 7061 7468   = Path(repopath",
            "-00002140: 292e 7265 736f 6c76 6528 290a 0a20 2020  ).resolve()..   ",
            "-00002150: 2074 7279 3a0a 2020 2020 2020 2020 7265   try:.        re",
            "-00002160: 6c70 6174 6820 3d20 7265 736f 6c76 6564  lpath = resolved",
            "-00002170: 5f70 6174 682e 7265 6c61 7469 7665 5f74  _path.relative_t",
            "-00002180: 6f28 7265 706f 7061 7468 290a 2020 2020  o(repopath).    ",
            "-00002190: 6578 6365 7074 2056 616c 7565 4572 726f  except ValueErro",
            "-000021a0: 723a 0a20 2020 2020 2020 2023 2049 6620  r:.        # If ",
            "-000021b0: 7061 7468 2069 7320 6120 7379 6d6c 696e  path is a symlin",
            "-000021c0: 6b20 7468 6174 2070 6f69 6e74 7320 746f  k that points to",
            "-000021d0: 2061 2066 696c 6520 6f75 7473 6964 6520   a file outside ",
            "-000021e0: 7468 6520 7265 706f 2c20 7765 0a20 2020  the repo, we.   ",
            "-000021f0: 2020 2020 2023 2077 616e 7420 7468 6520       # want the ",
            "-00002200: 7265 6c70 6174 6820 666f 7220 7468 6520  relpath for the ",
            "-00002210: 6c69 6e6b 2069 7473 656c 662c 206e 6f74  link itself, not",
            "-00002220: 2074 6865 2072 6573 6f6c 7665 6420 7461   the resolved ta",
            "-00002230: 7267 6574 0a20 2020 2020 2020 2069 6620  rget.        if ",
            "-00002240: 7061 7468 2e69 735f 7379 6d6c 696e 6b28  path.is_symlink(",
            "-00002250: 293a 0a20 2020 2020 2020 2020 2020 2070  ):.            p",
            "-00002260: 6172 656e 7420 3d20 7061 7468 2e70 6172  arent = path.par",
            "-00002270: 656e 742e 7265 736f 6c76 6528 290a 2020  ent.resolve().  ",
            "-00002280: 2020 2020 2020 2020 2020 7265 6c70 6174            relpat",
            "-00002290: 6820 3d20 2870 6172 656e 7420 2f20 7061  h = (parent / pa",
            "-000022a0: 7468 2e6e 616d 6529 2e72 656c 6174 6976  th.name).relativ",
            "-000022b0: 655f 746f 2872 6570 6f70 6174 6829 0a20  e_to(repopath). ",
            "-000022c0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   ",
            "-000022d0: 2020 2020 2020 2020 2072 6169 7365 0a20           raise. ",
            "-000022e0: 2020 2069 6620 7379 732e 706c 6174 666f     if sys.platfo",
            "-000022f0: 726d 203d 3d20 2277 696e 3332 223a 0a20  rm == \"win32\":. ",
            "-00002300: 2020 2020 2020 2072 6574 7572 6e20 7374         return st",
            "-00002310: 7228 7265 6c70 6174 6829 2e72 6570 6c61  r(relpath).repla",
            "-00002320: 6365 286f 732e 7061 7468 2e73 6570 2c20  ce(os.path.sep, ",
            "-00002330: 222f 2229 2e65 6e63 6f64 6528 7472 6565  \"/\").encode(tree",
            "-00002340: 5f65 6e63 6f64 696e 6729 0a20 2020 2065  _encoding).    e",
            "-00002350: 6c73 653a 0a20 2020 2020 2020 2072 6574  lse:.        ret",
            "-00002360: 7572 6e20 6279 7465 7328 7265 6c70 6174  urn bytes(relpat",
            "-00002370: 6829 0a0a 0a63 6c61 7373 2044 6976 6572  h)...class Diver",
            "-00002380: 6765 6442 7261 6e63 6865 7328 4572 726f  gedBranches(Erro",
            "-00002390: 7229 3a0a 2020 2020 2222 2242 7261 6e63  r):.    \"\"\"Branc",
            "-000023a0: 6865 7320 6861 7665 2064 6976 6572 6765  hes have diverge",
            "-000023b0: 6420 616e 6420 6661 7374 2d66 6f72 7761  d and fast-forwa",
            "-000023c0: 7264 2069 7320 6e6f 7420 706f 7373 6962  rd is not possib",
            "-000023d0: 6c65 2e22 2222 0a0a 2020 2020 6465 6620  le.\"\"\"..    def ",
            "-000023e0: 5f5f 696e 6974 5f5f 2873 656c 662c 2063  __init__(self, c",
            "-000023f0: 7572 7265 6e74 5f73 6861 2c20 6e65 775f  urrent_sha, new_",
            "-00002400: 7368 6129 202d 3e20 4e6f 6e65 3a0a 2020  sha) -> None:.  ",
            "-00002410: 2020 2020 2020 7365 6c66 2e63 7572 7265        self.curre",
            "-00002420: 6e74 5f73 6861 203d 2063 7572 7265 6e74  nt_sha = current",
            "-00002430: 5f73 6861 0a20 2020 2020 2020 2073 656c  _sha.        sel",
            "-00002440: 662e 6e65 775f 7368 6120 3d20 6e65 775f  f.new_sha = new_",
            "-00002450: 7368 610a 0a0a 6465 6620 6368 6563 6b5f  sha...def check_",
            "-00002460: 6469 7665 7267 6564 2872 6570 6f2c 2063  diverged(repo, c",
            "-00002470: 7572 7265 6e74 5f73 6861 2c20 6e65 775f  urrent_sha, new_",
            "-00002480: 7368 6129 202d 3e20 4e6f 6e65 3a0a 2020  sha) -> None:.  ",
            "-00002490: 2020 2222 2243 6865 636b 2069 6620 7570    \"\"\"Check if up",
            "-000024a0: 6461 7469 6e67 2074 6f20 6120 7368 6120  dating to a sha ",
            "-000024b0: 6361 6e20 6265 2064 6f6e 6520 7769 7468  can be done with",
            "-000024c0: 2066 6173 7420 666f 7277 6172 6469 6e67   fast forwarding",
            "-000024d0: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "-000024e0: 2020 2072 6570 6f3a 2052 6570 6f73 6974     repo: Reposit",
            "-000024f0: 6f72 7920 6f62 6a65 6374 0a20 2020 2020  ory object.     ",
            "-00002500: 2063 7572 7265 6e74 5f73 6861 3a20 4375   current_sha: Cu",
            "-00002510: 7272 656e 7420 6865 6164 2073 6861 0a20  rrent head sha. ",
            "-00002520: 2020 2020 206e 6577 5f73 6861 3a20 4e65       new_sha: Ne",
            "-00002530: 7720 6865 6164 2073 6861 0a20 2020 2022  w head sha.    \"",
            "-00002540: 2222 0a20 2020 2074 7279 3a0a 2020 2020  \"\".    try:.    ",
            "-00002550: 2020 2020 6361 6e20 3d20 6361 6e5f 6661      can = can_fa",
            "-00002560: 7374 5f66 6f72 7761 7264 2872 6570 6f2c  st_forward(repo,",
            "-00002570: 2063 7572 7265 6e74 5f73 6861 2c20 6e65   current_sha, ne",
            "-00002580: 775f 7368 6129 0a20 2020 2065 7863 6570  w_sha).    excep",
            "-00002590: 7420 4b65 7945 7272 6f72 3a0a 2020 2020  t KeyError:.    ",
            "-000025a0: 2020 2020 6361 6e20 3d20 4661 6c73 650a      can = False.",
            "-000025b0: 2020 2020 6966 206e 6f74 2063 616e 3a0a      if not can:.",
            "-000025c0: 2020 2020 2020 2020 7261 6973 6520 4469          raise Di",
            "-000025d0: 7665 7267 6564 4272 616e 6368 6573 2863  vergedBranches(c",
            "-000025e0: 7572 7265 6e74 5f73 6861 2c20 6e65 775f  urrent_sha, new_",
            "-000025f0: 7368 6129 0a0a 0a64 6566 2061 7263 6869  sha)...def archi",
            "-00002600: 7665 280a 2020 2020 7265 706f 2c0a 2020  ve(.    repo,.  ",
            "-00002610: 2020 636f 6d6d 6974 7469 7368 3d4e 6f6e    committish=Non",
            "-00002620: 652c 0a20 2020 206f 7574 7374 7265 616d  e,.    outstream",
            "-00002630: 3d64 6566 6175 6c74 5f62 7974 6573 5f6f  =default_bytes_o",
            "-00002640: 7574 5f73 7472 6561 6d2c 0a20 2020 2065  ut_stream,.    e",
            "-00002650: 7272 7374 7265 616d 3d64 6566 6175 6c74  rrstream=default",
            "-00002660: 5f62 7974 6573 5f65 7272 5f73 7472 6561  _bytes_err_strea",
            "-00002670: 6d2c 0a29 202d 3e20 4e6f 6e65 3a0a 2020  m,.) -> None:.  ",
            "-00002680: 2020 2222 2243 7265 6174 6520 616e 2061    \"\"\"Create an a",
            "-00002690: 7263 6869 7665 2e0a 0a20 2020 2041 7267  rchive...    Arg",
            "-000026a0: 733a 0a20 2020 2020 2072 6570 6f3a 2050  s:.      repo: P",
            "-000026b0: 6174 6820 6f66 2072 6570 6f73 6974 6f72  ath of repositor",
            "-000026c0: 7920 666f 7220 7768 6963 6820 746f 2067  y for which to g",
            "-000026d0: 656e 6572 6174 6520 616e 2061 7263 6869  enerate an archi",
            "-000026e0: 7665 2e0a 2020 2020 2020 636f 6d6d 6974  ve..      commit",
            "-000026f0: 7469 7368 3a20 436f 6d6d 6974 2053 4841  tish: Commit SHA",
            "-00002700: 3120 6f72 2072 6566 2074 6f20 7573 650a  1 or ref to use.",
            "-00002710: 2020 2020 2020 6f75 7473 7472 6561 6d3a        outstream:",
            "-00002720: 204f 7574 7075 7420 7374 7265 616d 2028   Output stream (",
            "-00002730: 6465 6661 756c 7473 2074 6f20 7374 646f  defaults to stdo",
            "-00002740: 7574 290a 2020 2020 2020 6572 7273 7472  ut).      errstr",
            "-00002750: 6561 6d3a 2045 7272 6f72 2073 7472 6561  eam: Error strea",
            "-00002760: 6d20 2864 6566 6175 6c74 7320 746f 2073  m (defaults to s",
            "-00002770: 7464 6572 7229 0a20 2020 2022 2222 0a20  tderr).    \"\"\". ",
            "-00002780: 2020 2069 6620 636f 6d6d 6974 7469 7368     if committish",
            "-00002790: 2069 7320 4e6f 6e65 3a0a 2020 2020 2020   is None:.      ",
            "-000027a0: 2020 636f 6d6d 6974 7469 7368 203d 2022    committish = \"",
            "-000027b0: 4845 4144 220a 2020 2020 7769 7468 206f  HEAD\".    with o",
            "-000027c0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "-000027d0: 2872 6570 6f29 2061 7320 7265 706f 5f6f  (repo) as repo_o",
            "-000027e0: 626a 3a0a 2020 2020 2020 2020 6320 3d20  bj:.        c = ",
            "-000027f0: 7061 7273 655f 636f 6d6d 6974 2872 6570  parse_commit(rep",
            "-00002800: 6f5f 6f62 6a2c 2063 6f6d 6d69 7474 6973  o_obj, committis",
            "-00002810: 6829 0a20 2020 2020 2020 2066 6f72 2063  h).        for c",
            "-00002820: 6875 6e6b 2069 6e20 7461 725f 7374 7265  hunk in tar_stre",
            "-00002830: 616d 280a 2020 2020 2020 2020 2020 2020  am(.            ",
            "-00002840: 7265 706f 5f6f 626a 2e6f 626a 6563 745f  repo_obj.object_",
            "-00002850: 7374 6f72 652c 2072 6570 6f5f 6f62 6a2e  store, repo_obj.",
            "-00002860: 6f62 6a65 6374 5f73 746f 7265 5b63 2e74  object_store[c.t",
            "-00002870: 7265 655d 2c20 632e 636f 6d6d 6974 5f74  ree], c.commit_t",
            "-00002880: 696d 650a 2020 2020 2020 2020 293a 0a20  ime.        ):. ",
            "-00002890: 2020 2020 2020 2020 2020 206f 7574 7374             outst",
            "-000028a0: 7265 616d 2e77 7269 7465 2863 6875 6e6b  ream.write(chunk",
            "-000028b0: 290a 0a0a 6465 6620 7570 6461 7465 5f73  )...def update_s",
            "-000028c0: 6572 7665 725f 696e 666f 2872 6570 6f3d  erver_info(repo=",
            "-000028d0: 222e 2229 202d 3e20 4e6f 6e65 3a0a 2020  \".\") -> None:.  ",
            "-000028e0: 2020 2222 2255 7064 6174 6520 7365 7276    \"\"\"Update serv",
            "-000028f0: 6572 2069 6e66 6f20 6669 6c65 7320 666f  er info files fo",
            "-00002900: 7220 6120 7265 706f 7369 746f 7279 2e0a  r a repository..",
            "-00002910: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "-00002920: 2072 6570 6f3a 2070 6174 6820 746f 2074   repo: path to t",
            "-00002930: 6865 2072 6570 6f73 6974 6f72 790a 2020  he repository.  ",
            "-00002940: 2020 2222 220a 2020 2020 7769 7468 206f    \"\"\".    with o",
            "-00002950: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "-00002960: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "-00002970: 2020 2020 2073 6572 7665 725f 7570 6461       server_upda",
            "-00002980: 7465 5f73 6572 7665 725f 696e 666f 2872  te_server_info(r",
            "-00002990: 290a 0a0a 6465 6620 7379 6d62 6f6c 6963  )...def symbolic",
            "-000029a0: 5f72 6566 2872 6570 6f2c 2072 6566 5f6e  _ref(repo, ref_n",
            "-000029b0: 616d 652c 2066 6f72 6365 3d46 616c 7365  ame, force=False",
            "-000029c0: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    \"",
            "-000029d0: 2222 5365 7420 6769 7420 7379 6d62 6f6c  \"\"Set git symbol",
            "-000029e0: 6963 2072 6566 2069 6e74 6f20 4845 4144  ic ref into HEAD",
            "-000029f0: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "-00002a00: 2020 2072 6570 6f3a 2070 6174 6820 746f     repo: path to",
            "-00002a10: 2074 6865 2072 6570 6f73 6974 6f72 790a   the repository.",
            "-00002a20: 2020 2020 2020 7265 665f 6e61 6d65 3a20        ref_name: ",
            "-00002a30: 7368 6f72 7420 6e61 6d65 206f 6620 7468  short name of th",
            "-00002a40: 6520 6e65 7720 7265 660a 2020 2020 2020  e new ref.      ",
            "-00002a50: 666f 7263 653a 2066 6f72 6365 2073 6574  force: force set",
            "-00002a60: 7469 6e67 7320 7769 7468 6f75 7420 6368  tings without ch",
            "-00002a70: 6563 6b69 6e67 2069 6620 6974 2065 7869  ecking if it exi",
            "-00002a80: 7374 7320 696e 2072 6566 732f 6865 6164  sts in refs/head",
            "-00002a90: 730a 2020 2020 2222 220a 2020 2020 7769  s.    \"\"\".    wi",
            "-00002aa0: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "-00002ab0: 7369 6e67 2872 6570 6f29 2061 7320 7265  sing(repo) as re",
            "-00002ac0: 706f 5f6f 626a 3a0a 2020 2020 2020 2020  po_obj:.        ",
            "-00002ad0: 7265 665f 7061 7468 203d 205f 6d61 6b65  ref_path = _make",
            "-00002ae0: 5f62 7261 6e63 685f 7265 6628 7265 665f  _branch_ref(ref_",
            "-00002af0: 6e61 6d65 290a 2020 2020 2020 2020 6966  name).        if",
            "-00002b00: 206e 6f74 2066 6f72 6365 2061 6e64 2072   not force and r",
            "-00002b10: 6566 5f70 6174 6820 6e6f 7420 696e 2072  ef_path not in r",
            "-00002b20: 6570 6f5f 6f62 6a2e 7265 6673 2e6b 6579  epo_obj.refs.key",
            "-00002b30: 7328 293a 0a20 2020 2020 2020 2020 2020  s():.           ",
            "-00002b40: 2072 6169 7365 2045 7272 6f72 2866 2266   raise Error(f\"f",
            "-00002b50: 6174 616c 3a20 7265 6620 607b 7265 665f  atal: ref `{ref_",
            "-00002b60: 6e61 6d65 7d60 2069 7320 6e6f 7420 6120  name}` is not a ",
            "-00002b70: 7265 6622 290a 2020 2020 2020 2020 7265  ref\").        re",
            "-00002b80: 706f 5f6f 626a 2e72 6566 732e 7365 745f  po_obj.refs.set_",
            "-00002b90: 7379 6d62 6f6c 6963 5f72 6566 2862 2248  symbolic_ref(b\"H",
            "-00002ba0: 4541 4422 2c20 7265 665f 7061 7468 290a  EAD\", ref_path).",
            "-00002bb0: 0a0a 6465 6620 7061 636b 5f72 6566 7328  ..def pack_refs(",
            "-00002bc0: 7265 706f 2c20 616c 6c3d 4661 6c73 6529  repo, all=False)",
            "-00002bd0: 202d 3e20 4e6f 6e65 3a0a 2020 2020 7769   -> None:.    wi",
            "-00002be0: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "-00002bf0: 7369 6e67 2872 6570 6f29 2061 7320 7265  sing(repo) as re",
            "-00002c00: 706f 5f6f 626a 3a0a 2020 2020 2020 2020  po_obj:.        ",
            "-00002c10: 7265 6673 203d 2072 6570 6f5f 6f62 6a2e  refs = repo_obj.",
            "-00002c20: 7265 6673 0a20 2020 2020 2020 2070 6163  refs.        pac",
            "-00002c30: 6b65 645f 7265 6673 203d 207b 0a20 2020  ked_refs = {.   ",
            "-00002c40: 2020 2020 2020 2020 2072 6566 3a20 7265           ref: re",
            "-00002c50: 6673 5b72 6566 5d0a 2020 2020 2020 2020  fs[ref].        ",
            "-00002c60: 2020 2020 666f 7220 7265 6620 696e 2072      for ref in r",
            "-00002c70: 6566 730a 2020 2020 2020 2020 2020 2020  efs.            ",
            "-00002c80: 6966 2028 616c 6c20 6f72 2072 6566 2e73  if (all or ref.s",
            "-00002c90: 7461 7274 7377 6974 6828 4c4f 4341 4c5f  tartswith(LOCAL_",
            "-00002ca0: 5441 475f 5052 4546 4958 2929 2061 6e64  TAG_PREFIX)) and",
            "-00002cb0: 2072 6566 2021 3d20 6222 4845 4144 220a   ref != b\"HEAD\".",
            "-00002cc0: 2020 2020 2020 2020 7d0a 0a20 2020 2020          }..     ",
            "-00002cd0: 2020 2072 6566 732e 6164 645f 7061 636b     refs.add_pack",
            "-00002ce0: 6564 5f72 6566 7328 7061 636b 6564 5f72  ed_refs(packed_r",
            "-00002cf0: 6566 7329 0a0a 0a64 6566 2063 6f6d 6d69  efs)...def commi",
            "-00002d00: 7428 0a20 2020 2072 6570 6f3d 222e 222c  t(.    repo=\".\",",
            "-00002d10: 0a20 2020 206d 6573 7361 6765 3d4e 6f6e  .    message=Non",
            "-00002d20: 652c 0a20 2020 2061 7574 686f 723d 4e6f  e,.    author=No",
            "-00002d30: 6e65 2c0a 2020 2020 6175 7468 6f72 5f74  ne,.    author_t",
            "-00002d40: 696d 657a 6f6e 653d 4e6f 6e65 2c0a 2020  imezone=None,.  ",
            "-00002d50: 2020 636f 6d6d 6974 7465 723d 4e6f 6e65    committer=None",
            "-00002d60: 2c0a 2020 2020 636f 6d6d 6974 5f74 696d  ,.    commit_tim",
            "-00002d70: 657a 6f6e 653d 4e6f 6e65 2c0a 2020 2020  ezone=None,.    ",
            "-00002d80: 656e 636f 6469 6e67 3d4e 6f6e 652c 0a20  encoding=None,. ",
            "-00002d90: 2020 206e 6f5f 7665 7269 6679 3d46 616c     no_verify=Fal",
            "-00002da0: 7365 2c0a 2020 2020 7369 676e 6f66 663d  se,.    signoff=",
            "-00002db0: 4661 6c73 652c 0a29 3a0a 2020 2020 2222  False,.):.    \"\"",
            "-00002dc0: 2243 7265 6174 6520 6120 6e65 7720 636f  \"Create a new co",
            "-00002dd0: 6d6d 6974 2e0a 0a20 2020 2041 7267 733a  mmit...    Args:",
            "-00002de0: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "-00002df0: 6820 746f 2072 6570 6f73 6974 6f72 790a  h to repository.",
            "-00002e00: 2020 2020 2020 6d65 7373 6167 653a 204f        message: O",
            "-00002e10: 7074 696f 6e61 6c20 636f 6d6d 6974 206d  ptional commit m",
            "-00002e20: 6573 7361 6765 0a20 2020 2020 2061 7574  essage.      aut",
            "-00002e30: 686f 723a 204f 7074 696f 6e61 6c20 6175  hor: Optional au",
            "-00002e40: 7468 6f72 206e 616d 6520 616e 6420 656d  thor name and em",
            "-00002e50: 6169 6c0a 2020 2020 2020 6175 7468 6f72  ail.      author",
            "-00002e60: 5f74 696d 657a 6f6e 653a 2041 7574 686f  _timezone: Autho",
            "-00002e70: 7220 7469 6d65 7374 616d 7020 7469 6d65  r timestamp time",
            "-00002e80: 7a6f 6e65 0a20 2020 2020 2063 6f6d 6d69  zone.      commi",
            "-00002e90: 7474 6572 3a20 4f70 7469 6f6e 616c 2063  tter: Optional c",
            "-00002ea0: 6f6d 6d69 7474 6572 206e 616d 6520 616e  ommitter name an",
            "-00002eb0: 6420 656d 6169 6c0a 2020 2020 2020 636f  d email.      co",
            "-00002ec0: 6d6d 6974 5f74 696d 657a 6f6e 653a 2043  mmit_timezone: C",
            "-00002ed0: 6f6d 6d69 7420 7469 6d65 7374 616d 7020  ommit timestamp ",
            "-00002ee0: 7469 6d65 7a6f 6e65 0a20 2020 2020 206e  timezone.      n",
            "-00002ef0: 6f5f 7665 7269 6679 3a20 536b 6970 2070  o_verify: Skip p",
            "-00002f00: 7265 2d63 6f6d 6d69 7420 616e 6420 636f  re-commit and co",
            "-00002f10: 6d6d 6974 2d6d 7367 2068 6f6f 6b73 0a20  mmit-msg hooks. ",
            "-00002f20: 2020 2020 2073 6967 6e6f 6666 3a20 4750       signoff: GP",
            "-00002f30: 4720 5369 676e 2074 6865 2063 6f6d 6d69  G Sign the commi",
            "-00002f40: 7420 2862 6f6f 6c2c 2064 6566 6175 6c74  t (bool, default",
            "-00002f50: 7320 746f 2046 616c 7365 2c0a 2020 2020  s to False,.    ",
            "-00002f60: 2020 2020 7061 7373 2054 7275 6520 746f      pass True to",
            "-00002f70: 2075 7365 2064 6566 6175 6c74 2047 5047   use default GPG",
            "-00002f80: 206b 6579 2c0a 2020 2020 2020 2020 7061   key,.        pa",
            "-00002f90: 7373 2061 2073 7472 2063 6f6e 7461 696e  ss a str contain",
            "-00002fa0: 696e 6720 4b65 7920 4944 2074 6f20 7573  ing Key ID to us",
            "-00002fb0: 6520 6120 7370 6563 6966 6963 2047 5047  e a specific GPG",
            "-00002fc0: 206b 6579 290a 2020 2020 5265 7475 726e   key).    Return",
            "-00002fd0: 733a 2053 4841 3120 6f66 2074 6865 206e  s: SHA1 of the n",
            "-00002fe0: 6577 2063 6f6d 6d69 740a 2020 2020 2222  ew commit.    \"\"",
            "-00002ff0: 220a 2020 2020 2320 4649 584d 453a 2053  \".    # FIXME: S",
            "-00003000: 7570 706f 7274 202d 2d61 6c6c 2061 7267  upport --all arg",
            "-00003010: 756d 656e 740a 2020 2020 6966 2067 6574  ument.    if get",
            "-00003020: 6174 7472 286d 6573 7361 6765 2c20 2265  attr(message, \"e",
            "-00003030: 6e63 6f64 6522 2c20 4e6f 6e65 293a 0a20  ncode\", None):. ",
            "-00003040: 2020 2020 2020 206d 6573 7361 6765 203d         message =",
            "-00003050: 206d 6573 7361 6765 2e65 6e63 6f64 6528   message.encode(",
            "-00003060: 656e 636f 6469 6e67 206f 7220 4445 4641  encoding or DEFA",
            "-00003070: 554c 545f 454e 434f 4449 4e47 290a 2020  ULT_ENCODING).  ",
            "-00003080: 2020 6966 2067 6574 6174 7472 2861 7574    if getattr(aut",
            "-00003090: 686f 722c 2022 656e 636f 6465 222c 204e  hor, \"encode\", N",
            "-000030a0: 6f6e 6529 3a0a 2020 2020 2020 2020 6175  one):.        au",
            "-000030b0: 7468 6f72 203d 2061 7574 686f 722e 656e  thor = author.en",
            "-000030c0: 636f 6465 2865 6e63 6f64 696e 6720 6f72  code(encoding or",
            "-000030d0: 2044 4546 4155 4c54 5f45 4e43 4f44 494e   DEFAULT_ENCODIN",
            "-000030e0: 4729 0a20 2020 2069 6620 6765 7461 7474  G).    if getatt",
            "-000030f0: 7228 636f 6d6d 6974 7465 722c 2022 656e  r(committer, \"en",
            "-00003100: 636f 6465 222c 204e 6f6e 6529 3a0a 2020  code\", None):.  ",
            "-00003110: 2020 2020 2020 636f 6d6d 6974 7465 7220        committer ",
            "-00003120: 3d20 636f 6d6d 6974 7465 722e 656e 636f  = committer.enco",
            "-00003130: 6465 2865 6e63 6f64 696e 6720 6f72 2044  de(encoding or D",
            "-00003140: 4546 4155 4c54 5f45 4e43 4f44 494e 4729  EFAULT_ENCODING)",
            "-00003150: 0a20 2020 206c 6f63 616c 5f74 696d 657a  .    local_timez",
            "-00003160: 6f6e 6520 3d20 6765 745f 7573 6572 5f74  one = get_user_t",
            "-00003170: 696d 657a 6f6e 6573 2829 0a20 2020 2069  imezones().    i",
            "-00003180: 6620 6175 7468 6f72 5f74 696d 657a 6f6e  f author_timezon",
            "-00003190: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     ",
            "-000031a0: 2020 2061 7574 686f 725f 7469 6d65 7a6f     author_timezo",
            "-000031b0: 6e65 203d 206c 6f63 616c 5f74 696d 657a  ne = local_timez",
            "-000031c0: 6f6e 655b 305d 0a20 2020 2069 6620 636f  one[0].    if co",
            "-000031d0: 6d6d 6974 5f74 696d 657a 6f6e 6520 6973  mmit_timezone is",
            "-000031e0: 204e 6f6e 653a 0a20 2020 2020 2020 2063   None:.        c",
            "-000031f0: 6f6d 6d69 745f 7469 6d65 7a6f 6e65 203d  ommit_timezone =",
            "-00003200: 206c 6f63 616c 5f74 696d 657a 6f6e 655b   local_timezone[",
            "-00003210: 315d 0a20 2020 2077 6974 6820 6f70 656e  1].    with open",
            "-00003220: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "-00003230: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "-00003240: 2020 7265 7475 726e 2072 2e64 6f5f 636f    return r.do_co",
            "-00003250: 6d6d 6974 280a 2020 2020 2020 2020 2020  mmit(.          ",
            "-00003260: 2020 6d65 7373 6167 653d 6d65 7373 6167    message=messag",
            "-00003270: 652c 0a20 2020 2020 2020 2020 2020 2061  e,.            a",
            "-00003280: 7574 686f 723d 6175 7468 6f72 2c0a 2020  uthor=author,.  ",
            "-00003290: 2020 2020 2020 2020 2020 6175 7468 6f72            author",
            "-000032a0: 5f74 696d 657a 6f6e 653d 6175 7468 6f72  _timezone=author",
            "-000032b0: 5f74 696d 657a 6f6e 652c 0a20 2020 2020  _timezone,.     ",
            "-000032c0: 2020 2020 2020 2063 6f6d 6d69 7474 6572         committer",
            "-000032d0: 3d63 6f6d 6d69 7474 6572 2c0a 2020 2020  =committer,.    ",
            "-000032e0: 2020 2020 2020 2020 636f 6d6d 6974 5f74          commit_t",
            "-000032f0: 696d 657a 6f6e 653d 636f 6d6d 6974 5f74  imezone=commit_t",
            "-00003300: 696d 657a 6f6e 652c 0a20 2020 2020 2020  imezone,.       ",
            "-00003310: 2020 2020 2065 6e63 6f64 696e 673d 656e       encoding=en",
            "-00003320: 636f 6469 6e67 2c0a 2020 2020 2020 2020  coding,.        ",
            "-00003330: 2020 2020 6e6f 5f76 6572 6966 793d 6e6f      no_verify=no",
            "-00003340: 5f76 6572 6966 792c 0a20 2020 2020 2020  _verify,.       ",
            "-00003350: 2020 2020 2073 6967 6e3d 7369 676e 6f66       sign=signof",
            "-00003360: 6620 6966 2069 7369 6e73 7461 6e63 6528  f if isinstance(",
            "-00003370: 7369 676e 6f66 662c 2028 7374 722c 2062  signoff, (str, b",
            "-00003380: 6f6f 6c29 2920 656c 7365 204e 6f6e 652c  ool)) else None,",
            "-00003390: 0a20 2020 2020 2020 2029 0a0a 0a64 6566  .        )...def",
            "-000033a0: 2063 6f6d 6d69 745f 7472 6565 2872 6570   commit_tree(rep",
            "-000033b0: 6f2c 2074 7265 652c 206d 6573 7361 6765  o, tree, message",
            "-000033c0: 3d4e 6f6e 652c 2061 7574 686f 723d 4e6f  =None, author=No",
            "-000033d0: 6e65 2c20 636f 6d6d 6974 7465 723d 4e6f  ne, committer=No",
            "-000033e0: 6e65 293a 0a20 2020 2022 2222 4372 6561  ne):.    \"\"\"Crea",
            "-000033f0: 7465 2061 206e 6577 2063 6f6d 6d69 7420  te a new commit ",
            "-00003400: 6f62 6a65 6374 2e0a 0a20 2020 2041 7267  object...    Arg",
            "-00003410: 733a 0a20 2020 2020 2072 6570 6f3a 2050  s:.      repo: P",
            "-00003420: 6174 6820 746f 2072 6570 6f73 6974 6f72  ath to repositor",
            "-00003430: 790a 2020 2020 2020 7472 6565 3a20 416e  y.      tree: An",
            "-00003440: 2065 7869 7374 696e 6720 7472 6565 206f   existing tree o",
            "-00003450: 626a 6563 740a 2020 2020 2020 6175 7468  bject.      auth",
            "-00003460: 6f72 3a20 4f70 7469 6f6e 616c 2061 7574  or: Optional aut",
            "-00003470: 686f 7220 6e61 6d65 2061 6e64 2065 6d61  hor name and ema",
            "-00003480: 696c 0a20 2020 2020 2063 6f6d 6d69 7474  il.      committ",
            "-00003490: 6572 3a20 4f70 7469 6f6e 616c 2063 6f6d  er: Optional com",
            "-000034a0: 6d69 7474 6572 206e 616d 6520 616e 6420  mitter name and ",
            "-000034b0: 656d 6169 6c0a 2020 2020 2222 220a 2020  email.    \"\"\".  ",
            "-000034c0: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "-000034d0: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "-000034e0: 7320 723a 0a20 2020 2020 2020 2072 6574  s r:.        ret",
            "-000034f0: 7572 6e20 722e 646f 5f63 6f6d 6d69 7428  urn r.do_commit(",
            "-00003500: 0a20 2020 2020 2020 2020 2020 206d 6573  .            mes",
            "-00003510: 7361 6765 3d6d 6573 7361 6765 2c20 7472  sage=message, tr",
            "-00003520: 6565 3d74 7265 652c 2063 6f6d 6d69 7474  ee=tree, committ",
            "-00003530: 6572 3d63 6f6d 6d69 7474 6572 2c20 6175  er=committer, au",
            "-00003540: 7468 6f72 3d61 7574 686f 720a 2020 2020  thor=author.    ",
            "-00003550: 2020 2020 290a 0a0a 6465 6620 696e 6974      )...def init",
            "-00003560: 2870 6174 683d 222e 222c 202a 2c20 6261  (path=\".\", *, ba",
            "-00003570: 7265 3d46 616c 7365 2c20 7379 6d6c 696e  re=False, symlin",
            "-00003580: 6b73 3a20 4f70 7469 6f6e 616c 5b62 6f6f  ks: Optional[boo",
            "-00003590: 6c5d 203d 204e 6f6e 6529 3a0a 2020 2020  l] = None):.    ",
            "-000035a0: 2222 2243 7265 6174 6520 6120 6e65 7720  \"\"\"Create a new ",
            "-000035b0: 6769 7420 7265 706f 7369 746f 7279 2e0a  git repository..",
            "-000035c0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "-000035d0: 2070 6174 683a 2050 6174 6820 746f 2072   path: Path to r",
            "-000035e0: 6570 6f73 6974 6f72 792e 0a20 2020 2020  epository..     ",
            "-000035f0: 2062 6172 653a 2057 6865 7468 6572 2074   bare: Whether t",
            "-00003600: 6f20 6372 6561 7465 2061 2062 6172 6520  o create a bare ",
            "-00003610: 7265 706f 7369 746f 7279 2e0a 2020 2020  repository..    ",
            "-00003620: 2020 7379 6d6c 696e 6b73 3a20 5768 6574    symlinks: Whet",
            "-00003630: 6865 7220 746f 2063 7265 6174 6520 6163  her to create ac",
            "-00003640: 7475 616c 2073 796d 6c69 6e6b 7320 2864  tual symlinks (d",
            "-00003650: 6566 6175 6c74 7320 746f 2061 7574 6f64  efaults to autod",
            "-00003660: 6574 6563 7429 0a20 2020 2052 6574 7572  etect).    Retur",
            "-00003670: 6e73 3a20 4120 5265 706f 2069 6e73 7461  ns: A Repo insta",
            "-00003680: 6e63 650a 2020 2020 2222 220a 2020 2020  nce.    \"\"\".    ",
            "-00003690: 6966 206e 6f74 206f 732e 7061 7468 2e65  if not os.path.e",
            "-000036a0: 7869 7374 7328 7061 7468 293a 0a20 2020  xists(path):.   ",
            "-000036b0: 2020 2020 206f 732e 6d6b 6469 7228 7061       os.mkdir(pa",
            "-000036c0: 7468 290a 0a20 2020 2069 6620 6261 7265  th)..    if bare",
            "-000036d0: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return",
            "-000036e0: 2052 6570 6f2e 696e 6974 5f62 6172 6528   Repo.init_bare(",
            "-000036f0: 7061 7468 290a 2020 2020 656c 7365 3a0a  path).    else:.",
            "-00003700: 2020 2020 2020 2020 7265 7475 726e 2052          return R",
            "-00003710: 6570 6f2e 696e 6974 2870 6174 682c 2073  epo.init(path, s",
            "-00003720: 796d 6c69 6e6b 733d 7379 6d6c 696e 6b73  ymlinks=symlinks",
            "-00003730: 290a 0a0a 6465 6620 636c 6f6e 6528 0a20  )...def clone(. ",
            "-00003740: 2020 2073 6f75 7263 652c 0a20 2020 2074     source,.    t",
            "-00003750: 6172 6765 743d 4e6f 6e65 2c0a 2020 2020  arget=None,.    ",
            "-00003760: 6261 7265 3d46 616c 7365 2c0a 2020 2020  bare=False,.    ",
            "-00003770: 6368 6563 6b6f 7574 3d4e 6f6e 652c 0a20  checkout=None,. ",
            "-00003780: 2020 2065 7272 7374 7265 616d 3d64 6566     errstream=def",
            "-00003790: 6175 6c74 5f62 7974 6573 5f65 7272 5f73  ault_bytes_err_s",
            "-000037a0: 7472 6561 6d2c 0a20 2020 206f 7574 7374  tream,.    outst",
            "-000037b0: 7265 616d 3d4e 6f6e 652c 0a20 2020 206f  ream=None,.    o",
            "-000037c0: 7269 6769 6e3a 204f 7074 696f 6e61 6c5b  rigin: Optional[",
            "-000037d0: 7374 725d 203d 2022 6f72 6967 696e 222c  str] = \"origin\",",
            "-000037e0: 0a20 2020 2064 6570 7468 3a20 4f70 7469  .    depth: Opti",
            "-000037f0: 6f6e 616c 5b69 6e74 5d20 3d20 4e6f 6e65  onal[int] = None",
            "-00003800: 2c0a 2020 2020 6272 616e 6368 3a20 4f70  ,.    branch: Op",
            "-00003810: 7469 6f6e 616c 5b55 6e69 6f6e 5b73 7472  tional[Union[str",
            "-00003820: 2c20 6279 7465 735d 5d20 3d20 4e6f 6e65  , bytes]] = None",
            "-00003830: 2c0a 2020 2020 636f 6e66 6967 3a20 4f70  ,.    config: Op",
            "-00003840: 7469 6f6e 616c 5b43 6f6e 6669 675d 203d  tional[Config] =",
            "-00003850: 204e 6f6e 652c 0a20 2020 2066 696c 7465   None,.    filte",
            "-00003860: 725f 7370 6563 3d4e 6f6e 652c 0a20 2020  r_spec=None,.   ",
            "-00003870: 2070 726f 746f 636f 6c5f 7665 7273 696f   protocol_versio",
            "-00003880: 6e3a 204f 7074 696f 6e61 6c5b 696e 745d  n: Optional[int]",
            "-00003890: 203d 204e 6f6e 652c 0a20 2020 202a 2a6b   = None,.    **k",
            "-000038a0: 7761 7267 732c 0a29 3a0a 2020 2020 2222  wargs,.):.    \"\"",
            "-000038b0: 2243 6c6f 6e65 2061 206c 6f63 616c 206f  \"Clone a local o",
            "-000038c0: 7220 7265 6d6f 7465 2067 6974 2072 6570  r remote git rep",
            "-000038d0: 6f73 6974 6f72 792e 0a0a 2020 2020 4172  ository...    Ar",
            "-000038e0: 6773 3a0a 2020 2020 2020 736f 7572 6365  gs:.      source",
            "-000038f0: 3a20 5061 7468 206f 7220 5552 4c20 666f  : Path or URL fo",
            "-00003900: 7220 736f 7572 6365 2072 6570 6f73 6974  r source reposit",
            "-00003910: 6f72 790a 2020 2020 2020 7461 7267 6574  ory.      target",
            "-00003920: 3a20 5061 7468 2074 6f20 7461 7267 6574  : Path to target",
            "-00003930: 2072 6570 6f73 6974 6f72 7920 286f 7074   repository (opt",
            "-00003940: 696f 6e61 6c29 0a20 2020 2020 2062 6172  ional).      bar",
            "-00003950: 653a 2057 6865 7468 6572 206f 7220 6e6f  e: Whether or no",
            "-00003960: 7420 746f 2063 7265 6174 6520 6120 6261  t to create a ba",
            "-00003970: 7265 2072 6570 6f73 6974 6f72 790a 2020  re repository.  ",
            "-00003980: 2020 2020 6368 6563 6b6f 7574 3a20 5768      checkout: Wh",
            "-00003990: 6574 6865 7220 6f72 206e 6f74 2074 6f20  ether or not to ",
            "-000039a0: 6368 6563 6b2d 6f75 7420 4845 4144 2061  check-out HEAD a",
            "-000039b0: 6674 6572 2063 6c6f 6e69 6e67 0a20 2020  fter cloning.   ",
            "-000039c0: 2020 2065 7272 7374 7265 616d 3a20 4f70     errstream: Op",
            "-000039d0: 7469 6f6e 616c 2073 7472 6561 6d20 746f  tional stream to",
            "-000039e0: 2077 7269 7465 2070 726f 6772 6573 7320   write progress ",
            "-000039f0: 746f 0a20 2020 2020 206f 7574 7374 7265  to.      outstre",
            "-00003a00: 616d 3a20 4f70 7469 6f6e 616c 2073 7472  am: Optional str",
            "-00003a10: 6561 6d20 746f 2077 7269 7465 2070 726f  eam to write pro",
            "-00003a20: 6772 6573 7320 746f 2028 6465 7072 6563  gress to (deprec",
            "-00003a30: 6174 6564 290a 2020 2020 2020 6f72 6967  ated).      orig",
            "-00003a40: 696e 3a20 4e61 6d65 206f 6620 7265 6d6f  in: Name of remo",
            "-00003a50: 7465 2066 726f 6d20 7468 6520 7265 706f  te from the repo",
            "-00003a60: 7369 746f 7279 2075 7365 6420 746f 2063  sitory used to c",
            "-00003a70: 6c6f 6e65 0a20 2020 2020 2064 6570 7468  lone.      depth",
            "-00003a80: 3a20 4465 7074 6820 746f 2066 6574 6368  : Depth to fetch",
            "-00003a90: 2061 740a 2020 2020 2020 6272 616e 6368   at.      branch",
            "-00003aa0: 3a20 4f70 7469 6f6e 616c 2062 7261 6e63  : Optional branc",
            "-00003ab0: 6820 6f72 2074 6167 2074 6f20 6265 2075  h or tag to be u",
            "-00003ac0: 7365 6420 6173 2048 4541 4420 696e 2074  sed as HEAD in t",
            "-00003ad0: 6865 206e 6577 2072 6570 6f73 6974 6f72  he new repositor",
            "-00003ae0: 790a 2020 2020 2020 2020 696e 7374 6561  y.        instea",
            "-00003af0: 6420 6f66 2074 6865 2063 6c6f 6e65 6420  d of the cloned ",
            "-00003b00: 7265 706f 7369 746f 7279 2773 2048 4541  repository's HEA",
            "-00003b10: 442e 0a20 2020 2020 2063 6f6e 6669 673a  D..      config:",
            "-00003b20: 2043 6f6e 6669 6775 7261 7469 6f6e 2074   Configuration t",
            "-00003b30: 6f20 7573 650a 2020 2020 2020 7265 6673  o use.      refs",
            "-00003b40: 7065 6373 3a20 7265 6673 7065 6373 2074  pecs: refspecs t",
            "-00003b50: 6f20 6665 7463 682e 2043 616e 2062 6520  o fetch. Can be ",
            "-00003b60: 6120 6279 7465 7374 7269 6e67 2c20 6120  a bytestring, a ",
            "-00003b70: 7374 7269 6e67 2c20 6f72 2061 206c 6973  string, or a lis",
            "-00003b80: 7420 6f66 0a20 2020 2020 2020 2062 7974  t of.        byt",
            "-00003b90: 6573 7472 696e 672f 7374 7269 6e67 2e0a  estring/string..",
            "-00003ba0: 2020 2020 2020 6669 6c74 6572 5f73 7065        filter_spe",
            "-00003bb0: 633a 2041 2067 6974 2d72 6576 2d6c 6973  c: A git-rev-lis",
            "-00003bc0: 742d 7374 796c 6520 6f62 6a65 6374 2066  t-style object f",
            "-00003bd0: 696c 7465 7220 7370 6563 2c20 6173 2061  ilter spec, as a",
            "-00003be0: 6e20 4153 4349 4920 7374 7269 6e67 2e0a  n ASCII string..",
            "-00003bf0: 2020 2020 2020 2020 4f6e 6c79 2075 7365          Only use",
            "-00003c00: 6420 6966 2074 6865 2073 6572 7665 7220  d if the server ",
            "-00003c10: 7375 7070 6f72 7473 2074 6865 2047 6974  supports the Git",
            "-00003c20: 2070 726f 746f 636f 6c2d 7632 2027 6669   protocol-v2 'fi",
            "-00003c30: 6c74 6572 270a 2020 2020 2020 2020 6665  lter'.        fe",
            "-00003c40: 6174 7572 652c 2061 6e64 2069 676e 6f72  ature, and ignor",
            "-00003c50: 6564 206f 7468 6572 7769 7365 2e0a 2020  ed otherwise..  ",
            "-00003c60: 2020 2020 7072 6f74 6f63 6f6c 5f76 6572      protocol_ver",
            "-00003c70: 7369 6f6e 3a20 6465 7369 7265 6420 4769  sion: desired Gi",
            "-00003c80: 7420 7072 6f74 6f63 6f6c 2076 6572 7369  t protocol versi",
            "-00003c90: 6f6e 2e20 4279 2064 6566 6175 6c74 2074  on. By default t",
            "-00003ca0: 6865 2068 6967 6865 7374 0a20 2020 2020  he highest.     ",
            "-00003cb0: 2020 206d 7574 7561 6c6c 7920 7375 7070     mutually supp",
            "-00003cc0: 6f72 7465 6420 7072 6f74 6f63 6f6c 2076  orted protocol v",
            "-00003cd0: 6572 7369 6f6e 2077 696c 6c20 6265 2075  ersion will be u",
            "-00003ce0: 7365 642e 0a20 2020 2052 6574 7572 6e73  sed..    Returns",
            "-00003cf0: 3a20 5468 6520 6e65 7720 7265 706f 7369  : The new reposi",
            "-00003d00: 746f 7279 0a20 2020 2022 2222 0a20 2020  tory.    \"\"\".   ",
            "-00003d10: 2069 6620 6f75 7473 7472 6561 6d20 6973   if outstream is",
            "-00003d20: 206e 6f74 204e 6f6e 653a 0a20 2020 2020   not None:.     ",
            "-00003d30: 2020 2069 6d70 6f72 7420 7761 726e 696e     import warnin",
            "-00003d40: 6773 0a0a 2020 2020 2020 2020 7761 726e  gs..        warn",
            "-00003d50: 696e 6773 2e77 6172 6e28 0a20 2020 2020  ings.warn(.     ",
            "-00003d60: 2020 2020 2020 2022 6f75 7473 7472 6561         \"outstrea",
            "-00003d70: 6d3d 2068 6173 2062 6565 6e20 6465 7072  m= has been depr",
            "-00003d80: 6563 6174 6564 2069 6e20 6661 766f 7572  ecated in favour",
            "-00003d90: 206f 6620 6572 7273 7472 6561 6d3d 2e22   of errstream=.\"",
            "-00003da0: 2c0a 2020 2020 2020 2020 2020 2020 4465  ,.            De",
            "-00003db0: 7072 6563 6174 696f 6e57 6172 6e69 6e67  precationWarning",
            "-00003dc0: 2c0a 2020 2020 2020 2020 2020 2020 7374  ,.            st",
            "-00003dd0: 6163 6b6c 6576 656c 3d33 2c0a 2020 2020  acklevel=3,.    ",
            "-00003de0: 2020 2020 290a 2020 2020 2020 2020 2320      ).        # ",
            "-00003df0: 544f 444f 286a 656c 6d65 7229 3a20 4361  TODO(jelmer): Ca",
            "-00003e00: 7074 7572 6520 6c6f 6767 696e 6720 6f75  pture logging ou",
            "-00003e10: 7470 7574 2061 6e64 2073 7472 6561 6d20  tput and stream ",
            "-00003e20: 746f 2065 7272 7374 7265 616d 0a0a 2020  to errstream..  ",
            "-00003e30: 2020 6966 2063 6f6e 6669 6720 6973 204e    if config is N",
            "-00003e40: 6f6e 653a 0a20 2020 2020 2020 2063 6f6e  one:.        con",
            "-00003e50: 6669 6720 3d20 5374 6163 6b65 6443 6f6e  fig = StackedCon",
            "-00003e60: 6669 672e 6465 6661 756c 7428 290a 0a20  fig.default().. ",
            "-00003e70: 2020 2069 6620 6368 6563 6b6f 7574 2069     if checkout i",
            "-00003e80: 7320 4e6f 6e65 3a0a 2020 2020 2020 2020  s None:.        ",
            "-00003e90: 6368 6563 6b6f 7574 203d 206e 6f74 2062  checkout = not b",
            "-00003ea0: 6172 650a 2020 2020 6966 2063 6865 636b  are.    if check",
            "-00003eb0: 6f75 7420 616e 6420 6261 7265 3a0a 2020  out and bare:.  ",
            "-00003ec0: 2020 2020 2020 7261 6973 6520 4572 726f        raise Erro",
            "-00003ed0: 7228 2263 6865 636b 6f75 7420 616e 6420  r(\"checkout and ",
            "-00003ee0: 6261 7265 2061 7265 2069 6e63 6f6d 7061  bare are incompa",
            "-00003ef0: 7469 626c 6522 290a 0a20 2020 2069 6620  tible\")..    if ",
            "-00003f00: 7461 7267 6574 2069 7320 4e6f 6e65 3a0a  target is None:.",
            "-00003f10: 2020 2020 2020 2020 7461 7267 6574 203d          target =",
            "-00003f20: 2073 6f75 7263 652e 7370 6c69 7428 222f   source.split(\"/",
            "-00003f30: 2229 5b2d 315d 0a0a 2020 2020 6966 2069  \")[-1]..    if i",
            "-00003f40: 7369 6e73 7461 6e63 6528 6272 616e 6368  sinstance(branch",
            "-00003f50: 2c20 7374 7229 3a0a 2020 2020 2020 2020  , str):.        ",
            "-00003f60: 6272 616e 6368 203d 2062 7261 6e63 682e  branch = branch.",
            "-00003f70: 656e 636f 6465 2844 4546 4155 4c54 5f45  encode(DEFAULT_E",
            "-00003f80: 4e43 4f44 494e 4729 0a0a 2020 2020 6d6b  NCODING)..    mk",
            "-00003f90: 6469 7220 3d20 6e6f 7420 6f73 2e70 6174  dir = not os.pat",
            "-00003fa0: 682e 6578 6973 7473 2874 6172 6765 7429  h.exists(target)",
            "-00003fb0: 0a0a 2020 2020 2863 6c69 656e 742c 2070  ..    (client, p",
            "-00003fc0: 6174 6829 203d 2067 6574 5f74 7261 6e73  ath) = get_trans",
            "-00003fd0: 706f 7274 5f61 6e64 5f70 6174 6828 736f  port_and_path(so",
            "-00003fe0: 7572 6365 2c20 636f 6e66 6967 3d63 6f6e  urce, config=con",
            "-00003ff0: 6669 672c 202a 2a6b 7761 7267 7329 0a0a  fig, **kwargs)..",
            "-00004000: 2020 2020 6966 2066 696c 7465 725f 7370      if filter_sp",
            "-00004010: 6563 3a0a 2020 2020 2020 2020 6669 6c74  ec:.        filt",
            "-00004020: 6572 5f73 7065 6320 3d20 6669 6c74 6572  er_spec = filter",
            "-00004030: 5f73 7065 632e 656e 636f 6465 2822 6173  _spec.encode(\"as",
            "-00004040: 6369 6922 290a 0a20 2020 2072 6574 7572  cii\")..    retur",
            "-00004050: 6e20 636c 6965 6e74 2e63 6c6f 6e65 280a  n client.clone(.",
            "-00004060: 2020 2020 2020 2020 7061 7468 2c0a 2020          path,.  ",
            "-00004070: 2020 2020 2020 7461 7267 6574 2c0a 2020        target,.  ",
            "-00004080: 2020 2020 2020 6d6b 6469 723d 6d6b 6469        mkdir=mkdi",
            "-00004090: 722c 0a20 2020 2020 2020 2062 6172 653d  r,.        bare=",
            "-000040a0: 6261 7265 2c0a 2020 2020 2020 2020 6f72  bare,.        or",
            "-000040b0: 6967 696e 3d6f 7269 6769 6e2c 0a20 2020  igin=origin,.   ",
            "-000040c0: 2020 2020 2063 6865 636b 6f75 743d 6368       checkout=ch",
            "-000040d0: 6563 6b6f 7574 2c0a 2020 2020 2020 2020  eckout,.        ",
            "-000040e0: 6272 616e 6368 3d62 7261 6e63 682c 0a20  branch=branch,. ",
            "-000040f0: 2020 2020 2020 2070 726f 6772 6573 733d         progress=",
            "-00004100: 6572 7273 7472 6561 6d2e 7772 6974 652c  errstream.write,",
            "-00004110: 0a20 2020 2020 2020 2064 6570 7468 3d64  .        depth=d",
            "-00004120: 6570 7468 2c0a 2020 2020 2020 2020 6669  epth,.        fi",
            "-00004130: 6c74 6572 5f73 7065 633d 6669 6c74 6572  lter_spec=filter",
            "-00004140: 5f73 7065 632c 0a20 2020 2020 2020 2070  _spec,.        p",
            "-00004150: 726f 746f 636f 6c5f 7665 7273 696f 6e3d  rotocol_version=",
            "-00004160: 7072 6f74 6f63 6f6c 5f76 6572 7369 6f6e  protocol_version",
            "-00004170: 2c0a 2020 2020 290a 0a0a 6465 6620 6164  ,.    )...def ad",
            "-00004180: 6428 7265 706f 3d22 2e22 2c20 7061 7468  d(repo=\".\", path",
            "-00004190: 733d 4e6f 6e65 293a 0a20 2020 2022 2222  s=None):.    \"\"\"",
            "-000041a0: 4164 6420 6669 6c65 7320 746f 2074 6865  Add files to the",
            "-000041b0: 2073 7461 6769 6e67 2061 7265 612e 0a0a   staging area...",
            "-000041c0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "-000041d0: 7265 706f 3a20 5265 706f 7369 746f 7279  repo: Repository",
            "-000041e0: 2066 6f72 2074 6865 2066 696c 6573 0a20   for the files. ",
            "-000041f0: 2020 2020 2070 6174 6873 3a20 5061 7468       paths: Path",
            "-00004200: 7320 746f 2061 6464 2e20 204e 6f20 7661  s to add.  No va",
            "-00004210: 6c75 6520 7061 7373 6564 2073 7461 6765  lue passed stage",
            "-00004220: 7320 616c 6c20 6d6f 6469 6669 6564 2066  s all modified f",
            "-00004230: 696c 6573 2e0a 2020 2020 5265 7475 726e  iles..    Return",
            "-00004240: 733a 2054 7570 6c65 2077 6974 6820 7365  s: Tuple with se",
            "-00004250: 7420 6f66 2061 6464 6564 2066 696c 6573  t of added files",
            "-00004260: 2061 6e64 2069 676e 6f72 6564 2066 696c   and ignored fil",
            "-00004270: 6573 0a0a 2020 2020 4966 2074 6865 2072  es..    If the r",
            "-00004280: 6570 6f73 6974 6f72 7920 636f 6e74 6169  epository contai",
            "-00004290: 6e73 2069 676e 6f72 6564 2064 6972 6563  ns ignored direc",
            "-000042a0: 746f 7269 6573 2c20 7468 6520 7265 7475  tories, the retu",
            "-000042b0: 726e 6564 2073 6574 2077 696c 6c0a 2020  rned set will.  ",
            "-000042c0: 2020 636f 6e74 6169 6e20 7468 6520 7061    contain the pa",
            "-000042d0: 7468 2074 6f20 616e 2069 676e 6f72 6564  th to an ignored",
            "-000042e0: 2064 6972 6563 746f 7279 2028 7769 7468   directory (with",
            "-000042f0: 2074 7261 696c 696e 6720 736c 6173 6829   trailing slash)",
            "-00004300: 2e20 496e 6469 7669 6475 616c 0a20 2020  . Individual.   ",
            "-00004310: 2066 696c 6573 2077 6974 6869 6e20 6967   files within ig",
            "-00004320: 6e6f 7265 6420 6469 7265 6374 6f72 6965  nored directorie",
            "-00004330: 7320 7769 6c6c 206e 6f74 2062 6520 7265  s will not be re",
            "-00004340: 7475 726e 6564 2e0a 2020 2020 2222 220a  turned..    \"\"\".",
            "-00004350: 2020 2020 6967 6e6f 7265 6420 3d20 7365      ignored = se",
            "-00004360: 7428 290a 2020 2020 7769 7468 206f 7065  t().    with ope",
            "-00004370: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "-00004380: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "-00004390: 2020 2072 6570 6f5f 7061 7468 203d 2050     repo_path = P",
            "-000043a0: 6174 6828 722e 7061 7468 292e 7265 736f  ath(r.path).reso",
            "-000043b0: 6c76 6528 290a 2020 2020 2020 2020 6967  lve().        ig",
            "-000043c0: 6e6f 7265 5f6d 616e 6167 6572 203d 2049  nore_manager = I",
            "-000043d0: 676e 6f72 6546 696c 7465 724d 616e 6167  gnoreFilterManag",
            "-000043e0: 6572 2e66 726f 6d5f 7265 706f 2872 290a  er.from_repo(r).",
            "-000043f0: 2020 2020 2020 2020 6966 206e 6f74 2070          if not p",
            "-00004400: 6174 6873 3a0a 2020 2020 2020 2020 2020  aths:.          ",
            "-00004410: 2020 7061 7468 7320 3d20 6c69 7374 280a    paths = list(.",
            "-00004420: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00004430: 6765 745f 756e 7472 6163 6b65 645f 7061  get_untracked_pa",
            "-00004440: 7468 7328 0a20 2020 2020 2020 2020 2020  ths(.           ",
            "-00004450: 2020 2020 2020 2020 2073 7472 2850 6174           str(Pat",
            "-00004460: 6828 6f73 2e67 6574 6377 6428 2929 2e72  h(os.getcwd()).r",
            "-00004470: 6573 6f6c 7665 2829 292c 0a20 2020 2020  esolve()),.     ",
            "-00004480: 2020 2020 2020 2020 2020 2020 2020 2073                 s",
            "-00004490: 7472 2872 6570 6f5f 7061 7468 292c 0a20  tr(repo_path),. ",
            "-000044a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000044b0: 2020 2072 2e6f 7065 6e5f 696e 6465 7828     r.open_index(",
            "-000044c0: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             ",
            "-000044d0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           ",
            "-000044e0: 2029 0a20 2020 2020 2020 2072 656c 7061   ).        relpa",
            "-000044f0: 7468 7320 3d20 5b5d 0a20 2020 2020 2020  ths = [].       ",
            "-00004500: 2069 6620 6e6f 7420 6973 696e 7374 616e   if not isinstan",
            "-00004510: 6365 2870 6174 6873 2c20 6c69 7374 293a  ce(paths, list):",
            "-00004520: 0a20 2020 2020 2020 2020 2020 2070 6174  .            pat",
            "-00004530: 6873 203d 205b 7061 7468 735d 0a20 2020  hs = [paths].   ",
            "-00004540: 2020 2020 2066 6f72 2070 2069 6e20 7061       for p in pa",
            "-00004550: 7468 733a 0a20 2020 2020 2020 2020 2020  ths:.           ",
            "-00004560: 2070 6174 6820 3d20 5061 7468 2870 290a   path = Path(p).",
            "-00004570: 2020 2020 2020 2020 2020 2020 7265 6c70              relp",
            "-00004580: 6174 6820 3d20 7374 7228 7061 7468 2e72  ath = str(path.r",
            "-00004590: 6573 6f6c 7665 2829 2e72 656c 6174 6976  esolve().relativ",
            "-000045a0: 655f 746f 2872 6570 6f5f 7061 7468 2929  e_to(repo_path))",
            "-000045b0: 0a20 2020 2020 2020 2020 2020 2023 2046  .            # F",
            "-000045c0: 4958 4d45 3a20 5375 7070 6f72 7420 7061  IXME: Support pa",
            "-000045d0: 7474 6572 6e73 0a20 2020 2020 2020 2020  tterns.         ",
            "-000045e0: 2020 2069 6620 7061 7468 2e69 735f 6469     if path.is_di",
            "-000045f0: 7228 293a 0a20 2020 2020 2020 2020 2020  r():.           ",
            "-00004600: 2020 2020 2072 656c 7061 7468 203d 206f       relpath = o",
            "-00004610: 732e 7061 7468 2e6a 6f69 6e28 7265 6c70  s.path.join(relp",
            "-00004620: 6174 682c 2022 2229 0a20 2020 2020 2020  ath, \"\").       ",
            "-00004630: 2020 2020 2069 6620 6967 6e6f 7265 5f6d       if ignore_m",
            "-00004640: 616e 6167 6572 2e69 735f 6967 6e6f 7265  anager.is_ignore",
            "-00004650: 6428 7265 6c70 6174 6829 3a0a 2020 2020  d(relpath):.    ",
            "-00004660: 2020 2020 2020 2020 2020 2020 6967 6e6f              igno",
            "-00004670: 7265 642e 6164 6428 7265 6c70 6174 6829  red.add(relpath)",
            "-00004680: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "-00004690: 2063 6f6e 7469 6e75 650a 2020 2020 2020   continue.      ",
            "-000046a0: 2020 2020 2020 7265 6c70 6174 6873 2e61        relpaths.a",
            "-000046b0: 7070 656e 6428 7265 6c70 6174 6829 0a20  ppend(relpath). ",
            "-000046c0: 2020 2020 2020 2072 2e73 7461 6765 2872         r.stage(r",
            "-000046d0: 656c 7061 7468 7329 0a20 2020 2072 6574  elpaths).    ret",
            "-000046e0: 7572 6e20 2872 656c 7061 7468 732c 2069  urn (relpaths, i",
            "-000046f0: 676e 6f72 6564 290a 0a0a 6465 6620 5f69  gnored)...def _i",
            "-00004700: 735f 7375 6264 6972 2873 7562 6469 722c  s_subdir(subdir,",
            "-00004710: 2070 6172 656e 7464 6972 293a 0a20 2020   parentdir):.   ",
            "-00004720: 2022 2222 4368 6563 6b20 7768 6574 6865   \"\"\"Check whethe",
            "-00004730: 7220 7375 6264 6972 2069 7320 7061 7265  r subdir is pare",
            "-00004740: 6e74 6469 7220 6f72 2061 2073 7562 6469  ntdir or a subdi",
            "-00004750: 7220 6f66 2070 6172 656e 7464 6972 2e0a  r of parentdir..",
            "-00004760: 0a20 2020 2049 6620 7061 7265 6e74 6469  .    If parentdi",
            "-00004770: 7220 6f72 2073 7562 6469 7220 6973 2061  r or subdir is a",
            "-00004780: 2072 656c 6174 6976 6520 7061 7468 2c20   relative path, ",
            "-00004790: 6974 2077 696c 6c20 6265 2064 6973 616d  it will be disam",
            "-000047a0: 6769 6275 6174 6564 0a20 2020 2072 656c  gibuated.    rel",
            "-000047b0: 6174 6976 6520 746f 2074 6865 2070 7764  ative to the pwd",
            "-000047c0: 2e0a 2020 2020 2222 220a 2020 2020 7061  ..    \"\"\".    pa",
            "-000047d0: 7265 6e74 6469 725f 6162 7320 3d20 6f73  rentdir_abs = os",
            "-000047e0: 2e70 6174 682e 7265 616c 7061 7468 2870  .path.realpath(p",
            "-000047f0: 6172 656e 7464 6972 2920 2b20 6f73 2e70  arentdir) + os.p",
            "-00004800: 6174 682e 7365 700a 2020 2020 7375 6264  ath.sep.    subd",
            "-00004810: 6972 5f61 6273 203d 206f 732e 7061 7468  ir_abs = os.path",
            "-00004820: 2e72 6561 6c70 6174 6828 7375 6264 6972  .realpath(subdir",
            "-00004830: 2920 2b20 6f73 2e70 6174 682e 7365 700a  ) + os.path.sep.",
            "-00004840: 2020 2020 7265 7475 726e 2073 7562 6469      return subdi",
            "-00004850: 725f 6162 732e 7374 6172 7473 7769 7468  r_abs.startswith",
            "-00004860: 2870 6172 656e 7464 6972 5f61 6273 290a  (parentdir_abs).",
            "-00004870: 0a0a 2320 544f 444f 3a20 6f70 7469 6f6e  ..# TODO: option",
            "-00004880: 2074 6f20 7265 6d6f 7665 2069 676e 6f72   to remove ignor",
            "-00004890: 6564 2066 696c 6573 2061 6c73 6f2c 2069  ed files also, i",
            "-000048a0: 6e20 6c69 6e65 2077 6974 6820 6067 6974  n line with `git",
            "-000048b0: 2063 6c65 616e 202d 6664 7860 0a64 6566   clean -fdx`.def",
            "-000048c0: 2063 6c65 616e 2872 6570 6f3d 222e 222c   clean(repo=\".\",",
            "-000048d0: 2074 6172 6765 745f 6469 723d 4e6f 6e65   target_dir=None",
            "-000048e0: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    \"",
            "-000048f0: 2222 5265 6d6f 7665 2061 6e79 2075 6e74  \"\"Remove any unt",
            "-00004900: 7261 636b 6564 2066 696c 6573 2066 726f  racked files fro",
            "-00004910: 6d20 7468 6520 7461 7267 6574 2064 6972  m the target dir",
            "-00004920: 6563 746f 7279 2072 6563 7572 7369 7665  ectory recursive",
            "-00004930: 6c79 2e0a 0a20 2020 2045 7175 6976 616c  ly...    Equival",
            "-00004940: 656e 7420 746f 2072 756e 6e69 6e67 2060  ent to running `",
            "-00004950: 6067 6974 2063 6c65 616e 202d 6664 6060  `git clean -fd``",
            "-00004960: 2069 6e20 7461 7267 6574 5f64 6972 2e0a   in target_dir..",
            "-00004970: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "-00004980: 2072 6570 6f3a 2052 6570 6f73 6974 6f72   repo: Repositor",
            "-00004990: 7920 7768 6572 6520 7468 6520 6669 6c65  y where the file",
            "-000049a0: 7320 6d61 7920 6265 2074 7261 636b 6564  s may be tracked",
            "-000049b0: 0a20 2020 2020 2074 6172 6765 745f 6469  .      target_di",
            "-000049c0: 723a 2044 6972 6563 746f 7279 2074 6f20  r: Directory to ",
            "-000049d0: 636c 6561 6e20 2d20 6375 7272 656e 7420  clean - current ",
            "-000049e0: 6469 7265 6374 6f72 7920 6966 204e 6f6e  directory if Non",
            "-000049f0: 650a 2020 2020 2222 220a 2020 2020 6966  e.    \"\"\".    if",
            "-00004a00: 2074 6172 6765 745f 6469 7220 6973 204e   target_dir is N",
            "-00004a10: 6f6e 653a 0a20 2020 2020 2020 2074 6172  one:.        tar",
            "-00004a20: 6765 745f 6469 7220 3d20 6f73 2e67 6574  get_dir = os.get",
            "-00004a30: 6377 6428 290a 0a20 2020 2077 6974 6820  cwd()..    with ",
            "-00004a40: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "-00004a50: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "-00004a60: 2020 2020 2020 6966 206e 6f74 205f 6973        if not _is",
            "-00004a70: 5f73 7562 6469 7228 7461 7267 6574 5f64  _subdir(target_d",
            "-00004a80: 6972 2c20 722e 7061 7468 293a 0a20 2020  ir, r.path):.   ",
            "-00004a90: 2020 2020 2020 2020 2072 6169 7365 2045           raise E",
            "-00004aa0: 7272 6f72 2822 7461 7267 6574 5f64 6972  rror(\"target_dir",
            "-00004ab0: 206d 7573 7420 6265 2069 6e20 7468 6520   must be in the ",
            "-00004ac0: 7265 706f 2773 2077 6f72 6b69 6e67 2064  repo's working d",
            "-00004ad0: 6972 2229 0a0a 2020 2020 2020 2020 636f  ir\")..        co",
            "-00004ae0: 6e66 6967 203d 2072 2e67 6574 5f63 6f6e  nfig = r.get_con",
            "-00004af0: 6669 675f 7374 6163 6b28 290a 2020 2020  fig_stack().    ",
            "-00004b00: 2020 2020 636f 6e66 6967 2e67 6574 5f62      config.get_b",
            "-00004b10: 6f6f 6c65 616e 2828 6222 636c 6561 6e22  oolean((b\"clean\"",
            "-00004b20: 2c29 2c20 6222 7265 7175 6972 6546 6f72  ,), b\"requireFor",
            "-00004b30: 6365 222c 2054 7275 6529 0a0a 2020 2020  ce\", True)..    ",
            "-00004b40: 2020 2020 2320 544f 444f 286a 656c 6d65      # TODO(jelme",
            "-00004b50: 7229 3a20 6966 2072 6571 7569 7265 5f66  r): if require_f",
            "-00004b60: 6f72 6365 2069 7320 7365 742c 2074 6865  orce is set, the",
            "-00004b70: 6e20 6d61 6b65 2073 7572 6520 7468 6174  n make sure that",
            "-00004b80: 202d 662c 202d 6920 6f72 0a20 2020 2020   -f, -i or.     ",
            "-00004b90: 2020 2023 202d 6e20 6973 2073 7065 6369     # -n is speci",
            "-00004ba0: 6669 6564 2e0a 0a20 2020 2020 2020 2069  fied...        i",
            "-00004bb0: 6e64 6578 203d 2072 2e6f 7065 6e5f 696e  ndex = r.open_in",
            "-00004bc0: 6465 7828 290a 2020 2020 2020 2020 6967  dex().        ig",
            "-00004bd0: 6e6f 7265 5f6d 616e 6167 6572 203d 2049  nore_manager = I",
            "-00004be0: 676e 6f72 6546 696c 7465 724d 616e 6167  gnoreFilterManag",
            "-00004bf0: 6572 2e66 726f 6d5f 7265 706f 2872 290a  er.from_repo(r).",
            "-00004c00: 0a20 2020 2020 2020 2070 6174 6873 5f69  .        paths_i",
            "-00004c10: 6e5f 7764 203d 205f 7761 6c6b 5f77 6f72  n_wd = _walk_wor",
            "-00004c20: 6b69 6e67 5f64 6972 5f70 6174 6873 2874  king_dir_paths(t",
            "-00004c30: 6172 6765 745f 6469 722c 2072 2e70 6174  arget_dir, r.pat",
            "-00004c40: 6829 0a20 2020 2020 2020 2023 2052 6576  h).        # Rev",
            "-00004c50: 6572 7365 2066 696c 6520 7669 7369 7420  erse file visit ",
            "-00004c60: 6f72 6465 722c 2073 6f20 7468 6174 2066  order, so that f",
            "-00004c70: 696c 6573 2061 6e64 2073 7562 6469 7265  iles and subdire",
            "-00004c80: 6374 6f72 6965 7320 6172 650a 2020 2020  ctories are.    ",
            "-00004c90: 2020 2020 2320 7265 6d6f 7665 6420 6265      # removed be",
            "-00004ca0: 666f 7265 2063 6f6e 7461 696e 696e 6720  fore containing ",
            "-00004cb0: 6469 7265 6374 6f72 790a 2020 2020 2020  directory.      ",
            "-00004cc0: 2020 666f 7220 6170 2c20 6973 5f64 6972    for ap, is_dir",
            "-00004cd0: 2069 6e20 7265 7665 7273 6564 286c 6973   in reversed(lis",
            "-00004ce0: 7428 7061 7468 735f 696e 5f77 6429 293a  t(paths_in_wd)):",
            "-00004cf0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if ",
            "-00004d00: 6973 5f64 6972 3a0a 2020 2020 2020 2020  is_dir:.        ",
            "-00004d10: 2020 2020 2020 2020 2320 416c 6c20 7375          # All su",
            "-00004d20: 6264 6972 6563 746f 7269 6573 2061 6e64  bdirectories and",
            "-00004d30: 2066 696c 6573 2068 6176 6520 6265 656e   files have been",
            "-00004d40: 2072 656d 6f76 6564 2069 6620 756e 7472   removed if untr",
            "-00004d50: 6163 6b65 642c 0a20 2020 2020 2020 2020  acked,.         ",
            "-00004d60: 2020 2020 2020 2023 2073 6f20 6469 7220         # so dir ",
            "-00004d70: 636f 6e74 6169 6e73 206e 6f20 7472 6163  contains no trac",
            "-00004d80: 6b65 6420 6669 6c65 7320 6966 6620 6974  ked files iff it",
            "-00004d90: 2069 7320 656d 7074 792e 0a20 2020 2020   is empty..     ",
            "-00004da0: 2020 2020 2020 2020 2020 2069 735f 656d             is_em",
            "-00004db0: 7074 7920 3d20 6c65 6e28 6f73 2e6c 6973  pty = len(os.lis",
            "-00004dc0: 7464 6972 2861 7029 2920 3d3d 2030 0a20  tdir(ap)) == 0. ",
            "-00004dd0: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "-00004de0: 6620 6973 5f65 6d70 7479 3a0a 2020 2020  f is_empty:.    ",
            "-00004df0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00004e00: 6f73 2e72 6d64 6972 2861 7029 0a20 2020  os.rmdir(ap).   ",
            "-00004e10: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. ",
            "-00004e20: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "-00004e30: 7020 3d20 7061 7468 5f74 6f5f 7472 6565  p = path_to_tree",
            "-00004e40: 5f70 6174 6828 722e 7061 7468 2c20 6170  _path(r.path, ap",
            "-00004e50: 290a 2020 2020 2020 2020 2020 2020 2020  ).              ",
            "-00004e60: 2020 6973 5f74 7261 636b 6564 203d 2069    is_tracked = i",
            "-00004e70: 7020 696e 2069 6e64 6578 0a0a 2020 2020  p in index..    ",
            "-00004e80: 2020 2020 2020 2020 2020 2020 7270 203d              rp =",
            "-00004e90: 206f 732e 7061 7468 2e72 656c 7061 7468   os.path.relpath",
            "-00004ea0: 2861 702c 2072 2e70 6174 6829 0a20 2020  (ap, r.path).   ",
            "-00004eb0: 2020 2020 2020 2020 2020 2020 2069 735f               is_",
            "-00004ec0: 6967 6e6f 7265 6420 3d20 6967 6e6f 7265  ignored = ignore",
            "-00004ed0: 5f6d 616e 6167 6572 2e69 735f 6967 6e6f  _manager.is_igno",
            "-00004ee0: 7265 6428 7270 290a 0a20 2020 2020 2020  red(rp)..       ",
            "-00004ef0: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not ",
            "-00004f00: 6973 5f74 7261 636b 6564 2061 6e64 206e  is_tracked and n",
            "-00004f10: 6f74 2069 735f 6967 6e6f 7265 643a 0a20  ot is_ignored:. ",
            "-00004f20: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00004f30: 2020 206f 732e 7265 6d6f 7665 2861 7029     os.remove(ap)",
            "-00004f40: 0a0a 0a64 6566 2072 656d 6f76 6528 7265  ...def remove(re",
            "-00004f50: 706f 3d22 2e22 2c20 7061 7468 733d 4e6f  po=\".\", paths=No",
            "-00004f60: 6e65 2c20 6361 6368 6564 3d46 616c 7365  ne, cached=False",
            "-00004f70: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    \"",
            "-00004f80: 2222 5265 6d6f 7665 2066 696c 6573 2066  \"\"Remove files f",
            "-00004f90: 726f 6d20 7468 6520 7374 6167 696e 6720  rom the staging ",
            "-00004fa0: 6172 6561 2e0a 0a20 2020 2041 7267 733a  area...    Args:",
            "-00004fb0: 0a20 2020 2020 2072 6570 6f3a 2052 6570  .      repo: Rep",
            "-00004fc0: 6f73 6974 6f72 7920 666f 7220 7468 6520  ository for the ",
            "-00004fd0: 6669 6c65 730a 2020 2020 2020 7061 7468  files.      path",
            "-00004fe0: 733a 2050 6174 6873 2074 6f20 7265 6d6f  s: Paths to remo",
            "-00004ff0: 7665 0a20 2020 2022 2222 0a20 2020 2077  ve.    \"\"\".    w",
            "-00005000: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "-00005010: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "-00005020: 3a0a 2020 2020 2020 2020 696e 6465 7820  :.        index ",
            "-00005030: 3d20 722e 6f70 656e 5f69 6e64 6578 2829  = r.open_index()",
            "-00005040: 0a20 2020 2020 2020 2066 6f72 2070 2069  .        for p i",
            "-00005050: 6e20 7061 7468 733a 0a20 2020 2020 2020  n paths:.       ",
            "-00005060: 2020 2020 2066 756c 6c5f 7061 7468 203d       full_path =",
            "-00005070: 206f 732e 6673 656e 636f 6465 286f 732e   os.fsencode(os.",
            "-00005080: 7061 7468 2e61 6273 7061 7468 2870 2929  path.abspath(p))",
            "-00005090: 0a20 2020 2020 2020 2020 2020 2074 7265  .            tre",
            "-000050a0: 655f 7061 7468 203d 2070 6174 685f 746f  e_path = path_to",
            "-000050b0: 5f74 7265 655f 7061 7468 2872 2e70 6174  _tree_path(r.pat",
            "-000050c0: 682c 2070 290a 2020 2020 2020 2020 2020  h, p).          ",
            "-000050d0: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         ",
            "-000050e0: 2020 2020 2020 2069 6e64 6578 5f73 6861         index_sha",
            "-000050f0: 203d 2069 6e64 6578 5b74 7265 655f 7061   = index[tree_pa",
            "-00005100: 7468 5d2e 7368 610a 2020 2020 2020 2020  th].sha.        ",
            "-00005110: 2020 2020 6578 6365 7074 204b 6579 4572      except KeyEr",
            "-00005120: 726f 7220 6173 2065 7863 3a0a 2020 2020  ror as exc:.    ",
            "-00005130: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "-00005140: 6520 4572 726f 7228 6622 7b70 7d20 6469  e Error(f\"{p} di",
            "-00005150: 6420 6e6f 7420 6d61 7463 6820 616e 7920  d not match any ",
            "-00005160: 6669 6c65 7322 2920 6672 6f6d 2065 7863  files\") from exc",
            "-00005170: 0a0a 2020 2020 2020 2020 2020 2020 6966  ..            if",
            "-00005180: 206e 6f74 2063 6163 6865 643a 0a20 2020   not cached:.   ",
            "-00005190: 2020 2020 2020 2020 2020 2020 2074 7279               try",
            "-000051a0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "-000051b0: 2020 2020 2020 7374 203d 206f 732e 6c73        st = os.ls",
            "-000051c0: 7461 7428 6675 6c6c 5f70 6174 6829 0a20  tat(full_path). ",
            "-000051d0: 2020 2020 2020 2020 2020 2020 2020 2065                 e",
            "-000051e0: 7863 6570 7420 4f53 4572 726f 723a 0a20  xcept OSError:. ",
            "+00000470: 650a 202a 2061 6464 0a20 2a20 6269 7365  e. * add. * bise",
            "+00000480: 6374 7b5f 7374 6172 742c 5f62 6164 2c5f  ct{_start,_bad,_",
            "+00000490: 676f 6f64 2c5f 736b 6970 2c5f 7265 7365  good,_skip,_rese",
            "+000004a0: 742c 5f6c 6f67 2c5f 7265 706c 6179 7d0a  t,_log,_replay}.",
            "+000004b0: 202a 2062 7261 6e63 687b 5f63 7265 6174   * branch{_creat",
            "+000004c0: 652c 5f64 656c 6574 652c 5f6c 6973 747d  e,_delete,_list}",
            "+000004d0: 0a20 2a20 6368 6563 6b5f 6967 6e6f 7265  . * check_ignore",
            "+000004e0: 0a20 2a20 6368 6563 6b6f 7574 0a20 2a20  . * checkout. * ",
            "+000004f0: 6368 6563 6b6f 7574 5f62 7261 6e63 680a  checkout_branch.",
            "+00000500: 202a 2063 6c6f 6e65 0a20 2a20 636f 6e65   * clone. * cone",
            "+00000510: 206d 6f64 657b 5f69 6e69 742c 205f 7365   mode{_init, _se",
            "+00000520: 742c 205f 6164 647d 0a20 2a20 636f 6d6d  t, _add}. * comm",
            "+00000530: 6974 0a20 2a20 636f 6d6d 6974 5f74 7265  it. * commit_tre",
            "+00000540: 650a 202a 2064 6165 6d6f 6e0a 202a 2064  e. * daemon. * d",
            "+00000550: 6573 6372 6962 650a 202a 2064 6966 665f  escribe. * diff_",
            "+00000560: 7472 6565 0a20 2a20 6665 7463 680a 202a  tree. * fetch. *",
            "+00000570: 2066 696c 7465 725f 6272 616e 6368 0a20   filter_branch. ",
            "+00000580: 2a20 666f 725f 6561 6368 5f72 6566 0a20  * for_each_ref. ",
            "+00000590: 2a20 696e 6974 0a20 2a20 6c73 5f66 696c  * init. * ls_fil",
            "+000005a0: 6573 0a20 2a20 6c73 5f72 656d 6f74 650a  es. * ls_remote.",
            "+000005b0: 202a 206c 735f 7472 6565 0a20 2a20 6d65   * ls_tree. * me",
            "+000005c0: 7267 650a 202a 206d 6572 6765 5f74 7265  rge. * merge_tre",
            "+000005d0: 650a 202a 206d 762f 6d6f 7665 0a20 2a20  e. * mv/move. * ",
            "+000005e0: 7072 756e 650a 202a 2070 756c 6c0a 202a  prune. * pull. *",
            "+000005f0: 2070 7573 680a 202a 2072 6d0a 202a 2072   push. * rm. * r",
            "+00000600: 656d 6f74 657b 5f61 6464 7d0a 202a 2072  emote{_add}. * r",
            "+00000610: 6563 6569 7665 5f70 6163 6b0a 202a 2072  eceive_pack. * r",
            "+00000620: 6573 6574 0a20 2a20 7265 7665 7274 0a20  eset. * revert. ",
            "+00000630: 2a20 7370 6172 7365 5f63 6865 636b 6f75  * sparse_checkou",
            "+00000640: 740a 202a 2073 7562 6d6f 6475 6c65 5f61  t. * submodule_a",
            "+00000650: 6464 0a20 2a20 7375 626d 6f64 756c 655f  dd. * submodule_",
            "+00000660: 696e 6974 0a20 2a20 7375 626d 6f64 756c  init. * submodul",
            "+00000670: 655f 6c69 7374 0a20 2a20 7265 765f 6c69  e_list. * rev_li",
            "+00000680: 7374 0a20 2a20 7461 677b 5f63 7265 6174  st. * tag{_creat",
            "+00000690: 652c 5f64 656c 6574 652c 5f6c 6973 747d  e,_delete,_list}",
            "+000006a0: 0a20 2a20 7570 6c6f 6164 5f70 6163 6b0a  . * upload_pack.",
            "+000006b0: 202a 2075 7064 6174 655f 7365 7276 6572   * update_server",
            "+000006c0: 5f69 6e66 6f0a 202a 2073 7461 7475 730a  _info. * status.",
            "+000006d0: 202a 2073 796d 626f 6c69 635f 7265 660a   * symbolic_ref.",
            "+000006e0: 0a54 6865 7365 2066 756e 6374 696f 6e73  .These functions",
            "+000006f0: 2061 7265 206d 6561 6e74 2074 6f20 6265   are meant to be",
            "+00000700: 6861 7665 2073 696d 696c 6172 6c79 2074  have similarly t",
            "+00000710: 6f20 7468 6520 6769 7420 7375 6263 6f6d  o the git subcom",
            "+00000720: 6d61 6e64 732e 0a44 6966 6665 7265 6e63  mands..Differenc",
            "+00000730: 6573 2069 6e20 6265 6861 7669 6f75 7220  es in behaviour ",
            "+00000740: 6172 6520 636f 6e73 6964 6572 6564 2062  are considered b",
            "+00000750: 7567 732e 0a0a 4e6f 7465 3a20 6f6e 6520  ugs...Note: one ",
            "+00000760: 6f66 2074 6865 2063 6f6e 7365 7175 656e  of the consequen",
            "+00000770: 6365 7320 6f66 2074 6869 7320 6973 2074  ces of this is t",
            "+00000780: 6861 7420 7061 7468 7320 7465 6e64 2074  hat paths tend t",
            "+00000790: 6f20 6265 0a69 6e74 6572 7072 6574 6564  o be.interpreted",
            "+000007a0: 2072 656c 6174 6976 6520 746f 2074 6865   relative to the",
            "+000007b0: 2063 7572 7265 6e74 2077 6f72 6b69 6e67   current working",
            "+000007c0: 2064 6972 6563 746f 7279 2072 6174 6865   directory rathe",
            "+000007d0: 7220 7468 616e 2072 656c 6174 6976 650a  r than relative.",
            "+000007e0: 746f 2074 6865 2072 6570 6f73 6974 6f72  to the repositor",
            "+000007f0: 7920 726f 6f74 2e0a 0a46 756e 6374 696f  y root...Functio",
            "+00000800: 6e73 2073 686f 756c 6420 6765 6e65 7261  ns should genera",
            "+00000810: 6c6c 7920 6163 6365 7074 2062 6f74 6820  lly accept both ",
            "+00000820: 756e 6963 6f64 6520 7374 7269 6e67 7320  unicode strings ",
            "+00000830: 616e 6420 6279 7465 7374 7269 6e67 730a  and bytestrings.",
            "+00000840: 2222 220a 0a69 6d70 6f72 7420 6461 7465  \"\"\"..import date",
            "+00000850: 7469 6d65 0a69 6d70 6f72 7420 666e 6d61  time.import fnma",
            "+00000860: 7463 680a 696d 706f 7274 206f 730a 696d  tch.import os.im",
            "+00000870: 706f 7274 2070 6f73 6978 7061 7468 0a69  port posixpath.i",
            "+00000880: 6d70 6f72 7420 7374 6174 0a69 6d70 6f72  mport stat.impor",
            "+00000890: 7420 7379 730a 696d 706f 7274 2074 696d  t sys.import tim",
            "+000008a0: 650a 6672 6f6d 2063 6f6c 6c65 6374 696f  e.from collectio",
            "+000008b0: 6e73 2069 6d70 6f72 7420 6e61 6d65 6474  ns import namedt",
            "+000008c0: 7570 6c65 0a66 726f 6d20 636f 6e74 6578  uple.from contex",
            "+000008d0: 746c 6962 2069 6d70 6f72 7420 636c 6f73  tlib import clos",
            "+000008e0: 696e 672c 2063 6f6e 7465 7874 6d61 6e61  ing, contextmana",
            "+000008f0: 6765 720a 6672 6f6d 2064 6174 6163 6c61  ger.from datacla",
            "+00000900: 7373 6573 2069 6d70 6f72 7420 6461 7461  sses import data",
            "+00000910: 636c 6173 730a 6672 6f6d 2069 6f20 696d  class.from io im",
            "+00000920: 706f 7274 2042 7974 6573 494f 2c20 5261  port BytesIO, Ra",
            "+00000930: 7749 4f42 6173 650a 6672 6f6d 2070 6174  wIOBase.from pat",
            "+00000940: 686c 6962 2069 6d70 6f72 7420 5061 7468  hlib import Path",
            "+00000950: 0a66 726f 6d20 7479 7069 6e67 2069 6d70  .from typing imp",
            "+00000960: 6f72 7420 4f70 7469 6f6e 616c 2c20 556e  ort Optional, Un",
            "+00000970: 696f 6e0a 0a66 726f 6d20 2e20 696d 706f  ion..from . impo",
            "+00000980: 7274 2072 6570 6c61 6365 5f6d 650a 6672  rt replace_me.fr",
            "+00000990: 6f6d 202e 6172 6368 6976 6520 696d 706f  om .archive impo",
            "+000009a0: 7274 2074 6172 5f73 7472 6561 6d0a 6672  rt tar_stream.fr",
            "+000009b0: 6f6d 202e 6269 7365 6374 2069 6d70 6f72  om .bisect impor",
            "+000009c0: 7420 4269 7365 6374 5374 6174 650a 6672  t BisectState.fr",
            "+000009d0: 6f6d 202e 636c 6965 6e74 2069 6d70 6f72  om .client impor",
            "+000009e0: 7420 6765 745f 7472 616e 7370 6f72 745f  t get_transport_",
            "+000009f0: 616e 645f 7061 7468 0a66 726f 6d20 2e63  and_path.from .c",
            "+00000a00: 6f6e 6669 6720 696d 706f 7274 2043 6f6e  onfig import Con",
            "+00000a10: 6669 672c 2043 6f6e 6669 6746 696c 652c  fig, ConfigFile,",
            "+00000a20: 2053 7461 636b 6564 436f 6e66 6967 2c20   StackedConfig, ",
            "+00000a30: 7265 6164 5f73 7562 6d6f 6475 6c65 730a  read_submodules.",
            "+00000a40: 6672 6f6d 202e 6469 6666 5f74 7265 6520  from .diff_tree ",
            "+00000a50: 696d 706f 7274 2028 0a20 2020 2043 4841  import (.    CHA",
            "+00000a60: 4e47 455f 4144 442c 0a20 2020 2043 4841  NGE_ADD,.    CHA",
            "+00000a70: 4e47 455f 434f 5059 2c0a 2020 2020 4348  NGE_COPY,.    CH",
            "+00000a80: 414e 4745 5f44 454c 4554 452c 0a20 2020  ANGE_DELETE,.   ",
            "+00000a90: 2043 4841 4e47 455f 4d4f 4449 4659 2c0a   CHANGE_MODIFY,.",
            "+00000aa0: 2020 2020 4348 414e 4745 5f52 454e 414d      CHANGE_RENAM",
            "+00000ab0: 452c 0a20 2020 2052 454e 414d 455f 4348  E,.    RENAME_CH",
            "+00000ac0: 414e 4745 5f54 5950 4553 2c0a 290a 6672  ANGE_TYPES,.).fr",
            "+00000ad0: 6f6d 202e 6572 726f 7273 2069 6d70 6f72  om .errors impor",
            "+00000ae0: 7420 5365 6e64 5061 636b 4572 726f 720a  t SendPackError.",
            "+00000af0: 6672 6f6d 202e 6772 6170 6820 696d 706f  from .graph impo",
            "+00000b00: 7274 2063 616e 5f66 6173 745f 666f 7277  rt can_fast_forw",
            "+00000b10: 6172 640a 6672 6f6d 202e 6967 6e6f 7265  ard.from .ignore",
            "+00000b20: 2069 6d70 6f72 7420 4967 6e6f 7265 4669   import IgnoreFi",
            "+00000b30: 6c74 6572 4d61 6e61 6765 720a 6672 6f6d  lterManager.from",
            "+00000b40: 202e 696e 6465 7820 696d 706f 7274 2028   .index import (",
            "+00000b50: 0a20 2020 205f 6673 5f74 6f5f 7472 6565  .    _fs_to_tree",
            "+00000b60: 5f70 6174 682c 0a20 2020 2062 6c6f 625f  _path,.    blob_",
            "+00000b70: 6672 6f6d 5f70 6174 685f 616e 645f 7374  from_path_and_st",
            "+00000b80: 6174 2c0a 2020 2020 6275 696c 645f 6669  at,.    build_fi",
            "+00000b90: 6c65 5f66 726f 6d5f 626c 6f62 2c0a 2020  le_from_blob,.  ",
            "+00000ba0: 2020 6765 745f 756e 7374 6167 6564 5f63    get_unstaged_c",
            "+00000bb0: 6861 6e67 6573 2c0a 2020 2020 7570 6461  hanges,.    upda",
            "+00000bc0: 7465 5f77 6f72 6b69 6e67 5f74 7265 652c  te_working_tree,",
            "+00000bd0: 0a29 0a66 726f 6d20 2e6f 626a 6563 745f  .).from .object_",
            "+00000be0: 7374 6f72 6520 696d 706f 7274 2074 7265  store import tre",
            "+00000bf0: 655f 6c6f 6f6b 7570 5f70 6174 680a 6672  e_lookup_path.fr",
            "+00000c00: 6f6d 202e 6f62 6a65 6374 7320 696d 706f  om .objects impo",
            "+00000c10: 7274 2028 0a20 2020 2043 6f6d 6d69 742c  rt (.    Commit,",
            "+00000c20: 0a20 2020 2054 6167 2c0a 2020 2020 666f  .    Tag,.    fo",
            "+00000c30: 726d 6174 5f74 696d 657a 6f6e 652c 0a20  rmat_timezone,. ",
            "+00000c40: 2020 2070 6172 7365 5f74 696d 657a 6f6e     parse_timezon",
            "+00000c50: 652c 0a20 2020 2070 7265 7474 795f 666f  e,.    pretty_fo",
            "+00000c60: 726d 6174 5f74 7265 655f 656e 7472 792c  rmat_tree_entry,",
            "+00000c70: 0a29 0a66 726f 6d20 2e6f 626a 6563 7473  .).from .objects",
            "+00000c80: 7065 6320 696d 706f 7274 2028 0a20 2020  pec import (.   ",
            "+00000c90: 2070 6172 7365 5f63 6f6d 6d69 742c 0a20   parse_commit,. ",
            "+00000ca0: 2020 2070 6172 7365 5f6f 626a 6563 742c     parse_object,",
            "+00000cb0: 0a20 2020 2070 6172 7365 5f72 6566 2c0a  .    parse_ref,.",
            "+00000cc0: 2020 2020 7061 7273 655f 7265 6674 7570      parse_reftup",
            "+00000cd0: 6c65 732c 0a20 2020 2070 6172 7365 5f74  les,.    parse_t",
            "+00000ce0: 7265 652c 0a29 0a66 726f 6d20 2e70 6163  ree,.).from .pac",
            "+00000cf0: 6b20 696d 706f 7274 2077 7269 7465 5f70  k import write_p",
            "+00000d00: 6163 6b5f 6672 6f6d 5f63 6f6e 7461 696e  ack_from_contain",
            "+00000d10: 6572 2c20 7772 6974 655f 7061 636b 5f69  er, write_pack_i",
            "+00000d20: 6e64 6578 0a66 726f 6d20 2e70 6174 6368  ndex.from .patch",
            "+00000d30: 2069 6d70 6f72 7420 7772 6974 655f 7472   import write_tr",
            "+00000d40: 6565 5f64 6966 660a 6672 6f6d 202e 7072  ee_diff.from .pr",
            "+00000d50: 6f74 6f63 6f6c 2069 6d70 6f72 7420 5a45  otocol import ZE",
            "+00000d60: 524f 5f53 4841 2c20 5072 6f74 6f63 6f6c  RO_SHA, Protocol",
            "+00000d70: 0a66 726f 6d20 2e72 6566 7320 696d 706f  .from .refs impo",
            "+00000d80: 7274 2028 0a20 2020 204c 4f43 414c 5f42  rt (.    LOCAL_B",
            "+00000d90: 5241 4e43 485f 5052 4546 4958 2c0a 2020  RANCH_PREFIX,.  ",
            "+00000da0: 2020 4c4f 4341 4c5f 4e4f 5445 535f 5052    LOCAL_NOTES_PR",
            "+00000db0: 4546 4958 2c0a 2020 2020 4c4f 4341 4c5f  EFIX,.    LOCAL_",
            "+00000dc0: 5441 475f 5052 4546 4958 2c0a 2020 2020  TAG_PREFIX,.    ",
            "+00000dd0: 5265 662c 0a20 2020 2053 796d 7265 664c  Ref,.    SymrefL",
            "+00000de0: 6f6f 702c 0a20 2020 205f 696d 706f 7274  oop,.    _import",
            "+00000df0: 5f72 656d 6f74 655f 7265 6673 2c0a 290a  _remote_refs,.).",
            "+00000e00: 6672 6f6d 202e 7265 706f 2069 6d70 6f72  from .repo impor",
            "+00000e10: 7420 4261 7365 5265 706f 2c20 5265 706f  t BaseRepo, Repo",
            "+00000e20: 2c20 6765 745f 7573 6572 5f69 6465 6e74  , get_user_ident",
            "+00000e30: 6974 790a 6672 6f6d 202e 7365 7276 6572  ity.from .server",
            "+00000e40: 2069 6d70 6f72 7420 280a 2020 2020 4669   import (.    Fi",
            "+00000e50: 6c65 5379 7374 656d 4261 636b 656e 642c  leSystemBackend,",
            "+00000e60: 0a20 2020 2052 6563 6569 7665 5061 636b  .    ReceivePack",
            "+00000e70: 4861 6e64 6c65 722c 0a20 2020 2054 4350  Handler,.    TCP",
            "+00000e80: 4769 7453 6572 7665 722c 0a20 2020 2055  GitServer,.    U",
            "+00000e90: 706c 6f61 6450 6163 6b48 616e 646c 6572  ploadPackHandler",
            "+00000ea0: 2c0a 290a 6672 6f6d 202e 7365 7276 6572  ,.).from .server",
            "+00000eb0: 2069 6d70 6f72 7420 7570 6461 7465 5f73   import update_s",
            "+00000ec0: 6572 7665 725f 696e 666f 2061 7320 7365  erver_info as se",
            "+00000ed0: 7276 6572 5f75 7064 6174 655f 7365 7276  rver_update_serv",
            "+00000ee0: 6572 5f69 6e66 6f0a 6672 6f6d 202e 7370  er_info.from .sp",
            "+00000ef0: 6172 7365 5f70 6174 7465 726e 7320 696d  arse_patterns im",
            "+00000f00: 706f 7274 2028 0a20 2020 2053 7061 7273  port (.    Spars",
            "+00000f10: 6543 6865 636b 6f75 7443 6f6e 666c 6963  eCheckoutConflic",
            "+00000f20: 7445 7272 6f72 2c0a 2020 2020 6170 706c  tError,.    appl",
            "+00000f30: 795f 696e 636c 7564 6564 5f70 6174 6873  y_included_paths",
            "+00000f40: 2c0a 2020 2020 6465 7465 726d 696e 655f  ,.    determine_",
            "+00000f50: 696e 636c 7564 6564 5f70 6174 6873 2c0a  included_paths,.",
            "+00000f60: 290a 0a23 204d 6f64 756c 6520 6c65 7665  )..# Module leve",
            "+00000f70: 6c20 7475 706c 6520 6465 6669 6e69 7469  l tuple definiti",
            "+00000f80: 6f6e 2066 6f72 2073 7461 7475 7320 6f75  on for status ou",
            "+00000f90: 7470 7574 0a47 6974 5374 6174 7573 203d  tput.GitStatus =",
            "+00000fa0: 206e 616d 6564 7475 706c 6528 2247 6974   namedtuple(\"Git",
            "+00000fb0: 5374 6174 7573 222c 2022 7374 6167 6564  Status\", \"staged",
            "+00000fc0: 2075 6e73 7461 6765 6420 756e 7472 6163   unstaged untrac",
            "+00000fd0: 6b65 6422 290a 0a0a 4064 6174 6163 6c61  ked\")...@datacla",
            "+00000fe0: 7373 0a63 6c61 7373 2043 6f75 6e74 4f62  ss.class CountOb",
            "+00000ff0: 6a65 6374 7352 6573 756c 743a 0a20 2020  jectsResult:.   ",
            "+00001000: 2022 2222 5265 7375 6c74 206f 6620 636f   \"\"\"Result of co",
            "+00001010: 756e 7469 6e67 206f 626a 6563 7473 2069  unting objects i",
            "+00001020: 6e20 6120 7265 706f 7369 746f 7279 2e0a  n a repository..",
            "+00001030: 0a20 2020 2041 7474 7269 6275 7465 733a  .    Attributes:",
            "+00001040: 0a20 2020 2020 2063 6f75 6e74 3a20 4e75  .      count: Nu",
            "+00001050: 6d62 6572 206f 6620 6c6f 6f73 6520 6f62  mber of loose ob",
            "+00001060: 6a65 6374 730a 2020 2020 2020 7369 7a65  jects.      size",
            "+00001070: 3a20 546f 7461 6c20 7369 7a65 206f 6620  : Total size of ",
            "+00001080: 6c6f 6f73 6520 6f62 6a65 6374 7320 696e  loose objects in",
            "+00001090: 2062 7974 6573 0a20 2020 2020 2069 6e5f   bytes.      in_",
            "+000010a0: 7061 636b 3a20 4e75 6d62 6572 206f 6620  pack: Number of ",
            "+000010b0: 6f62 6a65 6374 7320 696e 2070 6163 6b20  objects in pack ",
            "+000010c0: 6669 6c65 730a 2020 2020 2020 7061 636b  files.      pack",
            "+000010d0: 733a 204e 756d 6265 7220 6f66 2070 6163  s: Number of pac",
            "+000010e0: 6b20 6669 6c65 730a 2020 2020 2020 7369  k files.      si",
            "+000010f0: 7a65 5f70 6163 6b3a 2054 6f74 616c 2073  ze_pack: Total s",
            "+00001100: 697a 6520 6f66 2070 6163 6b20 6669 6c65  ize of pack file",
            "+00001110: 7320 696e 2062 7974 6573 0a20 2020 2022  s in bytes.    \"",
            "+00001120: 2222 0a0a 2020 2020 636f 756e 743a 2069  \"\"..    count: i",
            "+00001130: 6e74 0a20 2020 2073 697a 653a 2069 6e74  nt.    size: int",
            "+00001140: 0a20 2020 2069 6e5f 7061 636b 3a20 4f70  .    in_pack: Op",
            "+00001150: 7469 6f6e 616c 5b69 6e74 5d20 3d20 4e6f  tional[int] = No",
            "+00001160: 6e65 0a20 2020 2070 6163 6b73 3a20 4f70  ne.    packs: Op",
            "+00001170: 7469 6f6e 616c 5b69 6e74 5d20 3d20 4e6f  tional[int] = No",
            "+00001180: 6e65 0a20 2020 2073 697a 655f 7061 636b  ne.    size_pack",
            "+00001190: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] ",
            "+000011a0: 3d20 4e6f 6e65 0a0a 0a63 6c61 7373 204e  = None...class N",
            "+000011b0: 6f6e 6553 7472 6561 6d28 5261 7749 4f42  oneStream(RawIOB",
            "+000011c0: 6173 6529 3a0a 2020 2020 2222 2246 616c  ase):.    \"\"\"Fal",
            "+000011d0: 6c62 6163 6b20 6966 2073 7464 6f75 7420  lback if stdout ",
            "+000011e0: 6f72 2073 7464 6572 7220 6172 6520 756e  or stderr are un",
            "+000011f0: 6176 6169 6c61 626c 652c 2064 6f65 7320  available, does ",
            "+00001200: 6e6f 7468 696e 672e 2222 220a 0a20 2020  nothing.\"\"\"..   ",
            "+00001210: 2064 6566 2072 6561 6428 7365 6c66 2c20   def read(self, ",
            "+00001220: 7369 7a65 3d2d 3129 202d 3e20 4e6f 6e65  size=-1) -> None",
            "+00001230: 3a0a 2020 2020 2020 2020 7265 7475 726e  :.        return",
            "+00001240: 204e 6f6e 650a 0a20 2020 2064 6566 2072   None..    def r",
            "+00001250: 6561 6461 6c6c 2873 656c 6629 202d 3e20  eadall(self) -> ",
            "+00001260: 6279 7465 733a 0a20 2020 2020 2020 2072  bytes:.        r",
            "+00001270: 6574 7572 6e20 6222 220a 0a20 2020 2064  eturn b\"\"..    d",
            "+00001280: 6566 2072 6561 6469 6e74 6f28 7365 6c66  ef readinto(self",
            "+00001290: 2c20 6229 202d 3e20 4e6f 6e65 3a0a 2020  , b) -> None:.  ",
            "+000012a0: 2020 2020 2020 7265 7475 726e 204e 6f6e        return Non",
            "+000012b0: 650a 0a20 2020 2064 6566 2077 7269 7465  e..    def write",
            "+000012c0: 2873 656c 662c 2062 2920 2d3e 204e 6f6e  (self, b) -> Non",
            "+000012d0: 653a 0a20 2020 2020 2020 2072 6574 7572  e:.        retur",
            "+000012e0: 6e20 4e6f 6e65 0a0a 0a64 6566 6175 6c74  n None...default",
            "+000012f0: 5f62 7974 6573 5f6f 7574 5f73 7472 6561  _bytes_out_strea",
            "+00001300: 6d20 3d20 6765 7461 7474 7228 7379 732e  m = getattr(sys.",
            "+00001310: 7374 646f 7574 2c20 2262 7566 6665 7222  stdout, \"buffer\"",
            "+00001320: 2c20 4e6f 6e65 2920 6f72 204e 6f6e 6553  , None) or NoneS",
            "+00001330: 7472 6561 6d28 290a 6465 6661 756c 745f  tream().default_",
            "+00001340: 6279 7465 735f 6572 725f 7374 7265 616d  bytes_err_stream",
            "+00001350: 203d 2067 6574 6174 7472 2873 7973 2e73   = getattr(sys.s",
            "+00001360: 7464 6572 722c 2022 6275 6666 6572 222c  tderr, \"buffer\",",
            "+00001370: 204e 6f6e 6529 206f 7220 4e6f 6e65 5374   None) or NoneSt",
            "+00001380: 7265 616d 2829 0a0a 0a44 4546 4155 4c54  ream()...DEFAULT",
            "+00001390: 5f45 4e43 4f44 494e 4720 3d20 2275 7466  _ENCODING = \"utf",
            "+000013a0: 2d38 220a 0a0a 636c 6173 7320 4572 726f  -8\"...class Erro",
            "+000013b0: 7228 4578 6365 7074 696f 6e29 3a0a 2020  r(Exception):.  ",
            "+000013c0: 2020 2222 2250 6f72 6365 6c61 696e 2d62    \"\"\"Porcelain-b",
            "+000013d0: 6173 6564 2065 7272 6f72 2e22 2222 0a0a  ased error.\"\"\"..",
            "+000013e0: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__",
            "+000013f0: 2873 656c 662c 206d 7367 2920 2d3e 204e  (self, msg) -> N",
            "+00001400: 6f6e 653a 0a20 2020 2020 2020 2073 7570  one:.        sup",
            "+00001410: 6572 2829 2e5f 5f69 6e69 745f 5f28 6d73  er().__init__(ms",
            "+00001420: 6729 0a0a 0a63 6c61 7373 2052 656d 6f74  g)...class Remot",
            "+00001430: 6545 7869 7374 7328 4572 726f 7229 3a0a  eExists(Error):.",
            "+00001440: 2020 2020 2222 2252 6169 7365 6420 7768      \"\"\"Raised wh",
            "+00001450: 656e 2074 6865 2072 656d 6f74 6520 616c  en the remote al",
            "+00001460: 7265 6164 7920 6578 6973 7473 2e22 2222  ready exists.\"\"\"",
            "+00001470: 0a0a 0a63 6c61 7373 2054 696d 657a 6f6e  ...class Timezon",
            "+00001480: 6546 6f72 6d61 7445 7272 6f72 2845 7272  eFormatError(Err",
            "+00001490: 6f72 293a 0a20 2020 2022 2222 5261 6973  or):.    \"\"\"Rais",
            "+000014a0: 6564 2077 6865 6e20 7468 6520 7469 6d65  ed when the time",
            "+000014b0: 7a6f 6e65 2063 616e 6e6f 7420 6265 2064  zone cannot be d",
            "+000014c0: 6574 6572 6d69 6e65 6420 6672 6f6d 2061  etermined from a",
            "+000014d0: 2067 6976 656e 2073 7472 696e 672e 2222   given string.\"\"",
            "+000014e0: 220a 0a0a 636c 6173 7320 4368 6563 6b6f  \"...class Checko",
            "+000014f0: 7574 4572 726f 7228 4572 726f 7229 3a0a  utError(Error):.",
            "+00001500: 2020 2020 2222 2249 6e64 6963 6174 6573      \"\"\"Indicates",
            "+00001510: 2074 6861 7420 6120 6368 6563 6b6f 7574   that a checkout",
            "+00001520: 2063 616e 6e6f 7420 6265 2070 6572 666f   cannot be perfo",
            "+00001530: 726d 6564 2e22 2222 0a0a 0a64 6566 2070  rmed.\"\"\"...def p",
            "+00001540: 6172 7365 5f74 696d 657a 6f6e 655f 666f  arse_timezone_fo",
            "+00001550: 726d 6174 2874 7a5f 7374 7229 3a0a 2020  rmat(tz_str):.  ",
            "+00001560: 2020 2222 2250 6172 7365 2067 6976 656e    \"\"\"Parse given",
            "+00001570: 2073 7472 696e 6720 616e 6420 6174 7465   string and atte",
            "+00001580: 6d70 7420 746f 2072 6574 7572 6e20 6120  mpt to return a ",
            "+00001590: 7469 6d65 7a6f 6e65 206f 6666 7365 742e  timezone offset.",
            "+000015a0: 0a0a 2020 2020 4469 6666 6572 656e 7420  ..    Different ",
            "+000015b0: 666f 726d 6174 7320 6172 6520 636f 6e73  formats are cons",
            "+000015c0: 6964 6572 6564 2069 6e20 7468 6520 666f  idered in the fo",
            "+000015d0: 6c6c 6f77 696e 6720 6f72 6465 723a 0a0a  llowing order:..",
            "+000015e0: 2020 2020 202d 2047 6974 2069 6e74 6572       - Git inter",
            "+000015f0: 6e61 6c20 666f 726d 6174 3a20 3c75 6e69  nal format: <uni",
            "+00001600: 7820 7469 6d65 7374 616d 703e 203c 7469  x timestamp> <ti",
            "+00001610: 6d65 7a6f 6e65 206f 6666 7365 743e 0a20  mezone offset>. ",
            "+00001620: 2020 2020 2d20 5246 4320 3238 3232 3a20      - RFC 2822: ",
            "+00001630: 652e 672e 204d 6f6e 2c20 3230 204e 6f76  e.g. Mon, 20 Nov",
            "+00001640: 2031 3939 3520 3139 3a31 323a 3038 202d   1995 19:12:08 -",
            "+00001650: 3035 3030 0a20 2020 2020 2d20 4953 4f20  0500.     - ISO ",
            "+00001660: 3836 3031 3a20 652e 672e 2031 3939 352d  8601: e.g. 1995-",
            "+00001670: 3131 2d32 3054 3139 3a31 323a 3038 2d30  11-20T19:12:08-0",
            "+00001680: 3530 300a 0a20 2020 2041 7267 733a 0a20  500..    Args:. ",
            "+00001690: 2020 2020 2074 7a5f 7374 723a 2064 6174       tz_str: dat",
            "+000016a0: 6574 696d 6520 7374 7269 6e67 0a20 2020  etime string.   ",
            "+000016b0: 2052 6574 7572 6e73 3a20 5469 6d65 7a6f   Returns: Timezo",
            "+000016c0: 6e65 206f 6666 7365 7420 6173 2069 6e74  ne offset as int",
            "+000016d0: 6567 6572 0a20 2020 2052 6169 7365 733a  eger.    Raises:",
            "+000016e0: 0a20 2020 2020 2054 696d 657a 6f6e 6546  .      TimezoneF",
            "+000016f0: 6f72 6d61 7445 7272 6f72 3a20 6966 2074  ormatError: if t",
            "+00001700: 696d 657a 6f6e 6520 696e 666f 726d 6174  imezone informat",
            "+00001710: 696f 6e20 6361 6e6e 6f74 2062 6520 6578  ion cannot be ex",
            "+00001720: 7472 6163 7465 640a 2020 2020 2222 220a  tracted.    \"\"\".",
            "+00001730: 2020 2020 696d 706f 7274 2072 650a 0a20      import re.. ",
            "+00001740: 2020 2023 2047 6974 2069 6e74 6572 6e61     # Git interna",
            "+00001750: 6c20 666f 726d 6174 0a20 2020 2069 6e74  l format.    int",
            "+00001760: 6572 6e61 6c5f 666f 726d 6174 5f70 6174  ernal_format_pat",
            "+00001770: 7465 726e 203d 2072 652e 636f 6d70 696c  tern = re.compil",
            "+00001780: 6528 225e 5b30 2d39 5d2b 205b 2b2d 5d5b  e(\"^[0-9]+ [+-][",
            "+00001790: 302d 395d 7b2c 347d 2422 290a 2020 2020  0-9]{,4}$\").    ",
            "+000017a0: 6966 2072 652e 6d61 7463 6828 696e 7465  if re.match(inte",
            "+000017b0: 726e 616c 5f66 6f72 6d61 745f 7061 7474  rnal_format_patt",
            "+000017c0: 6572 6e2c 2074 7a5f 7374 7229 3a0a 2020  ern, tz_str):.  ",
            "+000017d0: 2020 2020 2020 7472 793a 0a20 2020 2020        try:.     ",
            "+000017e0: 2020 2020 2020 2074 7a5f 696e 7465 726e         tz_intern",
            "+000017f0: 616c 203d 2070 6172 7365 5f74 696d 657a  al = parse_timez",
            "+00001800: 6f6e 6528 747a 5f73 7472 2e73 706c 6974  one(tz_str.split",
            "+00001810: 2822 2022 295b 315d 2e65 6e63 6f64 6528  (\" \")[1].encode(",
            "+00001820: 4445 4641 554c 545f 454e 434f 4449 4e47  DEFAULT_ENCODING",
            "+00001830: 2929 0a20 2020 2020 2020 2020 2020 2072  )).            r",
            "+00001840: 6574 7572 6e20 747a 5f69 6e74 6572 6e61  eturn tz_interna",
            "+00001850: 6c5b 305d 0a20 2020 2020 2020 2065 7863  l[0].        exc",
            "+00001860: 6570 7420 5661 6c75 6545 7272 6f72 3a0a  ept ValueError:.",
            "+00001870: 2020 2020 2020 2020 2020 2020 7061 7373              pass",
            "+00001880: 0a0a 2020 2020 2320 5246 4320 3238 3232  ..    # RFC 2822",
            "+00001890: 0a20 2020 2069 6d70 6f72 7420 656d 6169  .    import emai",
            "+000018a0: 6c2e 7574 696c 730a 0a20 2020 2072 6663  l.utils..    rfc",
            "+000018b0: 5f32 3832 3220 3d20 656d 6169 6c2e 7574  _2822 = email.ut",
            "+000018c0: 696c 732e 7061 7273 6564 6174 655f 747a  ils.parsedate_tz",
            "+000018d0: 2874 7a5f 7374 7229 0a20 2020 2069 6620  (tz_str).    if ",
            "+000018e0: 7266 635f 3238 3232 3a0a 2020 2020 2020  rfc_2822:.      ",
            "+000018f0: 2020 7265 7475 726e 2072 6663 5f32 3832    return rfc_282",
            "+00001900: 325b 395d 0a0a 2020 2020 2320 4953 4f20  2[9]..    # ISO ",
            "+00001910: 3836 3031 0a0a 2020 2020 2320 5375 7070  8601..    # Supp",
            "+00001920: 6f72 7465 6420 6f66 6673 6574 733a 0a20  orted offsets:. ",
            "+00001930: 2020 2023 2073 4848 4d4d 2c20 7348 483a     # sHHMM, sHH:",
            "+00001940: 4d4d 2c20 7348 480a 2020 2020 6973 6f5f  MM, sHH.    iso_",
            "+00001950: 3836 3031 5f70 6174 7465 726e 203d 2072  8601_pattern = r",
            "+00001960: 652e 636f 6d70 696c 6528 0a20 2020 2020  e.compile(.     ",
            "+00001970: 2020 2022 5b30 2d39 5d20 3f28 5b2b 2d5d     \"[0-9] ?([+-]",
            "+00001980: 2928 5b30 2d39 5d7b 327d 2928 3f3a 3a28  )([0-9]{2})(?::(",
            "+00001990: 3f3d 5b30 2d39 5d7b 327d 2929 3f28 5b30  ?=[0-9]{2}))?([0",
            "+000019a0: 2d39 5d7b 327d 293f 2422 0a20 2020 2029  -9]{2})?$\".    )",
            "+000019b0: 0a20 2020 206d 6174 6368 203d 2072 652e  .    match = re.",
            "+000019c0: 7365 6172 6368 2869 736f 5f38 3630 315f  search(iso_8601_",
            "+000019d0: 7061 7474 6572 6e2c 2074 7a5f 7374 7229  pattern, tz_str)",
            "+000019e0: 0a20 2020 2074 6f74 616c 5f73 6563 7320  .    total_secs ",
            "+000019f0: 3d20 300a 2020 2020 6966 206d 6174 6368  = 0.    if match",
            "+00001a00: 3a0a 2020 2020 2020 2020 7369 676e 2c20  :.        sign, ",
            "+00001a10: 686f 7572 732c 206d 696e 7574 6573 203d  hours, minutes =",
            "+00001a20: 206d 6174 6368 2e67 726f 7570 7328 290a   match.groups().",
            "+00001a30: 2020 2020 2020 2020 746f 7461 6c5f 7365          total_se",
            "+00001a40: 6373 202b 3d20 696e 7428 686f 7572 7329  cs += int(hours)",
            "+00001a50: 202a 2033 3630 300a 2020 2020 2020 2020   * 3600.        ",
            "+00001a60: 6966 206d 696e 7574 6573 3a0a 2020 2020  if minutes:.    ",
            "+00001a70: 2020 2020 2020 2020 746f 7461 6c5f 7365          total_se",
            "+00001a80: 6373 202b 3d20 696e 7428 6d69 6e75 7465  cs += int(minute",
            "+00001a90: 7329 202a 2036 300a 2020 2020 2020 2020  s) * 60.        ",
            "+00001aa0: 746f 7461 6c5f 7365 6373 203d 202d 746f  total_secs = -to",
            "+00001ab0: 7461 6c5f 7365 6373 2069 6620 7369 676e  tal_secs if sign",
            "+00001ac0: 203d 3d20 222d 2220 656c 7365 2074 6f74   == \"-\" else tot",
            "+00001ad0: 616c 5f73 6563 730a 2020 2020 2020 2020  al_secs.        ",
            "+00001ae0: 7265 7475 726e 2074 6f74 616c 5f73 6563  return total_sec",
            "+00001af0: 730a 0a20 2020 2023 2059 5959 592e 4d4d  s..    # YYYY.MM",
            "+00001b00: 2e44 442c 204d 4d2f 4444 2f59 5959 592c  .DD, MM/DD/YYYY,",
            "+00001b10: 2044 442e 4d4d 2e59 5959 5920 636f 6e74   DD.MM.YYYY cont",
            "+00001b20: 6169 6e20 6e6f 2074 696d 657a 6f6e 6520  ain no timezone ",
            "+00001b30: 696e 666f 726d 6174 696f 6e0a 0a20 2020  information..   ",
            "+00001b40: 2072 6169 7365 2054 696d 657a 6f6e 6546   raise TimezoneF",
            "+00001b50: 6f72 6d61 7445 7272 6f72 2874 7a5f 7374  ormatError(tz_st",
            "+00001b60: 7229 0a0a 0a64 6566 2067 6574 5f75 7365  r)...def get_use",
            "+00001b70: 725f 7469 6d65 7a6f 6e65 7328 293a 0a20  r_timezones():. ",
            "+00001b80: 2020 2022 2222 5265 7472 6965 7665 206c     \"\"\"Retrieve l",
            "+00001b90: 6f63 616c 2074 696d 657a 6f6e 6520 6173  ocal timezone as",
            "+00001ba0: 2064 6573 6372 6962 6564 2069 6e0a 2020   described in.  ",
            "+00001bb0: 2020 6874 7470 733a 2f2f 7261 772e 6769    https://raw.gi",
            "+00001bc0: 7468 7562 7573 6572 636f 6e74 656e 742e  thubusercontent.",
            "+00001bd0: 636f 6d2f 6769 742f 6769 742f 7632 2e33  com/git/git/v2.3",
            "+00001be0: 2e30 2f44 6f63 756d 656e 7461 7469 6f6e  .0/Documentation",
            "+00001bf0: 2f64 6174 652d 666f 726d 6174 732e 7478  /date-formats.tx",
            "+00001c00: 740a 2020 2020 5265 7475 726e 733a 2041  t.    Returns: A",
            "+00001c10: 2074 7570 6c65 2063 6f6e 7461 696e 696e   tuple containin",
            "+00001c20: 6720 6175 7468 6f72 2074 696d 657a 6f6e  g author timezon",
            "+00001c30: 652c 2063 6f6d 6d69 7474 6572 2074 696d  e, committer tim",
            "+00001c40: 657a 6f6e 652e 0a20 2020 2022 2222 0a20  ezone..    \"\"\". ",
            "+00001c50: 2020 206c 6f63 616c 5f74 696d 657a 6f6e     local_timezon",
            "+00001c60: 6520 3d20 7469 6d65 2e6c 6f63 616c 7469  e = time.localti",
            "+00001c70: 6d65 2829 2e74 6d5f 676d 746f 6666 0a0a  me().tm_gmtoff..",
            "+00001c80: 2020 2020 6966 206f 732e 656e 7669 726f      if os.enviro",
            "+00001c90: 6e2e 6765 7428 2247 4954 5f41 5554 484f  n.get(\"GIT_AUTHO",
            "+00001ca0: 525f 4441 5445 2229 3a0a 2020 2020 2020  R_DATE\"):.      ",
            "+00001cb0: 2020 6175 7468 6f72 5f74 696d 657a 6f6e    author_timezon",
            "+00001cc0: 6520 3d20 7061 7273 655f 7469 6d65 7a6f  e = parse_timezo",
            "+00001cd0: 6e65 5f66 6f72 6d61 7428 6f73 2e65 6e76  ne_format(os.env",
            "+00001ce0: 6972 6f6e 5b22 4749 545f 4155 5448 4f52  iron[\"GIT_AUTHOR",
            "+00001cf0: 5f44 4154 4522 5d29 0a20 2020 2065 6c73  _DATE\"]).    els",
            "+00001d00: 653a 0a20 2020 2020 2020 2061 7574 686f  e:.        autho",
            "+00001d10: 725f 7469 6d65 7a6f 6e65 203d 206c 6f63  r_timezone = loc",
            "+00001d20: 616c 5f74 696d 657a 6f6e 650a 2020 2020  al_timezone.    ",
            "+00001d30: 6966 206f 732e 656e 7669 726f 6e2e 6765  if os.environ.ge",
            "+00001d40: 7428 2247 4954 5f43 4f4d 4d49 5454 4552  t(\"GIT_COMMITTER",
            "+00001d50: 5f44 4154 4522 293a 0a20 2020 2020 2020  _DATE\"):.       ",
            "+00001d60: 2063 6f6d 6d69 745f 7469 6d65 7a6f 6e65   commit_timezone",
            "+00001d70: 203d 2070 6172 7365 5f74 696d 657a 6f6e   = parse_timezon",
            "+00001d80: 655f 666f 726d 6174 286f 732e 656e 7669  e_format(os.envi",
            "+00001d90: 726f 6e5b 2247 4954 5f43 4f4d 4d49 5454  ron[\"GIT_COMMITT",
            "+00001da0: 4552 5f44 4154 4522 5d29 0a20 2020 2065  ER_DATE\"]).    e",
            "+00001db0: 6c73 653a 0a20 2020 2020 2020 2063 6f6d  lse:.        com",
            "+00001dc0: 6d69 745f 7469 6d65 7a6f 6e65 203d 206c  mit_timezone = l",
            "+00001dd0: 6f63 616c 5f74 696d 657a 6f6e 650a 0a20  ocal_timezone.. ",
            "+00001de0: 2020 2072 6574 7572 6e20 6175 7468 6f72     return author",
            "+00001df0: 5f74 696d 657a 6f6e 652c 2063 6f6d 6d69  _timezone, commi",
            "+00001e00: 745f 7469 6d65 7a6f 6e65 0a0a 0a64 6566  t_timezone...def",
            "+00001e10: 206f 7065 6e5f 7265 706f 2870 6174 685f   open_repo(path_",
            "+00001e20: 6f72 5f72 6570 6f3a 2055 6e69 6f6e 5b73  or_repo: Union[s",
            "+00001e30: 7472 2c20 6f73 2e50 6174 684c 696b 652c  tr, os.PathLike,",
            "+00001e40: 2042 6173 6552 6570 6f5d 293a 0a20 2020   BaseRepo]):.   ",
            "+00001e50: 2022 2222 4f70 656e 2061 6e20 6172 6775   \"\"\"Open an argu",
            "+00001e60: 6d65 6e74 2074 6861 7420 6361 6e20 6265  ment that can be",
            "+00001e70: 2061 2072 6570 6f73 6974 6f72 7920 6f72   a repository or",
            "+00001e80: 2061 2070 6174 6820 666f 7220 6120 7265   a path for a re",
            "+00001e90: 706f 7369 746f 7279 2e22 2222 0a20 2020  pository.\"\"\".   ",
            "+00001ea0: 2069 6620 6973 696e 7374 616e 6365 2870   if isinstance(p",
            "+00001eb0: 6174 685f 6f72 5f72 6570 6f2c 2042 6173  ath_or_repo, Bas",
            "+00001ec0: 6552 6570 6f29 3a0a 2020 2020 2020 2020  eRepo):.        ",
            "+00001ed0: 7265 7475 726e 2070 6174 685f 6f72 5f72  return path_or_r",
            "+00001ee0: 6570 6f0a 2020 2020 7265 7475 726e 2052  epo.    return R",
            "+00001ef0: 6570 6f28 7061 7468 5f6f 725f 7265 706f  epo(path_or_repo",
            "+00001f00: 290a 0a0a 4063 6f6e 7465 7874 6d61 6e61  )...@contextmana",
            "+00001f10: 6765 720a 6465 6620 5f6e 6f6f 705f 636f  ger.def _noop_co",
            "+00001f20: 6e74 6578 745f 6d61 6e61 6765 7228 6f62  ntext_manager(ob",
            "+00001f30: 6a29 3a0a 2020 2020 2222 2243 6f6e 7465  j):.    \"\"\"Conte",
            "+00001f40: 7874 206d 616e 6167 6572 2074 6861 7420  xt manager that ",
            "+00001f50: 6861 7320 7468 6520 7361 6d65 2061 7069  has the same api",
            "+00001f60: 2061 7320 636c 6f73 696e 6720 6275 7420   as closing but ",
            "+00001f70: 646f 6573 206e 6f74 6869 6e67 2e22 2222  does nothing.\"\"\"",
            "+00001f80: 0a20 2020 2079 6965 6c64 206f 626a 0a0a  .    yield obj..",
            "+00001f90: 0a64 6566 206f 7065 6e5f 7265 706f 5f63  .def open_repo_c",
            "+00001fa0: 6c6f 7369 6e67 2870 6174 685f 6f72 5f72  losing(path_or_r",
            "+00001fb0: 6570 6f3a 2055 6e69 6f6e 5b73 7472 2c20  epo: Union[str, ",
            "+00001fc0: 6f73 2e50 6174 684c 696b 652c 2042 6173  os.PathLike, Bas",
            "+00001fd0: 6552 6570 6f5d 293a 0a20 2020 2022 2222  eRepo]):.    \"\"\"",
            "+00001fe0: 4f70 656e 2061 6e20 6172 6775 6d65 6e74  Open an argument",
            "+00001ff0: 2074 6861 7420 6361 6e20 6265 2061 2072   that can be a r",
            "+00002000: 6570 6f73 6974 6f72 7920 6f72 2061 2070  epository or a p",
            "+00002010: 6174 6820 666f 7220 6120 7265 706f 7369  ath for a reposi",
            "+00002020: 746f 7279 2e0a 2020 2020 7265 7475 726e  tory..    return",
            "+00002030: 7320 6120 636f 6e74 6578 7420 6d61 6e61  s a context mana",
            "+00002040: 6765 7220 7468 6174 2077 696c 6c20 636c  ger that will cl",
            "+00002050: 6f73 6520 7468 6520 7265 706f 206f 6e20  ose the repo on ",
            "+00002060: 6578 6974 2069 6620 7468 6520 6172 6775  exit if the argu",
            "+00002070: 6d65 6e74 0a20 2020 2069 7320 6120 7061  ment.    is a pa",
            "+00002080: 7468 2c20 656c 7365 2064 6f65 7320 6e6f  th, else does no",
            "+00002090: 7468 696e 6720 6966 2074 6865 2061 7267  thing if the arg",
            "+000020a0: 756d 656e 7420 6973 2061 2072 6570 6f2e  ument is a repo.",
            "+000020b0: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    \"\"\".    if ",
            "+000020c0: 6973 696e 7374 616e 6365 2870 6174 685f  isinstance(path_",
            "+000020d0: 6f72 5f72 6570 6f2c 2042 6173 6552 6570  or_repo, BaseRep",
            "+000020e0: 6f29 3a0a 2020 2020 2020 2020 7265 7475  o):.        retu",
            "+000020f0: 726e 205f 6e6f 6f70 5f63 6f6e 7465 7874  rn _noop_context",
            "+00002100: 5f6d 616e 6167 6572 2870 6174 685f 6f72  _manager(path_or",
            "+00002110: 5f72 6570 6f29 0a20 2020 2072 6574 7572  _repo).    retur",
            "+00002120: 6e20 636c 6f73 696e 6728 5265 706f 2870  n closing(Repo(p",
            "+00002130: 6174 685f 6f72 5f72 6570 6f29 290a 0a0a  ath_or_repo))...",
            "+00002140: 6465 6620 7061 7468 5f74 6f5f 7472 6565  def path_to_tree",
            "+00002150: 5f70 6174 6828 7265 706f 7061 7468 2c20  _path(repopath, ",
            "+00002160: 7061 7468 2c20 7472 6565 5f65 6e63 6f64  path, tree_encod",
            "+00002170: 696e 673d 4445 4641 554c 545f 454e 434f  ing=DEFAULT_ENCO",
            "+00002180: 4449 4e47 293a 0a20 2020 2022 2222 436f  DING):.    \"\"\"Co",
            "+00002190: 6e76 6572 7420 6120 7061 7468 2074 6f20  nvert a path to ",
            "+000021a0: 6120 7061 7468 2075 7361 626c 6520 696e  a path usable in",
            "+000021b0: 2061 6e20 696e 6465 782c 2065 2e67 2e20   an index, e.g. ",
            "+000021c0: 6279 7465 7320 616e 6420 7265 6c61 7469  bytes and relati",
            "+000021d0: 7665 2074 6f0a 2020 2020 7468 6520 7265  ve to.    the re",
            "+000021e0: 706f 7369 746f 7279 2072 6f6f 742e 0a0a  pository root...",
            "+000021f0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+00002200: 7265 706f 7061 7468 3a20 5265 706f 7369  repopath: Reposi",
            "+00002210: 746f 7279 2070 6174 682c 2061 6273 6f6c  tory path, absol",
            "+00002220: 7574 6520 6f72 2072 656c 6174 6976 6520  ute or relative ",
            "+00002230: 746f 2074 6865 2063 7764 0a20 2020 2020  to the cwd.     ",
            "+00002240: 2070 6174 683a 2041 2070 6174 682c 2061   path: A path, a",
            "+00002250: 6273 6f6c 7574 6520 6f72 2072 656c 6174  bsolute or relat",
            "+00002260: 6976 6520 746f 2074 6865 2063 7764 0a20  ive to the cwd. ",
            "+00002270: 2020 2052 6574 7572 6e73 3a20 4120 7061     Returns: A pa",
            "+00002280: 7468 2066 6f72 6d61 7474 6564 2066 6f72  th formatted for",
            "+00002290: 2075 7365 2069 6e20 652e 672e 2061 6e20   use in e.g. an ",
            "+000022a0: 696e 6465 780a 2020 2020 2222 220a 2020  index.    \"\"\".  ",
            "+000022b0: 2020 2320 5265 736f 6c76 6520 6d69 6768    # Resolve migh",
            "+000022c0: 7420 7265 7475 726e 7320 6120 7265 6c61  t returns a rela",
            "+000022d0: 7469 7665 2070 6174 6820 6f6e 2057 696e  tive path on Win",
            "+000022e0: 646f 7773 0a20 2020 2023 2068 7474 7073  dows.    # https",
            "+000022f0: 3a2f 2f62 7567 732e 7079 7468 6f6e 2e6f  ://bugs.python.o",
            "+00002300: 7267 2f69 7373 7565 3338 3637 310a 2020  rg/issue38671.  ",
            "+00002310: 2020 6966 2073 7973 2e70 6c61 7466 6f72    if sys.platfor",
            "+00002320: 6d20 3d3d 2022 7769 6e33 3222 3a0a 2020  m == \"win32\":.  ",
            "+00002330: 2020 2020 2020 7061 7468 203d 206f 732e        path = os.",
            "+00002340: 7061 7468 2e61 6273 7061 7468 2870 6174  path.abspath(pat",
            "+00002350: 6829 0a0a 2020 2020 7061 7468 203d 2050  h)..    path = P",
            "+00002360: 6174 6828 7061 7468 290a 2020 2020 7265  ath(path).    re",
            "+00002370: 736f 6c76 6564 5f70 6174 6820 3d20 7061  solved_path = pa",
            "+00002380: 7468 2e72 6573 6f6c 7665 2829 0a0a 2020  th.resolve()..  ",
            "+00002390: 2020 2320 5265 736f 6c76 6520 616e 6420    # Resolve and ",
            "+000023a0: 6162 7370 6174 6820 7365 656d 7320 746f  abspath seems to",
            "+000023b0: 2062 6568 6176 6520 6469 6666 6572 656e   behave differen",
            "+000023c0: 746c 7920 7265 6761 7264 696e 6720 7379  tly regarding sy",
            "+000023d0: 6d6c 696e 6b73 2c0a 2020 2020 2320 6173  mlinks,.    # as",
            "+000023e0: 2077 6520 6172 6520 646f 696e 6720 6162   we are doing ab",
            "+000023f0: 7370 6174 6820 6f6e 2074 6865 2066 696c  spath on the fil",
            "+00002400: 6520 7061 7468 2c20 7765 206e 6565 6420  e path, we need ",
            "+00002410: 746f 2064 6f20 7468 6520 7361 6d65 206f  to do the same o",
            "+00002420: 6e0a 2020 2020 2320 7468 6520 7265 706f  n.    # the repo",
            "+00002430: 2070 6174 6820 6f72 2074 6865 7920 6d69   path or they mi",
            "+00002440: 6768 7420 6e6f 7420 6d61 7463 680a 2020  ght not match.  ",
            "+00002450: 2020 6966 2073 7973 2e70 6c61 7466 6f72    if sys.platfor",
            "+00002460: 6d20 3d3d 2022 7769 6e33 3222 3a0a 2020  m == \"win32\":.  ",
            "+00002470: 2020 2020 2020 7265 706f 7061 7468 203d        repopath =",
            "+00002480: 206f 732e 7061 7468 2e61 6273 7061 7468   os.path.abspath",
            "+00002490: 2872 6570 6f70 6174 6829 0a0a 2020 2020  (repopath)..    ",
            "+000024a0: 7265 706f 7061 7468 203d 2050 6174 6828  repopath = Path(",
            "+000024b0: 7265 706f 7061 7468 292e 7265 736f 6c76  repopath).resolv",
            "+000024c0: 6528 290a 0a20 2020 2074 7279 3a0a 2020  e()..    try:.  ",
            "+000024d0: 2020 2020 2020 7265 6c70 6174 6820 3d20        relpath = ",
            "+000024e0: 7265 736f 6c76 6564 5f70 6174 682e 7265  resolved_path.re",
            "+000024f0: 6c61 7469 7665 5f74 6f28 7265 706f 7061  lative_to(repopa",
            "+00002500: 7468 290a 2020 2020 6578 6365 7074 2056  th).    except V",
            "+00002510: 616c 7565 4572 726f 723a 0a20 2020 2020  alueError:.     ",
            "+00002520: 2020 2023 2049 6620 7061 7468 2069 7320     # If path is ",
            "+00002530: 6120 7379 6d6c 696e 6b20 7468 6174 2070  a symlink that p",
            "+00002540: 6f69 6e74 7320 746f 2061 2066 696c 6520  oints to a file ",
            "+00002550: 6f75 7473 6964 6520 7468 6520 7265 706f  outside the repo",
            "+00002560: 2c20 7765 0a20 2020 2020 2020 2023 2077  , we.        # w",
            "+00002570: 616e 7420 7468 6520 7265 6c70 6174 6820  ant the relpath ",
            "+00002580: 666f 7220 7468 6520 6c69 6e6b 2069 7473  for the link its",
            "+00002590: 656c 662c 206e 6f74 2074 6865 2072 6573  elf, not the res",
            "+000025a0: 6f6c 7665 6420 7461 7267 6574 0a20 2020  olved target.   ",
            "+000025b0: 2020 2020 2069 6620 7061 7468 2e69 735f       if path.is_",
            "+000025c0: 7379 6d6c 696e 6b28 293a 0a20 2020 2020  symlink():.     ",
            "+000025d0: 2020 2020 2020 2070 6172 656e 7420 3d20         parent = ",
            "+000025e0: 7061 7468 2e70 6172 656e 742e 7265 736f  path.parent.reso",
            "+000025f0: 6c76 6528 290a 2020 2020 2020 2020 2020  lve().          ",
            "+00002600: 2020 7265 6c70 6174 6820 3d20 2870 6172    relpath = (par",
            "+00002610: 656e 7420 2f20 7061 7468 2e6e 616d 6529  ent / path.name)",
            "+00002620: 2e72 656c 6174 6976 655f 746f 2872 6570  .relative_to(rep",
            "+00002630: 6f70 6174 6829 0a20 2020 2020 2020 2065  opath).        e",
            "+00002640: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           ",
            "+00002650: 2072 6169 7365 0a20 2020 2069 6620 7379   raise.    if sy",
            "+00002660: 732e 706c 6174 666f 726d 203d 3d20 2277  s.platform == \"w",
            "+00002670: 696e 3332 223a 0a20 2020 2020 2020 2072  in32\":.        r",
            "+00002680: 6574 7572 6e20 7374 7228 7265 6c70 6174  eturn str(relpat",
            "+00002690: 6829 2e72 6570 6c61 6365 286f 732e 7061  h).replace(os.pa",
            "+000026a0: 7468 2e73 6570 2c20 222f 2229 2e65 6e63  th.sep, \"/\").enc",
            "+000026b0: 6f64 6528 7472 6565 5f65 6e63 6f64 696e  ode(tree_encodin",
            "+000026c0: 6729 0a20 2020 2065 6c73 653a 0a20 2020  g).    else:.   ",
            "+000026d0: 2020 2020 2072 6574 7572 6e20 6279 7465       return byte",
            "+000026e0: 7328 7265 6c70 6174 6829 0a0a 0a63 6c61  s(relpath)...cla",
            "+000026f0: 7373 2044 6976 6572 6765 6442 7261 6e63  ss DivergedBranc",
            "+00002700: 6865 7328 4572 726f 7229 3a0a 2020 2020  hes(Error):.    ",
            "+00002710: 2222 2242 7261 6e63 6865 7320 6861 7665  \"\"\"Branches have",
            "+00002720: 2064 6976 6572 6765 6420 616e 6420 6661   diverged and fa",
            "+00002730: 7374 2d66 6f72 7761 7264 2069 7320 6e6f  st-forward is no",
            "+00002740: 7420 706f 7373 6962 6c65 2e22 2222 0a0a  t possible.\"\"\"..",
            "+00002750: 2020 2020 6465 6620 5f5f 696e 6974 5f5f      def __init__",
            "+00002760: 2873 656c 662c 2063 7572 7265 6e74 5f73  (self, current_s",
            "+00002770: 6861 2c20 6e65 775f 7368 6129 202d 3e20  ha, new_sha) -> ",
            "+00002780: 4e6f 6e65 3a0a 2020 2020 2020 2020 7365  None:.        se",
            "+00002790: 6c66 2e63 7572 7265 6e74 5f73 6861 203d  lf.current_sha =",
            "+000027a0: 2063 7572 7265 6e74 5f73 6861 0a20 2020   current_sha.   ",
            "+000027b0: 2020 2020 2073 656c 662e 6e65 775f 7368       self.new_sh",
            "+000027c0: 6120 3d20 6e65 775f 7368 610a 0a0a 6465  a = new_sha...de",
            "+000027d0: 6620 6368 6563 6b5f 6469 7665 7267 6564  f check_diverged",
            "+000027e0: 2872 6570 6f2c 2063 7572 7265 6e74 5f73  (repo, current_s",
            "+000027f0: 6861 2c20 6e65 775f 7368 6129 202d 3e20  ha, new_sha) -> ",
            "+00002800: 4e6f 6e65 3a0a 2020 2020 2222 2243 6865  None:.    \"\"\"Che",
            "+00002810: 636b 2069 6620 7570 6461 7469 6e67 2074  ck if updating t",
            "+00002820: 6f20 6120 7368 6120 6361 6e20 6265 2064  o a sha can be d",
            "+00002830: 6f6e 6520 7769 7468 2066 6173 7420 666f  one with fast fo",
            "+00002840: 7277 6172 6469 6e67 2e0a 0a20 2020 2041  rwarding...    A",
            "+00002850: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "+00002860: 2052 6570 6f73 6974 6f72 7920 6f62 6a65   Repository obje",
            "+00002870: 6374 0a20 2020 2020 2063 7572 7265 6e74  ct.      current",
            "+00002880: 5f73 6861 3a20 4375 7272 656e 7420 6865  _sha: Current he",
            "+00002890: 6164 2073 6861 0a20 2020 2020 206e 6577  ad sha.      new",
            "+000028a0: 5f73 6861 3a20 4e65 7720 6865 6164 2073  _sha: New head s",
            "+000028b0: 6861 0a20 2020 2022 2222 0a20 2020 2074  ha.    \"\"\".    t",
            "+000028c0: 7279 3a0a 2020 2020 2020 2020 6361 6e20  ry:.        can ",
            "+000028d0: 3d20 6361 6e5f 6661 7374 5f66 6f72 7761  = can_fast_forwa",
            "+000028e0: 7264 2872 6570 6f2c 2063 7572 7265 6e74  rd(repo, current",
            "+000028f0: 5f73 6861 2c20 6e65 775f 7368 6129 0a20  _sha, new_sha). ",
            "+00002900: 2020 2065 7863 6570 7420 4b65 7945 7272     except KeyErr",
            "+00002910: 6f72 3a0a 2020 2020 2020 2020 6361 6e20  or:.        can ",
            "+00002920: 3d20 4661 6c73 650a 2020 2020 6966 206e  = False.    if n",
            "+00002930: 6f74 2063 616e 3a0a 2020 2020 2020 2020  ot can:.        ",
            "+00002940: 7261 6973 6520 4469 7665 7267 6564 4272  raise DivergedBr",
            "+00002950: 616e 6368 6573 2863 7572 7265 6e74 5f73  anches(current_s",
            "+00002960: 6861 2c20 6e65 775f 7368 6129 0a0a 0a64  ha, new_sha)...d",
            "+00002970: 6566 2061 7263 6869 7665 280a 2020 2020  ef archive(.    ",
            "+00002980: 7265 706f 2c0a 2020 2020 636f 6d6d 6974  repo,.    commit",
            "+00002990: 7469 7368 3d4e 6f6e 652c 0a20 2020 206f  tish=None,.    o",
            "+000029a0: 7574 7374 7265 616d 3d64 6566 6175 6c74  utstream=default",
            "+000029b0: 5f62 7974 6573 5f6f 7574 5f73 7472 6561  _bytes_out_strea",
            "+000029c0: 6d2c 0a20 2020 2065 7272 7374 7265 616d  m,.    errstream",
            "+000029d0: 3d64 6566 6175 6c74 5f62 7974 6573 5f65  =default_bytes_e",
            "+000029e0: 7272 5f73 7472 6561 6d2c 0a29 202d 3e20  rr_stream,.) -> ",
            "+000029f0: 4e6f 6e65 3a0a 2020 2020 2222 2243 7265  None:.    \"\"\"Cre",
            "+00002a00: 6174 6520 616e 2061 7263 6869 7665 2e0a  ate an archive..",
            "+00002a10: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "+00002a20: 2072 6570 6f3a 2050 6174 6820 6f66 2072   repo: Path of r",
            "+00002a30: 6570 6f73 6974 6f72 7920 666f 7220 7768  epository for wh",
            "+00002a40: 6963 6820 746f 2067 656e 6572 6174 6520  ich to generate ",
            "+00002a50: 616e 2061 7263 6869 7665 2e0a 2020 2020  an archive..    ",
            "+00002a60: 2020 636f 6d6d 6974 7469 7368 3a20 436f    committish: Co",
            "+00002a70: 6d6d 6974 2053 4841 3120 6f72 2072 6566  mmit SHA1 or ref",
            "+00002a80: 2074 6f20 7573 650a 2020 2020 2020 6f75   to use.      ou",
            "+00002a90: 7473 7472 6561 6d3a 204f 7574 7075 7420  tstream: Output ",
            "+00002aa0: 7374 7265 616d 2028 6465 6661 756c 7473  stream (defaults",
            "+00002ab0: 2074 6f20 7374 646f 7574 290a 2020 2020   to stdout).    ",
            "+00002ac0: 2020 6572 7273 7472 6561 6d3a 2045 7272    errstream: Err",
            "+00002ad0: 6f72 2073 7472 6561 6d20 2864 6566 6175  or stream (defau",
            "+00002ae0: 6c74 7320 746f 2073 7464 6572 7229 0a20  lts to stderr). ",
            "+00002af0: 2020 2022 2222 0a20 2020 2069 6620 636f     \"\"\".    if co",
            "+00002b00: 6d6d 6974 7469 7368 2069 7320 4e6f 6e65  mmittish is None",
            "+00002b10: 3a0a 2020 2020 2020 2020 636f 6d6d 6974  :.        commit",
            "+00002b20: 7469 7368 203d 2022 4845 4144 220a 2020  tish = \"HEAD\".  ",
            "+00002b30: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+00002b40: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+00002b50: 7320 7265 706f 5f6f 626a 3a0a 2020 2020  s repo_obj:.    ",
            "+00002b60: 2020 2020 6320 3d20 7061 7273 655f 636f      c = parse_co",
            "+00002b70: 6d6d 6974 2872 6570 6f5f 6f62 6a2c 2063  mmit(repo_obj, c",
            "+00002b80: 6f6d 6d69 7474 6973 6829 0a20 2020 2020  ommittish).     ",
            "+00002b90: 2020 2066 6f72 2063 6875 6e6b 2069 6e20     for chunk in ",
            "+00002ba0: 7461 725f 7374 7265 616d 280a 2020 2020  tar_stream(.    ",
            "+00002bb0: 2020 2020 2020 2020 7265 706f 5f6f 626a          repo_obj",
            "+00002bc0: 2e6f 626a 6563 745f 7374 6f72 652c 2072  .object_store, r",
            "+00002bd0: 6570 6f5f 6f62 6a2e 6f62 6a65 6374 5f73  epo_obj.object_s",
            "+00002be0: 746f 7265 5b63 2e74 7265 655d 2c20 632e  tore[c.tree], c.",
            "+00002bf0: 636f 6d6d 6974 5f74 696d 650a 2020 2020  commit_time.    ",
            "+00002c00: 2020 2020 293a 0a20 2020 2020 2020 2020      ):.         ",
            "+00002c10: 2020 206f 7574 7374 7265 616d 2e77 7269     outstream.wri",
            "+00002c20: 7465 2863 6875 6e6b 290a 0a0a 6465 6620  te(chunk)...def ",
            "+00002c30: 7570 6461 7465 5f73 6572 7665 725f 696e  update_server_in",
            "+00002c40: 666f 2872 6570 6f3d 222e 2229 202d 3e20  fo(repo=\".\") -> ",
            "+00002c50: 4e6f 6e65 3a0a 2020 2020 2222 2255 7064  None:.    \"\"\"Upd",
            "+00002c60: 6174 6520 7365 7276 6572 2069 6e66 6f20  ate server info ",
            "+00002c70: 6669 6c65 7320 666f 7220 6120 7265 706f  files for a repo",
            "+00002c80: 7369 746f 7279 2e0a 0a20 2020 2041 7267  sitory...    Arg",
            "+00002c90: 733a 0a20 2020 2020 2072 6570 6f3a 2070  s:.      repo: p",
            "+00002ca0: 6174 6820 746f 2074 6865 2072 6570 6f73  ath to the repos",
            "+00002cb0: 6974 6f72 790a 2020 2020 2222 220a 2020  itory.    \"\"\".  ",
            "+00002cc0: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+00002cd0: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+00002ce0: 7320 723a 0a20 2020 2020 2020 2073 6572  s r:.        ser",
            "+00002cf0: 7665 725f 7570 6461 7465 5f73 6572 7665  ver_update_serve",
            "+00002d00: 725f 696e 666f 2872 290a 0a0a 6465 6620  r_info(r)...def ",
            "+00002d10: 7379 6d62 6f6c 6963 5f72 6566 2872 6570  symbolic_ref(rep",
            "+00002d20: 6f2c 2072 6566 5f6e 616d 652c 2066 6f72  o, ref_name, for",
            "+00002d30: 6365 3d46 616c 7365 2920 2d3e 204e 6f6e  ce=False) -> Non",
            "+00002d40: 653a 0a20 2020 2022 2222 5365 7420 6769  e:.    \"\"\"Set gi",
            "+00002d50: 7420 7379 6d62 6f6c 6963 2072 6566 2069  t symbolic ref i",
            "+00002d60: 6e74 6f20 4845 4144 2e0a 0a20 2020 2041  nto HEAD...    A",
            "+00002d70: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "+00002d80: 2070 6174 6820 746f 2074 6865 2072 6570   path to the rep",
            "+00002d90: 6f73 6974 6f72 790a 2020 2020 2020 7265  ository.      re",
            "+00002da0: 665f 6e61 6d65 3a20 7368 6f72 7420 6e61  f_name: short na",
            "+00002db0: 6d65 206f 6620 7468 6520 6e65 7720 7265  me of the new re",
            "+00002dc0: 660a 2020 2020 2020 666f 7263 653a 2066  f.      force: f",
            "+00002dd0: 6f72 6365 2073 6574 7469 6e67 7320 7769  orce settings wi",
            "+00002de0: 7468 6f75 7420 6368 6563 6b69 6e67 2069  thout checking i",
            "+00002df0: 6620 6974 2065 7869 7374 7320 696e 2072  f it exists in r",
            "+00002e00: 6566 732f 6865 6164 730a 2020 2020 2222  efs/heads.    \"\"",
            "+00002e10: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "+00002e20: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+00002e30: 6f29 2061 7320 7265 706f 5f6f 626a 3a0a  o) as repo_obj:.",
            "+00002e40: 2020 2020 2020 2020 7265 665f 7061 7468          ref_path",
            "+00002e50: 203d 205f 6d61 6b65 5f62 7261 6e63 685f   = _make_branch_",
            "+00002e60: 7265 6628 7265 665f 6e61 6d65 290a 2020  ref(ref_name).  ",
            "+00002e70: 2020 2020 2020 6966 206e 6f74 2066 6f72        if not for",
            "+00002e80: 6365 2061 6e64 2072 6566 5f70 6174 6820  ce and ref_path ",
            "+00002e90: 6e6f 7420 696e 2072 6570 6f5f 6f62 6a2e  not in repo_obj.",
            "+00002ea0: 7265 6673 2e6b 6579 7328 293a 0a20 2020  refs.keys():.   ",
            "+00002eb0: 2020 2020 2020 2020 2072 6169 7365 2045           raise E",
            "+00002ec0: 7272 6f72 2866 2266 6174 616c 3a20 7265  rror(f\"fatal: re",
            "+00002ed0: 6620 607b 7265 665f 6e61 6d65 7d60 2069  f `{ref_name}` i",
            "+00002ee0: 7320 6e6f 7420 6120 7265 6622 290a 2020  s not a ref\").  ",
            "+00002ef0: 2020 2020 2020 7265 706f 5f6f 626a 2e72        repo_obj.r",
            "+00002f00: 6566 732e 7365 745f 7379 6d62 6f6c 6963  efs.set_symbolic",
            "+00002f10: 5f72 6566 2862 2248 4541 4422 2c20 7265  _ref(b\"HEAD\", re",
            "+00002f20: 665f 7061 7468 290a 0a0a 6465 6620 7061  f_path)...def pa",
            "+00002f30: 636b 5f72 6566 7328 7265 706f 2c20 616c  ck_refs(repo, al",
            "+00002f40: 6c3d 4661 6c73 6529 202d 3e20 4e6f 6e65  l=False) -> None",
            "+00002f50: 3a0a 2020 2020 7769 7468 206f 7065 6e5f  :.    with open_",
            "+00002f60: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+00002f70: 6f29 2061 7320 7265 706f 5f6f 626a 3a0a  o) as repo_obj:.",
            "+00002f80: 2020 2020 2020 2020 7265 706f 5f6f 626a          repo_obj",
            "+00002f90: 2e72 6566 732e 7061 636b 5f72 6566 7328  .refs.pack_refs(",
            "+00002fa0: 616c 6c3d 616c 6c29 0a0a 0a64 6566 2063  all=all)...def c",
            "+00002fb0: 6f6d 6d69 7428 0a20 2020 2072 6570 6f3d  ommit(.    repo=",
            "+00002fc0: 222e 222c 0a20 2020 206d 6573 7361 6765  \".\",.    message",
            "+00002fd0: 3d4e 6f6e 652c 0a20 2020 2061 7574 686f  =None,.    autho",
            "+00002fe0: 723d 4e6f 6e65 2c0a 2020 2020 6175 7468  r=None,.    auth",
            "+00002ff0: 6f72 5f74 696d 657a 6f6e 653d 4e6f 6e65  or_timezone=None",
            "+00003000: 2c0a 2020 2020 636f 6d6d 6974 7465 723d  ,.    committer=",
            "+00003010: 4e6f 6e65 2c0a 2020 2020 636f 6d6d 6974  None,.    commit",
            "+00003020: 5f74 696d 657a 6f6e 653d 4e6f 6e65 2c0a  _timezone=None,.",
            "+00003030: 2020 2020 656e 636f 6469 6e67 3d4e 6f6e      encoding=Non",
            "+00003040: 652c 0a20 2020 206e 6f5f 7665 7269 6679  e,.    no_verify",
            "+00003050: 3d46 616c 7365 2c0a 2020 2020 7369 676e  =False,.    sign",
            "+00003060: 6f66 663d 4661 6c73 652c 0a29 3a0a 2020  off=False,.):.  ",
            "+00003070: 2020 2222 2243 7265 6174 6520 6120 6e65    \"\"\"Create a ne",
            "+00003080: 7720 636f 6d6d 6974 2e0a 0a20 2020 2041  w commit...    A",
            "+00003090: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "+000030a0: 2050 6174 6820 746f 2072 6570 6f73 6974   Path to reposit",
            "+000030b0: 6f72 790a 2020 2020 2020 6d65 7373 6167  ory.      messag",
            "+000030c0: 653a 204f 7074 696f 6e61 6c20 636f 6d6d  e: Optional comm",
            "+000030d0: 6974 206d 6573 7361 6765 0a20 2020 2020  it message.     ",
            "+000030e0: 2061 7574 686f 723a 204f 7074 696f 6e61   author: Optiona",
            "+000030f0: 6c20 6175 7468 6f72 206e 616d 6520 616e  l author name an",
            "+00003100: 6420 656d 6169 6c0a 2020 2020 2020 6175  d email.      au",
            "+00003110: 7468 6f72 5f74 696d 657a 6f6e 653a 2041  thor_timezone: A",
            "+00003120: 7574 686f 7220 7469 6d65 7374 616d 7020  uthor timestamp ",
            "+00003130: 7469 6d65 7a6f 6e65 0a20 2020 2020 2063  timezone.      c",
            "+00003140: 6f6d 6d69 7474 6572 3a20 4f70 7469 6f6e  ommitter: Option",
            "+00003150: 616c 2063 6f6d 6d69 7474 6572 206e 616d  al committer nam",
            "+00003160: 6520 616e 6420 656d 6169 6c0a 2020 2020  e and email.    ",
            "+00003170: 2020 636f 6d6d 6974 5f74 696d 657a 6f6e    commit_timezon",
            "+00003180: 653a 2043 6f6d 6d69 7420 7469 6d65 7374  e: Commit timest",
            "+00003190: 616d 7020 7469 6d65 7a6f 6e65 0a20 2020  amp timezone.   ",
            "+000031a0: 2020 206e 6f5f 7665 7269 6679 3a20 536b     no_verify: Sk",
            "+000031b0: 6970 2070 7265 2d63 6f6d 6d69 7420 616e  ip pre-commit an",
            "+000031c0: 6420 636f 6d6d 6974 2d6d 7367 2068 6f6f  d commit-msg hoo",
            "+000031d0: 6b73 0a20 2020 2020 2073 6967 6e6f 6666  ks.      signoff",
            "+000031e0: 3a20 4750 4720 5369 676e 2074 6865 2063  : GPG Sign the c",
            "+000031f0: 6f6d 6d69 7420 2862 6f6f 6c2c 2064 6566  ommit (bool, def",
            "+00003200: 6175 6c74 7320 746f 2046 616c 7365 2c0a  aults to False,.",
            "+00003210: 2020 2020 2020 2020 7061 7373 2054 7275          pass Tru",
            "+00003220: 6520 746f 2075 7365 2064 6566 6175 6c74  e to use default",
            "+00003230: 2047 5047 206b 6579 2c0a 2020 2020 2020   GPG key,.      ",
            "+00003240: 2020 7061 7373 2061 2073 7472 2063 6f6e    pass a str con",
            "+00003250: 7461 696e 696e 6720 4b65 7920 4944 2074  taining Key ID t",
            "+00003260: 6f20 7573 6520 6120 7370 6563 6966 6963  o use a specific",
            "+00003270: 2047 5047 206b 6579 290a 2020 2020 5265   GPG key).    Re",
            "+00003280: 7475 726e 733a 2053 4841 3120 6f66 2074  turns: SHA1 of t",
            "+00003290: 6865 206e 6577 2063 6f6d 6d69 740a 2020  he new commit.  ",
            "+000032a0: 2020 2222 220a 2020 2020 2320 4649 584d    \"\"\".    # FIXM",
            "+000032b0: 453a 2053 7570 706f 7274 202d 2d61 6c6c  E: Support --all",
            "+000032c0: 2061 7267 756d 656e 740a 2020 2020 6966   argument.    if",
            "+000032d0: 2067 6574 6174 7472 286d 6573 7361 6765   getattr(message",
            "+000032e0: 2c20 2265 6e63 6f64 6522 2c20 4e6f 6e65  , \"encode\", None",
            "+000032f0: 293a 0a20 2020 2020 2020 206d 6573 7361  ):.        messa",
            "+00003300: 6765 203d 206d 6573 7361 6765 2e65 6e63  ge = message.enc",
            "+00003310: 6f64 6528 656e 636f 6469 6e67 206f 7220  ode(encoding or ",
            "+00003320: 4445 4641 554c 545f 454e 434f 4449 4e47  DEFAULT_ENCODING",
            "+00003330: 290a 2020 2020 6966 2067 6574 6174 7472  ).    if getattr",
            "+00003340: 2861 7574 686f 722c 2022 656e 636f 6465  (author, \"encode",
            "+00003350: 222c 204e 6f6e 6529 3a0a 2020 2020 2020  \", None):.      ",
            "+00003360: 2020 6175 7468 6f72 203d 2061 7574 686f    author = autho",
            "+00003370: 722e 656e 636f 6465 2865 6e63 6f64 696e  r.encode(encodin",
            "+00003380: 6720 6f72 2044 4546 4155 4c54 5f45 4e43  g or DEFAULT_ENC",
            "+00003390: 4f44 494e 4729 0a20 2020 2069 6620 6765  ODING).    if ge",
            "+000033a0: 7461 7474 7228 636f 6d6d 6974 7465 722c  tattr(committer,",
            "+000033b0: 2022 656e 636f 6465 222c 204e 6f6e 6529   \"encode\", None)",
            "+000033c0: 3a0a 2020 2020 2020 2020 636f 6d6d 6974  :.        commit",
            "+000033d0: 7465 7220 3d20 636f 6d6d 6974 7465 722e  ter = committer.",
            "+000033e0: 656e 636f 6465 2865 6e63 6f64 696e 6720  encode(encoding ",
            "+000033f0: 6f72 2044 4546 4155 4c54 5f45 4e43 4f44  or DEFAULT_ENCOD",
            "+00003400: 494e 4729 0a20 2020 206c 6f63 616c 5f74  ING).    local_t",
            "+00003410: 696d 657a 6f6e 6520 3d20 6765 745f 7573  imezone = get_us",
            "+00003420: 6572 5f74 696d 657a 6f6e 6573 2829 0a20  er_timezones(). ",
            "+00003430: 2020 2069 6620 6175 7468 6f72 5f74 696d     if author_tim",
            "+00003440: 657a 6f6e 6520 6973 204e 6f6e 653a 0a20  ezone is None:. ",
            "+00003450: 2020 2020 2020 2061 7574 686f 725f 7469         author_ti",
            "+00003460: 6d65 7a6f 6e65 203d 206c 6f63 616c 5f74  mezone = local_t",
            "+00003470: 696d 657a 6f6e 655b 305d 0a20 2020 2069  imezone[0].    i",
            "+00003480: 6620 636f 6d6d 6974 5f74 696d 657a 6f6e  f commit_timezon",
            "+00003490: 6520 6973 204e 6f6e 653a 0a20 2020 2020  e is None:.     ",
            "+000034a0: 2020 2063 6f6d 6d69 745f 7469 6d65 7a6f     commit_timezo",
            "+000034b0: 6e65 203d 206c 6f63 616c 5f74 696d 657a  ne = local_timez",
            "+000034c0: 6f6e 655b 315d 0a20 2020 2077 6974 6820  one[1].    with ",
            "+000034d0: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "+000034e0: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "+000034f0: 2020 2020 2020 7265 7475 726e 2072 2e64        return r.d",
            "+00003500: 6f5f 636f 6d6d 6974 280a 2020 2020 2020  o_commit(.      ",
            "+00003510: 2020 2020 2020 6d65 7373 6167 653d 6d65        message=me",
            "+00003520: 7373 6167 652c 0a20 2020 2020 2020 2020  ssage,.         ",
            "+00003530: 2020 2061 7574 686f 723d 6175 7468 6f72     author=author",
            "+00003540: 2c0a 2020 2020 2020 2020 2020 2020 6175  ,.            au",
            "+00003550: 7468 6f72 5f74 696d 657a 6f6e 653d 6175  thor_timezone=au",
            "+00003560: 7468 6f72 5f74 696d 657a 6f6e 652c 0a20  thor_timezone,. ",
            "+00003570: 2020 2020 2020 2020 2020 2063 6f6d 6d69             commi",
            "+00003580: 7474 6572 3d63 6f6d 6d69 7474 6572 2c0a  tter=committer,.",
            "+00003590: 2020 2020 2020 2020 2020 2020 636f 6d6d              comm",
            "+000035a0: 6974 5f74 696d 657a 6f6e 653d 636f 6d6d  it_timezone=comm",
            "+000035b0: 6974 5f74 696d 657a 6f6e 652c 0a20 2020  it_timezone,.   ",
            "+000035c0: 2020 2020 2020 2020 2065 6e63 6f64 696e           encodin",
            "+000035d0: 673d 656e 636f 6469 6e67 2c0a 2020 2020  g=encoding,.    ",
            "+000035e0: 2020 2020 2020 2020 6e6f 5f76 6572 6966          no_verif",
            "+000035f0: 793d 6e6f 5f76 6572 6966 792c 0a20 2020  y=no_verify,.   ",
            "+00003600: 2020 2020 2020 2020 2073 6967 6e3d 7369           sign=si",
            "+00003610: 676e 6f66 6620 6966 2069 7369 6e73 7461  gnoff if isinsta",
            "+00003620: 6e63 6528 7369 676e 6f66 662c 2028 7374  nce(signoff, (st",
            "+00003630: 722c 2062 6f6f 6c29 2920 656c 7365 204e  r, bool)) else N",
            "+00003640: 6f6e 652c 0a20 2020 2020 2020 2029 0a0a  one,.        )..",
            "+00003650: 0a64 6566 2063 6f6d 6d69 745f 7472 6565  .def commit_tree",
            "+00003660: 2872 6570 6f2c 2074 7265 652c 206d 6573  (repo, tree, mes",
            "+00003670: 7361 6765 3d4e 6f6e 652c 2061 7574 686f  sage=None, autho",
            "+00003680: 723d 4e6f 6e65 2c20 636f 6d6d 6974 7465  r=None, committe",
            "+00003690: 723d 4e6f 6e65 293a 0a20 2020 2022 2222  r=None):.    \"\"\"",
            "+000036a0: 4372 6561 7465 2061 206e 6577 2063 6f6d  Create a new com",
            "+000036b0: 6d69 7420 6f62 6a65 6374 2e0a 0a20 2020  mit object...   ",
            "+000036c0: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "+000036d0: 6f3a 2050 6174 6820 746f 2072 6570 6f73  o: Path to repos",
            "+000036e0: 6974 6f72 790a 2020 2020 2020 7472 6565  itory.      tree",
            "+000036f0: 3a20 416e 2065 7869 7374 696e 6720 7472  : An existing tr",
            "+00003700: 6565 206f 626a 6563 740a 2020 2020 2020  ee object.      ",
            "+00003710: 6175 7468 6f72 3a20 4f70 7469 6f6e 616c  author: Optional",
            "+00003720: 2061 7574 686f 7220 6e61 6d65 2061 6e64   author name and",
            "+00003730: 2065 6d61 696c 0a20 2020 2020 2063 6f6d   email.      com",
            "+00003740: 6d69 7474 6572 3a20 4f70 7469 6f6e 616c  mitter: Optional",
            "+00003750: 2063 6f6d 6d69 7474 6572 206e 616d 6520   committer name ",
            "+00003760: 616e 6420 656d 6169 6c0a 2020 2020 2222  and email.    \"\"",
            "+00003770: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "+00003780: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+00003790: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "+000037a0: 2072 6574 7572 6e20 722e 646f 5f63 6f6d   return r.do_com",
            "+000037b0: 6d69 7428 0a20 2020 2020 2020 2020 2020  mit(.           ",
            "+000037c0: 206d 6573 7361 6765 3d6d 6573 7361 6765   message=message",
            "+000037d0: 2c20 7472 6565 3d74 7265 652c 2063 6f6d  , tree=tree, com",
            "+000037e0: 6d69 7474 6572 3d63 6f6d 6d69 7474 6572  mitter=committer",
            "+000037f0: 2c20 6175 7468 6f72 3d61 7574 686f 720a  , author=author.",
            "+00003800: 2020 2020 2020 2020 290a 0a0a 6465 6620          )...def ",
            "+00003810: 696e 6974 280a 2020 2020 7061 7468 3a20  init(.    path: ",
            "+00003820: 556e 696f 6e5b 7374 722c 206f 732e 5061  Union[str, os.Pa",
            "+00003830: 7468 4c69 6b65 5d20 3d20 222e 222c 202a  thLike] = \".\", *",
            "+00003840: 2c20 6261 7265 3d46 616c 7365 2c20 7379  , bare=False, sy",
            "+00003850: 6d6c 696e 6b73 3a20 4f70 7469 6f6e 616c  mlinks: Optional",
            "+00003860: 5b62 6f6f 6c5d 203d 204e 6f6e 650a 293a  [bool] = None.):",
            "+00003870: 0a20 2020 2022 2222 4372 6561 7465 2061  .    \"\"\"Create a",
            "+00003880: 206e 6577 2067 6974 2072 6570 6f73 6974   new git reposit",
            "+00003890: 6f72 792e 0a0a 2020 2020 4172 6773 3a0a  ory...    Args:.",
            "+000038a0: 2020 2020 2020 7061 7468 3a20 5061 7468        path: Path",
            "+000038b0: 2074 6f20 7265 706f 7369 746f 7279 2e0a   to repository..",
            "+000038c0: 2020 2020 2020 6261 7265 3a20 5768 6574        bare: Whet",
            "+000038d0: 6865 7220 746f 2063 7265 6174 6520 6120  her to create a ",
            "+000038e0: 6261 7265 2072 6570 6f73 6974 6f72 792e  bare repository.",
            "+000038f0: 0a20 2020 2020 2073 796d 6c69 6e6b 733a  .      symlinks:",
            "+00003900: 2057 6865 7468 6572 2074 6f20 6372 6561   Whether to crea",
            "+00003910: 7465 2061 6374 7561 6c20 7379 6d6c 696e  te actual symlin",
            "+00003920: 6b73 2028 6465 6661 756c 7473 2074 6f20  ks (defaults to ",
            "+00003930: 6175 746f 6465 7465 6374 290a 2020 2020  autodetect).    ",
            "+00003940: 5265 7475 726e 733a 2041 2052 6570 6f20  Returns: A Repo ",
            "+00003950: 696e 7374 616e 6365 0a20 2020 2022 2222  instance.    \"\"\"",
            "+00003960: 0a20 2020 2069 6620 6e6f 7420 6f73 2e70  .    if not os.p",
            "+00003970: 6174 682e 6578 6973 7473 2870 6174 6829  ath.exists(path)",
            "+00003980: 3a0a 2020 2020 2020 2020 6f73 2e6d 6b64  :.        os.mkd",
            "+00003990: 6972 2870 6174 6829 0a0a 2020 2020 6966  ir(path)..    if",
            "+000039a0: 2062 6172 653a 0a20 2020 2020 2020 2072   bare:.        r",
            "+000039b0: 6574 7572 6e20 5265 706f 2e69 6e69 745f  eturn Repo.init_",
            "+000039c0: 6261 7265 2870 6174 6829 0a20 2020 2065  bare(path).    e",
            "+000039d0: 6c73 653a 0a20 2020 2020 2020 2072 6574  lse:.        ret",
            "+000039e0: 7572 6e20 5265 706f 2e69 6e69 7428 7061  urn Repo.init(pa",
            "+000039f0: 7468 2c20 7379 6d6c 696e 6b73 3d73 796d  th, symlinks=sym",
            "+00003a00: 6c69 6e6b 7329 0a0a 0a64 6566 2063 6c6f  links)...def clo",
            "+00003a10: 6e65 280a 2020 2020 736f 7572 6365 2c0a  ne(.    source,.",
            "+00003a20: 2020 2020 7461 7267 6574 3a20 4f70 7469      target: Opti",
            "+00003a30: 6f6e 616c 5b55 6e69 6f6e 5b73 7472 2c20  onal[Union[str, ",
            "+00003a40: 6f73 2e50 6174 684c 696b 655d 5d20 3d20  os.PathLike]] = ",
            "+00003a50: 4e6f 6e65 2c0a 2020 2020 6261 7265 3d46  None,.    bare=F",
            "+00003a60: 616c 7365 2c0a 2020 2020 6368 6563 6b6f  alse,.    checko",
            "+00003a70: 7574 3d4e 6f6e 652c 0a20 2020 2065 7272  ut=None,.    err",
            "+00003a80: 7374 7265 616d 3d64 6566 6175 6c74 5f62  stream=default_b",
            "+00003a90: 7974 6573 5f65 7272 5f73 7472 6561 6d2c  ytes_err_stream,",
            "+00003aa0: 0a20 2020 206f 7574 7374 7265 616d 3d4e  .    outstream=N",
            "+00003ab0: 6f6e 652c 0a20 2020 206f 7269 6769 6e3a  one,.    origin:",
            "+00003ac0: 204f 7074 696f 6e61 6c5b 7374 725d 203d   Optional[str] =",
            "+00003ad0: 2022 6f72 6967 696e 222c 0a20 2020 2064   \"origin\",.    d",
            "+00003ae0: 6570 7468 3a20 4f70 7469 6f6e 616c 5b69  epth: Optional[i",
            "+00003af0: 6e74 5d20 3d20 4e6f 6e65 2c0a 2020 2020  nt] = None,.    ",
            "+00003b00: 6272 616e 6368 3a20 4f70 7469 6f6e 616c  branch: Optional",
            "+00003b10: 5b55 6e69 6f6e 5b73 7472 2c20 6279 7465  [Union[str, byte",
            "+00003b20: 735d 5d20 3d20 4e6f 6e65 2c0a 2020 2020  s]] = None,.    ",
            "+00003b30: 636f 6e66 6967 3a20 4f70 7469 6f6e 616c  config: Optional",
            "+00003b40: 5b43 6f6e 6669 675d 203d 204e 6f6e 652c  [Config] = None,",
            "+00003b50: 0a20 2020 2066 696c 7465 725f 7370 6563  .    filter_spec",
            "+00003b60: 3d4e 6f6e 652c 0a20 2020 2070 726f 746f  =None,.    proto",
            "+00003b70: 636f 6c5f 7665 7273 696f 6e3a 204f 7074  col_version: Opt",
            "+00003b80: 696f 6e61 6c5b 696e 745d 203d 204e 6f6e  ional[int] = Non",
            "+00003b90: 652c 0a20 2020 2072 6563 7572 7365 5f73  e,.    recurse_s",
            "+00003ba0: 7562 6d6f 6475 6c65 733a 2062 6f6f 6c20  ubmodules: bool ",
            "+00003bb0: 3d20 4661 6c73 652c 0a20 2020 202a 2a6b  = False,.    **k",
            "+00003bc0: 7761 7267 732c 0a29 3a0a 2020 2020 2222  wargs,.):.    \"\"",
            "+00003bd0: 2243 6c6f 6e65 2061 206c 6f63 616c 206f  \"Clone a local o",
            "+00003be0: 7220 7265 6d6f 7465 2067 6974 2072 6570  r remote git rep",
            "+00003bf0: 6f73 6974 6f72 792e 0a0a 2020 2020 4172  ository...    Ar",
            "+00003c00: 6773 3a0a 2020 2020 2020 736f 7572 6365  gs:.      source",
            "+00003c10: 3a20 5061 7468 206f 7220 5552 4c20 666f  : Path or URL fo",
            "+00003c20: 7220 736f 7572 6365 2072 6570 6f73 6974  r source reposit",
            "+00003c30: 6f72 790a 2020 2020 2020 7461 7267 6574  ory.      target",
            "+00003c40: 3a20 5061 7468 2074 6f20 7461 7267 6574  : Path to target",
            "+00003c50: 2072 6570 6f73 6974 6f72 7920 286f 7074   repository (opt",
            "+00003c60: 696f 6e61 6c29 0a20 2020 2020 2062 6172  ional).      bar",
            "+00003c70: 653a 2057 6865 7468 6572 206f 7220 6e6f  e: Whether or no",
            "+00003c80: 7420 746f 2063 7265 6174 6520 6120 6261  t to create a ba",
            "+00003c90: 7265 2072 6570 6f73 6974 6f72 790a 2020  re repository.  ",
            "+00003ca0: 2020 2020 6368 6563 6b6f 7574 3a20 5768      checkout: Wh",
            "+00003cb0: 6574 6865 7220 6f72 206e 6f74 2074 6f20  ether or not to ",
            "+00003cc0: 6368 6563 6b2d 6f75 7420 4845 4144 2061  check-out HEAD a",
            "+00003cd0: 6674 6572 2063 6c6f 6e69 6e67 0a20 2020  fter cloning.   ",
            "+00003ce0: 2020 2065 7272 7374 7265 616d 3a20 4f70     errstream: Op",
            "+00003cf0: 7469 6f6e 616c 2073 7472 6561 6d20 746f  tional stream to",
            "+00003d00: 2077 7269 7465 2070 726f 6772 6573 7320   write progress ",
            "+00003d10: 746f 0a20 2020 2020 206f 7574 7374 7265  to.      outstre",
            "+00003d20: 616d 3a20 4f70 7469 6f6e 616c 2073 7472  am: Optional str",
            "+00003d30: 6561 6d20 746f 2077 7269 7465 2070 726f  eam to write pro",
            "+00003d40: 6772 6573 7320 746f 2028 6465 7072 6563  gress to (deprec",
            "+00003d50: 6174 6564 290a 2020 2020 2020 6f72 6967  ated).      orig",
            "+00003d60: 696e 3a20 4e61 6d65 206f 6620 7265 6d6f  in: Name of remo",
            "+00003d70: 7465 2066 726f 6d20 7468 6520 7265 706f  te from the repo",
            "+00003d80: 7369 746f 7279 2075 7365 6420 746f 2063  sitory used to c",
            "+00003d90: 6c6f 6e65 0a20 2020 2020 2064 6570 7468  lone.      depth",
            "+00003da0: 3a20 4465 7074 6820 746f 2066 6574 6368  : Depth to fetch",
            "+00003db0: 2061 740a 2020 2020 2020 6272 616e 6368   at.      branch",
            "+00003dc0: 3a20 4f70 7469 6f6e 616c 2062 7261 6e63  : Optional branc",
            "+00003dd0: 6820 6f72 2074 6167 2074 6f20 6265 2075  h or tag to be u",
            "+00003de0: 7365 6420 6173 2048 4541 4420 696e 2074  sed as HEAD in t",
            "+00003df0: 6865 206e 6577 2072 6570 6f73 6974 6f72  he new repositor",
            "+00003e00: 790a 2020 2020 2020 2020 696e 7374 6561  y.        instea",
            "+00003e10: 6420 6f66 2074 6865 2063 6c6f 6e65 6420  d of the cloned ",
            "+00003e20: 7265 706f 7369 746f 7279 2773 2048 4541  repository's HEA",
            "+00003e30: 442e 0a20 2020 2020 2063 6f6e 6669 673a  D..      config:",
            "+00003e40: 2043 6f6e 6669 6775 7261 7469 6f6e 2074   Configuration t",
            "+00003e50: 6f20 7573 650a 2020 2020 2020 6669 6c74  o use.      filt",
            "+00003e60: 6572 5f73 7065 633a 2041 2067 6974 2d72  er_spec: A git-r",
            "+00003e70: 6576 2d6c 6973 742d 7374 796c 6520 6f62  ev-list-style ob",
            "+00003e80: 6a65 6374 2066 696c 7465 7220 7370 6563  ject filter spec",
            "+00003e90: 2c20 6173 2061 6e20 4153 4349 4920 7374  , as an ASCII st",
            "+00003ea0: 7269 6e67 2e0a 2020 2020 2020 2020 4f6e  ring..        On",
            "+00003eb0: 6c79 2075 7365 6420 6966 2074 6865 2073  ly used if the s",
            "+00003ec0: 6572 7665 7220 7375 7070 6f72 7473 2074  erver supports t",
            "+00003ed0: 6865 2047 6974 2070 726f 746f 636f 6c2d  he Git protocol-",
            "+00003ee0: 7632 2027 6669 6c74 6572 270a 2020 2020  v2 'filter'.    ",
            "+00003ef0: 2020 2020 6665 6174 7572 652c 2061 6e64      feature, and",
            "+00003f00: 2069 676e 6f72 6564 206f 7468 6572 7769   ignored otherwi",
            "+00003f10: 7365 2e0a 2020 2020 2020 7072 6f74 6f63  se..      protoc",
            "+00003f20: 6f6c 5f76 6572 7369 6f6e 3a20 6465 7369  ol_version: desi",
            "+00003f30: 7265 6420 4769 7420 7072 6f74 6f63 6f6c  red Git protocol",
            "+00003f40: 2076 6572 7369 6f6e 2e20 4279 2064 6566   version. By def",
            "+00003f50: 6175 6c74 2074 6865 2068 6967 6865 7374  ault the highest",
            "+00003f60: 0a20 2020 2020 2020 206d 7574 7561 6c6c  .        mutuall",
            "+00003f70: 7920 7375 7070 6f72 7465 6420 7072 6f74  y supported prot",
            "+00003f80: 6f63 6f6c 2076 6572 7369 6f6e 2077 696c  ocol version wil",
            "+00003f90: 6c20 6265 2075 7365 642e 0a20 2020 2020  l be used..     ",
            "+00003fa0: 2072 6563 7572 7365 5f73 7562 6d6f 6475   recurse_submodu",
            "+00003fb0: 6c65 733a 2057 6865 7468 6572 2074 6f20  les: Whether to ",
            "+00003fc0: 696e 6974 6961 6c69 7a65 2061 6e64 2063  initialize and c",
            "+00003fd0: 6c6f 6e65 2073 7562 6d6f 6475 6c65 730a  lone submodules.",
            "+00003fe0: 0a20 2020 204b 6579 776f 7264 2041 7267  .    Keyword Arg",
            "+00003ff0: 733a 0a20 2020 2020 2072 6566 7370 6563  s:.      refspec",
            "+00004000: 733a 2072 6566 7370 6563 7320 746f 2066  s: refspecs to f",
            "+00004010: 6574 6368 2e20 4361 6e20 6265 2061 2062  etch. Can be a b",
            "+00004020: 7974 6573 7472 696e 672c 2061 2073 7472  ytestring, a str",
            "+00004030: 696e 672c 206f 7220 6120 6c69 7374 206f  ing, or a list o",
            "+00004040: 660a 2020 2020 2020 2020 6279 7465 7374  f.        bytest",
            "+00004050: 7269 6e67 2f73 7472 696e 672e 0a0a 2020  ring/string...  ",
            "+00004060: 2020 5265 7475 726e 733a 2054 6865 206e    Returns: The n",
            "+00004070: 6577 2072 6570 6f73 6974 6f72 790a 2020  ew repository.  ",
            "+00004080: 2020 2222 220a 2020 2020 6966 206f 7574    \"\"\".    if out",
            "+00004090: 7374 7265 616d 2069 7320 6e6f 7420 4e6f  stream is not No",
            "+000040a0: 6e65 3a0a 2020 2020 2020 2020 696d 706f  ne:.        impo",
            "+000040b0: 7274 2077 6172 6e69 6e67 730a 0a20 2020  rt warnings..   ",
            "+000040c0: 2020 2020 2077 6172 6e69 6e67 732e 7761       warnings.wa",
            "+000040d0: 726e 280a 2020 2020 2020 2020 2020 2020  rn(.            ",
            "+000040e0: 226f 7574 7374 7265 616d 3d20 6861 7320  \"outstream= has ",
            "+000040f0: 6265 656e 2064 6570 7265 6361 7465 6420  been deprecated ",
            "+00004100: 696e 2066 6176 6f75 7220 6f66 2065 7272  in favour of err",
            "+00004110: 7374 7265 616d 3d2e 222c 0a20 2020 2020  stream=.\",.     ",
            "+00004120: 2020 2020 2020 2044 6570 7265 6361 7469         Deprecati",
            "+00004130: 6f6e 5761 726e 696e 672c 0a20 2020 2020  onWarning,.     ",
            "+00004140: 2020 2020 2020 2073 7461 636b 6c65 7665         stackleve",
            "+00004150: 6c3d 332c 0a20 2020 2020 2020 2029 0a20  l=3,.        ). ",
            "+00004160: 2020 2020 2020 2023 2054 4f44 4f28 6a65         # TODO(je",
            "+00004170: 6c6d 6572 293a 2043 6170 7475 7265 206c  lmer): Capture l",
            "+00004180: 6f67 6769 6e67 206f 7574 7075 7420 616e  ogging output an",
            "+00004190: 6420 7374 7265 616d 2074 6f20 6572 7273  d stream to errs",
            "+000041a0: 7472 6561 6d0a 0a20 2020 2069 6620 636f  tream..    if co",
            "+000041b0: 6e66 6967 2069 7320 4e6f 6e65 3a0a 2020  nfig is None:.  ",
            "+000041c0: 2020 2020 2020 636f 6e66 6967 203d 2053        config = S",
            "+000041d0: 7461 636b 6564 436f 6e66 6967 2e64 6566  tackedConfig.def",
            "+000041e0: 6175 6c74 2829 0a0a 2020 2020 6966 2063  ault()..    if c",
            "+000041f0: 6865 636b 6f75 7420 6973 204e 6f6e 653a  heckout is None:",
            "+00004200: 0a20 2020 2020 2020 2063 6865 636b 6f75  .        checkou",
            "+00004210: 7420 3d20 6e6f 7420 6261 7265 0a20 2020  t = not bare.   ",
            "+00004220: 2069 6620 6368 6563 6b6f 7574 2061 6e64   if checkout and",
            "+00004230: 2062 6172 653a 0a20 2020 2020 2020 2072   bare:.        r",
            "+00004240: 6169 7365 2045 7272 6f72 2822 6368 6563  aise Error(\"chec",
            "+00004250: 6b6f 7574 2061 6e64 2062 6172 6520 6172  kout and bare ar",
            "+00004260: 6520 696e 636f 6d70 6174 6962 6c65 2229  e incompatible\")",
            "+00004270: 0a0a 2020 2020 6966 2074 6172 6765 7420  ..    if target ",
            "+00004280: 6973 204e 6f6e 653a 0a20 2020 2020 2020  is None:.       ",
            "+00004290: 2074 6172 6765 7420 3d20 736f 7572 6365   target = source",
            "+000042a0: 2e73 706c 6974 2822 2f22 295b 2d31 5d0a  .split(\"/\")[-1].",
            "+000042b0: 0a20 2020 2069 6620 6973 696e 7374 616e  .    if isinstan",
            "+000042c0: 6365 2862 7261 6e63 682c 2073 7472 293a  ce(branch, str):",
            "+000042d0: 0a20 2020 2020 2020 2062 7261 6e63 6820  .        branch ",
            "+000042e0: 3d20 6272 616e 6368 2e65 6e63 6f64 6528  = branch.encode(",
            "+000042f0: 4445 4641 554c 545f 454e 434f 4449 4e47  DEFAULT_ENCODING",
            "+00004300: 290a 0a20 2020 206d 6b64 6972 203d 206e  )..    mkdir = n",
            "+00004310: 6f74 206f 732e 7061 7468 2e65 7869 7374  ot os.path.exist",
            "+00004320: 7328 7461 7267 6574 290a 0a20 2020 2028  s(target)..    (",
            "+00004330: 636c 6965 6e74 2c20 7061 7468 2920 3d20  client, path) = ",
            "+00004340: 6765 745f 7472 616e 7370 6f72 745f 616e  get_transport_an",
            "+00004350: 645f 7061 7468 2873 6f75 7263 652c 2063  d_path(source, c",
            "+00004360: 6f6e 6669 673d 636f 6e66 6967 2c20 2a2a  onfig=config, **",
            "+00004370: 6b77 6172 6773 290a 0a20 2020 2069 6620  kwargs)..    if ",
            "+00004380: 6669 6c74 6572 5f73 7065 633a 0a20 2020  filter_spec:.   ",
            "+00004390: 2020 2020 2066 696c 7465 725f 7370 6563       filter_spec",
            "+000043a0: 203d 2066 696c 7465 725f 7370 6563 2e65   = filter_spec.e",
            "+000043b0: 6e63 6f64 6528 2261 7363 6969 2229 0a0a  ncode(\"ascii\")..",
            "+000043c0: 2020 2020 7265 706f 203d 2063 6c69 656e      repo = clien",
            "+000043d0: 742e 636c 6f6e 6528 0a20 2020 2020 2020  t.clone(.       ",
            "+000043e0: 2070 6174 682c 0a20 2020 2020 2020 2074   path,.        t",
            "+000043f0: 6172 6765 742c 0a20 2020 2020 2020 206d  arget,.        m",
            "+00004400: 6b64 6972 3d6d 6b64 6972 2c0a 2020 2020  kdir=mkdir,.    ",
            "+00004410: 2020 2020 6261 7265 3d62 6172 652c 0a20      bare=bare,. ",
            "+00004420: 2020 2020 2020 206f 7269 6769 6e3d 6f72         origin=or",
            "+00004430: 6967 696e 2c0a 2020 2020 2020 2020 6368  igin,.        ch",
            "+00004440: 6563 6b6f 7574 3d63 6865 636b 6f75 742c  eckout=checkout,",
            "+00004450: 0a20 2020 2020 2020 2062 7261 6e63 683d  .        branch=",
            "+00004460: 6272 616e 6368 2c0a 2020 2020 2020 2020  branch,.        ",
            "+00004470: 7072 6f67 7265 7373 3d65 7272 7374 7265  progress=errstre",
            "+00004480: 616d 2e77 7269 7465 2c0a 2020 2020 2020  am.write,.      ",
            "+00004490: 2020 6465 7074 683d 6465 7074 682c 0a20    depth=depth,. ",
            "+000044a0: 2020 2020 2020 2066 696c 7465 725f 7370         filter_sp",
            "+000044b0: 6563 3d66 696c 7465 725f 7370 6563 2c0a  ec=filter_spec,.",
            "+000044c0: 2020 2020 2020 2020 7072 6f74 6f63 6f6c          protocol",
            "+000044d0: 5f76 6572 7369 6f6e 3d70 726f 746f 636f  _version=protoco",
            "+000044e0: 6c5f 7665 7273 696f 6e2c 0a20 2020 2029  l_version,.    )",
            "+000044f0: 0a0a 2020 2020 2320 496e 6974 6961 6c69  ..    # Initiali",
            "+00004500: 7a65 2061 6e64 2075 7064 6174 6520 7375  ze and update su",
            "+00004510: 626d 6f64 756c 6573 2069 6620 7265 7175  bmodules if requ",
            "+00004520: 6573 7465 640a 2020 2020 6966 2072 6563  ested.    if rec",
            "+00004530: 7572 7365 5f73 7562 6d6f 6475 6c65 7320  urse_submodules ",
            "+00004540: 616e 6420 6e6f 7420 6261 7265 3a0a 2020  and not bare:.  ",
            "+00004550: 2020 2020 2020 7472 793a 0a20 2020 2020        try:.     ",
            "+00004560: 2020 2020 2020 2073 7562 6d6f 6475 6c65         submodule",
            "+00004570: 5f69 6e69 7428 7265 706f 290a 2020 2020  _init(repo).    ",
            "+00004580: 2020 2020 2020 2020 7375 626d 6f64 756c          submodul",
            "+00004590: 655f 7570 6461 7465 2872 6570 6f2c 2069  e_update(repo, i",
            "+000045a0: 6e69 743d 5472 7565 290a 2020 2020 2020  nit=True).      ",
            "+000045b0: 2020 6578 6365 7074 2046 696c 654e 6f74    except FileNot",
            "+000045c0: 466f 756e 6445 7272 6f72 2061 7320 653a  FoundError as e:",
            "+000045d0: 0a20 2020 2020 2020 2020 2020 2023 202e  .            # .",
            "+000045e0: 6769 746d 6f64 756c 6573 2066 696c 6520  gitmodules file ",
            "+000045f0: 646f 6573 6e27 7420 6578 6973 7420 2d20  doesn't exist - ",
            "+00004600: 6e6f 2073 7562 6d6f 6475 6c65 7320 746f  no submodules to",
            "+00004610: 2070 726f 6365 7373 0a20 2020 2020 2020   process.       ",
            "+00004620: 2020 2020 2069 6d70 6f72 7420 6c6f 6767       import logg",
            "+00004630: 696e 670a 0a20 2020 2020 2020 2020 2020  ing..           ",
            "+00004640: 206c 6f67 6769 6e67 2e64 6562 7567 2822   logging.debug(\"",
            "+00004650: 4e6f 202e 6769 746d 6f64 756c 6573 2066  No .gitmodules f",
            "+00004660: 696c 6520 666f 756e 643a 2025 7322 2c20  ile found: %s\", ",
            "+00004670: 6529 0a20 2020 2020 2020 2065 7863 6570  e).        excep",
            "+00004680: 7420 4b65 7945 7272 6f72 2061 7320 653a  t KeyError as e:",
            "+00004690: 0a20 2020 2020 2020 2020 2020 2023 2053  .            # S",
            "+000046a0: 7562 6d6f 6475 6c65 2063 6f6e 6669 6775  ubmodule configu",
            "+000046b0: 7261 7469 6f6e 206d 6973 7369 6e67 0a20  ration missing. ",
            "+000046c0: 2020 2020 2020 2020 2020 2069 6d70 6f72             impor",
            "+000046d0: 7420 6c6f 6767 696e 670a 0a20 2020 2020  t logging..     ",
            "+000046e0: 2020 2020 2020 206c 6f67 6769 6e67 2e77         logging.w",
            "+000046f0: 6172 6e69 6e67 2822 5375 626d 6f64 756c  arning(\"Submodul",
            "+00004700: 6520 636f 6e66 6967 7572 6174 696f 6e20  e configuration ",
            "+00004710: 6572 726f 723a 2025 7322 2c20 6529 0a20  error: %s\", e). ",
            "+00004720: 2020 2020 2020 2020 2020 2069 6620 6572             if er",
            "+00004730: 7273 7472 6561 6d3a 0a20 2020 2020 2020  rstream:.       ",
            "+00004740: 2020 2020 2020 2020 2065 7272 7374 7265           errstre",
            "+00004750: 616d 2e77 7269 7465 280a 2020 2020 2020  am.write(.      ",
            "+00004760: 2020 2020 2020 2020 2020 2020 2020 6622                f\"",
            "+00004770: 5761 726e 696e 673a 2053 7562 6d6f 6475  Warning: Submodu",
            "+00004780: 6c65 2063 6f6e 6669 6775 7261 7469 6f6e  le configuration",
            "+00004790: 2065 7272 6f72 3a20 7b65 7d5c 6e22 2e65   error: {e}\\n\".e",
            "+000047a0: 6e63 6f64 6528 290a 2020 2020 2020 2020  ncode().        ",
            "+000047b0: 2020 2020 2020 2020 290a 0a20 2020 2072          )..    r",
            "+000047c0: 6574 7572 6e20 7265 706f 0a0a 0a64 6566  eturn repo...def",
            "+000047d0: 2061 6464 2872 6570 6f3a 2055 6e69 6f6e   add(repo: Union",
            "+000047e0: 5b73 7472 2c20 6f73 2e50 6174 684c 696b  [str, os.PathLik",
            "+000047f0: 652c 2042 6173 6552 6570 6f5d 203d 2022  e, BaseRepo] = \"",
            "+00004800: 2e22 2c20 7061 7468 733d 4e6f 6e65 293a  .\", paths=None):",
            "+00004810: 0a20 2020 2022 2222 4164 6420 6669 6c65  .    \"\"\"Add file",
            "+00004820: 7320 746f 2074 6865 2073 7461 6769 6e67  s to the staging",
            "+00004830: 2061 7265 612e 0a0a 2020 2020 4172 6773   area...    Args",
            "+00004840: 3a0a 2020 2020 2020 7265 706f 3a20 5265  :.      repo: Re",
            "+00004850: 706f 7369 746f 7279 2066 6f72 2074 6865  pository for the",
            "+00004860: 2066 696c 6573 0a20 2020 2020 2070 6174   files.      pat",
            "+00004870: 6873 3a20 5061 7468 7320 746f 2061 6464  hs: Paths to add",
            "+00004880: 2e20 4966 204e 6f6e 652c 2073 7461 6765  . If None, stage",
            "+00004890: 7320 616c 6c20 756e 7472 6163 6b65 6420  s all untracked ",
            "+000048a0: 616e 6420 6d6f 6469 6669 6564 2066 696c  and modified fil",
            "+000048b0: 6573 2066 726f 6d20 7468 650a 2020 2020  es from the.    ",
            "+000048c0: 2020 2020 6375 7272 656e 7420 776f 726b      current work",
            "+000048d0: 696e 6720 6469 7265 6374 6f72 7920 286d  ing directory (m",
            "+000048e0: 696d 6963 6b69 6e67 2027 6769 7420 6164  imicking 'git ad",
            "+000048f0: 6420 2e27 2062 6568 6176 696f 7229 2e0a  d .' behavior)..",
            "+00004900: 2020 2020 5265 7475 726e 733a 2054 7570      Returns: Tup",
            "+00004910: 6c65 2077 6974 6820 7365 7420 6f66 2061  le with set of a",
            "+00004920: 6464 6564 2066 696c 6573 2061 6e64 2069  dded files and i",
            "+00004930: 676e 6f72 6564 2066 696c 6573 0a0a 2020  gnored files..  ",
            "+00004940: 2020 4966 2074 6865 2072 6570 6f73 6974    If the reposit",
            "+00004950: 6f72 7920 636f 6e74 6169 6e73 2069 676e  ory contains ign",
            "+00004960: 6f72 6564 2064 6972 6563 746f 7269 6573  ored directories",
            "+00004970: 2c20 7468 6520 7265 7475 726e 6564 2073  , the returned s",
            "+00004980: 6574 2077 696c 6c0a 2020 2020 636f 6e74  et will.    cont",
            "+00004990: 6169 6e20 7468 6520 7061 7468 2074 6f20  ain the path to ",
            "+000049a0: 616e 2069 676e 6f72 6564 2064 6972 6563  an ignored direc",
            "+000049b0: 746f 7279 2028 7769 7468 2074 7261 696c  tory (with trail",
            "+000049c0: 696e 6720 736c 6173 6829 2e20 496e 6469  ing slash). Indi",
            "+000049d0: 7669 6475 616c 0a20 2020 2066 696c 6573  vidual.    files",
            "+000049e0: 2077 6974 6869 6e20 6967 6e6f 7265 6420   within ignored ",
            "+000049f0: 6469 7265 6374 6f72 6965 7320 7769 6c6c  directories will",
            "+00004a00: 206e 6f74 2062 6520 7265 7475 726e 6564   not be returned",
            "+00004a10: 2e0a 0a20 2020 204e 6f74 653a 2057 6865  ...    Note: Whe",
            "+00004a20: 6e20 7061 7468 733d 4e6f 6e65 2c20 7468  n paths=None, th",
            "+00004a30: 6973 2066 756e 6374 696f 6e20 6164 6473  is function adds",
            "+00004a40: 2061 6c6c 2075 6e74 7261 636b 6564 2061   all untracked a",
            "+00004a50: 6e64 206d 6f64 6966 6965 6420 6669 6c65  nd modified file",
            "+00004a60: 730a 2020 2020 6672 6f6d 2074 6865 2065  s.    from the e",
            "+00004a70: 6e74 6972 6520 7265 706f 7369 746f 7279  ntire repository",
            "+00004a80: 2c20 6d69 6d69 636b 696e 6720 2767 6974  , mimicking 'git",
            "+00004a90: 2061 6464 202d 4127 2062 6568 6176 696f   add -A' behavio",
            "+00004aa0: 722e 0a20 2020 2022 2222 0a20 2020 2069  r..    \"\"\".    i",
            "+00004ab0: 676e 6f72 6564 203d 2073 6574 2829 0a20  gnored = set(). ",
            "+00004ac0: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "+00004ad0: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "+00004ae0: 6173 2072 3a0a 2020 2020 2020 2020 7265  as r:.        re",
            "+00004af0: 706f 5f70 6174 6820 3d20 5061 7468 2872  po_path = Path(r",
            "+00004b00: 2e70 6174 6829 2e72 6573 6f6c 7665 2829  .path).resolve()",
            "+00004b10: 0a20 2020 2020 2020 2069 676e 6f72 655f  .        ignore_",
            "+00004b20: 6d61 6e61 6765 7220 3d20 4967 6e6f 7265  manager = Ignore",
            "+00004b30: 4669 6c74 6572 4d61 6e61 6765 722e 6672  FilterManager.fr",
            "+00004b40: 6f6d 5f72 6570 6f28 7229 0a0a 2020 2020  om_repo(r)..    ",
            "+00004b50: 2020 2020 2320 4765 7420 756e 7374 6167      # Get unstag",
            "+00004b60: 6564 2063 6861 6e67 6573 206f 6e63 6520  ed changes once ",
            "+00004b70: 666f 7220 7468 6520 656e 7469 7265 206f  for the entire o",
            "+00004b80: 7065 7261 7469 6f6e 0a20 2020 2020 2020  peration.       ",
            "+00004b90: 2069 6e64 6578 203d 2072 2e6f 7065 6e5f   index = r.open_",
            "+00004ba0: 696e 6465 7828 290a 2020 2020 2020 2020  index().        ",
            "+00004bb0: 6e6f 726d 616c 697a 6572 203d 2072 2e67  normalizer = r.g",
            "+00004bc0: 6574 5f62 6c6f 625f 6e6f 726d 616c 697a  et_blob_normaliz",
            "+00004bd0: 6572 2829 0a20 2020 2020 2020 2066 696c  er().        fil",
            "+00004be0: 7465 725f 6361 6c6c 6261 636b 203d 206e  ter_callback = n",
            "+00004bf0: 6f72 6d61 6c69 7a65 722e 6368 6563 6b69  ormalizer.checki",
            "+00004c00: 6e5f 6e6f 726d 616c 697a 650a 2020 2020  n_normalize.    ",
            "+00004c10: 2020 2020 616c 6c5f 756e 7374 6167 6564      all_unstaged",
            "+00004c20: 5f70 6174 6873 203d 206c 6973 7428 6765  _paths = list(ge",
            "+00004c30: 745f 756e 7374 6167 6564 5f63 6861 6e67  t_unstaged_chang",
            "+00004c40: 6573 2869 6e64 6578 2c20 722e 7061 7468  es(index, r.path",
            "+00004c50: 2c20 6669 6c74 6572 5f63 616c 6c62 6163  , filter_callbac",
            "+00004c60: 6b29 290a 0a20 2020 2020 2020 2069 6620  k))..        if ",
            "+00004c70: 6e6f 7420 7061 7468 733a 0a20 2020 2020  not paths:.     ",
            "+00004c80: 2020 2020 2020 2023 2057 6865 6e20 6e6f         # When no",
            "+00004c90: 2070 6174 6873 2073 7065 6369 6669 6564   paths specified",
            "+00004ca0: 2c20 6164 6420 616c 6c20 756e 7472 6163  , add all untrac",
            "+00004cb0: 6b65 6420 616e 6420 6d6f 6469 6669 6564  ked and modified",
            "+00004cc0: 2066 696c 6573 2066 726f 6d20 7265 706f   files from repo",
            "+00004cd0: 2072 6f6f 740a 2020 2020 2020 2020 2020   root.          ",
            "+00004ce0: 2020 7061 7468 7320 3d20 5b73 7472 2872    paths = [str(r",
            "+00004cf0: 6570 6f5f 7061 7468 295d 0a20 2020 2020  epo_path)].     ",
            "+00004d00: 2020 2072 656c 7061 7468 7320 3d20 5b5d     relpaths = []",
            "+00004d10: 0a20 2020 2020 2020 2069 6620 6e6f 7420  .        if not ",
            "+00004d20: 6973 696e 7374 616e 6365 2870 6174 6873  isinstance(paths",
            "+00004d30: 2c20 6c69 7374 293a 0a20 2020 2020 2020  , list):.       ",
            "+00004d40: 2020 2020 2070 6174 6873 203d 205b 7061       paths = [pa",
            "+00004d50: 7468 735d 0a20 2020 2020 2020 2066 6f72  ths].        for",
            "+00004d60: 2070 2069 6e20 7061 7468 733a 0a20 2020   p in paths:.   ",
            "+00004d70: 2020 2020 2020 2020 2070 6174 6820 3d20           path = ",
            "+00004d80: 5061 7468 2870 290a 2020 2020 2020 2020  Path(p).        ",
            "+00004d90: 2020 2020 6966 206e 6f74 2070 6174 682e      if not path.",
            "+00004da0: 6973 5f61 6273 6f6c 7574 6528 293a 0a20  is_absolute():. ",
            "+00004db0: 2020 2020 2020 2020 2020 2020 2020 2023                 #",
            "+00004dc0: 204d 616b 6520 7265 6c61 7469 7665 2070   Make relative p",
            "+00004dd0: 6174 6873 2072 656c 6174 6976 6520 746f  aths relative to",
            "+00004de0: 2074 6865 2072 6570 6f20 6469 7265 6374   the repo direct",
            "+00004df0: 6f72 790a 2020 2020 2020 2020 2020 2020  ory.            ",
            "+00004e00: 2020 2020 7061 7468 203d 2072 6570 6f5f      path = repo_",
            "+00004e10: 7061 7468 202f 2070 6174 680a 0a20 2020  path / path..   ",
            "+00004e20: 2020 2020 2020 2020 2023 2044 6f6e 2774           # Don't",
            "+00004e30: 2072 6573 6f6c 7665 2073 796d 6c69 6e6b   resolve symlink",
            "+00004e40: 7320 636f 6d70 6c65 7465 6c79 202d 206f  s completely - o",
            "+00004e50: 6e6c 7920 7265 736f 6c76 6520 7468 6520  nly resolve the ",
            "+00004e60: 7061 7265 6e74 2064 6972 6563 746f 7279  parent directory",
            "+00004e70: 0a20 2020 2020 2020 2020 2020 2023 2074  .            # t",
            "+00004e80: 6f20 6176 6f69 6420 6973 7375 6573 2077  o avoid issues w",
            "+00004e90: 6865 6e20 7379 6d6c 696e 6b73 2070 6f69  hen symlinks poi",
            "+00004ea0: 6e74 206f 7574 7369 6465 2074 6865 2072  nt outside the r",
            "+00004eb0: 6570 6f73 6974 6f72 790a 2020 2020 2020  epository.      ",
            "+00004ec0: 2020 2020 2020 6966 2070 6174 682e 6973        if path.is",
            "+00004ed0: 5f73 796d 6c69 6e6b 2829 3a0a 2020 2020  _symlink():.    ",
            "+00004ee0: 2020 2020 2020 2020 2020 2020 2320 466f              # Fo",
            "+00004ef0: 7220 7379 6d6c 696e 6b73 2c20 7265 736f  r symlinks, reso",
            "+00004f00: 6c76 6520 6f6e 6c79 2074 6865 2070 6172  lve only the par",
            "+00004f10: 656e 7420 6469 7265 6374 6f72 790a 2020  ent directory.  ",
            "+00004f20: 2020 2020 2020 2020 2020 2020 2020 7061                pa",
            "+00004f30: 7265 6e74 5f72 6573 6f6c 7665 6420 3d20  rent_resolved = ",
            "+00004f40: 7061 7468 2e70 6172 656e 742e 7265 736f  path.parent.reso",
            "+00004f50: 6c76 6528 290a 2020 2020 2020 2020 2020  lve().          ",
            "+00004f60: 2020 2020 2020 7265 736f 6c76 6564 5f70        resolved_p",
            "+00004f70: 6174 6820 3d20 7061 7265 6e74 5f72 6573  ath = parent_res",
            "+00004f80: 6f6c 7665 6420 2f20 7061 7468 2e6e 616d  olved / path.nam",
            "+00004f90: 650a 2020 2020 2020 2020 2020 2020 656c  e.            el",
            "+00004fa0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            ",
            "+00004fb0: 2020 2020 2320 466f 7220 7265 6775 6c61      # For regula",
            "+00004fc0: 7220 6669 6c65 732f 6469 7273 2c20 7265  r files/dirs, re",
            "+00004fd0: 736f 6c76 6520 6e6f 726d 616c 6c79 0a20  solve normally. ",
            "+00004fe0: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+00004ff0: 6573 6f6c 7665 645f 7061 7468 203d 2070  esolved_path = p",
            "+00005000: 6174 682e 7265 736f 6c76 6528 290a 0a20  ath.resolve().. ",
            "+00005010: 2020 2020 2020 2020 2020 2074 7279 3a0a             try:.",
            "+00005020: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005030: 7265 6c70 6174 6820 3d20 7374 7228 7265  relpath = str(re",
            "+00005040: 736f 6c76 6564 5f70 6174 682e 7265 6c61  solved_path.rela",
            "+00005050: 7469 7665 5f74 6f28 7265 706f 5f70 6174  tive_to(repo_pat",
            "+00005060: 6829 292e 7265 706c 6163 6528 6f73 2e73  h)).replace(os.s",
            "+00005070: 6570 2c20 222f 2229 0a20 2020 2020 2020  ep, \"/\").       ",
            "+00005080: 2020 2020 2065 7863 6570 7420 5661 6c75       except Valu",
            "+00005090: 6545 7272 6f72 2061 7320 653a 0a20 2020  eError as e:.   ",
            "+000050a0: 2020 2020 2020 2020 2020 2020 2023 2050               # P",
            "+000050b0: 6174 6820 6973 206e 6f74 2077 6974 6869  ath is not withi",
            "+000050c0: 6e20 7468 6520 7265 706f 7369 746f 7279  n the repository",
            "+000050d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000050e0: 2072 6169 7365 2056 616c 7565 4572 726f   raise ValueErro",
            "+000050f0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             ",
            "+00005100: 2020 2020 2020 2066 2250 6174 6820 7b70         f\"Path {p",
            "+00005110: 7d20 6973 206e 6f74 2077 6974 6869 6e20  } is not within ",
            "+00005120: 7265 706f 7369 746f 7279 207b 7265 706f  repository {repo",
            "+00005130: 5f70 6174 687d 220a 2020 2020 2020 2020  _path}\".        ",
            "+00005140: 2020 2020 2020 2020 2920 6672 6f6d 2065          ) from e",
            "+00005150: 0a0a 2020 2020 2020 2020 2020 2020 2320  ..            # ",
            "+00005160: 4861 6e64 6c65 2064 6972 6563 746f 7269  Handle directori",
            "+00005170: 6573 2062 7920 7363 616e 6e69 6e67 2074  es by scanning t",
            "+00005180: 6865 6972 2063 6f6e 7465 6e74 730a 2020  heir contents.  ",
            "+00005190: 2020 2020 2020 2020 2020 6966 2072 6573            if res",
            "+000051a0: 6f6c 7665 645f 7061 7468 2e69 735f 6469  olved_path.is_di",
            "+000051b0: 7228 293a 0a20 2020 2020 2020 2020 2020  r():.           ",
            "+000051c0: 2020 2020 2023 2043 6865 636b 2069 6620       # Check if ",
            "+000051d0: 7468 6520 6469 7265 6374 6f72 7920 6974  the directory it",
            "+000051e0: 7365 6c66 2069 7320 6967 6e6f 7265 640a  self is ignored.",
            " 000051f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005200: 2020 2070 6173 730a 2020 2020 2020 2020     pass.        ",
            "-00005210: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  ",
            "-00005220: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005230: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         ",
            "-00005240: 2020 2020 2020 2020 2020 2020 2020 2062                 b",
            "-00005250: 6c6f 6220 3d20 626c 6f62 5f66 726f 6d5f  lob = blob_from_",
            "-00005260: 7061 7468 5f61 6e64 5f73 7461 7428 6675  path_and_stat(fu",
            "-00005270: 6c6c 5f70 6174 682c 2073 7429 0a20 2020  ll_path, st).   ",
            "-00005280: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005290: 2065 7863 6570 7420 4f53 4572 726f 723a   except OSError:",
            "-000052a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "-000052b0: 2020 2020 2020 2020 2070 6173 730a 2020           pass.  ",
            "+00005200: 6469 725f 7265 6c70 6174 6820 3d20 706f  dir_relpath = po",
            "+00005210: 7369 7870 6174 682e 6a6f 696e 2872 656c  sixpath.join(rel",
            "+00005220: 7061 7468 2c20 2222 2920 6966 2072 656c  path, \"\") if rel",
            "+00005230: 7061 7468 2021 3d20 222e 2220 656c 7365  path != \".\" else",
            "+00005240: 2022 220a 2020 2020 2020 2020 2020 2020   \"\".            ",
            "+00005250: 2020 2020 6966 2064 6972 5f72 656c 7061      if dir_relpa",
            "+00005260: 7468 2061 6e64 2069 676e 6f72 655f 6d61  th and ignore_ma",
            "+00005270: 6e61 6765 722e 6973 5f69 676e 6f72 6564  nager.is_ignored",
            "+00005280: 2864 6972 5f72 656c 7061 7468 293a 0a20  (dir_relpath):. ",
            "+00005290: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000052a0: 2020 2069 676e 6f72 6564 2e61 6464 2864     ignored.add(d",
            "+000052b0: 6972 5f72 656c 7061 7468 290a 2020 2020  ir_relpath).    ",
            " 000052c0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000052d0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        ",
            "-000052e0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000052f0: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           ",
            "-00005300: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005310: 2063 6f6d 6d69 7474 6564 5f73 6861 203d   committed_sha =",
            "-00005320: 2074 7265 655f 6c6f 6f6b 7570 5f70 6174   tree_lookup_pat",
            "-00005330: 6828 0a20 2020 2020 2020 2020 2020 2020  h(.             ",
            "-00005340: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005350: 2020 2072 2e5f 5f67 6574 6974 656d 5f5f     r.__getitem__",
            "-00005360: 2c20 725b 722e 6865 6164 2829 5d2e 7472  , r[r.head()].tr",
            "-00005370: 6565 2c20 7472 6565 5f70 6174 680a 2020  ee, tree_path.  ",
            "+000052d0: 636f 6e74 696e 7565 0a0a 2020 2020 2020  continue..      ",
            "+000052e0: 2020 2020 2020 2020 2020 2320 5768 656e            # When",
            "+000052f0: 2061 6464 696e 6720 6120 6469 7265 6374   adding a direct",
            "+00005300: 6f72 792c 2061 6464 2061 6c6c 2075 6e74  ory, add all unt",
            "+00005310: 7261 636b 6564 2066 696c 6573 2077 6974  racked files wit",
            "+00005320: 6869 6e20 6974 0a20 2020 2020 2020 2020  hin it.         ",
            "+00005330: 2020 2020 2020 2063 7572 7265 6e74 5f75         current_u",
            "+00005340: 6e74 7261 636b 6564 203d 206c 6973 7428  ntracked = list(",
            "+00005350: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00005360: 2020 2020 2067 6574 5f75 6e74 7261 636b       get_untrack",
            "+00005370: 6564 5f70 6174 6873 280a 2020 2020 2020  ed_paths(.      ",
            " 00005380: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005390: 2020 2020 2020 2020 2020 295b 315d 0a20            )[1]. ",
            "-000053a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000053b0: 2020 2020 2020 2065 7863 6570 7420 4b65         except Ke",
            "-000053c0: 7945 7272 6f72 3a0a 2020 2020 2020 2020  yError:.        ",
            "+00005390: 2020 7374 7228 7265 736f 6c76 6564 5f70    str(resolved_p",
            "+000053a0: 6174 6829 2c0a 2020 2020 2020 2020 2020  ath),.          ",
            "+000053b0: 2020 2020 2020 2020 2020 2020 2020 7374                st",
            "+000053c0: 7228 7265 706f 5f70 6174 6829 2c0a 2020  r(repo_path),.  ",
            " 000053d0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000053e0: 2020 2020 636f 6d6d 6974 7465 645f 7368      committed_sh",
            "-000053f0: 6120 3d20 4e6f 6e65 0a0a 2020 2020 2020  a = None..      ",
            "-00005400: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005410: 2020 6966 2062 6c6f 622e 6964 2021 3d20    if blob.id != ",
            "-00005420: 696e 6465 785f 7368 6120 616e 6420 696e  index_sha and in",
            "-00005430: 6465 785f 7368 6120 213d 2063 6f6d 6d69  dex_sha != commi",
            "-00005440: 7474 6564 5f73 6861 3a0a 2020 2020 2020  tted_sha:.      ",
            "+000053e0: 2020 2020 2020 696e 6465 782c 0a20 2020        index,.   ",
            "+000053f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005400: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             ",
            "+00005410: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           ",
            "+00005420: 2020 2020 2066 6f72 2075 6e74 7261 636b       for untrack",
            "+00005430: 6564 5f70 6174 6820 696e 2063 7572 7265  ed_path in curre",
            "+00005440: 6e74 5f75 6e74 7261 636b 6564 3a0a 2020  nt_untracked:.  ",
            " 00005450: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005460: 2020 2020 2020 7261 6973 6520 4572 726f        raise Erro",
            "-00005470: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             ",
            "-00005480: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005490: 2020 2022 6669 6c65 2068 6173 2073 7461     \"file has sta",
            "-000054a0: 6765 6420 636f 6e74 656e 7420 6469 6666  ged content diff",
            "-000054b0: 6572 696e 6720 220a 2020 2020 2020 2020  ering \".        ",
            "+00005460: 2020 2320 4966 2077 6527 7265 2073 6361    # If we're sca",
            "+00005470: 6e6e 696e 6720 6120 7375 6264 6972 6563  nning a subdirec",
            "+00005480: 746f 7279 2c20 6164 6a75 7374 2074 6865  tory, adjust the",
            "+00005490: 2070 6174 680a 2020 2020 2020 2020 2020   path.          ",
            "+000054a0: 2020 2020 2020 2020 2020 6966 2072 656c            if rel",
            "+000054b0: 7061 7468 2021 3d20 222e 223a 0a20 2020  path != \".\":.   ",
            " 000054c0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000054d0: 2020 2020 2020 2020 6622 6672 6f6d 2062          f\"from b",
            "-000054e0: 6f74 6820 7468 6520 6669 6c65 2061 6e64  oth the file and",
            "-000054f0: 2068 6561 643a 207b 707d 220a 2020 2020   head: {p}\".    ",
            "-00005500: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005510: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     ",
            "-00005520: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00005530: 2020 2069 6620 696e 6465 785f 7368 6120     if index_sha ",
            "-00005540: 213d 2063 6f6d 6d69 7474 6564 5f73 6861  != committed_sha",
            "-00005550: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "-00005560: 2020 2020 2020 2020 2020 2020 2020 7261                ra",
            "-00005570: 6973 6520 4572 726f 7228 6622 6669 6c65  ise Error(f\"file",
            "-00005580: 2068 6173 2073 7461 6765 6420 6368 616e   has staged chan",
            "-00005590: 6765 733a 207b 707d 2229 0a20 2020 2020  ges: {p}\").     ",
            "-000055a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000055b0: 2020 206f 732e 7265 6d6f 7665 2866 756c     os.remove(ful",
            "-000055c0: 6c5f 7061 7468 290a 2020 2020 2020 2020  l_path).        ",
            "-000055d0: 2020 2020 6465 6c20 696e 6465 785b 7472      del index[tr",
            "-000055e0: 6565 5f70 6174 685d 0a20 2020 2020 2020  ee_path].       ",
            "-000055f0: 2069 6e64 6578 2e77 7269 7465 2829 0a0a   index.write()..",
            "-00005600: 0a72 6d20 3d20 7265 6d6f 7665 0a0a 0a64  .rm = remove...d",
            "-00005610: 6566 2063 6f6d 6d69 745f 6465 636f 6465  ef commit_decode",
            "-00005620: 2863 6f6d 6d69 742c 2063 6f6e 7465 6e74  (commit, content",
            "-00005630: 732c 2064 6566 6175 6c74 5f65 6e63 6f64  s, default_encod",
            "-00005640: 696e 673d 4445 4641 554c 545f 454e 434f  ing=DEFAULT_ENCO",
            "-00005650: 4449 4e47 293a 0a20 2020 2069 6620 636f  DING):.    if co",
            "-00005660: 6d6d 6974 2e65 6e63 6f64 696e 673a 0a20  mmit.encoding:. ",
            "-00005670: 2020 2020 2020 2065 6e63 6f64 696e 6720         encoding ",
            "-00005680: 3d20 636f 6d6d 6974 2e65 6e63 6f64 696e  = commit.encodin",
            "-00005690: 672e 6465 636f 6465 2822 6173 6369 6922  g.decode(\"ascii\"",
            "-000056a0: 290a 2020 2020 656c 7365 3a0a 2020 2020  ).    else:.    ",
            "-000056b0: 2020 2020 656e 636f 6469 6e67 203d 2064      encoding = d",
            "-000056c0: 6566 6175 6c74 5f65 6e63 6f64 696e 670a  efault_encoding.",
            "-000056d0: 2020 2020 7265 7475 726e 2063 6f6e 7465      return conte",
            "-000056e0: 6e74 732e 6465 636f 6465 2865 6e63 6f64  nts.decode(encod",
            "-000056f0: 696e 672c 2022 7265 706c 6163 6522 290a  ing, \"replace\").",
            "-00005700: 0a0a 6465 6620 636f 6d6d 6974 5f65 6e63  ..def commit_enc",
            "-00005710: 6f64 6528 636f 6d6d 6974 2c20 636f 6e74  ode(commit, cont",
            "-00005720: 656e 7473 2c20 6465 6661 756c 745f 656e  ents, default_en",
            "-00005730: 636f 6469 6e67 3d44 4546 4155 4c54 5f45  coding=DEFAULT_E",
            "-00005740: 4e43 4f44 494e 4729 3a0a 2020 2020 6966  NCODING):.    if",
            "-00005750: 2063 6f6d 6d69 742e 656e 636f 6469 6e67   commit.encoding",
            "-00005760: 3a0a 2020 2020 2020 2020 656e 636f 6469  :.        encodi",
            "-00005770: 6e67 203d 2063 6f6d 6d69 742e 656e 636f  ng = commit.enco",
            "-00005780: 6469 6e67 2e64 6563 6f64 6528 2261 7363  ding.decode(\"asc",
            "-00005790: 6969 2229 0a20 2020 2065 6c73 653a 0a20  ii\").    else:. ",
            "-000057a0: 2020 2020 2020 2065 6e63 6f64 696e 6720         encoding ",
            "-000057b0: 3d20 6465 6661 756c 745f 656e 636f 6469  = default_encodi",
            "-000057c0: 6e67 0a20 2020 2072 6574 7572 6e20 636f  ng.    return co",
            "-000057d0: 6e74 656e 7473 2e65 6e63 6f64 6528 656e  ntents.encode(en",
            "-000057e0: 636f 6469 6e67 290a 0a0a 6465 6620 7072  coding)...def pr",
            "-000057f0: 696e 745f 636f 6d6d 6974 2863 6f6d 6d69  int_commit(commi",
            "-00005800: 742c 2064 6563 6f64 652c 206f 7574 7374  t, decode, outst",
            "-00005810: 7265 616d 3d73 7973 2e73 7464 6f75 7429  ream=sys.stdout)",
            "-00005820: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2222   -> None:.    \"\"",
            "-00005830: 2257 7269 7465 2061 2068 756d 616e 2d72  \"Write a human-r",
            "-00005840: 6561 6461 626c 6520 636f 6d6d 6974 206c  eadable commit l",
            "-00005850: 6f67 2065 6e74 7279 2e0a 0a20 2020 2041  og entry...    A",
            "-00005860: 7267 733a 0a20 2020 2020 2063 6f6d 6d69  rgs:.      commi",
            "-00005870: 743a 2041 2060 436f 6d6d 6974 6020 6f62  t: A `Commit` ob",
            "-00005880: 6a65 6374 0a20 2020 2020 206f 7574 7374  ject.      outst",
            "-00005890: 7265 616d 3a20 4120 7374 7265 616d 2066  ream: A stream f",
            "-000058a0: 696c 6520 746f 2077 7269 7465 2074 6f0a  ile to write to.",
            "-000058b0: 2020 2020 2222 220a 2020 2020 6f75 7473      \"\"\".    outs",
            "-000058c0: 7472 6561 6d2e 7772 6974 6528 222d 2220  tream.write(\"-\" ",
            "-000058d0: 2a20 3530 202b 2022 5c6e 2229 0a20 2020  * 50 + \"\\n\").   ",
            "-000058e0: 206f 7574 7374 7265 616d 2e77 7269 7465   outstream.write",
            "-000058f0: 2822 636f 6d6d 6974 3a20 2220 2b20 636f  (\"commit: \" + co",
            "-00005900: 6d6d 6974 2e69 642e 6465 636f 6465 2822  mmit.id.decode(\"",
            "-00005910: 6173 6369 6922 2920 2b20 225c 6e22 290a  ascii\") + \"\\n\").",
            "-00005920: 2020 2020 6966 206c 656e 2863 6f6d 6d69      if len(commi",
            "-00005930: 742e 7061 7265 6e74 7329 203e 2031 3a0a  t.parents) > 1:.",
            "-00005940: 2020 2020 2020 2020 6f75 7473 7472 6561          outstrea",
            "-00005950: 6d2e 7772 6974 6528 0a20 2020 2020 2020  m.write(.       ",
            "-00005960: 2020 2020 2022 6d65 7267 653a 2022 0a20       \"merge: \". ",
            "-00005970: 2020 2020 2020 2020 2020 202b 2022 2e2e             + \"..",
            "-00005980: 2e22 2e6a 6f69 6e28 5b63 2e64 6563 6f64  .\".join([c.decod",
            "-00005990: 6528 2261 7363 6969 2229 2066 6f72 2063  e(\"ascii\") for c",
            "-000059a0: 2069 6e20 636f 6d6d 6974 2e70 6172 656e   in commit.paren",
            "-000059b0: 7473 5b31 3a5d 5d29 0a20 2020 2020 2020  ts[1:]]).       ",
            "-000059c0: 2020 2020 202b 2022 5c6e 220a 2020 2020       + \"\\n\".    ",
            "-000059d0: 2020 2020 290a 2020 2020 6f75 7473 7472      ).    outstr",
            "-000059e0: 6561 6d2e 7772 6974 6528 2241 7574 686f  eam.write(\"Autho",
            "-000059f0: 723a 2022 202b 2064 6563 6f64 6528 636f  r: \" + decode(co",
            "-00005a00: 6d6d 6974 2e61 7574 686f 7229 202b 2022  mmit.author) + \"",
            "-00005a10: 5c6e 2229 0a20 2020 2069 6620 636f 6d6d  \\n\").    if comm",
            "-00005a20: 6974 2e61 7574 686f 7220 213d 2063 6f6d  it.author != com",
            "-00005a30: 6d69 742e 636f 6d6d 6974 7465 723a 0a20  mit.committer:. ",
            "-00005a40: 2020 2020 2020 206f 7574 7374 7265 616d         outstream",
            "-00005a50: 2e77 7269 7465 2822 436f 6d6d 6974 7465  .write(\"Committe",
            "-00005a60: 723a 2022 202b 2064 6563 6f64 6528 636f  r: \" + decode(co",
            "-00005a70: 6d6d 6974 2e63 6f6d 6d69 7474 6572 2920  mmit.committer) ",
            "-00005a80: 2b20 225c 6e22 290a 0a20 2020 2074 696d  + \"\\n\")..    tim",
            "-00005a90: 655f 7475 706c 6520 3d20 7469 6d65 2e67  e_tuple = time.g",
            "-00005aa0: 6d74 696d 6528 636f 6d6d 6974 2e61 7574  mtime(commit.aut",
            "-00005ab0: 686f 725f 7469 6d65 202b 2063 6f6d 6d69  hor_time + commi",
            "-00005ac0: 742e 6175 7468 6f72 5f74 696d 657a 6f6e  t.author_timezon",
            "-00005ad0: 6529 0a20 2020 2074 696d 655f 7374 7220  e).    time_str ",
            "-00005ae0: 3d20 7469 6d65 2e73 7472 6674 696d 6528  = time.strftime(",
            "-00005af0: 2225 6120 2562 2025 6420 2559 2025 483a  \"%a %b %d %Y %H:",
            "-00005b00: 254d 3a25 5322 2c20 7469 6d65 5f74 7570  %M:%S\", time_tup",
            "-00005b10: 6c65 290a 2020 2020 7469 6d65 7a6f 6e65  le).    timezone",
            "-00005b20: 5f73 7472 203d 2066 6f72 6d61 745f 7469  _str = format_ti",
            "-00005b30: 6d65 7a6f 6e65 2863 6f6d 6d69 742e 6175  mezone(commit.au",
            "-00005b40: 7468 6f72 5f74 696d 657a 6f6e 6529 2e64  thor_timezone).d",
            "-00005b50: 6563 6f64 6528 2261 7363 6969 2229 0a20  ecode(\"ascii\"). ",
            "-00005b60: 2020 206f 7574 7374 7265 616d 2e77 7269     outstream.wri",
            "-00005b70: 7465 2822 4461 7465 3a20 2020 2220 2b20  te(\"Date:   \" + ",
            "-00005b80: 7469 6d65 5f73 7472 202b 2022 2022 202b  time_str + \" \" +",
            "-00005b90: 2074 696d 657a 6f6e 655f 7374 7220 2b20   timezone_str + ",
            "-00005ba0: 225c 6e22 290a 2020 2020 6f75 7473 7472  \"\\n\").    outstr",
            "-00005bb0: 6561 6d2e 7772 6974 6528 225c 6e22 290a  eam.write(\"\\n\").",
            "-00005bc0: 2020 2020 6f75 7473 7472 6561 6d2e 7772      outstream.wr",
            "-00005bd0: 6974 6528 6465 636f 6465 2863 6f6d 6d69  ite(decode(commi",
            "-00005be0: 742e 6d65 7373 6167 6529 202b 2022 5c6e  t.message) + \"\\n",
            "-00005bf0: 2229 0a20 2020 206f 7574 7374 7265 616d  \").    outstream",
            "-00005c00: 2e77 7269 7465 2822 5c6e 2229 0a0a 0a64  .write(\"\\n\")...d",
            "-00005c10: 6566 2070 7269 6e74 5f74 6167 2874 6167  ef print_tag(tag",
            "-00005c20: 2c20 6465 636f 6465 2c20 6f75 7473 7472  , decode, outstr",
            "-00005c30: 6561 6d3d 7379 732e 7374 646f 7574 2920  eam=sys.stdout) ",
            "-00005c40: 2d3e 204e 6f6e 653a 0a20 2020 2022 2222  -> None:.    \"\"\"",
            "-00005c50: 5772 6974 6520 6120 6875 6d61 6e2d 7265  Write a human-re",
            "-00005c60: 6164 6162 6c65 2074 6167 2e0a 0a20 2020  adable tag...   ",
            "-00005c70: 2041 7267 733a 0a20 2020 2020 2074 6167   Args:.      tag",
            "-00005c80: 3a20 4120 6054 6167 6020 6f62 6a65 6374  : A `Tag` object",
            "-00005c90: 0a20 2020 2020 2064 6563 6f64 653a 2046  .      decode: F",
            "-00005ca0: 756e 6374 696f 6e20 666f 7220 6465 636f  unction for deco",
            "-00005cb0: 6469 6e67 2062 7974 6573 2074 6f20 756e  ding bytes to un",
            "-00005cc0: 6963 6f64 6520 7374 7269 6e67 0a20 2020  icode string.   ",
            "-00005cd0: 2020 206f 7574 7374 7265 616d 3a20 4120     outstream: A ",
            "-00005ce0: 7374 7265 616d 2074 6f20 7772 6974 6520  stream to write ",
            "-00005cf0: 746f 0a20 2020 2022 2222 0a20 2020 206f  to.    \"\"\".    o",
            "-00005d00: 7574 7374 7265 616d 2e77 7269 7465 2822  utstream.write(\"",
            "-00005d10: 5461 6767 6572 3a20 2220 2b20 6465 636f  Tagger: \" + deco",
            "-00005d20: 6465 2874 6167 2e74 6167 6765 7229 202b  de(tag.tagger) +",
            "-00005d30: 2022 5c6e 2229 0a20 2020 2074 696d 655f   \"\\n\").    time_",
            "-00005d40: 7475 706c 6520 3d20 7469 6d65 2e67 6d74  tuple = time.gmt",
            "-00005d50: 696d 6528 7461 672e 7461 675f 7469 6d65  ime(tag.tag_time",
            "-00005d60: 202b 2074 6167 2e74 6167 5f74 696d 657a   + tag.tag_timez",
            "-00005d70: 6f6e 6529 0a20 2020 2074 696d 655f 7374  one).    time_st",
            "-00005d80: 7220 3d20 7469 6d65 2e73 7472 6674 696d  r = time.strftim",
            "-00005d90: 6528 2225 6120 2562 2025 6420 2559 2025  e(\"%a %b %d %Y %",
            "-00005da0: 483a 254d 3a25 5322 2c20 7469 6d65 5f74  H:%M:%S\", time_t",
            "-00005db0: 7570 6c65 290a 2020 2020 7469 6d65 7a6f  uple).    timezo",
            "-00005dc0: 6e65 5f73 7472 203d 2066 6f72 6d61 745f  ne_str = format_",
            "-00005dd0: 7469 6d65 7a6f 6e65 2874 6167 2e74 6167  timezone(tag.tag",
            "-00005de0: 5f74 696d 657a 6f6e 6529 2e64 6563 6f64  _timezone).decod",
            "-00005df0: 6528 2261 7363 6969 2229 0a20 2020 206f  e(\"ascii\").    o",
            "-00005e00: 7574 7374 7265 616d 2e77 7269 7465 2822  utstream.write(\"",
            "-00005e10: 4461 7465 3a20 2020 2220 2b20 7469 6d65  Date:   \" + time",
            "-00005e20: 5f73 7472 202b 2022 2022 202b 2074 696d  _str + \" \" + tim",
            "-00005e30: 657a 6f6e 655f 7374 7220 2b20 225c 6e22  ezone_str + \"\\n\"",
            "-00005e40: 290a 2020 2020 6f75 7473 7472 6561 6d2e  ).    outstream.",
            "-00005e50: 7772 6974 6528 225c 6e22 290a 2020 2020  write(\"\\n\").    ",
            "-00005e60: 6f75 7473 7472 6561 6d2e 7772 6974 6528  outstream.write(",
            "-00005e70: 6465 636f 6465 2874 6167 2e6d 6573 7361  decode(tag.messa",
            "-00005e80: 6765 2929 0a20 2020 206f 7574 7374 7265  ge)).    outstre",
            "-00005e90: 616d 2e77 7269 7465 2822 5c6e 2229 0a0a  am.write(\"\\n\")..",
            "-00005ea0: 0a64 6566 2073 686f 775f 626c 6f62 2872  .def show_blob(r",
            "-00005eb0: 6570 6f2c 2062 6c6f 622c 2064 6563 6f64  epo, blob, decod",
            "-00005ec0: 652c 206f 7574 7374 7265 616d 3d73 7973  e, outstream=sys",
            "-00005ed0: 2e73 7464 6f75 7429 202d 3e20 4e6f 6e65  .stdout) -> None",
            "-00005ee0: 3a0a 2020 2020 2222 2257 7269 7465 2061  :.    \"\"\"Write a",
            "-00005ef0: 2062 6c6f 6220 746f 2061 2073 7472 6561   blob to a strea",
            "-00005f00: 6d2e 0a0a 2020 2020 4172 6773 3a0a 2020  m...    Args:.  ",
            "-00005f10: 2020 2020 7265 706f 3a20 4120 6052 6570      repo: A `Rep",
            "-00005f20: 6f60 206f 626a 6563 740a 2020 2020 2020  o` object.      ",
            "-00005f30: 626c 6f62 3a20 4120 6042 6c6f 6260 206f  blob: A `Blob` o",
            "-00005f40: 626a 6563 740a 2020 2020 2020 6465 636f  bject.      deco",
            "-00005f50: 6465 3a20 4675 6e63 7469 6f6e 2066 6f72  de: Function for",
            "-00005f60: 2064 6563 6f64 696e 6720 6279 7465 7320   decoding bytes ",
            "-00005f70: 746f 2075 6e69 636f 6465 2073 7472 696e  to unicode strin",
            "-00005f80: 670a 2020 2020 2020 6f75 7473 7472 6561  g.      outstrea",
            "-00005f90: 6d3a 2041 2073 7472 6561 6d20 6669 6c65  m: A stream file",
            "-00005fa0: 2074 6f20 7772 6974 6520 746f 0a20 2020   to write to.   ",
            "-00005fb0: 2022 2222 0a20 2020 206f 7574 7374 7265   \"\"\".    outstre",
            "-00005fc0: 616d 2e77 7269 7465 2864 6563 6f64 6528  am.write(decode(",
            "-00005fd0: 626c 6f62 2e64 6174 6129 290a 0a0a 6465  blob.data))...de",
            "-00005fe0: 6620 7368 6f77 5f63 6f6d 6d69 7428 7265  f show_commit(re",
            "-00005ff0: 706f 2c20 636f 6d6d 6974 2c20 6465 636f  po, commit, deco",
            "-00006000: 6465 2c20 6f75 7473 7472 6561 6d3d 7379  de, outstream=sy",
            "-00006010: 732e 7374 646f 7574 2920 2d3e 204e 6f6e  s.stdout) -> Non",
            "-00006020: 653a 0a20 2020 2022 2222 5368 6f77 2061  e:.    \"\"\"Show a",
            "-00006030: 2063 6f6d 6d69 7420 746f 2061 2073 7472   commit to a str",
            "-00006040: 6561 6d2e 0a0a 2020 2020 4172 6773 3a0a  eam...    Args:.",
            "-00006050: 2020 2020 2020 7265 706f 3a20 4120 6052        repo: A `R",
            "-00006060: 6570 6f60 206f 626a 6563 740a 2020 2020  epo` object.    ",
            "-00006070: 2020 636f 6d6d 6974 3a20 4120 6043 6f6d    commit: A `Com",
            "-00006080: 6d69 7460 206f 626a 6563 740a 2020 2020  mit` object.    ",
            "-00006090: 2020 6465 636f 6465 3a20 4675 6e63 7469    decode: Functi",
            "-000060a0: 6f6e 2066 6f72 2064 6563 6f64 696e 6720  on for decoding ",
            "-000060b0: 6279 7465 7320 746f 2075 6e69 636f 6465  bytes to unicode",
            "-000060c0: 2073 7472 696e 670a 2020 2020 2020 6f75   string.      ou",
            "-000060d0: 7473 7472 6561 6d3a 2053 7472 6561 6d20  tstream: Stream ",
            "-000060e0: 746f 2077 7269 7465 2074 6f0a 2020 2020  to write to.    ",
            "-000060f0: 2222 220a 2020 2020 7072 696e 745f 636f  \"\"\".    print_co",
            "-00006100: 6d6d 6974 2863 6f6d 6d69 742c 2064 6563  mmit(commit, dec",
            "-00006110: 6f64 653d 6465 636f 6465 2c20 6f75 7473  ode=decode, outs",
            "-00006120: 7472 6561 6d3d 6f75 7473 7472 6561 6d29  tream=outstream)",
            "-00006130: 0a20 2020 2069 6620 636f 6d6d 6974 2e70  .    if commit.p",
            "-00006140: 6172 656e 7473 3a0a 2020 2020 2020 2020  arents:.        ",
            "-00006150: 7061 7265 6e74 5f63 6f6d 6d69 7420 3d20  parent_commit = ",
            "-00006160: 7265 706f 5b63 6f6d 6d69 742e 7061 7265  repo[commit.pare",
            "-00006170: 6e74 735b 305d 5d0a 2020 2020 2020 2020  nts[0]].        ",
            "-00006180: 6261 7365 5f74 7265 6520 3d20 7061 7265  base_tree = pare",
            "-00006190: 6e74 5f63 6f6d 6d69 742e 7472 6565 0a20  nt_commit.tree. ",
            "-000061a0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       ",
            "-000061b0: 2062 6173 655f 7472 6565 203d 204e 6f6e   base_tree = Non",
            "-000061c0: 650a 2020 2020 6469 6666 7374 7265 616d  e.    diffstream",
            "-000061d0: 203d 2042 7974 6573 494f 2829 0a20 2020   = BytesIO().   ",
            "-000061e0: 2077 7269 7465 5f74 7265 655f 6469 6666   write_tree_diff",
            "-000061f0: 2864 6966 6673 7472 6561 6d2c 2072 6570  (diffstream, rep",
            "-00006200: 6f2e 6f62 6a65 6374 5f73 746f 7265 2c20  o.object_store, ",
            "-00006210: 6261 7365 5f74 7265 652c 2063 6f6d 6d69  base_tree, commi",
            "-00006220: 742e 7472 6565 290a 2020 2020 6469 6666  t.tree).    diff",
            "-00006230: 7374 7265 616d 2e73 6565 6b28 3029 0a20  stream.seek(0). ",
            "-00006240: 2020 206f 7574 7374 7265 616d 2e77 7269     outstream.wri",
            "-00006250: 7465 2863 6f6d 6d69 745f 6465 636f 6465  te(commit_decode",
            "-00006260: 2863 6f6d 6d69 742c 2064 6966 6673 7472  (commit, diffstr",
            "-00006270: 6561 6d2e 6765 7476 616c 7565 2829 2929  eam.getvalue()))",
            "-00006280: 0a0a 0a64 6566 2073 686f 775f 7472 6565  ...def show_tree",
            "-00006290: 2872 6570 6f2c 2074 7265 652c 2064 6563  (repo, tree, dec",
            "-000062a0: 6f64 652c 206f 7574 7374 7265 616d 3d73  ode, outstream=s",
            "-000062b0: 7973 2e73 7464 6f75 7429 202d 3e20 4e6f  ys.stdout) -> No",
            "-000062c0: 6e65 3a0a 2020 2020 2222 2250 7269 6e74  ne:.    \"\"\"Print",
            "-000062d0: 2061 2074 7265 6520 746f 2061 2073 7472   a tree to a str",
            "-000062e0: 6561 6d2e 0a0a 2020 2020 4172 6773 3a0a  eam...    Args:.",
            "-000062f0: 2020 2020 2020 7265 706f 3a20 4120 6052        repo: A `R",
            "-00006300: 6570 6f60 206f 626a 6563 740a 2020 2020  epo` object.    ",
            "-00006310: 2020 7472 6565 3a20 4120 6054 7265 6560    tree: A `Tree`",
            "-00006320: 206f 626a 6563 740a 2020 2020 2020 6465   object.      de",
            "-00006330: 636f 6465 3a20 4675 6e63 7469 6f6e 2066  code: Function f",
            "-00006340: 6f72 2064 6563 6f64 696e 6720 6279 7465  or decoding byte",
            "-00006350: 7320 746f 2075 6e69 636f 6465 2073 7472  s to unicode str",
            "-00006360: 696e 670a 2020 2020 2020 6f75 7473 7472  ing.      outstr",
            "-00006370: 6561 6d3a 2053 7472 6561 6d20 746f 2077  eam: Stream to w",
            "-00006380: 7269 7465 2074 6f0a 2020 2020 2222 220a  rite to.    \"\"\".",
            "-00006390: 2020 2020 666f 7220 6e20 696e 2074 7265      for n in tre",
            "-000063a0: 653a 0a20 2020 2020 2020 206f 7574 7374  e:.        outst",
            "-000063b0: 7265 616d 2e77 7269 7465 2864 6563 6f64  ream.write(decod",
            "-000063c0: 6528 6e29 202b 2022 5c6e 2229 0a0a 0a64  e(n) + \"\\n\")...d",
            "-000063d0: 6566 2073 686f 775f 7461 6728 7265 706f  ef show_tag(repo",
            "-000063e0: 2c20 7461 672c 2064 6563 6f64 652c 206f  , tag, decode, o",
            "-000063f0: 7574 7374 7265 616d 3d73 7973 2e73 7464  utstream=sys.std",
            "-00006400: 6f75 7429 202d 3e20 4e6f 6e65 3a0a 2020  out) -> None:.  ",
            "-00006410: 2020 2222 2250 7269 6e74 2061 2074 6167    \"\"\"Print a tag",
            "-00006420: 2074 6f20 6120 7374 7265 616d 2e0a 0a20   to a stream... ",
            "-00006430: 2020 2041 7267 733a 0a20 2020 2020 2072     Args:.      r",
            "-00006440: 6570 6f3a 2041 2060 5265 706f 6020 6f62  epo: A `Repo` ob",
            "-00006450: 6a65 6374 0a20 2020 2020 2074 6167 3a20  ject.      tag: ",
            "-00006460: 4120 6054 6167 6020 6f62 6a65 6374 0a20  A `Tag` object. ",
            "-00006470: 2020 2020 2064 6563 6f64 653a 2046 756e       decode: Fun",
            "-00006480: 6374 696f 6e20 666f 7220 6465 636f 6469  ction for decodi",
            "-00006490: 6e67 2062 7974 6573 2074 6f20 756e 6963  ng bytes to unic",
            "-000064a0: 6f64 6520 7374 7269 6e67 0a20 2020 2020  ode string.     ",
            "-000064b0: 206f 7574 7374 7265 616d 3a20 5374 7265   outstream: Stre",
            "-000064c0: 616d 2074 6f20 7772 6974 6520 746f 0a20  am to write to. ",
            "-000064d0: 2020 2022 2222 0a20 2020 2070 7269 6e74     \"\"\".    print",
            "-000064e0: 5f74 6167 2874 6167 2c20 6465 636f 6465  _tag(tag, decode",
            "-000064f0: 2c20 6f75 7473 7472 6561 6d29 0a20 2020  , outstream).   ",
            "-00006500: 2073 686f 775f 6f62 6a65 6374 2872 6570   show_object(rep",
            "-00006510: 6f2c 2072 6570 6f5b 7461 672e 6f62 6a65  o, repo[tag.obje",
            "-00006520: 6374 5b31 5d5d 2c20 6465 636f 6465 2c20  ct[1]], decode, ",
            "-00006530: 6f75 7473 7472 6561 6d29 0a0a 0a64 6566  outstream)...def",
            "-00006540: 2073 686f 775f 6f62 6a65 6374 2872 6570   show_object(rep",
            "-00006550: 6f2c 206f 626a 2c20 6465 636f 6465 2c20  o, obj, decode, ",
            "-00006560: 6f75 7473 7472 6561 6d29 3a0a 2020 2020  outstream):.    ",
            "-00006570: 7265 7475 726e 207b 0a20 2020 2020 2020  return {.       ",
            "-00006580: 2062 2274 7265 6522 3a20 7368 6f77 5f74   b\"tree\": show_t",
            "-00006590: 7265 652c 0a20 2020 2020 2020 2062 2262  ree,.        b\"b",
            "-000065a0: 6c6f 6222 3a20 7368 6f77 5f62 6c6f 622c  lob\": show_blob,",
            "-000065b0: 0a20 2020 2020 2020 2062 2263 6f6d 6d69  .        b\"commi",
            "-000065c0: 7422 3a20 7368 6f77 5f63 6f6d 6d69 742c  t\": show_commit,",
            "-000065d0: 0a20 2020 2020 2020 2062 2274 6167 223a  .        b\"tag\":",
            "-000065e0: 2073 686f 775f 7461 672c 0a20 2020 207d   show_tag,.    }",
            "-000065f0: 5b6f 626a 2e74 7970 655f 6e61 6d65 5d28  [obj.type_name](",
            "-00006600: 7265 706f 2c20 6f62 6a2c 2064 6563 6f64  repo, obj, decod",
            "-00006610: 652c 206f 7574 7374 7265 616d 290a 0a0a  e, outstream)...",
            "-00006620: 6465 6620 7072 696e 745f 6e61 6d65 5f73  def print_name_s",
            "-00006630: 7461 7475 7328 6368 616e 6765 7329 3a0a  tatus(changes):.",
            "-00006640: 2020 2020 2222 2250 7269 6e74 2061 2073      \"\"\"Print a s",
            "-00006650: 696d 706c 6520 7374 6174 7573 2073 756d  imple status sum",
            "-00006660: 6d61 7279 2c20 6c69 7374 696e 6720 6368  mary, listing ch",
            "-00006670: 616e 6765 6420 6669 6c65 732e 2222 220a  anged files.\"\"\".",
            "-00006680: 2020 2020 666f 7220 6368 616e 6765 2069      for change i",
            "-00006690: 6e20 6368 616e 6765 733a 0a20 2020 2020  n changes:.     ",
            "-000066a0: 2020 2069 6620 6e6f 7420 6368 616e 6765     if not change",
            "-000066b0: 3a0a 2020 2020 2020 2020 2020 2020 636f  :.            co",
            "-000066c0: 6e74 696e 7565 0a20 2020 2020 2020 2069  ntinue.        i",
            "-000066d0: 6620 6973 696e 7374 616e 6365 2863 6861  f isinstance(cha",
            "-000066e0: 6e67 652c 206c 6973 7429 3a0a 2020 2020  nge, list):.    ",
            "-000066f0: 2020 2020 2020 2020 6368 616e 6765 203d          change =",
            "-00006700: 2063 6861 6e67 655b 305d 0a20 2020 2020   change[0].     ",
            "-00006710: 2020 2069 6620 6368 616e 6765 2e74 7970     if change.typ",
            "-00006720: 6520 3d3d 2043 4841 4e47 455f 4144 443a  e == CHANGE_ADD:",
            "-00006730: 0a20 2020 2020 2020 2020 2020 2070 6174  .            pat",
            "-00006740: 6831 203d 2063 6861 6e67 652e 6e65 772e  h1 = change.new.",
            "-00006750: 7061 7468 0a20 2020 2020 2020 2020 2020  path.           ",
            "-00006760: 2070 6174 6832 203d 2022 220a 2020 2020   path2 = \"\".    ",
            "-00006770: 2020 2020 2020 2020 6b69 6e64 203d 2022          kind = \"",
            "-00006780: 4122 0a20 2020 2020 2020 2065 6c69 6620  A\".        elif ",
            "-00006790: 6368 616e 6765 2e74 7970 6520 3d3d 2043  change.type == C",
            "-000067a0: 4841 4e47 455f 4445 4c45 5445 3a0a 2020  HANGE_DELETE:.  ",
            "-000067b0: 2020 2020 2020 2020 2020 7061 7468 3120            path1 ",
            "-000067c0: 3d20 6368 616e 6765 2e6f 6c64 2e70 6174  = change.old.pat",
            "-000067d0: 680a 2020 2020 2020 2020 2020 2020 7061  h.            pa",
            "-000067e0: 7468 3220 3d20 2222 0a20 2020 2020 2020  th2 = \"\".       ",
            "-000067f0: 2020 2020 206b 696e 6420 3d20 2244 220a       kind = \"D\".",
            "-00006800: 2020 2020 2020 2020 656c 6966 2063 6861          elif cha",
            "-00006810: 6e67 652e 7479 7065 203d 3d20 4348 414e  nge.type == CHAN",
            "-00006820: 4745 5f4d 4f44 4946 593a 0a20 2020 2020  GE_MODIFY:.     ",
            "-00006830: 2020 2020 2020 2070 6174 6831 203d 2063         path1 = c",
            "-00006840: 6861 6e67 652e 6e65 772e 7061 7468 0a20  hange.new.path. ",
            "-00006850: 2020 2020 2020 2020 2020 2070 6174 6832             path2",
            "-00006860: 203d 2022 220a 2020 2020 2020 2020 2020   = \"\".          ",
            "-00006870: 2020 6b69 6e64 203d 2022 4d22 0a20 2020    kind = \"M\".   ",
            "-00006880: 2020 2020 2065 6c69 6620 6368 616e 6765       elif change",
            "-00006890: 2e74 7970 6520 696e 2052 454e 414d 455f  .type in RENAME_",
            "-000068a0: 4348 414e 4745 5f54 5950 4553 3a0a 2020  CHANGE_TYPES:.  ",
            "-000068b0: 2020 2020 2020 2020 2020 7061 7468 3120            path1 ",
            "-000068c0: 3d20 6368 616e 6765 2e6f 6c64 2e70 6174  = change.old.pat",
            "-000068d0: 680a 2020 2020 2020 2020 2020 2020 7061  h.            pa",
            "-000068e0: 7468 3220 3d20 6368 616e 6765 2e6e 6577  th2 = change.new",
            "-000068f0: 2e70 6174 680a 2020 2020 2020 2020 2020  .path.          ",
            "-00006900: 2020 6966 2063 6861 6e67 652e 7479 7065    if change.type",
            "-00006910: 203d 3d20 4348 414e 4745 5f52 454e 414d   == CHANGE_RENAM",
            "-00006920: 453a 0a20 2020 2020 2020 2020 2020 2020  E:.             ",
            "-00006930: 2020 206b 696e 6420 3d20 2252 220a 2020     kind = \"R\".  ",
            "-00006940: 2020 2020 2020 2020 2020 656c 6966 2063            elif c",
            "-00006950: 6861 6e67 652e 7479 7065 203d 3d20 4348  hange.type == CH",
            "-00006960: 414e 4745 5f43 4f50 593a 0a20 2020 2020  ANGE_COPY:.     ",
            "-00006970: 2020 2020 2020 2020 2020 206b 696e 6420             kind ",
            "-00006980: 3d20 2243 220a 2020 2020 2020 2020 7969  = \"C\".        yi",
            "-00006990: 656c 6420 2225 2d38 7325 2d32 3073 252d  eld \"%-8s%-20s%-",
            "-000069a0: 3230 7322 2025 2028 6b69 6e64 2c20 7061  20s\" % (kind, pa",
            "-000069b0: 7468 312c 2070 6174 6832 2920 2023 206e  th1, path2)  # n",
            "-000069c0: 6f71 613a 2055 5030 3331 0a0a 0a64 6566  oqa: UP031...def",
            "-000069d0: 206c 6f67 280a 2020 2020 7265 706f 3d22   log(.    repo=\"",
            "-000069e0: 2e22 2c0a 2020 2020 7061 7468 733d 4e6f  .\",.    paths=No",
            "-000069f0: 6e65 2c0a 2020 2020 6f75 7473 7472 6561  ne,.    outstrea",
            "-00006a00: 6d3d 7379 732e 7374 646f 7574 2c0a 2020  m=sys.stdout,.  ",
            "-00006a10: 2020 6d61 785f 656e 7472 6965 733d 4e6f    max_entries=No",
            "-00006a20: 6e65 2c0a 2020 2020 7265 7665 7273 653d  ne,.    reverse=",
            "-00006a30: 4661 6c73 652c 0a20 2020 206e 616d 655f  False,.    name_",
            "-00006a40: 7374 6174 7573 3d46 616c 7365 2c0a 2920  status=False,.) ",
            "-00006a50: 2d3e 204e 6f6e 653a 0a20 2020 2022 2222  -> None:.    \"\"\"",
            "-00006a60: 5772 6974 6520 636f 6d6d 6974 206c 6f67  Write commit log",
            "-00006a70: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  ",
            "-00006a80: 2020 2020 7265 706f 3a20 5061 7468 2074      repo: Path t",
            "-00006a90: 6f20 7265 706f 7369 746f 7279 0a20 2020  o repository.   ",
            "-00006aa0: 2020 2070 6174 6873 3a20 4f70 7469 6f6e     paths: Option",
            "-00006ab0: 616c 2073 6574 206f 6620 7370 6563 6966  al set of specif",
            "-00006ac0: 6963 2070 6174 6873 2074 6f20 7072 696e  ic paths to prin",
            "-00006ad0: 7420 656e 7472 6965 7320 666f 720a 2020  t entries for.  ",
            "-00006ae0: 2020 2020 6f75 7473 7472 6561 6d3a 2053      outstream: S",
            "-00006af0: 7472 6561 6d20 746f 2077 7269 7465 206c  tream to write l",
            "-00006b00: 6f67 206f 7574 7075 7420 746f 0a20 2020  og output to.   ",
            "-00006b10: 2020 2072 6576 6572 7365 3a20 5265 7665     reverse: Reve",
            "-00006b20: 7273 6520 6f72 6465 7220 696e 2077 6869  rse order in whi",
            "-00006b30: 6368 2065 6e74 7269 6573 2061 7265 2070  ch entries are p",
            "-00006b40: 7269 6e74 6564 0a20 2020 2020 206e 616d  rinted.      nam",
            "-00006b50: 655f 7374 6174 7573 3a20 5072 696e 7420  e_status: Print ",
            "-00006b60: 6e61 6d65 2073 7461 7475 730a 2020 2020  name status.    ",
            "-00006b70: 2020 6d61 785f 656e 7472 6965 733a 204f    max_entries: O",
            "-00006b80: 7074 696f 6e61 6c20 6d61 7869 6d75 6d20  ptional maximum ",
            "-00006b90: 6e75 6d62 6572 206f 6620 656e 7472 6965  number of entrie",
            "-00006ba0: 7320 746f 2064 6973 706c 6179 0a20 2020  s to display.   ",
            "-00006bb0: 2022 2222 0a20 2020 2077 6974 6820 6f70   \"\"\".    with op",
            "-00006bc0: 656e 5f72 6570 6f5f 636c 6f73 696e 6728  en_repo_closing(",
            "-00006bd0: 7265 706f 2920 6173 2072 3a0a 2020 2020  repo) as r:.    ",
            "-00006be0: 2020 2020 7761 6c6b 6572 203d 2072 2e67      walker = r.g",
            "-00006bf0: 6574 5f77 616c 6b65 7228 6d61 785f 656e  et_walker(max_en",
            "-00006c00: 7472 6965 733d 6d61 785f 656e 7472 6965  tries=max_entrie",
            "-00006c10: 732c 2070 6174 6873 3d70 6174 6873 2c20  s, paths=paths, ",
            "-00006c20: 7265 7665 7273 653d 7265 7665 7273 6529  reverse=reverse)",
            "-00006c30: 0a20 2020 2020 2020 2066 6f72 2065 6e74  .        for ent",
            "-00006c40: 7279 2069 6e20 7761 6c6b 6572 3a0a 0a20  ry in walker:.. ",
            "-00006c50: 2020 2020 2020 2020 2020 2064 6566 2064             def d",
            "-00006c60: 6563 6f64 6528 7829 3a0a 2020 2020 2020  ecode(x):.      ",
            "-00006c70: 2020 2020 2020 2020 2020 7265 7475 726e            return",
            "-00006c80: 2063 6f6d 6d69 745f 6465 636f 6465 2865   commit_decode(e",
            "-00006c90: 6e74 7279 2e63 6f6d 6d69 742c 2078 290a  ntry.commit, x).",
            "-00006ca0: 0a20 2020 2020 2020 2020 2020 2070 7269  .            pri",
            "-00006cb0: 6e74 5f63 6f6d 6d69 7428 656e 7472 792e  nt_commit(entry.",
            "-00006cc0: 636f 6d6d 6974 2c20 6465 636f 6465 2c20  commit, decode, ",
            "-00006cd0: 6f75 7473 7472 6561 6d29 0a20 2020 2020  outstream).     ",
            "-00006ce0: 2020 2020 2020 2069 6620 6e61 6d65 5f73         if name_s",
            "-00006cf0: 7461 7475 733a 0a20 2020 2020 2020 2020  tatus:.         ",
            "-00006d00: 2020 2020 2020 206f 7574 7374 7265 616d         outstream",
            "-00006d10: 2e77 7269 7465 6c69 6e65 7328 0a20 2020  .writelines(.   ",
            "-00006d20: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00006d30: 205b 6c69 6e65 202b 2022 5c6e 2220 666f   [line + \"\\n\" fo",
            "-00006d40: 7220 6c69 6e65 2069 6e20 7072 696e 745f  r line in print_",
            "-00006d50: 6e61 6d65 5f73 7461 7475 7328 656e 7472  name_status(entr",
            "-00006d60: 792e 6368 616e 6765 7328 2929 5d0a 2020  y.changes())].  ",
            "-00006d70: 2020 2020 2020 2020 2020 2020 2020 290a                ).",
            "-00006d80: 0a0a 2320 544f 444f 286a 656c 6d65 7229  ..# TODO(jelmer)",
            "-00006d90: 3a20 6265 7474 6572 2064 6566 6175 6c74  : better default",
            "-00006da0: 2066 6f72 2065 6e63 6f64 696e 673f 0a64   for encoding?.d",
            "-00006db0: 6566 2073 686f 7728 0a20 2020 2072 6570  ef show(.    rep",
            "-00006dc0: 6f3d 222e 222c 0a20 2020 206f 626a 6563  o=\".\",.    objec",
            "-00006dd0: 7473 3d4e 6f6e 652c 0a20 2020 206f 7574  ts=None,.    out",
            "-00006de0: 7374 7265 616d 3d73 7973 2e73 7464 6f75  stream=sys.stdou",
            "-00006df0: 742c 0a20 2020 2064 6566 6175 6c74 5f65  t,.    default_e",
            "-00006e00: 6e63 6f64 696e 673d 4445 4641 554c 545f  ncoding=DEFAULT_",
            "-00006e10: 454e 434f 4449 4e47 2c0a 2920 2d3e 204e  ENCODING,.) -> N",
            "-00006e20: 6f6e 653a 0a20 2020 2022 2222 5072 696e  one:.    \"\"\"Prin",
            "-00006e30: 7420 7468 6520 6368 616e 6765 7320 696e  t the changes in",
            "-00006e40: 2061 2063 6f6d 6d69 742e 0a0a 2020 2020   a commit...    ",
            "-00006e50: 4172 6773 3a0a 2020 2020 2020 7265 706f  Args:.      repo",
            "-00006e60: 3a20 5061 7468 2074 6f20 7265 706f 7369  : Path to reposi",
            "-00006e70: 746f 7279 0a20 2020 2020 206f 626a 6563  tory.      objec",
            "-00006e80: 7473 3a20 4f62 6a65 6374 7320 746f 2073  ts: Objects to s",
            "-00006e90: 686f 7720 2864 6566 6175 6c74 7320 746f  how (defaults to",
            "-00006ea0: 205b 4845 4144 5d29 0a20 2020 2020 206f   [HEAD]).      o",
            "-00006eb0: 7574 7374 7265 616d 3a20 5374 7265 616d  utstream: Stream",
            "-00006ec0: 2074 6f20 7772 6974 6520 746f 0a20 2020   to write to.   ",
            "-00006ed0: 2020 2064 6566 6175 6c74 5f65 6e63 6f64     default_encod",
            "-00006ee0: 696e 673a 2044 6566 6175 6c74 2065 6e63  ing: Default enc",
            "-00006ef0: 6f64 696e 6720 746f 2075 7365 2069 6620  oding to use if ",
            "-00006f00: 6e6f 6e65 2069 7320 7365 7420 696e 2074  none is set in t",
            "-00006f10: 6865 0a20 2020 2020 2020 2063 6f6d 6d69  he.        commi",
            "-00006f20: 740a 2020 2020 2222 220a 2020 2020 6966  t.    \"\"\".    if",
            "-00006f30: 206f 626a 6563 7473 2069 7320 4e6f 6e65   objects is None",
            "-00006f40: 3a0a 2020 2020 2020 2020 6f62 6a65 6374  :.        object",
            "-00006f50: 7320 3d20 5b22 4845 4144 225d 0a20 2020  s = [\"HEAD\"].   ",
            "-00006f60: 2069 6620 6e6f 7420 6973 696e 7374 616e   if not isinstan",
            "-00006f70: 6365 286f 626a 6563 7473 2c20 6c69 7374  ce(objects, list",
            "-00006f80: 293a 0a20 2020 2020 2020 206f 626a 6563  ):.        objec",
            "-00006f90: 7473 203d 205b 6f62 6a65 6374 735d 0a20  ts = [objects]. ",
            "-00006fa0: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "-00006fb0: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "-00006fc0: 6173 2072 3a0a 2020 2020 2020 2020 666f  as r:.        fo",
            "-00006fd0: 7220 6f62 6a65 6374 6973 6820 696e 206f  r objectish in o",
            "-00006fe0: 626a 6563 7473 3a0a 2020 2020 2020 2020  bjects:.        ",
            "-00006ff0: 2020 2020 6f20 3d20 7061 7273 655f 6f62      o = parse_ob",
            "-00007000: 6a65 6374 2872 2c20 6f62 6a65 6374 6973  ject(r, objectis",
            "-00007010: 6829 0a20 2020 2020 2020 2020 2020 2069  h).            i",
            "-00007020: 6620 6973 696e 7374 616e 6365 286f 2c20  f isinstance(o, ",
            "-00007030: 436f 6d6d 6974 293a 0a0a 2020 2020 2020  Commit):..      ",
            "-00007040: 2020 2020 2020 2020 2020 6465 6620 6465            def de",
            "-00007050: 636f 6465 2878 293a 0a20 2020 2020 2020  code(x):.       ",
            "-00007060: 2020 2020 2020 2020 2020 2020 2072 6574               ret",
            "-00007070: 7572 6e20 636f 6d6d 6974 5f64 6563 6f64  urn commit_decod",
            "-00007080: 6528 6f2c 2078 2c20 6465 6661 756c 745f  e(o, x, default_",
            "-00007090: 656e 636f 6469 6e67 290a 0a20 2020 2020  encoding)..     ",
            "-000070a0: 2020 2020 2020 2065 6c73 653a 0a0a 2020         else:..  ",
            "-000070b0: 2020 2020 2020 2020 2020 2020 2020 6465                de",
            "-000070c0: 6620 6465 636f 6465 2878 293a 0a20 2020  f decode(x):.   ",
            "-000070d0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000070e0: 2072 6574 7572 6e20 782e 6465 636f 6465   return x.decode",
            "-000070f0: 2864 6566 6175 6c74 5f65 6e63 6f64 696e  (default_encodin",
            "-00007100: 6729 0a0a 2020 2020 2020 2020 2020 2020  g)..            ",
            "-00007110: 7368 6f77 5f6f 626a 6563 7428 722c 206f  show_object(r, o",
            "-00007120: 2c20 6465 636f 6465 2c20 6f75 7473 7472  , decode, outstr",
            "-00007130: 6561 6d29 0a0a 0a64 6566 2064 6966 665f  eam)...def diff_",
            "-00007140: 7472 6565 2872 6570 6f2c 206f 6c64 5f74  tree(repo, old_t",
            "-00007150: 7265 652c 206e 6577 5f74 7265 652c 206f  ree, new_tree, o",
            "-00007160: 7574 7374 7265 616d 3d64 6566 6175 6c74  utstream=default",
            "-00007170: 5f62 7974 6573 5f6f 7574 5f73 7472 6561  _bytes_out_strea",
            "-00007180: 6d29 202d 3e20 4e6f 6e65 3a0a 2020 2020  m) -> None:.    ",
            "-00007190: 2222 2243 6f6d 7061 7265 7320 7468 6520  \"\"\"Compares the ",
            "-000071a0: 636f 6e74 656e 7420 616e 6420 6d6f 6465  content and mode",
            "-000071b0: 206f 6620 626c 6f62 7320 666f 756e 6420   of blobs found ",
            "-000071c0: 7669 6120 7477 6f20 7472 6565 206f 626a  via two tree obj",
            "-000071d0: 6563 7473 2e0a 0a20 2020 2041 7267 733a  ects...    Args:",
            "-000071e0: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "-000071f0: 6820 746f 2072 6570 6f73 6974 6f72 790a  h to repository.",
            "-00007200: 2020 2020 2020 6f6c 645f 7472 6565 3a20        old_tree: ",
            "-00007210: 4964 206f 6620 6f6c 6420 7472 6565 0a20  Id of old tree. ",
            "-00007220: 2020 2020 206e 6577 5f74 7265 653a 2049       new_tree: I",
            "-00007230: 6420 6f66 206e 6577 2074 7265 650a 2020  d of new tree.  ",
            "-00007240: 2020 2020 6f75 7473 7472 6561 6d3a 2053      outstream: S",
            "-00007250: 7472 6561 6d20 746f 2077 7269 7465 2074  tream to write t",
            "-00007260: 6f0a 2020 2020 2222 220a 2020 2020 7769  o.    \"\"\".    wi",
            "-00007270: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "-00007280: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "-00007290: 0a20 2020 2020 2020 2077 7269 7465 5f74  .        write_t",
            "-000072a0: 7265 655f 6469 6666 286f 7574 7374 7265  ree_diff(outstre",
            "-000072b0: 616d 2c20 722e 6f62 6a65 6374 5f73 746f  am, r.object_sto",
            "-000072c0: 7265 2c20 6f6c 645f 7472 6565 2c20 6e65  re, old_tree, ne",
            "-000072d0: 775f 7472 6565 290a 0a0a 6465 6620 7265  w_tree)...def re",
            "-000072e0: 765f 6c69 7374 2872 6570 6f2c 2063 6f6d  v_list(repo, com",
            "-000072f0: 6d69 7473 2c20 6f75 7473 7472 6561 6d3d  mits, outstream=",
            "-00007300: 7379 732e 7374 646f 7574 2920 2d3e 204e  sys.stdout) -> N",
            "-00007310: 6f6e 653a 0a20 2020 2022 2222 4c69 7374  one:.    \"\"\"List",
            "-00007320: 7320 636f 6d6d 6974 206f 626a 6563 7473  s commit objects",
            "-00007330: 2069 6e20 7265 7665 7273 6520 6368 726f   in reverse chro",
            "-00007340: 6e6f 6c6f 6769 6361 6c20 6f72 6465 722e  nological order.",
            "-00007350: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            "-00007360: 2020 7265 706f 3a20 5061 7468 2074 6f20    repo: Path to ",
            "-00007370: 7265 706f 7369 746f 7279 0a20 2020 2020  repository.     ",
            "-00007380: 2063 6f6d 6d69 7473 3a20 436f 6d6d 6974   commits: Commit",
            "-00007390: 7320 6f76 6572 2077 6869 6368 2074 6f20  s over which to ",
            "-000073a0: 6974 6572 6174 650a 2020 2020 2020 6f75  iterate.      ou",
            "-000073b0: 7473 7472 6561 6d3a 2053 7472 6561 6d20  tstream: Stream ",
            "-000073c0: 746f 2077 7269 7465 2074 6f0a 2020 2020  to write to.    ",
            "-000073d0: 2222 220a 2020 2020 7769 7468 206f 7065  \"\"\".    with ope",
            "-000073e0: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "-000073f0: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "-00007400: 2020 2066 6f72 2065 6e74 7279 2069 6e20     for entry in ",
            "-00007410: 722e 6765 745f 7761 6c6b 6572 2869 6e63  r.get_walker(inc",
            "-00007420: 6c75 6465 3d5b 725b 635d 2e69 6420 666f  lude=[r[c].id fo",
            "-00007430: 7220 6320 696e 2063 6f6d 6d69 7473 5d29  r c in commits])",
            "-00007440: 3a0a 2020 2020 2020 2020 2020 2020 6f75  :.            ou",
            "-00007450: 7473 7472 6561 6d2e 7772 6974 6528 656e  tstream.write(en",
            "-00007460: 7472 792e 636f 6d6d 6974 2e69 6420 2b20  try.commit.id + ",
            "-00007470: 6222 5c6e 2229 0a0a 0a64 6566 205f 6361  b\"\\n\")...def _ca",
            "-00007480: 6e6f 6e69 6361 6c5f 7061 7274 2875 726c  nonical_part(url",
            "-00007490: 3a20 7374 7229 202d 3e20 7374 723a 0a20  : str) -> str:. ",
            "-000074a0: 2020 206e 616d 6520 3d20 7572 6c2e 7273     name = url.rs",
            "-000074b0: 706c 6974 2822 2f22 2c20 3129 5b2d 315d  plit(\"/\", 1)[-1]",
            "-000074c0: 0a20 2020 2069 6620 6e61 6d65 2e65 6e64  .    if name.end",
            "-000074d0: 7377 6974 6828 222e 6769 7422 293a 0a20  swith(\".git\"):. ",
            "-000074e0: 2020 2020 2020 206e 616d 6520 3d20 6e61         name = na",
            "-000074f0: 6d65 5b3a 2d34 5d0a 2020 2020 7265 7475  me[:-4].    retu",
            "-00007500: 726e 206e 616d 650a 0a0a 6465 6620 7375  rn name...def su",
            "-00007510: 626d 6f64 756c 655f 6164 6428 7265 706f  bmodule_add(repo",
            "-00007520: 2c20 7572 6c2c 2070 6174 683d 4e6f 6e65  , url, path=None",
            "-00007530: 2c20 6e61 6d65 3d4e 6f6e 6529 202d 3e20  , name=None) -> ",
            "-00007540: 4e6f 6e65 3a0a 2020 2020 2222 2241 6464  None:.    \"\"\"Add",
            "-00007550: 2061 206e 6577 2073 7562 6d6f 6475 6c65   a new submodule",
            "-00007560: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "-00007570: 2020 2072 6570 6f3a 2050 6174 6820 746f     repo: Path to",
            "-00007580: 2072 6570 6f73 6974 6f72 790a 2020 2020   repository.    ",
            "-00007590: 2020 7572 6c3a 2055 524c 206f 6620 7265    url: URL of re",
            "-000075a0: 706f 7369 746f 7279 2074 6f20 6164 6420  pository to add ",
            "-000075b0: 6173 2073 7562 6d6f 6475 6c65 0a20 2020  as submodule.   ",
            "-000075c0: 2020 2070 6174 683a 2050 6174 6820 7768     path: Path wh",
            "-000075d0: 6572 6520 7375 626d 6f64 756c 6520 7368  ere submodule sh",
            "-000075e0: 6f75 6c64 206c 6976 650a 2020 2020 2222  ould live.    \"\"",
            "-000075f0: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "-00007600: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "-00007610: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "-00007620: 2069 6620 7061 7468 2069 7320 4e6f 6e65   if path is None",
            "-00007630: 3a0a 2020 2020 2020 2020 2020 2020 7061  :.            pa",
            "-00007640: 7468 203d 206f 732e 7061 7468 2e72 656c  th = os.path.rel",
            "-00007650: 7061 7468 285f 6361 6e6f 6e69 6361 6c5f  path(_canonical_",
            "-00007660: 7061 7274 2875 726c 292c 2072 2e70 6174  part(url), r.pat",
            "-00007670: 6829 0a20 2020 2020 2020 2069 6620 6e61  h).        if na",
            "-00007680: 6d65 2069 7320 4e6f 6e65 3a0a 2020 2020  me is None:.    ",
            "-00007690: 2020 2020 2020 2020 6e61 6d65 203d 2070          name = p",
            "-000076a0: 6174 680a 0a20 2020 2020 2020 2023 2054  ath..        # T",
            "-000076b0: 4f44 4f28 6a65 6c6d 6572 293a 204d 6f76  ODO(jelmer): Mov",
            "-000076c0: 6520 7468 6973 206c 6f67 6963 2074 6f20  e this logic to ",
            "-000076d0: 6475 6c77 6963 682e 7375 626d 6f64 756c  dulwich.submodul",
            "-000076e0: 650a 2020 2020 2020 2020 6769 746d 6f64  e.        gitmod",
            "-000076f0: 756c 6573 5f70 6174 6820 3d20 6f73 2e70  ules_path = os.p",
            "-00007700: 6174 682e 6a6f 696e 2872 2e70 6174 682c  ath.join(r.path,",
            "-00007710: 2022 2e67 6974 6d6f 6475 6c65 7322 290a   \".gitmodules\").",
            "-00007720: 2020 2020 2020 2020 7472 793a 0a20 2020          try:.   ",
            "-00007730: 2020 2020 2020 2020 2063 6f6e 6669 6720           config ",
            "-00007740: 3d20 436f 6e66 6967 4669 6c65 2e66 726f  = ConfigFile.fro",
            "-00007750: 6d5f 7061 7468 2867 6974 6d6f 6475 6c65  m_path(gitmodule",
            "-00007760: 735f 7061 7468 290a 2020 2020 2020 2020  s_path).        ",
            "-00007770: 6578 6365 7074 2046 696c 654e 6f74 466f  except FileNotFo",
            "-00007780: 756e 6445 7272 6f72 3a0a 2020 2020 2020  undError:.      ",
            "-00007790: 2020 2020 2020 636f 6e66 6967 203d 2043        config = C",
            "-000077a0: 6f6e 6669 6746 696c 6528 290a 2020 2020  onfigFile().    ",
            "-000077b0: 2020 2020 2020 2020 636f 6e66 6967 2e70          config.p",
            "-000077c0: 6174 6820 3d20 6769 746d 6f64 756c 6573  ath = gitmodules",
            "-000077d0: 5f70 6174 680a 2020 2020 2020 2020 636f  _path.        co",
            "-000077e0: 6e66 6967 2e73 6574 2828 2273 7562 6d6f  nfig.set((\"submo",
            "-000077f0: 6475 6c65 222c 206e 616d 6529 2c20 2275  dule\", name), \"u",
            "-00007800: 726c 222c 2075 726c 290a 2020 2020 2020  rl\", url).      ",
            "-00007810: 2020 636f 6e66 6967 2e73 6574 2828 2273    config.set((\"s",
            "-00007820: 7562 6d6f 6475 6c65 222c 206e 616d 6529  ubmodule\", name)",
            "-00007830: 2c20 2270 6174 6822 2c20 7061 7468 290a  , \"path\", path).",
            "-00007840: 2020 2020 2020 2020 636f 6e66 6967 2e77          config.w",
            "-00007850: 7269 7465 5f74 6f5f 7061 7468 2829 0a0a  rite_to_path()..",
            "-00007860: 0a64 6566 2073 7562 6d6f 6475 6c65 5f69  .def submodule_i",
            "-00007870: 6e69 7428 7265 706f 2920 2d3e 204e 6f6e  nit(repo) -> Non",
            "-00007880: 653a 0a20 2020 2022 2222 496e 6974 6961  e:.    \"\"\"Initia",
            "-00007890: 6c69 7a65 2073 7562 6d6f 6475 6c65 732e  lize submodules.",
            "-000078a0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            "-000078b0: 2020 7265 706f 3a20 5061 7468 2074 6f20    repo: Path to ",
            "-000078c0: 7265 706f 7369 746f 7279 0a20 2020 2022  repository.    \"",
            "-000078d0: 2222 0a20 2020 2077 6974 6820 6f70 656e  \"\".    with open",
            "-000078e0: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "-000078f0: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "-00007900: 2020 636f 6e66 6967 203d 2072 2e67 6574    config = r.get",
            "-00007910: 5f63 6f6e 6669 6728 290a 2020 2020 2020  _config().      ",
            "-00007920: 2020 6769 746d 6f64 756c 6573 5f70 6174    gitmodules_pat",
            "-00007930: 6820 3d20 6f73 2e70 6174 682e 6a6f 696e  h = os.path.join",
            "-00007940: 2872 2e70 6174 682c 2022 2e67 6974 6d6f  (r.path, \".gitmo",
            "-00007950: 6475 6c65 7322 290a 2020 2020 2020 2020  dules\").        ",
            "-00007960: 666f 7220 7061 7468 2c20 7572 6c2c 206e  for path, url, n",
            "-00007970: 616d 6520 696e 2072 6561 645f 7375 626d  ame in read_subm",
            "-00007980: 6f64 756c 6573 2867 6974 6d6f 6475 6c65  odules(gitmodule",
            "-00007990: 735f 7061 7468 293a 0a20 2020 2020 2020  s_path):.       ",
            "-000079a0: 2020 2020 2063 6f6e 6669 672e 7365 7428       config.set(",
            "-000079b0: 2862 2273 7562 6d6f 6475 6c65 222c 206e  (b\"submodule\", n",
            "-000079c0: 616d 6529 2c20 6222 6163 7469 7665 222c  ame), b\"active\",",
            "-000079d0: 2054 7275 6529 0a20 2020 2020 2020 2020   True).         ",
            "-000079e0: 2020 2063 6f6e 6669 672e 7365 7428 2862     config.set((b",
            "-000079f0: 2273 7562 6d6f 6475 6c65 222c 206e 616d  \"submodule\", nam",
            "-00007a00: 6529 2c20 6222 7572 6c22 2c20 7572 6c29  e), b\"url\", url)",
            "-00007a10: 0a20 2020 2020 2020 2063 6f6e 6669 672e  .        config.",
            "-00007a20: 7772 6974 655f 746f 5f70 6174 6828 290a  write_to_path().",
            "-00007a30: 0a0a 6465 6620 7375 626d 6f64 756c 655f  ..def submodule_",
            "-00007a40: 6c69 7374 2872 6570 6f29 3a0a 2020 2020  list(repo):.    ",
            "-00007a50: 2222 224c 6973 7420 7375 626d 6f64 756c  \"\"\"List submodul",
            "-00007a60: 6573 2e0a 0a20 2020 2041 7267 733a 0a20  es...    Args:. ",
            "-00007a70: 2020 2020 2072 6570 6f3a 2050 6174 6820       repo: Path ",
            "-00007a80: 746f 2072 6570 6f73 6974 6f72 790a 2020  to repository.  ",
            "-00007a90: 2020 2222 220a 2020 2020 6672 6f6d 202e    \"\"\".    from .",
            "-00007aa0: 7375 626d 6f64 756c 6520 696d 706f 7274  submodule import",
            "-00007ab0: 2069 7465 725f 6361 6368 6564 5f73 7562   iter_cached_sub",
            "-00007ac0: 6d6f 6475 6c65 730a 0a20 2020 2077 6974  modules..    wit",
            "-00007ad0: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "-00007ae0: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "-00007af0: 2020 2020 2020 2020 666f 7220 7061 7468          for path",
            "-00007b00: 2c20 7368 6120 696e 2069 7465 725f 6361  , sha in iter_ca",
            "-00007b10: 6368 6564 5f73 7562 6d6f 6475 6c65 7328  ched_submodules(",
            "-00007b20: 722e 6f62 6a65 6374 5f73 746f 7265 2c20  r.object_store, ",
            "-00007b30: 725b 722e 6865 6164 2829 5d2e 7472 6565  r[r.head()].tree",
            "-00007b40: 293a 0a20 2020 2020 2020 2020 2020 2079  ):.            y",
            "-00007b50: 6965 6c64 2070 6174 682c 2073 6861 2e64  ield path, sha.d",
            "-00007b60: 6563 6f64 6528 4445 4641 554c 545f 454e  ecode(DEFAULT_EN",
            "-00007b70: 434f 4449 4e47 290a 0a0a 6465 6620 7461  CODING)...def ta",
            "-00007b80: 675f 6372 6561 7465 280a 2020 2020 7265  g_create(.    re",
            "-00007b90: 706f 2c0a 2020 2020 7461 672c 0a20 2020  po,.    tag,.   ",
            "-00007ba0: 2061 7574 686f 723d 4e6f 6e65 2c0a 2020   author=None,.  ",
            "-00007bb0: 2020 6d65 7373 6167 653d 4e6f 6e65 2c0a    message=None,.",
            "-00007bc0: 2020 2020 616e 6e6f 7461 7465 643d 4661      annotated=Fa",
            "-00007bd0: 6c73 652c 0a20 2020 206f 626a 6563 7469  lse,.    objecti",
            "-00007be0: 7368 3d22 4845 4144 222c 0a20 2020 2074  sh=\"HEAD\",.    t",
            "-00007bf0: 6167 5f74 696d 653d 4e6f 6e65 2c0a 2020  ag_time=None,.  ",
            "-00007c00: 2020 7461 675f 7469 6d65 7a6f 6e65 3d4e    tag_timezone=N",
            "-00007c10: 6f6e 652c 0a20 2020 2073 6967 6e3d 4661  one,.    sign=Fa",
            "-00007c20: 6c73 652c 0a20 2020 2065 6e63 6f64 696e  lse,.    encodin",
            "-00007c30: 673d 4445 4641 554c 545f 454e 434f 4449  g=DEFAULT_ENCODI",
            "-00007c40: 4e47 2c0a 2920 2d3e 204e 6f6e 653a 0a20  NG,.) -> None:. ",
            "-00007c50: 2020 2022 2222 4372 6561 7465 7320 6120     \"\"\"Creates a ",
            "-00007c60: 7461 6720 696e 2067 6974 2076 6961 2064  tag in git via d",
            "-00007c70: 756c 7769 6368 2063 616c 6c73 2e0a 0a20  ulwich calls... ",
            "-00007c80: 2020 2041 7267 733a 0a20 2020 2020 2072     Args:.      r",
            "-00007c90: 6570 6f3a 2050 6174 6820 746f 2072 6570  epo: Path to rep",
            "-00007ca0: 6f73 6974 6f72 790a 2020 2020 2020 7461  ository.      ta",
            "-00007cb0: 673a 2074 6167 2073 7472 696e 670a 2020  g: tag string.  ",
            "-00007cc0: 2020 2020 6175 7468 6f72 3a20 7461 6720      author: tag ",
            "-00007cd0: 6175 7468 6f72 2028 6f70 7469 6f6e 616c  author (optional",
            "-00007ce0: 2c20 6966 2061 6e6e 6f74 6174 6564 2069  , if annotated i",
            "-00007cf0: 7320 7365 7429 0a20 2020 2020 206d 6573  s set).      mes",
            "-00007d00: 7361 6765 3a20 7461 6720 6d65 7373 6167  sage: tag messag",
            "-00007d10: 6520 286f 7074 696f 6e61 6c29 0a20 2020  e (optional).   ",
            "-00007d20: 2020 2061 6e6e 6f74 6174 6564 3a20 7768     annotated: wh",
            "-00007d30: 6574 6865 7220 746f 2063 7265 6174 6520  ether to create ",
            "-00007d40: 616e 2061 6e6e 6f74 6174 6564 2074 6167  an annotated tag",
            "-00007d50: 0a20 2020 2020 206f 626a 6563 7469 7368  .      objectish",
            "-00007d60: 3a20 6f62 6a65 6374 2074 6865 2074 6167  : object the tag",
            "-00007d70: 2073 686f 756c 6420 706f 696e 7420 6174   should point at",
            "-00007d80: 2c20 6465 6661 756c 7473 2074 6f20 4845  , defaults to HE",
            "-00007d90: 4144 0a20 2020 2020 2074 6167 5f74 696d  AD.      tag_tim",
            "-00007da0: 653a 204f 7074 696f 6e61 6c20 7469 6d65  e: Optional time",
            "-00007db0: 2066 6f72 2061 6e6e 6f74 6174 6564 2074   for annotated t",
            "-00007dc0: 6167 0a20 2020 2020 2074 6167 5f74 696d  ag.      tag_tim",
            "-00007dd0: 657a 6f6e 653a 204f 7074 696f 6e61 6c20  ezone: Optional ",
            "-00007de0: 7469 6d65 7a6f 6e65 2066 6f72 2061 6e6e  timezone for ann",
            "-00007df0: 6f74 6174 6564 2074 6167 0a20 2020 2020  otated tag.     ",
            "-00007e00: 2073 6967 6e3a 2047 5047 2053 6967 6e20   sign: GPG Sign ",
            "-00007e10: 7468 6520 7461 6720 2862 6f6f 6c2c 2064  the tag (bool, d",
            "-00007e20: 6566 6175 6c74 7320 746f 2046 616c 7365  efaults to False",
            "-00007e30: 2c0a 2020 2020 2020 2020 7061 7373 2054  ,.        pass T",
            "-00007e40: 7275 6520 746f 2075 7365 2064 6566 6175  rue to use defau",
            "-00007e50: 6c74 2047 5047 206b 6579 2c0a 2020 2020  lt GPG key,.    ",
            "-00007e60: 2020 2020 7061 7373 2061 2073 7472 2063      pass a str c",
            "-00007e70: 6f6e 7461 696e 696e 6720 4b65 7920 4944  ontaining Key ID",
            "-00007e80: 2074 6f20 7573 6520 6120 7370 6563 6966   to use a specif",
            "-00007e90: 6963 2047 5047 206b 6579 290a 2020 2020  ic GPG key).    ",
            "-00007ea0: 2222 220a 2020 2020 7769 7468 206f 7065  \"\"\".    with ope",
            "-00007eb0: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "-00007ec0: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "-00007ed0: 2020 206f 626a 6563 7420 3d20 7061 7273     object = pars",
            "-00007ee0: 655f 6f62 6a65 6374 2872 2c20 6f62 6a65  e_object(r, obje",
            "-00007ef0: 6374 6973 6829 0a0a 2020 2020 2020 2020  ctish)..        ",
            "-00007f00: 6966 2061 6e6e 6f74 6174 6564 3a0a 2020  if annotated:.  ",
            "-00007f10: 2020 2020 2020 2020 2020 2320 4372 6561            # Crea",
            "-00007f20: 7465 2074 6865 2074 6167 206f 626a 6563  te the tag objec",
            "-00007f30: 740a 2020 2020 2020 2020 2020 2020 7461  t.            ta",
            "-00007f40: 675f 6f62 6a20 3d20 5461 6728 290a 2020  g_obj = Tag().  ",
            "-00007f50: 2020 2020 2020 2020 2020 6966 2061 7574            if aut",
            "-00007f60: 686f 7220 6973 204e 6f6e 653a 0a20 2020  hor is None:.   ",
            "-00007f70: 2020 2020 2020 2020 2020 2020 2061 7574               aut",
            "-00007f80: 686f 7220 3d20 6765 745f 7573 6572 5f69  hor = get_user_i",
            "-00007f90: 6465 6e74 6974 7928 722e 6765 745f 636f  dentity(r.get_co",
            "-00007fa0: 6e66 6967 5f73 7461 636b 2829 290a 2020  nfig_stack()).  ",
            "-00007fb0: 2020 2020 2020 2020 2020 7461 675f 6f62            tag_ob",
            "-00007fc0: 6a2e 7461 6767 6572 203d 2061 7574 686f  j.tagger = autho",
            "-00007fd0: 720a 2020 2020 2020 2020 2020 2020 7461  r.            ta",
            "-00007fe0: 675f 6f62 6a2e 6d65 7373 6167 6520 3d20  g_obj.message = ",
            "-00007ff0: 6d65 7373 6167 6520 2b20 225c 6e22 2e65  message + \"\\n\".e",
            "-00008000: 6e63 6f64 6528 656e 636f 6469 6e67 290a  ncode(encoding).",
            "-00008010: 2020 2020 2020 2020 2020 2020 7461 675f              tag_",
            "-00008020: 6f62 6a2e 6e61 6d65 203d 2074 6167 0a20  obj.name = tag. ",
            "-00008030: 2020 2020 2020 2020 2020 2074 6167 5f6f             tag_o",
            "-00008040: 626a 2e6f 626a 6563 7420 3d20 2874 7970  bj.object = (typ",
            "-00008050: 6528 6f62 6a65 6374 292c 206f 626a 6563  e(object), objec",
            "-00008060: 742e 6964 290a 2020 2020 2020 2020 2020  t.id).          ",
            "-00008070: 2020 6966 2074 6167 5f74 696d 6520 6973    if tag_time is",
            "-00008080: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         ",
            "-00008090: 2020 2020 2020 2074 6167 5f74 696d 6520         tag_time ",
            "-000080a0: 3d20 696e 7428 7469 6d65 2e74 696d 6528  = int(time.time(",
            "-000080b0: 2929 0a20 2020 2020 2020 2020 2020 2074  )).            t",
            "-000080c0: 6167 5f6f 626a 2e74 6167 5f74 696d 6520  ag_obj.tag_time ",
            "-000080d0: 3d20 7461 675f 7469 6d65 0a20 2020 2020  = tag_time.     ",
            "-000080e0: 2020 2020 2020 2069 6620 7461 675f 7469         if tag_ti",
            "-000080f0: 6d65 7a6f 6e65 2069 7320 4e6f 6e65 3a0a  mezone is None:.",
            "-00008100: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00008110: 7461 675f 7469 6d65 7a6f 6e65 203d 2067  tag_timezone = g",
            "-00008120: 6574 5f75 7365 725f 7469 6d65 7a6f 6e65  et_user_timezone",
            "-00008130: 7328 295b 315d 0a20 2020 2020 2020 2020  s()[1].         ",
            "-00008140: 2020 2065 6c69 6620 6973 696e 7374 616e     elif isinstan",
            "-00008150: 6365 2874 6167 5f74 696d 657a 6f6e 652c  ce(tag_timezone,",
            "-00008160: 2073 7472 293a 0a20 2020 2020 2020 2020   str):.         ",
            "-00008170: 2020 2020 2020 2074 6167 5f74 696d 657a         tag_timez",
            "-00008180: 6f6e 6520 3d20 7061 7273 655f 7469 6d65  one = parse_time",
            "-00008190: 7a6f 6e65 2874 6167 5f74 696d 657a 6f6e  zone(tag_timezon",
            "-000081a0: 6529 0a20 2020 2020 2020 2020 2020 2074  e).            t",
            "-000081b0: 6167 5f6f 626a 2e74 6167 5f74 696d 657a  ag_obj.tag_timez",
            "-000081c0: 6f6e 6520 3d20 7461 675f 7469 6d65 7a6f  one = tag_timezo",
            "-000081d0: 6e65 0a20 2020 2020 2020 2020 2020 2069  ne.            i",
            "-000081e0: 6620 7369 676e 3a0a 2020 2020 2020 2020  f sign:.        ",
            "-000081f0: 2020 2020 2020 2020 7461 675f 6f62 6a2e          tag_obj.",
            "-00008200: 7369 676e 2873 6967 6e20 6966 2069 7369  sign(sign if isi",
            "-00008210: 6e73 7461 6e63 6528 7369 676e 2c20 7374  nstance(sign, st",
            "-00008220: 7229 2065 6c73 6520 4e6f 6e65 290a 0a20  r) else None).. ",
            "-00008230: 2020 2020 2020 2020 2020 2072 2e6f 626a             r.obj",
            "-00008240: 6563 745f 7374 6f72 652e 6164 645f 6f62  ect_store.add_ob",
            "-00008250: 6a65 6374 2874 6167 5f6f 626a 290a 2020  ject(tag_obj).  ",
            "-00008260: 2020 2020 2020 2020 2020 7461 675f 6964            tag_id",
            "-00008270: 203d 2074 6167 5f6f 626a 2e69 640a 2020   = tag_obj.id.  ",
            "-00008280: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    ",
            "-00008290: 2020 2020 2020 2020 7461 675f 6964 203d          tag_id =",
            "-000082a0: 206f 626a 6563 742e 6964 0a0a 2020 2020   object.id..    ",
            "-000082b0: 2020 2020 722e 7265 6673 5b5f 6d61 6b65      r.refs[_make",
            "-000082c0: 5f74 6167 5f72 6566 2874 6167 295d 203d  _tag_ref(tag)] =",
            "-000082d0: 2074 6167 5f69 640a 0a0a 6465 6620 7461   tag_id...def ta",
            "-000082e0: 675f 6c69 7374 2872 6570 6f2c 206f 7574  g_list(repo, out",
            "-000082f0: 7374 7265 616d 3d73 7973 2e73 7464 6f75  stream=sys.stdou",
            "-00008300: 7429 3a0a 2020 2020 2222 224c 6973 7420  t):.    \"\"\"List ",
            "-00008310: 616c 6c20 7461 6773 2e0a 0a20 2020 2041  all tags...    A",
            "-00008320: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "-00008330: 2050 6174 6820 746f 2072 6570 6f73 6974   Path to reposit",
            "-00008340: 6f72 790a 2020 2020 2020 6f75 7473 7472  ory.      outstr",
            "-00008350: 6561 6d3a 2053 7472 6561 6d20 746f 2077  eam: Stream to w",
            "-00008360: 7269 7465 2074 6167 7320 746f 0a20 2020  rite tags to.   ",
            "-00008370: 2022 2222 0a20 2020 2077 6974 6820 6f70   \"\"\".    with op",
            "-00008380: 656e 5f72 6570 6f5f 636c 6f73 696e 6728  en_repo_closing(",
            "-00008390: 7265 706f 2920 6173 2072 3a0a 2020 2020  repo) as r:.    ",
            "-000083a0: 2020 2020 7461 6773 203d 2073 6f72 7465      tags = sorte",
            "-000083b0: 6428 722e 7265 6673 2e61 735f 6469 6374  d(r.refs.as_dict",
            "-000083c0: 2862 2272 6566 732f 7461 6773 2229 290a  (b\"refs/tags\")).",
            "-000083d0: 2020 2020 2020 2020 7265 7475 726e 2074          return t",
            "-000083e0: 6167 730a 0a0a 6465 6620 7461 675f 6465  ags...def tag_de",
            "-000083f0: 6c65 7465 2872 6570 6f2c 206e 616d 6529  lete(repo, name)",
            "-00008400: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2222   -> None:.    \"\"",
            "-00008410: 2252 656d 6f76 6520 6120 7461 672e 0a0a  \"Remove a tag...",
            "-00008420: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "-00008430: 7265 706f 3a20 5061 7468 2074 6f20 7265  repo: Path to re",
            "-00008440: 706f 7369 746f 7279 0a20 2020 2020 206e  pository.      n",
            "-00008450: 616d 653a 204e 616d 6520 6f66 2074 6167  ame: Name of tag",
            "-00008460: 2074 6f20 7265 6d6f 7665 0a20 2020 2022   to remove.    \"",
            "-00008470: 2222 0a20 2020 2077 6974 6820 6f70 656e  \"\".    with open",
            "-00008480: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "-00008490: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "-000084a0: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(",
            "-000084b0: 6e61 6d65 2c20 6279 7465 7329 3a0a 2020  name, bytes):.  ",
            "-000084c0: 2020 2020 2020 2020 2020 6e61 6d65 7320            names ",
            "-000084d0: 3d20 5b6e 616d 655d 0a20 2020 2020 2020  = [name].       ",
            "-000084e0: 2065 6c69 6620 6973 696e 7374 616e 6365   elif isinstance",
            "-000084f0: 286e 616d 652c 206c 6973 7429 3a0a 2020  (name, list):.  ",
            "-00008500: 2020 2020 2020 2020 2020 6e61 6d65 7320            names ",
            "-00008510: 3d20 6e61 6d65 0a20 2020 2020 2020 2065  = name.        e",
            "-00008520: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           ",
            "-00008530: 2072 6169 7365 2045 7272 6f72 2866 2255   raise Error(f\"U",
            "-00008540: 6e65 7870 6563 7465 6420 7461 6720 6e61  nexpected tag na",
            "-00008550: 6d65 2074 7970 6520 7b6e 616d 6521 727d  me type {name!r}",
            "-00008560: 2229 0a20 2020 2020 2020 2066 6f72 206e  \").        for n",
            "-00008570: 616d 6520 696e 206e 616d 6573 3a0a 2020  ame in names:.  ",
            "-00008580: 2020 2020 2020 2020 2020 6465 6c20 722e            del r.",
            "-00008590: 7265 6673 5b5f 6d61 6b65 5f74 6167 5f72  refs[_make_tag_r",
            "-000085a0: 6566 286e 616d 6529 5d0a 0a0a 6465 6620  ef(name)]...def ",
            "-000085b0: 7265 7365 7428 7265 706f 2c20 6d6f 6465  reset(repo, mode",
            "-000085c0: 2c20 7472 6565 6973 683d 2248 4541 4422  , treeish=\"HEAD\"",
            "-000085d0: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    \"",
            "-000085e0: 2222 5265 7365 7420 6375 7272 656e 7420  \"\"Reset current ",
            "-000085f0: 4845 4144 2074 6f20 7468 6520 7370 6563  HEAD to the spec",
            "-00008600: 6966 6965 6420 7374 6174 652e 0a0a 2020  ified state...  ",
            "-00008610: 2020 4172 6773 3a0a 2020 2020 2020 7265    Args:.      re",
            "-00008620: 706f 3a20 5061 7468 2074 6f20 7265 706f  po: Path to repo",
            "-00008630: 7369 746f 7279 0a20 2020 2020 206d 6f64  sitory.      mod",
            "-00008640: 653a 204d 6f64 6520 2822 6861 7264 222c  e: Mode (\"hard\",",
            "-00008650: 2022 736f 6674 222c 2022 6d69 7865 6422   \"soft\", \"mixed\"",
            "-00008660: 290a 2020 2020 2020 7472 6565 6973 683a  ).      treeish:",
            "-00008670: 2054 7265 6569 7368 2074 6f20 7265 7365   Treeish to rese",
            "-00008680: 7420 746f 0a20 2020 2022 2222 0a20 2020  t to.    \"\"\".   ",
            "-00008690: 2069 6620 6d6f 6465 2021 3d20 2268 6172   if mode != \"har",
            "-000086a0: 6422 3a0a 2020 2020 2020 2020 7261 6973  d\":.        rais",
            "-000086b0: 6520 4572 726f 7228 2268 6172 6420 6973  e Error(\"hard is",
            "-000086c0: 2074 6865 206f 6e6c 7920 6d6f 6465 2063   the only mode c",
            "-000086d0: 7572 7265 6e74 6c79 2073 7570 706f 7274  urrently support",
            "-000086e0: 6564 2229 0a0a 2020 2020 7769 7468 206f  ed\")..    with o",
            "-000086f0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "-00008700: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "-00008710: 2020 2020 2074 7265 6520 3d20 7061 7273       tree = pars",
            "-00008720: 655f 7472 6565 2872 2c20 7472 6565 6973  e_tree(r, treeis",
            "-00008730: 6829 0a20 2020 2020 2020 2072 2e72 6573  h).        r.res",
            "-00008740: 6574 5f69 6e64 6578 2874 7265 652e 6964  et_index(tree.id",
            "-00008750: 290a 0a0a 6465 6620 6765 745f 7265 6d6f  )...def get_remo",
            "-00008760: 7465 5f72 6570 6f28 0a20 2020 2072 6570  te_repo(.    rep",
            "-00008770: 6f3a 2052 6570 6f2c 2072 656d 6f74 655f  o: Repo, remote_",
            "-00008780: 6c6f 6361 7469 6f6e 3a20 4f70 7469 6f6e  location: Option",
            "-00008790: 616c 5b55 6e69 6f6e 5b73 7472 2c20 6279  al[Union[str, by",
            "-000087a0: 7465 735d 5d20 3d20 4e6f 6e65 0a29 202d  tes]] = None.) -",
            "-000087b0: 3e20 7475 706c 655b 4f70 7469 6f6e 616c  > tuple[Optional",
            "-000087c0: 5b73 7472 5d2c 2073 7472 5d3a 0a20 2020  [str], str]:.   ",
            "-000087d0: 2063 6f6e 6669 6720 3d20 7265 706f 2e67   config = repo.g",
            "-000087e0: 6574 5f63 6f6e 6669 6728 290a 2020 2020  et_config().    ",
            "-000087f0: 6966 2072 656d 6f74 655f 6c6f 6361 7469  if remote_locati",
            "-00008800: 6f6e 2069 7320 4e6f 6e65 3a0a 2020 2020  on is None:.    ",
            "-00008810: 2020 2020 7265 6d6f 7465 5f6c 6f63 6174      remote_locat",
            "-00008820: 696f 6e20 3d20 6765 745f 6272 616e 6368  ion = get_branch",
            "-00008830: 5f72 656d 6f74 6528 7265 706f 290a 2020  _remote(repo).  ",
            "-00008840: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(",
            "-00008850: 7265 6d6f 7465 5f6c 6f63 6174 696f 6e2c  remote_location,",
            "-00008860: 2073 7472 293a 0a20 2020 2020 2020 2065   str):.        e",
            "-00008870: 6e63 6f64 6564 5f6c 6f63 6174 696f 6e20  ncoded_location ",
            "-00008880: 3d20 7265 6d6f 7465 5f6c 6f63 6174 696f  = remote_locatio",
            "-00008890: 6e2e 656e 636f 6465 2829 0a20 2020 2065  n.encode().    e",
            "-000088a0: 6c73 653a 0a20 2020 2020 2020 2065 6e63  lse:.        enc",
            "-000088b0: 6f64 6564 5f6c 6f63 6174 696f 6e20 3d20  oded_location = ",
            "-000088c0: 7265 6d6f 7465 5f6c 6f63 6174 696f 6e0a  remote_location.",
            "-000088d0: 0a20 2020 2073 6563 7469 6f6e 203d 2028  .    section = (",
            "-000088e0: 6222 7265 6d6f 7465 222c 2065 6e63 6f64  b\"remote\", encod",
            "-000088f0: 6564 5f6c 6f63 6174 696f 6e29 0a0a 2020  ed_location)..  ",
            "-00008900: 2020 7265 6d6f 7465 5f6e 616d 653a 204f    remote_name: O",
            "-00008910: 7074 696f 6e61 6c5b 7374 725d 203d 204e  ptional[str] = N",
            "-00008920: 6f6e 650a 0a20 2020 2069 6620 636f 6e66  one..    if conf",
            "-00008930: 6967 2e68 6173 5f73 6563 7469 6f6e 2873  ig.has_section(s",
            "-00008940: 6563 7469 6f6e 293a 0a20 2020 2020 2020  ection):.       ",
            "-00008950: 2072 656d 6f74 655f 6e61 6d65 203d 2065   remote_name = e",
            "-00008960: 6e63 6f64 6564 5f6c 6f63 6174 696f 6e2e  ncoded_location.",
            "-00008970: 6465 636f 6465 2829 0a20 2020 2020 2020  decode().       ",
            "-00008980: 2065 6e63 6f64 6564 5f6c 6f63 6174 696f   encoded_locatio",
            "-00008990: 6e20 3d20 636f 6e66 6967 2e67 6574 2873  n = config.get(s",
            "-000089a0: 6563 7469 6f6e 2c20 2275 726c 2229 0a20  ection, \"url\"). ",
            "-000089b0: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       ",
            "-000089c0: 2072 656d 6f74 655f 6e61 6d65 203d 204e   remote_name = N",
            "-000089d0: 6f6e 650a 0a20 2020 2072 6574 7572 6e20  one..    return ",
            "-000089e0: 2872 656d 6f74 655f 6e61 6d65 2c20 656e  (remote_name, en",
            "-000089f0: 636f 6465 645f 6c6f 6361 7469 6f6e 2e64  coded_location.d",
            "-00008a00: 6563 6f64 6528 2929 0a0a 0a64 6566 2070  ecode())...def p",
            "-00008a10: 7573 6828 0a20 2020 2072 6570 6f2c 0a20  ush(.    repo,. ",
            "-00008a20: 2020 2072 656d 6f74 655f 6c6f 6361 7469     remote_locati",
            "-00008a30: 6f6e 3d4e 6f6e 652c 0a20 2020 2072 6566  on=None,.    ref",
            "-00008a40: 7370 6563 733d 4e6f 6e65 2c0a 2020 2020  specs=None,.    ",
            "-00008a50: 6f75 7473 7472 6561 6d3d 6465 6661 756c  outstream=defaul",
            "-00008a60: 745f 6279 7465 735f 6f75 745f 7374 7265  t_bytes_out_stre",
            "-00008a70: 616d 2c0a 2020 2020 6572 7273 7472 6561  am,.    errstrea",
            "-00008a80: 6d3d 6465 6661 756c 745f 6279 7465 735f  m=default_bytes_",
            "-00008a90: 6572 725f 7374 7265 616d 2c0a 2020 2020  err_stream,.    ",
            "-00008aa0: 666f 7263 653d 4661 6c73 652c 0a20 2020  force=False,.   ",
            "-00008ab0: 202a 2a6b 7761 7267 732c 0a29 202d 3e20   **kwargs,.) -> ",
            "-00008ac0: 4e6f 6e65 3a0a 2020 2020 2222 2252 656d  None:.    \"\"\"Rem",
            "-00008ad0: 6f74 6520 7075 7368 2077 6974 6820 6475  ote push with du",
            "-00008ae0: 6c77 6963 6820 7669 6120 6475 6c77 6963  lwich via dulwic",
            "-00008af0: 682e 636c 6965 6e74 2e0a 0a20 2020 2041  h.client...    A",
            "-00008b00: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "-00008b10: 2050 6174 6820 746f 2072 6570 6f73 6974   Path to reposit",
            "-00008b20: 6f72 790a 2020 2020 2020 7265 6d6f 7465  ory.      remote",
            "-00008b30: 5f6c 6f63 6174 696f 6e3a 204c 6f63 6174  _location: Locat",
            "-00008b40: 696f 6e20 6f66 2074 6865 2072 656d 6f74  ion of the remot",
            "-00008b50: 650a 2020 2020 2020 7265 6673 7065 6373  e.      refspecs",
            "-00008b60: 3a20 5265 6673 2074 6f20 7075 7368 2074  : Refs to push t",
            "-00008b70: 6f20 7265 6d6f 7465 0a20 2020 2020 206f  o remote.      o",
            "-00008b80: 7574 7374 7265 616d 3a20 4120 7374 7265  utstream: A stre",
            "-00008b90: 616d 2066 696c 6520 746f 2077 7269 7465  am file to write",
            "-00008ba0: 206f 7574 7075 740a 2020 2020 2020 6572   output.      er",
            "-00008bb0: 7273 7472 6561 6d3a 2041 2073 7472 6561  rstream: A strea",
            "-00008bc0: 6d20 6669 6c65 2074 6f20 7772 6974 6520  m file to write ",
            "-00008bd0: 6572 726f 7273 0a20 2020 2020 2066 6f72  errors.      for",
            "-00008be0: 6365 3a20 466f 7263 6520 6f76 6572 7772  ce: Force overwr",
            "-00008bf0: 6974 696e 6720 7265 6673 0a20 2020 2022  iting refs.    \"",
            "-00008c00: 2222 0a20 2020 2023 204f 7065 6e20 7468  \"\".    # Open th",
            "-00008c10: 6520 7265 706f 0a20 2020 2077 6974 6820  e repo.    with ",
            "-00008c20: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "-00008c30: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "-00008c40: 2020 2020 2020 6966 2072 6566 7370 6563        if refspec",
            "-00008c50: 7320 6973 204e 6f6e 653a 0a20 2020 2020  s is None:.     ",
            "-00008c60: 2020 2020 2020 2072 6566 7370 6563 7320         refspecs ",
            "-00008c70: 3d20 5b61 6374 6976 655f 6272 616e 6368  = [active_branch",
            "-00008c80: 2872 295d 0a20 2020 2020 2020 2028 7265  (r)].        (re",
            "-00008c90: 6d6f 7465 5f6e 616d 652c 2072 656d 6f74  mote_name, remot",
            "-00008ca0: 655f 6c6f 6361 7469 6f6e 2920 3d20 6765  e_location) = ge",
            "-00008cb0: 745f 7265 6d6f 7465 5f72 6570 6f28 722c  t_remote_repo(r,",
            "-00008cc0: 2072 656d 6f74 655f 6c6f 6361 7469 6f6e   remote_location",
            "-00008cd0: 290a 0a20 2020 2020 2020 2023 2047 6574  )..        # Get",
            "-00008ce0: 2074 6865 2063 6c69 656e 7420 616e 6420   the client and ",
            "-00008cf0: 7061 7468 0a20 2020 2020 2020 2063 6c69  path.        cli",
            "-00008d00: 656e 742c 2070 6174 6820 3d20 6765 745f  ent, path = get_",
            "-00008d10: 7472 616e 7370 6f72 745f 616e 645f 7061  transport_and_pa",
            "-00008d20: 7468 280a 2020 2020 2020 2020 2020 2020  th(.            ",
            "-00008d30: 7265 6d6f 7465 5f6c 6f63 6174 696f 6e2c  remote_location,",
            "-00008d40: 2063 6f6e 6669 673d 722e 6765 745f 636f   config=r.get_co",
            "-00008d50: 6e66 6967 5f73 7461 636b 2829 2c20 2a2a  nfig_stack(), **",
            "-00008d60: 6b77 6172 6773 0a20 2020 2020 2020 2029  kwargs.        )",
            "-00008d70: 0a0a 2020 2020 2020 2020 7365 6c65 6374  ..        select",
            "-00008d80: 6564 5f72 6566 7320 3d20 5b5d 0a20 2020  ed_refs = [].   ",
            "-00008d90: 2020 2020 2072 656d 6f74 655f 6368 616e       remote_chan",
            "-00008da0: 6765 645f 7265 6673 203d 207b 7d0a 0a20  ged_refs = {}.. ",
            "-00008db0: 2020 2020 2020 2064 6566 2075 7064 6174         def updat",
            "-00008dc0: 655f 7265 6673 2872 6566 7329 3a0a 2020  e_refs(refs):.  ",
            "-00008dd0: 2020 2020 2020 2020 2020 7365 6c65 6374            select",
            "-00008de0: 6564 5f72 6566 732e 6578 7465 6e64 2870  ed_refs.extend(p",
            "-00008df0: 6172 7365 5f72 6566 7475 706c 6573 2872  arse_reftuples(r",
            "-00008e00: 2e72 6566 732c 2072 6566 732c 2072 6566  .refs, refs, ref",
            "-00008e10: 7370 6563 732c 2066 6f72 6365 3d66 6f72  specs, force=for",
            "-00008e20: 6365 2929 0a20 2020 2020 2020 2020 2020  ce)).           ",
            "-00008e30: 206e 6577 5f72 6566 7320 3d20 7b7d 0a20   new_refs = {}. ",
            "-00008e40: 2020 2020 2020 2020 2020 2023 2054 4f44             # TOD",
            "-00008e50: 4f3a 2048 616e 646c 6520 7365 6c65 6374  O: Handle select",
            "-00008e60: 6564 5f72 6566 7320 3d3d 207b 4e6f 6e65  ed_refs == {None",
            "-00008e70: 3a20 4e6f 6e65 7d0a 2020 2020 2020 2020  : None}.        ",
            "-00008e80: 2020 2020 666f 7220 6c68 2c20 7268 2c20      for lh, rh, ",
            "-00008e90: 666f 7263 655f 7265 6620 696e 2073 656c  force_ref in sel",
            "-00008ea0: 6563 7465 645f 7265 6673 3a0a 2020 2020  ected_refs:.    ",
            "-00008eb0: 2020 2020 2020 2020 2020 2020 6966 206c              if l",
            "-00008ec0: 6820 6973 204e 6f6e 653a 0a20 2020 2020  h is None:.     ",
            "-00008ed0: 2020 2020 2020 2020 2020 2020 2020 206e                 n",
            "-00008ee0: 6577 5f72 6566 735b 7268 5d20 3d20 5a45  ew_refs[rh] = ZE",
            "-00008ef0: 524f 5f53 4841 0a20 2020 2020 2020 2020  RO_SHA.         ",
            "-00008f00: 2020 2020 2020 2020 2020 2072 656d 6f74             remot",
            "-00008f10: 655f 6368 616e 6765 645f 7265 6673 5b72  e_changed_refs[r",
            "-00008f20: 685d 203d 204e 6f6e 650a 2020 2020 2020  h] = None.      ",
            "-00008f30: 2020 2020 2020 2020 2020 656c 7365 3a0a            else:.",
            "-00008f40: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00008f50: 2020 2020 7472 793a 0a20 2020 2020 2020      try:.       ",
            "-00008f60: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00008f70: 206c 6f63 616c 7368 6120 3d20 722e 7265   localsha = r.re",
            "-00008f80: 6673 5b6c 685d 0a20 2020 2020 2020 2020  fs[lh].         ",
            "-00008f90: 2020 2020 2020 2020 2020 2065 7863 6570             excep",
            "-00008fa0: 7420 4b65 7945 7272 6f72 2061 7320 6578  t KeyError as ex",
            "-00008fb0: 633a 0a20 2020 2020 2020 2020 2020 2020  c:.             ",
            "-00008fc0: 2020 2020 2020 2020 2020 2072 6169 7365             raise",
            "-00008fd0: 2045 7272 6f72 2866 224e 6f20 7661 6c69   Error(f\"No vali",
            "-00008fe0: 6420 7265 6620 7b6c 687d 2069 6e20 6c6f  d ref {lh} in lo",
            "-00008ff0: 6361 6c20 7265 706f 7369 746f 7279 2229  cal repository\")",
            "-00009000: 2066 726f 6d20 6578 630a 2020 2020 2020   from exc.      ",
            "-00009010: 2020 2020 2020 2020 2020 2020 2020 6966                if",
            "-00009020: 206e 6f74 2066 6f72 6365 5f72 6566 2061   not force_ref a",
            "-00009030: 6e64 2072 6820 696e 2072 6566 733a 0a20  nd rh in refs:. ",
            "-00009040: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009050: 2020 2020 2020 2063 6865 636b 5f64 6976         check_div",
            "-00009060: 6572 6765 6428 722c 2072 6566 735b 7268  erged(r, refs[rh",
            "-00009070: 5d2c 206c 6f63 616c 7368 6129 0a20 2020  ], localsha).   ",
            "-00009080: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009090: 206e 6577 5f72 6566 735b 7268 5d20 3d20   new_refs[rh] = ",
            "-000090a0: 6c6f 6361 6c73 6861 0a20 2020 2020 2020  localsha.       ",
            "-000090b0: 2020 2020 2020 2020 2020 2020 2072 656d               rem",
            "-000090c0: 6f74 655f 6368 616e 6765 645f 7265 6673  ote_changed_refs",
            "-000090d0: 5b72 685d 203d 206c 6f63 616c 7368 610a  [rh] = localsha.",
            "-000090e0: 2020 2020 2020 2020 2020 2020 7265 7475              retu",
            "-000090f0: 726e 206e 6577 5f72 6566 730a 0a20 2020  rn new_refs..   ",
            "-00009100: 2020 2020 2065 7272 5f65 6e63 6f64 696e       err_encodin",
            "-00009110: 6720 3d20 6765 7461 7474 7228 6572 7273  g = getattr(errs",
            "-00009120: 7472 6561 6d2c 2022 656e 636f 6469 6e67  tream, \"encoding",
            "-00009130: 222c 204e 6f6e 6529 206f 7220 4445 4641  \", None) or DEFA",
            "-00009140: 554c 545f 454e 434f 4449 4e47 0a20 2020  ULT_ENCODING.   ",
            "-00009150: 2020 2020 2072 656d 6f74 655f 6c6f 6361       remote_loca",
            "-00009160: 7469 6f6e 203d 2063 6c69 656e 742e 6765  tion = client.ge",
            "-00009170: 745f 7572 6c28 7061 7468 290a 2020 2020  t_url(path).    ",
            "-00009180: 2020 2020 7472 793a 0a20 2020 2020 2020      try:.       ",
            "-00009190: 2020 2020 2072 6573 756c 7420 3d20 636c       result = cl",
            "-000091a0: 6965 6e74 2e73 656e 645f 7061 636b 280a  ient.send_pack(.",
            "-000091b0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000091c0: 7061 7468 2c0a 2020 2020 2020 2020 2020  path,.          ",
            "-000091d0: 2020 2020 2020 7570 6461 7465 5f72 6566        update_ref",
            "-000091e0: 732c 0a20 2020 2020 2020 2020 2020 2020  s,.             ",
            "-000091f0: 2020 2067 656e 6572 6174 655f 7061 636b     generate_pack",
            "-00009200: 5f64 6174 613d 722e 6765 6e65 7261 7465  _data=r.generate",
            "-00009210: 5f70 6163 6b5f 6461 7461 2c0a 2020 2020  _pack_data,.    ",
            "-00009220: 2020 2020 2020 2020 2020 2020 7072 6f67              prog",
            "-00009230: 7265 7373 3d65 7272 7374 7265 616d 2e77  ress=errstream.w",
            "-00009240: 7269 7465 2c0a 2020 2020 2020 2020 2020  rite,.          ",
            "-00009250: 2020 290a 2020 2020 2020 2020 6578 6365    ).        exce",
            "-00009260: 7074 2053 656e 6450 6163 6b45 7272 6f72  pt SendPackError",
            "-00009270: 2061 7320 6578 633a 0a20 2020 2020 2020   as exc:.       ",
            "-00009280: 2020 2020 2072 6169 7365 2045 7272 6f72       raise Error",
            "-00009290: 280a 2020 2020 2020 2020 2020 2020 2020  (.              ",
            "-000092a0: 2020 2250 7573 6820 746f 2022 202b 2072    \"Push to \" + r",
            "-000092b0: 656d 6f74 655f 6c6f 6361 7469 6f6e 202b  emote_location +",
            "-000092c0: 2022 2066 6169 6c65 6420 2d3e 2022 202b   \" failed -> \" +",
            "-000092d0: 2065 7863 2e61 7267 735b 305d 2e64 6563   exc.args[0].dec",
            "-000092e0: 6f64 6528 292c 0a20 2020 2020 2020 2020  ode(),.         ",
            "-000092f0: 2020 2029 2066 726f 6d20 6578 630a 2020     ) from exc.  ",
            "-00009300: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    ",
            "-00009310: 2020 2020 2020 2020 6572 7273 7472 6561          errstrea",
            "-00009320: 6d2e 7772 6974 6528 0a20 2020 2020 2020  m.write(.       ",
            "-00009330: 2020 2020 2020 2020 2062 2250 7573 6820           b\"Push ",
            "-00009340: 746f 2022 202b 2072 656d 6f74 655f 6c6f  to \" + remote_lo",
            "-00009350: 6361 7469 6f6e 2e65 6e63 6f64 6528 6572  cation.encode(er",
            "-00009360: 725f 656e 636f 6469 6e67 2920 2b20 6222  r_encoding) + b\"",
            "-00009370: 2073 7563 6365 7373 6675 6c2e 5c6e 220a   successful.\\n\".",
            "-00009380: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. ",
            "-00009390: 2020 2020 2020 2066 6f72 2072 6566 2c20         for ref, ",
            "-000093a0: 6572 726f 7220 696e 2028 7265 7375 6c74  error in (result",
            "-000093b0: 2e72 6566 5f73 7461 7475 7320 6f72 207b  .ref_status or {",
            "-000093c0: 7d29 2e69 7465 6d73 2829 3a0a 2020 2020  }).items():.    ",
            "-000093d0: 2020 2020 2020 2020 6966 2065 7272 6f72          if error",
            "-000093e0: 2069 7320 6e6f 7420 4e6f 6e65 3a0a 2020   is not None:.  ",
            "-000093f0: 2020 2020 2020 2020 2020 2020 2020 6572                er",
            "-00009400: 7273 7472 6561 6d2e 7772 6974 6528 0a20  rstream.write(. ",
            "-00009410: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009420: 2020 2062 2250 7573 6820 6f66 2072 6566     b\"Push of ref",
            "-00009430: 2025 7320 6661 696c 6564 3a20 2573 5c6e   %s failed: %s\\n",
            "-00009440: 2220 2520 2872 6566 2c20 6572 726f 722e  \" % (ref, error.",
            "-00009450: 656e 636f 6465 2865 7272 5f65 6e63 6f64  encode(err_encod",
            "-00009460: 696e 6729 290a 2020 2020 2020 2020 2020  ing)).          ",
            "-00009470: 2020 2020 2020 290a 2020 2020 2020 2020        ).        ",
            "-00009480: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      ",
            "-00009490: 2020 2020 2020 2020 2020 6572 7273 7472            errstr",
            "-000094a0: 6561 6d2e 7772 6974 6528 6222 5265 6620  eam.write(b\"Ref ",
            "-000094b0: 2573 2075 7064 6174 6564 5c6e 2220 2520  %s updated\\n\" % ",
            "-000094c0: 7265 6629 0a0a 2020 2020 2020 2020 6966  ref)..        if",
            "-000094d0: 2072 656d 6f74 655f 6e61 6d65 2069 7320   remote_name is ",
            "-000094e0: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      ",
            "-000094f0: 2020 2020 2020 5f69 6d70 6f72 745f 7265        _import_re",
            "-00009500: 6d6f 7465 5f72 6566 7328 722e 7265 6673  mote_refs(r.refs",
            "-00009510: 2c20 7265 6d6f 7465 5f6e 616d 652c 2072  , remote_name, r",
            "-00009520: 656d 6f74 655f 6368 616e 6765 645f 7265  emote_changed_re",
            "-00009530: 6673 290a 0a0a 6465 6620 7075 6c6c 280a  fs)...def pull(.",
            "-00009540: 2020 2020 7265 706f 2c0a 2020 2020 7265      repo,.    re",
            "-00009550: 6d6f 7465 5f6c 6f63 6174 696f 6e3d 4e6f  mote_location=No",
            "-00009560: 6e65 2c0a 2020 2020 7265 6673 7065 6373  ne,.    refspecs",
            "-00009570: 3d4e 6f6e 652c 0a20 2020 206f 7574 7374  =None,.    outst",
            "-00009580: 7265 616d 3d64 6566 6175 6c74 5f62 7974  ream=default_byt",
            "-00009590: 6573 5f6f 7574 5f73 7472 6561 6d2c 0a20  es_out_stream,. ",
            "-000095a0: 2020 2065 7272 7374 7265 616d 3d64 6566     errstream=def",
            "-000095b0: 6175 6c74 5f62 7974 6573 5f65 7272 5f73  ault_bytes_err_s",
            "-000095c0: 7472 6561 6d2c 0a20 2020 2066 6173 745f  tream,.    fast_",
            "-000095d0: 666f 7277 6172 643d 5472 7565 2c0a 2020  forward=True,.  ",
            "-000095e0: 2020 666f 7263 653d 4661 6c73 652c 0a20    force=False,. ",
            "-000095f0: 2020 2066 696c 7465 725f 7370 6563 3d4e     filter_spec=N",
            "-00009600: 6f6e 652c 0a20 2020 2070 726f 746f 636f  one,.    protoco",
            "-00009610: 6c5f 7665 7273 696f 6e3d 4e6f 6e65 2c0a  l_version=None,.",
            "-00009620: 2020 2020 2a2a 6b77 6172 6773 2c0a 2920      **kwargs,.) ",
            "-00009630: 2d3e 204e 6f6e 653a 0a20 2020 2022 2222  -> None:.    \"\"\"",
            "-00009640: 5075 6c6c 2066 726f 6d20 7265 6d6f 7465  Pull from remote",
            "-00009650: 2076 6961 2064 756c 7769 6368 2e63 6c69   via dulwich.cli",
            "-00009660: 656e 742e 0a0a 2020 2020 4172 6773 3a0a  ent...    Args:.",
            "-00009670: 2020 2020 2020 7265 706f 3a20 5061 7468        repo: Path",
            "-00009680: 2074 6f20 7265 706f 7369 746f 7279 0a20   to repository. ",
            "-00009690: 2020 2020 2072 656d 6f74 655f 6c6f 6361       remote_loca",
            "-000096a0: 7469 6f6e 3a20 4c6f 6361 7469 6f6e 206f  tion: Location o",
            "-000096b0: 6620 7468 6520 7265 6d6f 7465 0a20 2020  f the remote.   ",
            "-000096c0: 2020 2072 6566 7370 6563 733a 2072 6566     refspecs: ref",
            "-000096d0: 7370 6563 7320 746f 2066 6574 6368 2e20  specs to fetch. ",
            "-000096e0: 4361 6e20 6265 2061 2062 7974 6573 7472  Can be a bytestr",
            "-000096f0: 696e 672c 2061 2073 7472 696e 672c 206f  ing, a string, o",
            "-00009700: 7220 6120 6c69 7374 206f 660a 2020 2020  r a list of.    ",
            "-00009710: 2020 2020 6279 7465 7374 7269 6e67 2f73      bytestring/s",
            "-00009720: 7472 696e 672e 0a20 2020 2020 206f 7574  tring..      out",
            "-00009730: 7374 7265 616d 3a20 4120 7374 7265 616d  stream: A stream",
            "-00009740: 2066 696c 6520 746f 2077 7269 7465 2074   file to write t",
            "-00009750: 6f20 6f75 7470 7574 0a20 2020 2020 2065  o output.      e",
            "-00009760: 7272 7374 7265 616d 3a20 4120 7374 7265  rrstream: A stre",
            "-00009770: 616d 2066 696c 6520 746f 2077 7269 7465  am file to write",
            "-00009780: 2074 6f20 6572 726f 7273 0a20 2020 2020   to errors.     ",
            "-00009790: 2066 696c 7465 725f 7370 6563 3a20 4120   filter_spec: A ",
            "-000097a0: 6769 742d 7265 762d 6c69 7374 2d73 7479  git-rev-list-sty",
            "-000097b0: 6c65 206f 626a 6563 7420 6669 6c74 6572  le object filter",
            "-000097c0: 2073 7065 632c 2061 7320 616e 2041 5343   spec, as an ASC",
            "-000097d0: 4949 2073 7472 696e 672e 0a20 2020 2020  II string..     ",
            "-000097e0: 2020 204f 6e6c 7920 7573 6564 2069 6620     Only used if ",
            "-000097f0: 7468 6520 7365 7276 6572 2073 7570 706f  the server suppo",
            "-00009800: 7274 7320 7468 6520 4769 7420 7072 6f74  rts the Git prot",
            "-00009810: 6f63 6f6c 2d76 3220 2766 696c 7465 7227  ocol-v2 'filter'",
            "-00009820: 0a20 2020 2020 2020 2066 6561 7475 7265  .        feature",
            "-00009830: 2c20 616e 6420 6967 6e6f 7265 6420 6f74  , and ignored ot",
            "-00009840: 6865 7277 6973 652e 0a20 2020 2020 2070  herwise..      p",
            "-00009850: 726f 746f 636f 6c5f 7665 7273 696f 6e3a  rotocol_version:",
            "-00009860: 2064 6573 6972 6564 2047 6974 2070 726f   desired Git pro",
            "-00009870: 746f 636f 6c20 7665 7273 696f 6e2e 2042  tocol version. B",
            "-00009880: 7920 6465 6661 756c 7420 7468 6520 6869  y default the hi",
            "-00009890: 6768 6573 740a 2020 2020 2020 2020 6d75  ghest.        mu",
            "-000098a0: 7475 616c 6c79 2073 7570 706f 7274 6564  tually supported",
            "-000098b0: 2070 726f 746f 636f 6c20 7665 7273 696f   protocol versio",
            "-000098c0: 6e20 7769 6c6c 2062 6520 7573 6564 0a20  n will be used. ",
            "-000098d0: 2020 2022 2222 0a20 2020 2023 204f 7065     \"\"\".    # Ope",
            "-000098e0: 6e20 7468 6520 7265 706f 0a20 2020 2077  n the repo.    w",
            "-000098f0: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "-00009900: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "-00009910: 3a0a 2020 2020 2020 2020 2872 656d 6f74  :.        (remot",
            "-00009920: 655f 6e61 6d65 2c20 7265 6d6f 7465 5f6c  e_name, remote_l",
            "-00009930: 6f63 6174 696f 6e29 203d 2067 6574 5f72  ocation) = get_r",
            "-00009940: 656d 6f74 655f 7265 706f 2872 2c20 7265  emote_repo(r, re",
            "-00009950: 6d6f 7465 5f6c 6f63 6174 696f 6e29 0a0a  mote_location)..",
            "-00009960: 2020 2020 2020 2020 7365 6c65 6374 6564          selected",
            "-00009970: 5f72 6566 7320 3d20 5b5d 0a0a 2020 2020  _refs = []..    ",
            "-00009980: 2020 2020 6966 2072 6566 7370 6563 7320      if refspecs ",
            "-00009990: 6973 204e 6f6e 653a 0a20 2020 2020 2020  is None:.       ",
            "-000099a0: 2020 2020 2072 6566 7370 6563 7320 3d20       refspecs = ",
            "-000099b0: 5b62 2248 4541 4422 5d0a 0a20 2020 2020  [b\"HEAD\"]..     ",
            "-000099c0: 2020 2064 6566 2064 6574 6572 6d69 6e65     def determine",
            "-000099d0: 5f77 616e 7473 2872 656d 6f74 655f 7265  _wants(remote_re",
            "-000099e0: 6673 2c20 2a61 7267 732c 202a 2a6b 7761  fs, *args, **kwa",
            "-000099f0: 7267 7329 3a0a 2020 2020 2020 2020 2020  rgs):.          ",
            "-00009a00: 2020 7365 6c65 6374 6564 5f72 6566 732e    selected_refs.",
            "-00009a10: 6578 7465 6e64 280a 2020 2020 2020 2020  extend(.        ",
            "-00009a20: 2020 2020 2020 2020 7061 7273 655f 7265          parse_re",
            "-00009a30: 6674 7570 6c65 7328 7265 6d6f 7465 5f72  ftuples(remote_r",
            "-00009a40: 6566 732c 2072 2e72 6566 732c 2072 6566  efs, r.refs, ref",
            "-00009a50: 7370 6563 732c 2066 6f72 6365 3d66 6f72  specs, force=for",
            "-00009a60: 6365 290a 2020 2020 2020 2020 2020 2020  ce).            ",
            "-00009a70: 290a 2020 2020 2020 2020 2020 2020 7265  ).            re",
            "-00009a80: 7475 726e 205b 0a20 2020 2020 2020 2020  turn [.         ",
            "-00009a90: 2020 2020 2020 2072 656d 6f74 655f 7265         remote_re",
            "-00009aa0: 6673 5b6c 685d 0a20 2020 2020 2020 2020  fs[lh].         ",
            "-00009ab0: 2020 2020 2020 2066 6f72 2028 6c68 2c20         for (lh, ",
            "-00009ac0: 7268 2c20 666f 7263 655f 7265 6629 2069  rh, force_ref) i",
            "-00009ad0: 6e20 7365 6c65 6374 6564 5f72 6566 730a  n selected_refs.",
            "-00009ae0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009af0: 6966 2072 656d 6f74 655f 7265 6673 5b6c  if remote_refs[l",
            "-00009b00: 685d 206e 6f74 2069 6e20 722e 6f62 6a65  h] not in r.obje",
            "-00009b10: 6374 5f73 746f 7265 0a20 2020 2020 2020  ct_store.       ",
            "-00009b20: 2020 2020 205d 0a0a 2020 2020 2020 2020       ]..        ",
            "-00009b30: 636c 6965 6e74 2c20 7061 7468 203d 2067  client, path = g",
            "-00009b40: 6574 5f74 7261 6e73 706f 7274 5f61 6e64  et_transport_and",
            "-00009b50: 5f70 6174 6828 0a20 2020 2020 2020 2020  _path(.         ",
            "-00009b60: 2020 2072 656d 6f74 655f 6c6f 6361 7469     remote_locati",
            "-00009b70: 6f6e 2c20 636f 6e66 6967 3d72 2e67 6574  on, config=r.get",
            "-00009b80: 5f63 6f6e 6669 675f 7374 6163 6b28 292c  _config_stack(),",
            "-00009b90: 202a 2a6b 7761 7267 730a 2020 2020 2020   **kwargs.      ",
            "-00009ba0: 2020 290a 2020 2020 2020 2020 6966 2066    ).        if f",
            "-00009bb0: 696c 7465 725f 7370 6563 3a0a 2020 2020  ilter_spec:.    ",
            "-00009bc0: 2020 2020 2020 2020 6669 6c74 6572 5f73          filter_s",
            "-00009bd0: 7065 6320 3d20 6669 6c74 6572 5f73 7065  pec = filter_spe",
            "-00009be0: 632e 656e 636f 6465 2822 6173 6369 6922  c.encode(\"ascii\"",
            "-00009bf0: 290a 2020 2020 2020 2020 6665 7463 685f  ).        fetch_",
            "-00009c00: 7265 7375 6c74 203d 2063 6c69 656e 742e  result = client.",
            "-00009c10: 6665 7463 6828 0a20 2020 2020 2020 2020  fetch(.         ",
            "-00009c20: 2020 2070 6174 682c 0a20 2020 2020 2020     path,.       ",
            "-00009c30: 2020 2020 2072 2c0a 2020 2020 2020 2020       r,.        ",
            "-00009c40: 2020 2020 7072 6f67 7265 7373 3d65 7272      progress=err",
            "-00009c50: 7374 7265 616d 2e77 7269 7465 2c0a 2020  stream.write,.  ",
            "-00009c60: 2020 2020 2020 2020 2020 6465 7465 726d            determ",
            "-00009c70: 696e 655f 7761 6e74 733d 6465 7465 726d  ine_wants=determ",
            "-00009c80: 696e 655f 7761 6e74 732c 0a20 2020 2020  ine_wants,.     ",
            "-00009c90: 2020 2020 2020 2066 696c 7465 725f 7370         filter_sp",
            "-00009ca0: 6563 3d66 696c 7465 725f 7370 6563 2c0a  ec=filter_spec,.",
            "-00009cb0: 2020 2020 2020 2020 2020 2020 7072 6f74              prot",
            "-00009cc0: 6f63 6f6c 5f76 6572 7369 6f6e 3d70 726f  ocol_version=pro",
            "-00009cd0: 746f 636f 6c5f 7665 7273 696f 6e2c 0a20  tocol_version,. ",
            "-00009ce0: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       ",
            "-00009cf0: 2066 6f72 206c 682c 2072 682c 2066 6f72   for lh, rh, for",
            "-00009d00: 6365 5f72 6566 2069 6e20 7365 6c65 6374  ce_ref in select",
            "-00009d10: 6564 5f72 6566 733a 0a20 2020 2020 2020  ed_refs:.       ",
            "-00009d20: 2020 2020 2069 6620 6e6f 7420 666f 7263       if not forc",
            "-00009d30: 655f 7265 6620 616e 6420 7268 2069 6e20  e_ref and rh in ",
            "-00009d40: 722e 7265 6673 3a0a 2020 2020 2020 2020  r.refs:.        ",
            "-00009d50: 2020 2020 2020 2020 7472 793a 0a20 2020          try:.   ",
            "-00009d60: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009d70: 2063 6865 636b 5f64 6976 6572 6765 6428   check_diverged(",
            "-00009d80: 722c 2072 2e72 6566 732e 666f 6c6c 6f77  r, r.refs.follow",
            "-00009d90: 2872 6829 5b31 5d2c 2066 6574 6368 5f72  (rh)[1], fetch_r",
            "-00009da0: 6573 756c 742e 7265 6673 5b6c 685d 290a  esult.refs[lh]).",
            "-00009db0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009dc0: 6578 6365 7074 2044 6976 6572 6765 6442  except DivergedB",
            "-00009dd0: 7261 6e63 6865 7320 6173 2065 7863 3a0a  ranches as exc:.",
            "-00009de0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009df0: 2020 2020 6966 2066 6173 745f 666f 7277      if fast_forw",
            "-00009e00: 6172 643a 0a20 2020 2020 2020 2020 2020  ard:.           ",
            "-00009e10: 2020 2020 2020 2020 2020 2020 2072 6169               rai",
            "-00009e20: 7365 0a20 2020 2020 2020 2020 2020 2020  se.             ",
            "-00009e30: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   ",
            "-00009e40: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00009e50: 2020 2020 2072 6169 7365 204e 6f74 496d       raise NotIm",
            "-00009e60: 706c 656d 656e 7465 6445 7272 6f72 2822  plementedError(\"",
            "-00009e70: 6d65 7267 6520 6973 206e 6f74 2079 6574  merge is not yet",
            "-00009e80: 2073 7570 706f 7274 6564 2229 2066 726f   supported\") fro",
            "-00009e90: 6d20 6578 630a 2020 2020 2020 2020 2020  m exc.          ",
            "-00009ea0: 2020 722e 7265 6673 5b72 685d 203d 2066    r.refs[rh] = f",
            "-00009eb0: 6574 6368 5f72 6573 756c 742e 7265 6673  etch_result.refs",
            "-00009ec0: 5b6c 685d 0a20 2020 2020 2020 2069 6620  [lh].        if ",
            "-00009ed0: 7365 6c65 6374 6564 5f72 6566 733a 0a20  selected_refs:. ",
            "-00009ee0: 2020 2020 2020 2020 2020 2072 5b62 2248             r[b\"H",
            "-00009ef0: 4541 4422 5d20 3d20 6665 7463 685f 7265  EAD\"] = fetch_re",
            "-00009f00: 7375 6c74 2e72 6566 735b 7365 6c65 6374  sult.refs[select",
            "-00009f10: 6564 5f72 6566 735b 305d 5b31 5d5d 0a0a  ed_refs[0][1]]..",
            "-00009f20: 2020 2020 2020 2020 2320 5065 7266 6f72          # Perfor",
            "-00009f30: 6d20 2767 6974 2063 6865 636b 6f75 7420  m 'git checkout ",
            "-00009f40: 2e27 202d 2073 796e 6373 2073 7461 6765  .' - syncs stage",
            "-00009f50: 6420 6368 616e 6765 730a 2020 2020 2020  d changes.      ",
            "-00009f60: 2020 7472 6565 203d 2072 5b62 2248 4541    tree = r[b\"HEA",
            "-00009f70: 4422 5d2e 7472 6565 0a20 2020 2020 2020  D\"].tree.       ",
            "-00009f80: 2072 2e72 6573 6574 5f69 6e64 6578 2874   r.reset_index(t",
            "-00009f90: 7265 653d 7472 6565 290a 2020 2020 2020  ree=tree).      ",
            "-00009fa0: 2020 6966 2072 656d 6f74 655f 6e61 6d65    if remote_name",
            "-00009fb0: 2069 7320 6e6f 7420 4e6f 6e65 3a0a 2020   is not None:.  ",
            "-00009fc0: 2020 2020 2020 2020 2020 5f69 6d70 6f72            _impor",
            "-00009fd0: 745f 7265 6d6f 7465 5f72 6566 7328 722e  t_remote_refs(r.",
            "-00009fe0: 7265 6673 2c20 7265 6d6f 7465 5f6e 616d  refs, remote_nam",
            "-00009ff0: 652c 2066 6574 6368 5f72 6573 756c 742e  e, fetch_result.",
            "-0000a000: 7265 6673 290a 0a0a 6465 6620 7374 6174  refs)...def stat",
            "-0000a010: 7573 2872 6570 6f3d 222e 222c 2069 676e  us(repo=\".\", ign",
            "-0000a020: 6f72 6564 3d46 616c 7365 2c20 756e 7472  ored=False, untr",
            "-0000a030: 6163 6b65 645f 6669 6c65 733d 2261 6c6c  acked_files=\"all",
            "-0000a040: 2229 3a0a 2020 2020 2222 2252 6574 7572  \"):.    \"\"\"Retur",
            "-0000a050: 6e73 2073 7461 6765 642c 2075 6e73 7461  ns staged, unsta",
            "-0000a060: 6765 642c 2061 6e64 2075 6e74 7261 636b  ged, and untrack",
            "-0000a070: 6564 2063 6861 6e67 6573 2072 656c 6174  ed changes relat",
            "-0000a080: 6976 6520 746f 2074 6865 2048 4541 442e  ive to the HEAD.",
            "+000054d0: 2020 2020 2075 6e74 7261 636b 6564 5f70       untracked_p",
            "+000054e0: 6174 6820 3d20 706f 7369 7870 6174 682e  ath = posixpath.",
            "+000054f0: 6a6f 696e 2872 656c 7061 7468 2c20 756e  join(relpath, un",
            "+00005500: 7472 6163 6b65 645f 7061 7468 290a 0a20  tracked_path).. ",
            "+00005510: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005520: 2020 2069 6620 6e6f 7420 6967 6e6f 7265     if not ignore",
            "+00005530: 5f6d 616e 6167 6572 2e69 735f 6967 6e6f  _manager.is_igno",
            "+00005540: 7265 6428 756e 7472 6163 6b65 645f 7061  red(untracked_pa",
            "+00005550: 7468 293a 0a20 2020 2020 2020 2020 2020  th):.           ",
            "+00005560: 2020 2020 2020 2020 2020 2020 2072 656c               rel",
            "+00005570: 7061 7468 732e 6170 7065 6e64 2875 6e74  paths.append(unt",
            "+00005580: 7261 636b 6564 5f70 6174 6829 0a20 2020  racked_path).   ",
            "+00005590: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000055a0: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         ",
            "+000055b0: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "+000055c0: 676e 6f72 6564 2e61 6464 2875 6e74 7261  gnored.add(untra",
            "+000055d0: 636b 6564 5f70 6174 6829 0a0a 2020 2020  cked_path)..    ",
            "+000055e0: 2020 2020 2020 2020 2020 2020 2320 416c              # Al",
            "+000055f0: 736f 2061 6464 2075 6e73 7461 6765 6420  so add unstaged ",
            "+00005600: 286d 6f64 6966 6965 6429 2066 696c 6573  (modified) files",
            "+00005610: 2077 6974 6869 6e20 7468 6973 2064 6972   within this dir",
            "+00005620: 6563 746f 7279 0a20 2020 2020 2020 2020  ectory.         ",
            "+00005630: 2020 2020 2020 2066 6f72 2075 6e73 7461         for unsta",
            "+00005640: 6765 645f 7061 7468 2069 6e20 616c 6c5f  ged_path in all_",
            "+00005650: 756e 7374 6167 6564 5f70 6174 6873 3a0a  unstaged_paths:.",
            "+00005660: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005670: 2020 2020 6966 2069 7369 6e73 7461 6e63      if isinstanc",
            "+00005680: 6528 756e 7374 6167 6564 5f70 6174 682c  e(unstaged_path,",
            "+00005690: 2062 7974 6573 293a 0a20 2020 2020 2020   bytes):.       ",
            "+000056a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000056b0: 2075 6e73 7461 6765 645f 7061 7468 5f73   unstaged_path_s",
            "+000056c0: 7472 203d 2075 6e73 7461 6765 645f 7061  tr = unstaged_pa",
            "+000056d0: 7468 2e64 6563 6f64 6528 2275 7466 2d38  th.decode(\"utf-8",
            "+000056e0: 2229 0a20 2020 2020 2020 2020 2020 2020  \").             ",
            "+000056f0: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   ",
            "+00005700: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005710: 2020 2020 2075 6e73 7461 6765 645f 7061       unstaged_pa",
            "+00005720: 7468 5f73 7472 203d 2075 6e73 7461 6765  th_str = unstage",
            "+00005730: 645f 7061 7468 0a0a 2020 2020 2020 2020  d_path..        ",
            "+00005740: 2020 2020 2020 2020 2020 2020 2320 4368              # Ch",
            "+00005750: 6563 6b20 6966 2074 6869 7320 756e 7374  eck if this unst",
            "+00005760: 6167 6564 2066 696c 6520 6973 2077 6974  aged file is wit",
            "+00005770: 6869 6e20 7468 6520 6469 7265 6374 6f72  hin the director",
            "+00005780: 7920 7765 2772 6520 7072 6f63 6573 7369  y we're processi",
            "+00005790: 6e67 0a20 2020 2020 2020 2020 2020 2020  ng.             ",
            "+000057a0: 2020 2020 2020 2075 6e73 7461 6765 645f         unstaged_",
            "+000057b0: 6675 6c6c 5f70 6174 6820 3d20 7265 706f  full_path = repo",
            "+000057c0: 5f70 6174 6820 2f20 756e 7374 6167 6564  _path / unstaged",
            "+000057d0: 5f70 6174 685f 7374 720a 2020 2020 2020  _path_str.      ",
            "+000057e0: 2020 2020 2020 2020 2020 2020 2020 7472                tr",
            "+000057f0: 793a 0a20 2020 2020 2020 2020 2020 2020  y:.             ",
            "+00005800: 2020 2020 2020 2020 2020 2075 6e73 7461             unsta",
            "+00005810: 6765 645f 6675 6c6c 5f70 6174 682e 7265  ged_full_path.re",
            "+00005820: 6c61 7469 7665 5f74 6f28 7265 736f 6c76  lative_to(resolv",
            "+00005830: 6564 5f70 6174 6829 0a20 2020 2020 2020  ed_path).       ",
            "+00005840: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005850: 2023 2046 696c 6520 6973 2077 6974 6869   # File is withi",
            "+00005860: 6e20 7468 6973 2064 6972 6563 746f 7279  n this directory",
            "+00005870: 2c20 6164 6420 6974 0a20 2020 2020 2020  , add it.       ",
            "+00005880: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005890: 2069 6620 6e6f 7420 6967 6e6f 7265 5f6d   if not ignore_m",
            "+000058a0: 616e 6167 6572 2e69 735f 6967 6e6f 7265  anager.is_ignore",
            "+000058b0: 6428 756e 7374 6167 6564 5f70 6174 685f  d(unstaged_path_",
            "+000058c0: 7374 7229 3a0a 2020 2020 2020 2020 2020  str):.          ",
            "+000058d0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000058e0: 2020 7265 6c70 6174 6873 2e61 7070 656e    relpaths.appen",
            "+000058f0: 6428 756e 7374 6167 6564 5f70 6174 685f  d(unstaged_path_",
            "+00005900: 7374 7229 0a20 2020 2020 2020 2020 2020  str).           ",
            "+00005910: 2020 2020 2020 2020 2020 2020 2065 6c73               els",
            "+00005920: 653a 0a20 2020 2020 2020 2020 2020 2020  e:.             ",
            "+00005930: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "+00005940: 676e 6f72 6564 2e61 6464 2875 6e73 7461  gnored.add(unsta",
            "+00005950: 6765 645f 7061 7468 5f73 7472 290a 2020  ged_path_str).  ",
            "+00005960: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00005970: 2020 6578 6365 7074 2056 616c 7565 4572    except ValueEr",
            "+00005980: 726f 723a 0a20 2020 2020 2020 2020 2020  ror:.           ",
            "+00005990: 2020 2020 2020 2020 2020 2020 2023 2046               # F",
            "+000059a0: 696c 6520 6973 206e 6f74 2077 6974 6869  ile is not withi",
            "+000059b0: 6e20 7468 6973 2064 6972 6563 746f 7279  n this directory",
            "+000059c0: 2c20 736b 6970 2069 740a 2020 2020 2020  , skip it.      ",
            "+000059d0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000059e0: 2020 636f 6e74 696e 7565 0a20 2020 2020    continue.     ",
            "+000059f0: 2020 2020 2020 2020 2020 2063 6f6e 7469             conti",
            "+00005a00: 6e75 650a 0a20 2020 2020 2020 2020 2020  nue..           ",
            "+00005a10: 2023 2046 4958 4d45 3a20 5375 7070 6f72   # FIXME: Suppor",
            "+00005a20: 7420 7061 7474 6572 6e73 0a20 2020 2020  t patterns.     ",
            "+00005a30: 2020 2020 2020 2069 6620 6967 6e6f 7265         if ignore",
            "+00005a40: 5f6d 616e 6167 6572 2e69 735f 6967 6e6f  _manager.is_igno",
            "+00005a50: 7265 6428 7265 6c70 6174 6829 3a0a 2020  red(relpath):.  ",
            "+00005a60: 2020 2020 2020 2020 2020 2020 2020 6967                ig",
            "+00005a70: 6e6f 7265 642e 6164 6428 7265 6c70 6174  nored.add(relpat",
            "+00005a80: 6829 0a20 2020 2020 2020 2020 2020 2020  h).             ",
            "+00005a90: 2020 2063 6f6e 7469 6e75 650a 2020 2020     continue.    ",
            "+00005aa0: 2020 2020 2020 2020 7265 6c70 6174 6873          relpaths",
            "+00005ab0: 2e61 7070 656e 6428 7265 6c70 6174 6829  .append(relpath)",
            "+00005ac0: 0a20 2020 2020 2020 2072 2e73 7461 6765  .        r.stage",
            "+00005ad0: 2872 656c 7061 7468 7329 0a20 2020 2072  (relpaths).    r",
            "+00005ae0: 6574 7572 6e20 2872 656c 7061 7468 732c  eturn (relpaths,",
            "+00005af0: 2069 676e 6f72 6564 290a 0a0a 6465 6620   ignored)...def ",
            "+00005b00: 5f69 735f 7375 6264 6972 2873 7562 6469  _is_subdir(subdi",
            "+00005b10: 722c 2070 6172 656e 7464 6972 293a 0a20  r, parentdir):. ",
            "+00005b20: 2020 2022 2222 4368 6563 6b20 7768 6574     \"\"\"Check whet",
            "+00005b30: 6865 7220 7375 6264 6972 2069 7320 7061  her subdir is pa",
            "+00005b40: 7265 6e74 6469 7220 6f72 2061 2073 7562  rentdir or a sub",
            "+00005b50: 6469 7220 6f66 2070 6172 656e 7464 6972  dir of parentdir",
            "+00005b60: 2e0a 0a20 2020 2049 6620 7061 7265 6e74  ...    If parent",
            "+00005b70: 6469 7220 6f72 2073 7562 6469 7220 6973  dir or subdir is",
            "+00005b80: 2061 2072 656c 6174 6976 6520 7061 7468   a relative path",
            "+00005b90: 2c20 6974 2077 696c 6c20 6265 2064 6973  , it will be dis",
            "+00005ba0: 616d 6769 6275 6174 6564 0a20 2020 2072  amgibuated.    r",
            "+00005bb0: 656c 6174 6976 6520 746f 2074 6865 2070  elative to the p",
            "+00005bc0: 7764 2e0a 2020 2020 2222 220a 2020 2020  wd..    \"\"\".    ",
            "+00005bd0: 7061 7265 6e74 6469 725f 6162 7320 3d20  parentdir_abs = ",
            "+00005be0: 6f73 2e70 6174 682e 7265 616c 7061 7468  os.path.realpath",
            "+00005bf0: 2870 6172 656e 7464 6972 2920 2b20 6f73  (parentdir) + os",
            "+00005c00: 2e70 6174 682e 7365 700a 2020 2020 7375  .path.sep.    su",
            "+00005c10: 6264 6972 5f61 6273 203d 206f 732e 7061  bdir_abs = os.pa",
            "+00005c20: 7468 2e72 6561 6c70 6174 6828 7375 6264  th.realpath(subd",
            "+00005c30: 6972 2920 2b20 6f73 2e70 6174 682e 7365  ir) + os.path.se",
            "+00005c40: 700a 2020 2020 7265 7475 726e 2073 7562  p.    return sub",
            "+00005c50: 6469 725f 6162 732e 7374 6172 7473 7769  dir_abs.startswi",
            "+00005c60: 7468 2870 6172 656e 7464 6972 5f61 6273  th(parentdir_abs",
            "+00005c70: 290a 0a0a 2320 544f 444f 3a20 6f70 7469  )...# TODO: opti",
            "+00005c80: 6f6e 2074 6f20 7265 6d6f 7665 2069 676e  on to remove ign",
            "+00005c90: 6f72 6564 2066 696c 6573 2061 6c73 6f2c  ored files also,",
            "+00005ca0: 2069 6e20 6c69 6e65 2077 6974 6820 6067   in line with `g",
            "+00005cb0: 6974 2063 6c65 616e 202d 6664 7860 0a64  it clean -fdx`.d",
            "+00005cc0: 6566 2063 6c65 616e 2872 6570 6f3d 222e  ef clean(repo=\".",
            "+00005cd0: 222c 2074 6172 6765 745f 6469 723d 4e6f  \", target_dir=No",
            "+00005ce0: 6e65 2920 2d3e 204e 6f6e 653a 0a20 2020  ne) -> None:.   ",
            "+00005cf0: 2022 2222 5265 6d6f 7665 2061 6e79 2075   \"\"\"Remove any u",
            "+00005d00: 6e74 7261 636b 6564 2066 696c 6573 2066  ntracked files f",
            "+00005d10: 726f 6d20 7468 6520 7461 7267 6574 2064  rom the target d",
            "+00005d20: 6972 6563 746f 7279 2072 6563 7572 7369  irectory recursi",
            "+00005d30: 7665 6c79 2e0a 0a20 2020 2045 7175 6976  vely...    Equiv",
            "+00005d40: 616c 656e 7420 746f 2072 756e 6e69 6e67  alent to running",
            "+00005d50: 2060 6067 6974 2063 6c65 616e 202d 6664   ``git clean -fd",
            "+00005d60: 6060 2069 6e20 7461 7267 6574 5f64 6972  `` in target_dir",
            "+00005d70: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "+00005d80: 2020 2072 6570 6f3a 2052 6570 6f73 6974     repo: Reposit",
            "+00005d90: 6f72 7920 7768 6572 6520 7468 6520 6669  ory where the fi",
            "+00005da0: 6c65 7320 6d61 7920 6265 2074 7261 636b  les may be track",
            "+00005db0: 6564 0a20 2020 2020 2074 6172 6765 745f  ed.      target_",
            "+00005dc0: 6469 723a 2044 6972 6563 746f 7279 2074  dir: Directory t",
            "+00005dd0: 6f20 636c 6561 6e20 2d20 6375 7272 656e  o clean - curren",
            "+00005de0: 7420 6469 7265 6374 6f72 7920 6966 204e  t directory if N",
            "+00005df0: 6f6e 650a 2020 2020 2222 220a 2020 2020  one.    \"\"\".    ",
            "+00005e00: 6966 2074 6172 6765 745f 6469 7220 6973  if target_dir is",
            "+00005e10: 204e 6f6e 653a 0a20 2020 2020 2020 2074   None:.        t",
            "+00005e20: 6172 6765 745f 6469 7220 3d20 6f73 2e67  arget_dir = os.g",
            "+00005e30: 6574 6377 6428 290a 0a20 2020 2077 6974  etcwd()..    wit",
            "+00005e40: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "+00005e50: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "+00005e60: 2020 2020 2020 2020 6966 206e 6f74 205f          if not _",
            "+00005e70: 6973 5f73 7562 6469 7228 7461 7267 6574  is_subdir(target",
            "+00005e80: 5f64 6972 2c20 722e 7061 7468 293a 0a20  _dir, r.path):. ",
            "+00005e90: 2020 2020 2020 2020 2020 2072 6169 7365             raise",
            "+00005ea0: 2045 7272 6f72 2822 7461 7267 6574 5f64   Error(\"target_d",
            "+00005eb0: 6972 206d 7573 7420 6265 2069 6e20 7468  ir must be in th",
            "+00005ec0: 6520 7265 706f 2773 2077 6f72 6b69 6e67  e repo's working",
            "+00005ed0: 2064 6972 2229 0a0a 2020 2020 2020 2020   dir\")..        ",
            "+00005ee0: 636f 6e66 6967 203d 2072 2e67 6574 5f63  config = r.get_c",
            "+00005ef0: 6f6e 6669 675f 7374 6163 6b28 290a 2020  onfig_stack().  ",
            "+00005f00: 2020 2020 2020 636f 6e66 6967 2e67 6574        config.get",
            "+00005f10: 5f62 6f6f 6c65 616e 2828 6222 636c 6561  _boolean((b\"clea",
            "+00005f20: 6e22 2c29 2c20 6222 7265 7175 6972 6546  n\",), b\"requireF",
            "+00005f30: 6f72 6365 222c 2054 7275 6529 0a0a 2020  orce\", True)..  ",
            "+00005f40: 2020 2020 2020 2320 544f 444f 286a 656c        # TODO(jel",
            "+00005f50: 6d65 7229 3a20 6966 2072 6571 7569 7265  mer): if require",
            "+00005f60: 5f66 6f72 6365 2069 7320 7365 742c 2074  _force is set, t",
            "+00005f70: 6865 6e20 6d61 6b65 2073 7572 6520 7468  hen make sure th",
            "+00005f80: 6174 202d 662c 202d 6920 6f72 0a20 2020  at -f, -i or.   ",
            "+00005f90: 2020 2020 2023 202d 6e20 6973 2073 7065       # -n is spe",
            "+00005fa0: 6369 6669 6564 2e0a 0a20 2020 2020 2020  cified...       ",
            "+00005fb0: 2069 6e64 6578 203d 2072 2e6f 7065 6e5f   index = r.open_",
            "+00005fc0: 696e 6465 7828 290a 2020 2020 2020 2020  index().        ",
            "+00005fd0: 6967 6e6f 7265 5f6d 616e 6167 6572 203d  ignore_manager =",
            "+00005fe0: 2049 676e 6f72 6546 696c 7465 724d 616e   IgnoreFilterMan",
            "+00005ff0: 6167 6572 2e66 726f 6d5f 7265 706f 2872  ager.from_repo(r",
            "+00006000: 290a 0a20 2020 2020 2020 2070 6174 6873  )..        paths",
            "+00006010: 5f69 6e5f 7764 203d 205f 7761 6c6b 5f77  _in_wd = _walk_w",
            "+00006020: 6f72 6b69 6e67 5f64 6972 5f70 6174 6873  orking_dir_paths",
            "+00006030: 2874 6172 6765 745f 6469 722c 2072 2e70  (target_dir, r.p",
            "+00006040: 6174 6829 0a20 2020 2020 2020 2023 2052  ath).        # R",
            "+00006050: 6576 6572 7365 2066 696c 6520 7669 7369  everse file visi",
            "+00006060: 7420 6f72 6465 722c 2073 6f20 7468 6174  t order, so that",
            "+00006070: 2066 696c 6573 2061 6e64 2073 7562 6469   files and subdi",
            "+00006080: 7265 6374 6f72 6965 7320 6172 650a 2020  rectories are.  ",
            "+00006090: 2020 2020 2020 2320 7265 6d6f 7665 6420        # removed ",
            "+000060a0: 6265 666f 7265 2063 6f6e 7461 696e 696e  before containin",
            "+000060b0: 6720 6469 7265 6374 6f72 790a 2020 2020  g directory.    ",
            "+000060c0: 2020 2020 666f 7220 6170 2c20 6973 5f64      for ap, is_d",
            "+000060d0: 6972 2069 6e20 7265 7665 7273 6564 286c  ir in reversed(l",
            "+000060e0: 6973 7428 7061 7468 735f 696e 5f77 6429  ist(paths_in_wd)",
            "+000060f0: 293a 0a20 2020 2020 2020 2020 2020 2069  ):.            i",
            "+00006100: 6620 6973 5f64 6972 3a0a 2020 2020 2020  f is_dir:.      ",
            "+00006110: 2020 2020 2020 2020 2020 2320 416c 6c20            # All ",
            "+00006120: 7375 6264 6972 6563 746f 7269 6573 2061  subdirectories a",
            "+00006130: 6e64 2066 696c 6573 2068 6176 6520 6265  nd files have be",
            "+00006140: 656e 2072 656d 6f76 6564 2069 6620 756e  en removed if un",
            "+00006150: 7472 6163 6b65 642c 0a20 2020 2020 2020  tracked,.       ",
            "+00006160: 2020 2020 2020 2020 2023 2073 6f20 6469           # so di",
            "+00006170: 7220 636f 6e74 6169 6e73 206e 6f20 7472  r contains no tr",
            "+00006180: 6163 6b65 6420 6669 6c65 7320 6966 6620  acked files iff ",
            "+00006190: 6974 2069 7320 656d 7074 792e 0a20 2020  it is empty..   ",
            "+000061a0: 2020 2020 2020 2020 2020 2020 2069 735f               is_",
            "+000061b0: 656d 7074 7920 3d20 6c65 6e28 6f73 2e6c  empty = len(os.l",
            "+000061c0: 6973 7464 6972 2861 7029 2920 3d3d 2030  istdir(ap)) == 0",
            "+000061d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000061e0: 2069 6620 6973 5f65 6d70 7479 3a0a 2020   if is_empty:.  ",
            "+000061f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006200: 2020 6f73 2e72 6d64 6972 2861 7029 0a20    os.rmdir(ap). ",
            "+00006210: 2020 2020 2020 2020 2020 2065 6c73 653a             else:",
            "+00006220: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00006230: 2069 7020 3d20 7061 7468 5f74 6f5f 7472   ip = path_to_tr",
            "+00006240: 6565 5f70 6174 6828 722e 7061 7468 2c20  ee_path(r.path, ",
            "+00006250: 6170 290a 2020 2020 2020 2020 2020 2020  ap).            ",
            "+00006260: 2020 2020 6973 5f74 7261 636b 6564 203d      is_tracked =",
            "+00006270: 2069 7020 696e 2069 6e64 6578 0a0a 2020   ip in index..  ",
            "+00006280: 2020 2020 2020 2020 2020 2020 2020 7270                rp",
            "+00006290: 203d 206f 732e 7061 7468 2e72 656c 7061   = os.path.relpa",
            "+000062a0: 7468 2861 702c 2072 2e70 6174 6829 0a20  th(ap, r.path). ",
            "+000062b0: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "+000062c0: 735f 6967 6e6f 7265 6420 3d20 6967 6e6f  s_ignored = igno",
            "+000062d0: 7265 5f6d 616e 6167 6572 2e69 735f 6967  re_manager.is_ig",
            "+000062e0: 6e6f 7265 6428 7270 290a 0a20 2020 2020  nored(rp)..     ",
            "+000062f0: 2020 2020 2020 2020 2020 2069 6620 6e6f             if no",
            "+00006300: 7420 6973 5f74 7261 636b 6564 2061 6e64  t is_tracked and",
            "+00006310: 206e 6f74 2069 735f 6967 6e6f 7265 643a   not is_ignored:",
            "+00006320: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00006330: 2020 2020 206f 732e 7265 6d6f 7665 2861       os.remove(a",
            "+00006340: 7029 0a0a 0a64 6566 2072 656d 6f76 6528  p)...def remove(",
            "+00006350: 7265 706f 3d22 2e22 2c20 7061 7468 733d  repo=\".\", paths=",
            "+00006360: 4e6f 6e65 2c20 6361 6368 6564 3d46 616c  None, cached=Fal",
            "+00006370: 7365 2920 2d3e 204e 6f6e 653a 0a20 2020  se) -> None:.   ",
            "+00006380: 2022 2222 5265 6d6f 7665 2066 696c 6573   \"\"\"Remove files",
            "+00006390: 2066 726f 6d20 7468 6520 7374 6167 696e   from the stagin",
            "+000063a0: 6720 6172 6561 2e0a 0a20 2020 2041 7267  g area...    Arg",
            "+000063b0: 733a 0a20 2020 2020 2072 6570 6f3a 2052  s:.      repo: R",
            "+000063c0: 6570 6f73 6974 6f72 7920 666f 7220 7468  epository for th",
            "+000063d0: 6520 6669 6c65 730a 2020 2020 2020 7061  e files.      pa",
            "+000063e0: 7468 733a 2050 6174 6873 2074 6f20 7265  ths: Paths to re",
            "+000063f0: 6d6f 7665 2e20 4361 6e20 6265 2061 6273  move. Can be abs",
            "+00006400: 6f6c 7574 6520 6f72 2072 656c 6174 6976  olute or relativ",
            "+00006410: 6520 746f 2074 6865 2072 6570 6f73 6974  e to the reposit",
            "+00006420: 6f72 7920 726f 6f74 2e0a 2020 2020 2222  ory root..    \"\"",
            "+00006430: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "+00006440: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+00006450: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "+00006460: 2069 6e64 6578 203d 2072 2e6f 7065 6e5f   index = r.open_",
            "+00006470: 696e 6465 7828 290a 2020 2020 2020 2020  index().        ",
            "+00006480: 626c 6f62 5f6e 6f72 6d61 6c69 7a65 7220  blob_normalizer ",
            "+00006490: 3d20 722e 6765 745f 626c 6f62 5f6e 6f72  = r.get_blob_nor",
            "+000064a0: 6d61 6c69 7a65 7228 290a 0a20 2020 2020  malizer()..     ",
            "+000064b0: 2020 2066 6f72 2070 2069 6e20 7061 7468     for p in path",
            "+000064c0: 733a 0a20 2020 2020 2020 2020 2020 2023  s:.            #",
            "+000064d0: 2049 6620 7061 7468 2069 7320 6162 736f   If path is abso",
            "+000064e0: 6c75 7465 2c20 7573 6520 6974 2061 732d  lute, use it as-",
            "+000064f0: 6973 2e20 4f74 6865 7277 6973 652c 2074  is. Otherwise, t",
            "+00006500: 7265 6174 2069 7420 6173 2072 656c 6174  reat it as relat",
            "+00006510: 6976 6520 746f 2072 6570 6f0a 2020 2020  ive to repo.    ",
            "+00006520: 2020 2020 2020 2020 6966 206f 732e 7061          if os.pa",
            "+00006530: 7468 2e69 7361 6273 2870 293a 0a20 2020  th.isabs(p):.   ",
            "+00006540: 2020 2020 2020 2020 2020 2020 2066 756c               ful",
            "+00006550: 6c5f 7061 7468 203d 2070 0a20 2020 2020  l_path = p.     ",
            "+00006560: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   ",
            "+00006570: 2020 2020 2020 2020 2020 2020 2023 2054               # T",
            "+00006580: 7265 6174 2072 656c 6174 6976 6520 7061  reat relative pa",
            "+00006590: 7468 7320 6173 2072 656c 6174 6976 6520  ths as relative ",
            "+000065a0: 746f 2074 6865 2072 6570 6f73 6974 6f72  to the repositor",
            "+000065b0: 7920 726f 6f74 0a20 2020 2020 2020 2020  y root.         ",
            "+000065c0: 2020 2020 2020 2066 756c 6c5f 7061 7468         full_path",
            "+000065d0: 203d 206f 732e 7061 7468 2e6a 6f69 6e28   = os.path.join(",
            "+000065e0: 722e 7061 7468 2c20 7029 0a20 2020 2020  r.path, p).     ",
            "+000065f0: 2020 2020 2020 2074 7265 655f 7061 7468         tree_path",
            "+00006600: 203d 2070 6174 685f 746f 5f74 7265 655f   = path_to_tree_",
            "+00006610: 7061 7468 2872 2e70 6174 682c 2066 756c  path(r.path, ful",
            "+00006620: 6c5f 7061 7468 290a 2020 2020 2020 2020  l_path).        ",
            "+00006630: 2020 2020 2320 436f 6e76 6572 7420 746f      # Convert to",
            "+00006640: 2062 7974 6573 2066 6f72 2066 696c 6520   bytes for file ",
            "+00006650: 6f70 6572 6174 696f 6e73 0a20 2020 2020  operations.     ",
            "+00006660: 2020 2020 2020 2066 756c 6c5f 7061 7468         full_path",
            "+00006670: 5f62 7974 6573 203d 206f 732e 6673 656e  _bytes = os.fsen",
            "+00006680: 636f 6465 2866 756c 6c5f 7061 7468 290a  code(full_path).",
            "+00006690: 2020 2020 2020 2020 2020 2020 7472 793a              try:",
            "+000066a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000066b0: 2069 6e64 6578 5f73 6861 203d 2069 6e64   index_sha = ind",
            "+000066c0: 6578 5b74 7265 655f 7061 7468 5d2e 7368  ex[tree_path].sh",
            "+000066d0: 610a 2020 2020 2020 2020 2020 2020 6578  a.            ex",
            "+000066e0: 6365 7074 204b 6579 4572 726f 7220 6173  cept KeyError as",
            "+000066f0: 2065 7863 3a0a 2020 2020 2020 2020 2020   exc:.          ",
            "+00006700: 2020 2020 2020 7261 6973 6520 4572 726f        raise Erro",
            "+00006710: 7228 6622 7b70 7d20 6469 6420 6e6f 7420  r(f\"{p} did not ",
            "+00006720: 6d61 7463 6820 616e 7920 6669 6c65 7322  match any files\"",
            "+00006730: 2920 6672 6f6d 2065 7863 0a0a 2020 2020  ) from exc..    ",
            "+00006740: 2020 2020 2020 2020 6966 206e 6f74 2063          if not c",
            "+00006750: 6163 6865 643a 0a20 2020 2020 2020 2020  ached:.         ",
            "+00006760: 2020 2020 2020 2074 7279 3a0a 2020 2020         try:.    ",
            "+00006770: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006780: 7374 203d 206f 732e 6c73 7461 7428 6675  st = os.lstat(fu",
            "+00006790: 6c6c 5f70 6174 685f 6279 7465 7329 0a20  ll_path_bytes). ",
            "+000067a0: 2020 2020 2020 2020 2020 2020 2020 2065                 e",
            "+000067b0: 7863 6570 7420 4f53 4572 726f 723a 0a20  xcept OSError:. ",
            "+000067c0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000067d0: 2020 2070 6173 730a 2020 2020 2020 2020     pass.        ",
            "+000067e0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  ",
            "+000067f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006800: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         ",
            "+00006810: 2020 2020 2020 2020 2020 2020 2020 2062                 b",
            "+00006820: 6c6f 6220 3d20 626c 6f62 5f66 726f 6d5f  lob = blob_from_",
            "+00006830: 7061 7468 5f61 6e64 5f73 7461 7428 6675  path_and_stat(fu",
            "+00006840: 6c6c 5f70 6174 685f 6279 7465 732c 2073  ll_path_bytes, s",
            "+00006850: 7429 0a20 2020 2020 2020 2020 2020 2020  t).             ",
            "+00006860: 2020 2020 2020 2020 2020 2023 2041 7070             # App",
            "+00006870: 6c79 2063 6865 636b 696e 206e 6f72 6d61  ly checkin norma",
            "+00006880: 6c69 7a61 7469 6f6e 2074 6f20 636f 6d70  lization to comp",
            "+00006890: 6172 6520 6170 706c 6573 2074 6f20 6170  are apples to ap",
            "+000068a0: 706c 6573 0a20 2020 2020 2020 2020 2020  ples.           ",
            "+000068b0: 2020 2020 2020 2020 2020 2020 2069 6620               if ",
            "+000068c0: 626c 6f62 5f6e 6f72 6d61 6c69 7a65 7220  blob_normalizer ",
            "+000068d0: 6973 206e 6f74 204e 6f6e 653a 0a20 2020  is not None:.   ",
            "+000068e0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000068f0: 2020 2020 2020 2020 2062 6c6f 6220 3d20           blob = ",
            "+00006900: 626c 6f62 5f6e 6f72 6d61 6c69 7a65 722e  blob_normalizer.",
            "+00006910: 6368 6563 6b69 6e5f 6e6f 726d 616c 697a  checkin_normaliz",
            "+00006920: 6528 626c 6f62 2c20 7472 6565 5f70 6174  e(blob, tree_pat",
            "+00006930: 6829 0a20 2020 2020 2020 2020 2020 2020  h).             ",
            "+00006940: 2020 2020 2020 2065 7863 6570 7420 4f53         except OS",
            "+00006950: 4572 726f 723a 0a20 2020 2020 2020 2020  Error:.         ",
            "+00006960: 2020 2020 2020 2020 2020 2020 2020 2070                 p",
            "+00006970: 6173 730a 2020 2020 2020 2020 2020 2020  ass.            ",
            "+00006980: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  ",
            "+00006990: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000069a0: 2020 2020 2020 7472 793a 0a20 2020 2020        try:.     ",
            "+000069b0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000069c0: 2020 2020 2020 2063 6f6d 6d69 7474 6564         committed",
            "+000069d0: 5f73 6861 203d 2074 7265 655f 6c6f 6f6b  _sha = tree_look",
            "+000069e0: 7570 5f70 6174 6828 0a20 2020 2020 2020  up_path(.       ",
            "+000069f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006a00: 2020 2020 2020 2020 2072 2e5f 5f67 6574           r.__get",
            "+00006a10: 6974 656d 5f5f 2c20 725b 722e 6865 6164  item__, r[r.head",
            "+00006a20: 2829 5d2e 7472 6565 2c20 7472 6565 5f70  ()].tree, tree_p",
            "+00006a30: 6174 680a 2020 2020 2020 2020 2020 2020  ath.            ",
            "+00006a40: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006a50: 295b 315d 0a20 2020 2020 2020 2020 2020  )[1].           ",
            "+00006a60: 2020 2020 2020 2020 2020 2020 2065 7863               exc",
            "+00006a70: 6570 7420 4b65 7945 7272 6f72 3a0a 2020  ept KeyError:.  ",
            "+00006a80: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006a90: 2020 2020 2020 2020 2020 636f 6d6d 6974            commit",
            "+00006aa0: 7465 645f 7368 6120 3d20 4e6f 6e65 0a0a  ted_sha = None..",
            "+00006ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006ac0: 2020 2020 2020 2020 6966 2062 6c6f 622e          if blob.",
            "+00006ad0: 6964 2021 3d20 696e 6465 785f 7368 6120  id != index_sha ",
            "+00006ae0: 616e 6420 696e 6465 785f 7368 6120 213d  and index_sha !=",
            "+00006af0: 2063 6f6d 6d69 7474 6564 5f73 6861 3a0a   committed_sha:.",
            "+00006b00: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006b10: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+00006b20: 6520 4572 726f 7228 0a20 2020 2020 2020  e Error(.       ",
            "+00006b30: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006b40: 2020 2020 2020 2020 2022 6669 6c65 2068           \"file h",
            "+00006b50: 6173 2073 7461 6765 6420 636f 6e74 656e  as staged conten",
            "+00006b60: 7420 6469 6666 6572 696e 6720 220a 2020  t differing \".  ",
            "+00006b70: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006b80: 2020 2020 2020 2020 2020 2020 2020 6622                f\"",
            "+00006b90: 6672 6f6d 2062 6f74 6820 7468 6520 6669  from both the fi",
            "+00006ba0: 6c65 2061 6e64 2068 6561 643a 207b 707d  le and head: {p}",
            "+00006bb0: 220a 2020 2020 2020 2020 2020 2020 2020  \".              ",
            "+00006bc0: 2020 2020 2020 2020 2020 2020 2020 290a                ).",
            "+00006bd0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00006be0: 2020 2020 2020 2020 2069 6620 696e 6465           if inde",
            "+00006bf0: 785f 7368 6120 213d 2063 6f6d 6d69 7474  x_sha != committ",
            "+00006c00: 6564 5f73 6861 3a0a 2020 2020 2020 2020  ed_sha:.        ",
            "+00006c10: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00006c20: 2020 2020 7261 6973 6520 4572 726f 7228      raise Error(",
            "+00006c30: 6622 6669 6c65 2068 6173 2073 7461 6765  f\"file has stage",
            "+00006c40: 6420 6368 616e 6765 733a 207b 707d 2229  d changes: {p}\")",
            "+00006c50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00006c60: 2020 2020 2020 2020 206f 732e 7265 6d6f           os.remo",
            "+00006c70: 7665 2866 756c 6c5f 7061 7468 5f62 7974  ve(full_path_byt",
            "+00006c80: 6573 290a 2020 2020 2020 2020 2020 2020  es).            ",
            "+00006c90: 6465 6c20 696e 6465 785b 7472 6565 5f70  del index[tree_p",
            "+00006ca0: 6174 685d 0a20 2020 2020 2020 2069 6e64  ath].        ind",
            "+00006cb0: 6578 2e77 7269 7465 2829 0a0a 0a72 6d20  ex.write()...rm ",
            "+00006cc0: 3d20 7265 6d6f 7665 0a0a 0a64 6566 206d  = remove...def m",
            "+00006cd0: 7628 0a20 2020 2072 6570 6f3a 2055 6e69  v(.    repo: Uni",
            "+00006ce0: 6f6e 5b73 7472 2c20 6f73 2e50 6174 684c  on[str, os.PathL",
            "+00006cf0: 696b 652c 2042 6173 6552 6570 6f5d 2c0a  ike, BaseRepo],.",
            "+00006d00: 2020 2020 736f 7572 6365 3a20 556e 696f      source: Unio",
            "+00006d10: 6e5b 7374 722c 2062 7974 6573 2c20 6f73  n[str, bytes, os",
            "+00006d20: 2e50 6174 684c 696b 655d 2c0a 2020 2020  .PathLike],.    ",
            "+00006d30: 6465 7374 696e 6174 696f 6e3a 2055 6e69  destination: Uni",
            "+00006d40: 6f6e 5b73 7472 2c20 6279 7465 732c 206f  on[str, bytes, o",
            "+00006d50: 732e 5061 7468 4c69 6b65 5d2c 0a20 2020  s.PathLike],.   ",
            "+00006d60: 2066 6f72 6365 3a20 626f 6f6c 203d 2046   force: bool = F",
            "+00006d70: 616c 7365 2c0a 2920 2d3e 204e 6f6e 653a  alse,.) -> None:",
            "+00006d80: 0a20 2020 2022 2222 4d6f 7665 206f 7220  .    \"\"\"Move or ",
            "+00006d90: 7265 6e61 6d65 2061 2066 696c 652c 2064  rename a file, d",
            "+00006da0: 6972 6563 746f 7279 2c20 6f72 2073 796d  irectory, or sym",
            "+00006db0: 6c69 6e6b 2e0a 0a20 2020 2041 7267 733a  link...    Args:",
            "+00006dc0: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "+00006dd0: 6820 746f 2074 6865 2072 6570 6f73 6974  h to the reposit",
            "+00006de0: 6f72 790a 2020 2020 2020 736f 7572 6365  ory.      source",
            "+00006df0: 3a20 5061 7468 2074 6f20 6d6f 7665 2066  : Path to move f",
            "+00006e00: 726f 6d0a 2020 2020 2020 6465 7374 696e  rom.      destin",
            "+00006e10: 6174 696f 6e3a 2050 6174 6820 746f 206d  ation: Path to m",
            "+00006e20: 6f76 6520 746f 0a20 2020 2020 2066 6f72  ove to.      for",
            "+00006e30: 6365 3a20 466f 7263 6520 6d6f 7665 2065  ce: Force move e",
            "+00006e40: 7665 6e20 6966 2064 6573 7469 6e61 7469  ven if destinati",
            "+00006e50: 6f6e 2065 7869 7374 730a 0a20 2020 2052  on exists..    R",
            "+00006e60: 6169 7365 733a 0a20 2020 2020 2045 7272  aises:.      Err",
            "+00006e70: 6f72 3a20 4966 2073 6f75 7263 6520 646f  or: If source do",
            "+00006e80: 6573 6e27 7420 6578 6973 742c 2069 7320  esn't exist, is ",
            "+00006e90: 6e6f 7420 7472 6163 6b65 642c 206f 7220  not tracked, or ",
            "+00006ea0: 6465 7374 696e 6174 696f 6e20 616c 7265  destination alre",
            "+00006eb0: 6164 7920 6578 6973 7473 2028 7769 7468  ady exists (with",
            "+00006ec0: 6f75 7420 666f 7263 6529 0a20 2020 2022  out force).    \"",
            "+00006ed0: 2222 0a20 2020 2077 6974 6820 6f70 656e  \"\".    with open",
            "+00006ee0: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "+00006ef0: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "+00006f00: 2020 696e 6465 7820 3d20 722e 6f70 656e    index = r.open",
            "+00006f10: 5f69 6e64 6578 2829 0a0a 2020 2020 2020  _index()..      ",
            "+00006f20: 2020 2320 4861 6e64 6c65 2070 6174 6873    # Handle paths",
            "+00006f30: 202d 2063 6f6e 7665 7274 2074 6f20 7374   - convert to st",
            "+00006f40: 7269 6e67 2069 6620 6e65 6365 7373 6172  ring if necessar",
            "+00006f50: 790a 2020 2020 2020 2020 6966 2069 7369  y.        if isi",
            "+00006f60: 6e73 7461 6e63 6528 736f 7572 6365 2c20  nstance(source, ",
            "+00006f70: 6279 7465 7329 3a0a 2020 2020 2020 2020  bytes):.        ",
            "+00006f80: 2020 2020 736f 7572 6365 203d 2073 6f75      source = sou",
            "+00006f90: 7263 652e 6465 636f 6465 2873 7973 2e67  rce.decode(sys.g",
            "+00006fa0: 6574 6669 6c65 7379 7374 656d 656e 636f  etfilesystemenco",
            "+00006fb0: 6469 6e67 2829 290a 2020 2020 2020 2020  ding()).        ",
            "+00006fc0: 656c 6966 2068 6173 6174 7472 2873 6f75  elif hasattr(sou",
            "+00006fd0: 7263 652c 2022 5f5f 6673 7061 7468 5f5f  rce, \"__fspath__",
            "+00006fe0: 2229 3a0a 2020 2020 2020 2020 2020 2020  \"):.            ",
            "+00006ff0: 736f 7572 6365 203d 206f 732e 6673 7061  source = os.fspa",
            "+00007000: 7468 2873 6f75 7263 6529 0a20 2020 2020  th(source).     ",
            "+00007010: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       ",
            "+00007020: 2020 2020 2073 6f75 7263 6520 3d20 7374       source = st",
            "+00007030: 7228 736f 7572 6365 290a 0a20 2020 2020  r(source)..     ",
            "+00007040: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance",
            "+00007050: 2864 6573 7469 6e61 7469 6f6e 2c20 6279  (destination, by",
            "+00007060: 7465 7329 3a0a 2020 2020 2020 2020 2020  tes):.          ",
            "+00007070: 2020 6465 7374 696e 6174 696f 6e20 3d20    destination = ",
            "+00007080: 6465 7374 696e 6174 696f 6e2e 6465 636f  destination.deco",
            "+00007090: 6465 2873 7973 2e67 6574 6669 6c65 7379  de(sys.getfilesy",
            "+000070a0: 7374 656d 656e 636f 6469 6e67 2829 290a  stemencoding()).",
            "+000070b0: 2020 2020 2020 2020 656c 6966 2068 6173          elif has",
            "+000070c0: 6174 7472 2864 6573 7469 6e61 7469 6f6e  attr(destination",
            "+000070d0: 2c20 225f 5f66 7370 6174 685f 5f22 293a  , \"__fspath__\"):",
            "+000070e0: 0a20 2020 2020 2020 2020 2020 2064 6573  .            des",
            "+000070f0: 7469 6e61 7469 6f6e 203d 206f 732e 6673  tination = os.fs",
            "+00007100: 7061 7468 2864 6573 7469 6e61 7469 6f6e  path(destination",
            "+00007110: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.",
            "+00007120: 2020 2020 2020 2020 2020 2020 6465 7374              dest",
            "+00007130: 696e 6174 696f 6e20 3d20 7374 7228 6465  ination = str(de",
            "+00007140: 7374 696e 6174 696f 6e29 0a0a 2020 2020  stination)..    ",
            "+00007150: 2020 2020 2320 4765 7420 6675 6c6c 2070      # Get full p",
            "+00007160: 6174 6873 0a20 2020 2020 2020 2069 6620  aths.        if ",
            "+00007170: 6f73 2e70 6174 682e 6973 6162 7328 736f  os.path.isabs(so",
            "+00007180: 7572 6365 293a 0a20 2020 2020 2020 2020  urce):.         ",
            "+00007190: 2020 2073 6f75 7263 655f 6675 6c6c 5f70     source_full_p",
            "+000071a0: 6174 6820 3d20 736f 7572 6365 0a20 2020  ath = source.   ",
            "+000071b0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     ",
            "+000071c0: 2020 2020 2020 2023 2054 7265 6174 2072         # Treat r",
            "+000071d0: 656c 6174 6976 6520 7061 7468 7320 6173  elative paths as",
            "+000071e0: 2072 656c 6174 6976 6520 746f 2074 6865   relative to the",
            "+000071f0: 2072 6570 6f73 6974 6f72 7920 726f 6f74   repository root",
            "+00007200: 0a20 2020 2020 2020 2020 2020 2073 6f75  .            sou",
            "+00007210: 7263 655f 6675 6c6c 5f70 6174 6820 3d20  rce_full_path = ",
            "+00007220: 6f73 2e70 6174 682e 6a6f 696e 2872 2e70  os.path.join(r.p",
            "+00007230: 6174 682c 2073 6f75 7263 6529 0a0a 2020  ath, source)..  ",
            "+00007240: 2020 2020 2020 6966 206f 732e 7061 7468        if os.path",
            "+00007250: 2e69 7361 6273 2864 6573 7469 6e61 7469  .isabs(destinati",
            "+00007260: 6f6e 293a 0a20 2020 2020 2020 2020 2020  on):.           ",
            "+00007270: 2064 6573 7469 6e61 7469 6f6e 5f66 756c   destination_ful",
            "+00007280: 6c5f 7061 7468 203d 2064 6573 7469 6e61  l_path = destina",
            "+00007290: 7469 6f6e 0a20 2020 2020 2020 2065 6c73  tion.        els",
            "+000072a0: 653a 0a20 2020 2020 2020 2020 2020 2023  e:.            #",
            "+000072b0: 2054 7265 6174 2072 656c 6174 6976 6520   Treat relative ",
            "+000072c0: 7061 7468 7320 6173 2072 656c 6174 6976  paths as relativ",
            "+000072d0: 6520 746f 2074 6865 2072 6570 6f73 6974  e to the reposit",
            "+000072e0: 6f72 7920 726f 6f74 0a20 2020 2020 2020  ory root.       ",
            "+000072f0: 2020 2020 2064 6573 7469 6e61 7469 6f6e       destination",
            "+00007300: 5f66 756c 6c5f 7061 7468 203d 206f 732e  _full_path = os.",
            "+00007310: 7061 7468 2e6a 6f69 6e28 722e 7061 7468  path.join(r.path",
            "+00007320: 2c20 6465 7374 696e 6174 696f 6e29 0a0a  , destination)..",
            "+00007330: 2020 2020 2020 2020 2320 4368 6563 6b20          # Check ",
            "+00007340: 6966 2064 6573 7469 6e61 7469 6f6e 2069  if destination i",
            "+00007350: 7320 6120 6469 7265 6374 6f72 790a 2020  s a directory.  ",
            "+00007360: 2020 2020 2020 6966 206f 732e 7061 7468        if os.path",
            "+00007370: 2e69 7364 6972 2864 6573 7469 6e61 7469  .isdir(destinati",
            "+00007380: 6f6e 5f66 756c 6c5f 7061 7468 293a 0a20  on_full_path):. ",
            "+00007390: 2020 2020 2020 2020 2020 2023 204d 6f76             # Mov",
            "+000073a0: 6520 736f 7572 6365 2069 6e74 6f20 6465  e source into de",
            "+000073b0: 7374 696e 6174 696f 6e20 6469 7265 6374  stination direct",
            "+000073c0: 6f72 790a 2020 2020 2020 2020 2020 2020  ory.            ",
            "+000073d0: 6261 7365 6e61 6d65 203d 206f 732e 7061  basename = os.pa",
            "+000073e0: 7468 2e62 6173 656e 616d 6528 736f 7572  th.basename(sour",
            "+000073f0: 6365 5f66 756c 6c5f 7061 7468 290a 2020  ce_full_path).  ",
            "+00007400: 2020 2020 2020 2020 2020 6465 7374 696e            destin",
            "+00007410: 6174 696f 6e5f 6675 6c6c 5f70 6174 6820  ation_full_path ",
            "+00007420: 3d20 6f73 2e70 6174 682e 6a6f 696e 2864  = os.path.join(d",
            "+00007430: 6573 7469 6e61 7469 6f6e 5f66 756c 6c5f  estination_full_",
            "+00007440: 7061 7468 2c20 6261 7365 6e61 6d65 290a  path, basename).",
            "+00007450: 0a20 2020 2020 2020 2023 2043 6f6e 7665  .        # Conve",
            "+00007460: 7274 2074 6f20 7472 6565 2070 6174 6873  rt to tree paths",
            "+00007470: 2066 6f72 2069 6e64 6578 0a20 2020 2020   for index.     ",
            "+00007480: 2020 2073 6f75 7263 655f 7472 6565 5f70     source_tree_p",
            "+00007490: 6174 6820 3d20 7061 7468 5f74 6f5f 7472  ath = path_to_tr",
            "+000074a0: 6565 5f70 6174 6828 722e 7061 7468 2c20  ee_path(r.path, ",
            "+000074b0: 736f 7572 6365 5f66 756c 6c5f 7061 7468  source_full_path",
            "+000074c0: 290a 2020 2020 2020 2020 6465 7374 696e  ).        destin",
            "+000074d0: 6174 696f 6e5f 7472 6565 5f70 6174 6820  ation_tree_path ",
            "+000074e0: 3d20 7061 7468 5f74 6f5f 7472 6565 5f70  = path_to_tree_p",
            "+000074f0: 6174 6828 722e 7061 7468 2c20 6465 7374  ath(r.path, dest",
            "+00007500: 696e 6174 696f 6e5f 6675 6c6c 5f70 6174  ination_full_pat",
            "+00007510: 6829 0a0a 2020 2020 2020 2020 2320 4368  h)..        # Ch",
            "+00007520: 6563 6b20 6966 2073 6f75 7263 6520 6578  eck if source ex",
            "+00007530: 6973 7473 2069 6e20 696e 6465 780a 2020  ists in index.  ",
            "+00007540: 2020 2020 2020 6966 2073 6f75 7263 655f        if source_",
            "+00007550: 7472 6565 5f70 6174 6820 6e6f 7420 696e  tree_path not in",
            "+00007560: 2069 6e64 6578 3a0a 2020 2020 2020 2020   index:.        ",
            "+00007570: 2020 2020 7261 6973 6520 4572 726f 7228      raise Error(",
            "+00007580: 6622 736f 7572 6365 2027 7b73 6f75 7263  f\"source '{sourc",
            "+00007590: 657d 2720 6973 206e 6f74 2075 6e64 6572  e}' is not under",
            "+000075a0: 2076 6572 7369 6f6e 2063 6f6e 7472 6f6c   version control",
            "+000075b0: 2229 0a0a 2020 2020 2020 2020 2320 4368  \")..        # Ch",
            "+000075c0: 6563 6b20 6966 2073 6f75 7263 6520 6578  eck if source ex",
            "+000075d0: 6973 7473 2069 6e20 6669 6c65 7379 7374  ists in filesyst",
            "+000075e0: 656d 0a20 2020 2020 2020 2069 6620 6e6f  em.        if no",
            "+000075f0: 7420 6f73 2e70 6174 682e 6578 6973 7473  t os.path.exists",
            "+00007600: 2873 6f75 7263 655f 6675 6c6c 5f70 6174  (source_full_pat",
            "+00007610: 6829 3a0a 2020 2020 2020 2020 2020 2020  h):.            ",
            "+00007620: 7261 6973 6520 4572 726f 7228 6622 736f  raise Error(f\"so",
            "+00007630: 7572 6365 2027 7b73 6f75 7263 657d 2720  urce '{source}' ",
            "+00007640: 646f 6573 206e 6f74 2065 7869 7374 2229  does not exist\")",
            "+00007650: 0a0a 2020 2020 2020 2020 2320 4368 6563  ..        # Chec",
            "+00007660: 6b20 6966 2064 6573 7469 6e61 7469 6f6e  k if destination",
            "+00007670: 2061 6c72 6561 6479 2065 7869 7374 730a   already exists.",
            "+00007680: 2020 2020 2020 2020 6966 206f 732e 7061          if os.pa",
            "+00007690: 7468 2e65 7869 7374 7328 6465 7374 696e  th.exists(destin",
            "+000076a0: 6174 696f 6e5f 6675 6c6c 5f70 6174 6829  ation_full_path)",
            "+000076b0: 2061 6e64 206e 6f74 2066 6f72 6365 3a0a   and not force:.",
            "+000076c0: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+000076d0: 6520 4572 726f 7228 6622 6465 7374 696e  e Error(f\"destin",
            "+000076e0: 6174 696f 6e20 277b 6465 7374 696e 6174  ation '{destinat",
            "+000076f0: 696f 6e7d 2720 616c 7265 6164 7920 6578  ion}' already ex",
            "+00007700: 6973 7473 2028 7573 6520 2d66 2074 6f20  ists (use -f to ",
            "+00007710: 666f 7263 6529 2229 0a0a 2020 2020 2020  force)\")..      ",
            "+00007720: 2020 2320 4368 6563 6b20 6966 2064 6573    # Check if des",
            "+00007730: 7469 6e61 7469 6f6e 2069 7320 616c 7265  tination is alre",
            "+00007740: 6164 7920 696e 2069 6e64 6578 0a20 2020  ady in index.   ",
            "+00007750: 2020 2020 2069 6620 6465 7374 696e 6174       if destinat",
            "+00007760: 696f 6e5f 7472 6565 5f70 6174 6820 696e  ion_tree_path in",
            "+00007770: 2069 6e64 6578 2061 6e64 206e 6f74 2066   index and not f",
            "+00007780: 6f72 6365 3a0a 2020 2020 2020 2020 2020  orce:.          ",
            "+00007790: 2020 7261 6973 6520 4572 726f 7228 0a20    raise Error(. ",
            "+000077a0: 2020 2020 2020 2020 2020 2020 2020 2066                 f",
            "+000077b0: 2264 6573 7469 6e61 7469 6f6e 2027 7b64  \"destination '{d",
            "+000077c0: 6573 7469 6e61 7469 6f6e 7d27 2061 6c72  estination}' alr",
            "+000077d0: 6561 6479 2065 7869 7374 7320 696e 2069  eady exists in i",
            "+000077e0: 6e64 6578 2028 7573 6520 2d66 2074 6f20  ndex (use -f to ",
            "+000077f0: 666f 7263 6529 220a 2020 2020 2020 2020  force)\".        ",
            "+00007800: 2020 2020 290a 0a20 2020 2020 2020 2023      )..        #",
            "+00007810: 2047 6574 2074 6865 2069 6e64 6578 2065   Get the index e",
            "+00007820: 6e74 7279 2066 6f72 2074 6865 2073 6f75  ntry for the sou",
            "+00007830: 7263 650a 2020 2020 2020 2020 736f 7572  rce.        sour",
            "+00007840: 6365 5f65 6e74 7279 203d 2069 6e64 6578  ce_entry = index",
            "+00007850: 5b73 6f75 7263 655f 7472 6565 5f70 6174  [source_tree_pat",
            "+00007860: 685d 0a0a 2020 2020 2020 2020 2320 436f  h]..        # Co",
            "+00007870: 6e76 6572 7420 746f 2062 7974 6573 2066  nvert to bytes f",
            "+00007880: 6f72 2066 696c 6520 6f70 6572 6174 696f  or file operatio",
            "+00007890: 6e73 0a20 2020 2020 2020 2073 6f75 7263  ns.        sourc",
            "+000078a0: 655f 6675 6c6c 5f70 6174 685f 6279 7465  e_full_path_byte",
            "+000078b0: 7320 3d20 6f73 2e66 7365 6e63 6f64 6528  s = os.fsencode(",
            "+000078c0: 736f 7572 6365 5f66 756c 6c5f 7061 7468  source_full_path",
            "+000078d0: 290a 2020 2020 2020 2020 6465 7374 696e  ).        destin",
            "+000078e0: 6174 696f 6e5f 6675 6c6c 5f70 6174 685f  ation_full_path_",
            "+000078f0: 6279 7465 7320 3d20 6f73 2e66 7365 6e63  bytes = os.fsenc",
            "+00007900: 6f64 6528 6465 7374 696e 6174 696f 6e5f  ode(destination_",
            "+00007910: 6675 6c6c 5f70 6174 6829 0a0a 2020 2020  full_path)..    ",
            "+00007920: 2020 2020 2320 4372 6561 7465 2070 6172      # Create par",
            "+00007930: 656e 7420 6469 7265 6374 6f72 7920 666f  ent directory fo",
            "+00007940: 7220 6465 7374 696e 6174 696f 6e20 6966  r destination if",
            "+00007950: 206e 6565 6465 640a 2020 2020 2020 2020   needed.        ",
            "+00007960: 6465 7374 5f64 6972 203d 206f 732e 7061  dest_dir = os.pa",
            "+00007970: 7468 2e64 6972 6e61 6d65 2864 6573 7469  th.dirname(desti",
            "+00007980: 6e61 7469 6f6e 5f66 756c 6c5f 7061 7468  nation_full_path",
            "+00007990: 5f62 7974 6573 290a 2020 2020 2020 2020  _bytes).        ",
            "+000079a0: 6966 2064 6573 745f 6469 7220 616e 6420  if dest_dir and ",
            "+000079b0: 6e6f 7420 6f73 2e70 6174 682e 6578 6973  not os.path.exis",
            "+000079c0: 7473 2864 6573 745f 6469 7229 3a0a 2020  ts(dest_dir):.  ",
            "+000079d0: 2020 2020 2020 2020 2020 6f73 2e6d 616b            os.mak",
            "+000079e0: 6564 6972 7328 6465 7374 5f64 6972 290a  edirs(dest_dir).",
            "+000079f0: 0a20 2020 2020 2020 2023 204d 6f76 6520  .        # Move ",
            "+00007a00: 7468 6520 6669 6c65 2069 6e20 7468 6520  the file in the ",
            "+00007a10: 6669 6c65 7379 7374 656d 0a20 2020 2020  filesystem.     ",
            "+00007a20: 2020 2069 6620 6f73 2e70 6174 682e 6578     if os.path.ex",
            "+00007a30: 6973 7473 2864 6573 7469 6e61 7469 6f6e  ists(destination",
            "+00007a40: 5f66 756c 6c5f 7061 7468 5f62 7974 6573  _full_path_bytes",
            "+00007a50: 2920 616e 6420 666f 7263 653a 0a20 2020  ) and force:.   ",
            "+00007a60: 2020 2020 2020 2020 206f 732e 7265 6d6f           os.remo",
            "+00007a70: 7665 2864 6573 7469 6e61 7469 6f6e 5f66  ve(destination_f",
            "+00007a80: 756c 6c5f 7061 7468 5f62 7974 6573 290a  ull_path_bytes).",
            "+00007a90: 2020 2020 2020 2020 6f73 2e72 656e 616d          os.renam",
            "+00007aa0: 6528 736f 7572 6365 5f66 756c 6c5f 7061  e(source_full_pa",
            "+00007ab0: 7468 5f62 7974 6573 2c20 6465 7374 696e  th_bytes, destin",
            "+00007ac0: 6174 696f 6e5f 6675 6c6c 5f70 6174 685f  ation_full_path_",
            "+00007ad0: 6279 7465 7329 0a0a 2020 2020 2020 2020  bytes)..        ",
            "+00007ae0: 2320 5570 6461 7465 2074 6865 2069 6e64  # Update the ind",
            "+00007af0: 6578 0a20 2020 2020 2020 2064 656c 2069  ex.        del i",
            "+00007b00: 6e64 6578 5b73 6f75 7263 655f 7472 6565  ndex[source_tree",
            "+00007b10: 5f70 6174 685d 0a20 2020 2020 2020 2069  _path].        i",
            "+00007b20: 6e64 6578 5b64 6573 7469 6e61 7469 6f6e  ndex[destination",
            "+00007b30: 5f74 7265 655f 7061 7468 5d20 3d20 736f  _tree_path] = so",
            "+00007b40: 7572 6365 5f65 6e74 7279 0a20 2020 2020  urce_entry.     ",
            "+00007b50: 2020 2069 6e64 6578 2e77 7269 7465 2829     index.write()",
            "+00007b60: 0a0a 0a6d 6f76 6520 3d20 6d76 0a0a 0a64  ...move = mv...d",
            "+00007b70: 6566 2063 6f6d 6d69 745f 6465 636f 6465  ef commit_decode",
            "+00007b80: 2863 6f6d 6d69 742c 2063 6f6e 7465 6e74  (commit, content",
            "+00007b90: 732c 2064 6566 6175 6c74 5f65 6e63 6f64  s, default_encod",
            "+00007ba0: 696e 673d 4445 4641 554c 545f 454e 434f  ing=DEFAULT_ENCO",
            "+00007bb0: 4449 4e47 293a 0a20 2020 2069 6620 636f  DING):.    if co",
            "+00007bc0: 6d6d 6974 2e65 6e63 6f64 696e 673a 0a20  mmit.encoding:. ",
            "+00007bd0: 2020 2020 2020 2065 6e63 6f64 696e 6720         encoding ",
            "+00007be0: 3d20 636f 6d6d 6974 2e65 6e63 6f64 696e  = commit.encodin",
            "+00007bf0: 672e 6465 636f 6465 2822 6173 6369 6922  g.decode(\"ascii\"",
            "+00007c00: 290a 2020 2020 656c 7365 3a0a 2020 2020  ).    else:.    ",
            "+00007c10: 2020 2020 656e 636f 6469 6e67 203d 2064      encoding = d",
            "+00007c20: 6566 6175 6c74 5f65 6e63 6f64 696e 670a  efault_encoding.",
            "+00007c30: 2020 2020 7265 7475 726e 2063 6f6e 7465      return conte",
            "+00007c40: 6e74 732e 6465 636f 6465 2865 6e63 6f64  nts.decode(encod",
            "+00007c50: 696e 672c 2022 7265 706c 6163 6522 290a  ing, \"replace\").",
            "+00007c60: 0a0a 6465 6620 636f 6d6d 6974 5f65 6e63  ..def commit_enc",
            "+00007c70: 6f64 6528 636f 6d6d 6974 2c20 636f 6e74  ode(commit, cont",
            "+00007c80: 656e 7473 2c20 6465 6661 756c 745f 656e  ents, default_en",
            "+00007c90: 636f 6469 6e67 3d44 4546 4155 4c54 5f45  coding=DEFAULT_E",
            "+00007ca0: 4e43 4f44 494e 4729 3a0a 2020 2020 6966  NCODING):.    if",
            "+00007cb0: 2063 6f6d 6d69 742e 656e 636f 6469 6e67   commit.encoding",
            "+00007cc0: 3a0a 2020 2020 2020 2020 656e 636f 6469  :.        encodi",
            "+00007cd0: 6e67 203d 2063 6f6d 6d69 742e 656e 636f  ng = commit.enco",
            "+00007ce0: 6469 6e67 2e64 6563 6f64 6528 2261 7363  ding.decode(\"asc",
            "+00007cf0: 6969 2229 0a20 2020 2065 6c73 653a 0a20  ii\").    else:. ",
            "+00007d00: 2020 2020 2020 2065 6e63 6f64 696e 6720         encoding ",
            "+00007d10: 3d20 6465 6661 756c 745f 656e 636f 6469  = default_encodi",
            "+00007d20: 6e67 0a20 2020 2072 6574 7572 6e20 636f  ng.    return co",
            "+00007d30: 6e74 656e 7473 2e65 6e63 6f64 6528 656e  ntents.encode(en",
            "+00007d40: 636f 6469 6e67 290a 0a0a 6465 6620 7072  coding)...def pr",
            "+00007d50: 696e 745f 636f 6d6d 6974 2863 6f6d 6d69  int_commit(commi",
            "+00007d60: 742c 2064 6563 6f64 652c 206f 7574 7374  t, decode, outst",
            "+00007d70: 7265 616d 3d73 7973 2e73 7464 6f75 7429  ream=sys.stdout)",
            "+00007d80: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2222   -> None:.    \"\"",
            "+00007d90: 2257 7269 7465 2061 2068 756d 616e 2d72  \"Write a human-r",
            "+00007da0: 6561 6461 626c 6520 636f 6d6d 6974 206c  eadable commit l",
            "+00007db0: 6f67 2065 6e74 7279 2e0a 0a20 2020 2041  og entry...    A",
            "+00007dc0: 7267 733a 0a20 2020 2020 2063 6f6d 6d69  rgs:.      commi",
            "+00007dd0: 743a 2041 2060 436f 6d6d 6974 6020 6f62  t: A `Commit` ob",
            "+00007de0: 6a65 6374 0a20 2020 2020 206f 7574 7374  ject.      outst",
            "+00007df0: 7265 616d 3a20 4120 7374 7265 616d 2066  ream: A stream f",
            "+00007e00: 696c 6520 746f 2077 7269 7465 2074 6f0a  ile to write to.",
            "+00007e10: 2020 2020 2222 220a 2020 2020 6f75 7473      \"\"\".    outs",
            "+00007e20: 7472 6561 6d2e 7772 6974 6528 222d 2220  tream.write(\"-\" ",
            "+00007e30: 2a20 3530 202b 2022 5c6e 2229 0a20 2020  * 50 + \"\\n\").   ",
            "+00007e40: 206f 7574 7374 7265 616d 2e77 7269 7465   outstream.write",
            "+00007e50: 2822 636f 6d6d 6974 3a20 2220 2b20 636f  (\"commit: \" + co",
            "+00007e60: 6d6d 6974 2e69 642e 6465 636f 6465 2822  mmit.id.decode(\"",
            "+00007e70: 6173 6369 6922 2920 2b20 225c 6e22 290a  ascii\") + \"\\n\").",
            "+00007e80: 2020 2020 6966 206c 656e 2863 6f6d 6d69      if len(commi",
            "+00007e90: 742e 7061 7265 6e74 7329 203e 2031 3a0a  t.parents) > 1:.",
            "+00007ea0: 2020 2020 2020 2020 6f75 7473 7472 6561          outstrea",
            "+00007eb0: 6d2e 7772 6974 6528 0a20 2020 2020 2020  m.write(.       ",
            "+00007ec0: 2020 2020 2022 6d65 7267 653a 2022 0a20       \"merge: \". ",
            "+00007ed0: 2020 2020 2020 2020 2020 202b 2022 2e2e             + \"..",
            "+00007ee0: 2e22 2e6a 6f69 6e28 5b63 2e64 6563 6f64  .\".join([c.decod",
            "+00007ef0: 6528 2261 7363 6969 2229 2066 6f72 2063  e(\"ascii\") for c",
            "+00007f00: 2069 6e20 636f 6d6d 6974 2e70 6172 656e   in commit.paren",
            "+00007f10: 7473 5b31 3a5d 5d29 0a20 2020 2020 2020  ts[1:]]).       ",
            "+00007f20: 2020 2020 202b 2022 5c6e 220a 2020 2020       + \"\\n\".    ",
            "+00007f30: 2020 2020 290a 2020 2020 6f75 7473 7472      ).    outstr",
            "+00007f40: 6561 6d2e 7772 6974 6528 2241 7574 686f  eam.write(\"Autho",
            "+00007f50: 723a 2022 202b 2064 6563 6f64 6528 636f  r: \" + decode(co",
            "+00007f60: 6d6d 6974 2e61 7574 686f 7229 202b 2022  mmit.author) + \"",
            "+00007f70: 5c6e 2229 0a20 2020 2069 6620 636f 6d6d  \\n\").    if comm",
            "+00007f80: 6974 2e61 7574 686f 7220 213d 2063 6f6d  it.author != com",
            "+00007f90: 6d69 742e 636f 6d6d 6974 7465 723a 0a20  mit.committer:. ",
            "+00007fa0: 2020 2020 2020 206f 7574 7374 7265 616d         outstream",
            "+00007fb0: 2e77 7269 7465 2822 436f 6d6d 6974 7465  .write(\"Committe",
            "+00007fc0: 723a 2022 202b 2064 6563 6f64 6528 636f  r: \" + decode(co",
            "+00007fd0: 6d6d 6974 2e63 6f6d 6d69 7474 6572 2920  mmit.committer) ",
            "+00007fe0: 2b20 225c 6e22 290a 0a20 2020 2074 696d  + \"\\n\")..    tim",
            "+00007ff0: 655f 7475 706c 6520 3d20 7469 6d65 2e67  e_tuple = time.g",
            "+00008000: 6d74 696d 6528 636f 6d6d 6974 2e61 7574  mtime(commit.aut",
            "+00008010: 686f 725f 7469 6d65 202b 2063 6f6d 6d69  hor_time + commi",
            "+00008020: 742e 6175 7468 6f72 5f74 696d 657a 6f6e  t.author_timezon",
            "+00008030: 6529 0a20 2020 2074 696d 655f 7374 7220  e).    time_str ",
            "+00008040: 3d20 7469 6d65 2e73 7472 6674 696d 6528  = time.strftime(",
            "+00008050: 2225 6120 2562 2025 6420 2559 2025 483a  \"%a %b %d %Y %H:",
            "+00008060: 254d 3a25 5322 2c20 7469 6d65 5f74 7570  %M:%S\", time_tup",
            "+00008070: 6c65 290a 2020 2020 7469 6d65 7a6f 6e65  le).    timezone",
            "+00008080: 5f73 7472 203d 2066 6f72 6d61 745f 7469  _str = format_ti",
            "+00008090: 6d65 7a6f 6e65 2863 6f6d 6d69 742e 6175  mezone(commit.au",
            "+000080a0: 7468 6f72 5f74 696d 657a 6f6e 6529 2e64  thor_timezone).d",
            "+000080b0: 6563 6f64 6528 2261 7363 6969 2229 0a20  ecode(\"ascii\"). ",
            "+000080c0: 2020 206f 7574 7374 7265 616d 2e77 7269     outstream.wri",
            "+000080d0: 7465 2822 4461 7465 3a20 2020 2220 2b20  te(\"Date:   \" + ",
            "+000080e0: 7469 6d65 5f73 7472 202b 2022 2022 202b  time_str + \" \" +",
            "+000080f0: 2074 696d 657a 6f6e 655f 7374 7220 2b20   timezone_str + ",
            "+00008100: 225c 6e22 290a 2020 2020 6966 2063 6f6d  \"\\n\").    if com",
            "+00008110: 6d69 742e 6d65 7373 6167 653a 0a20 2020  mit.message:.   ",
            "+00008120: 2020 2020 206f 7574 7374 7265 616d 2e77       outstream.w",
            "+00008130: 7269 7465 2822 5c6e 2229 0a20 2020 2020  rite(\"\\n\").     ",
            "+00008140: 2020 206f 7574 7374 7265 616d 2e77 7269     outstream.wri",
            "+00008150: 7465 2864 6563 6f64 6528 636f 6d6d 6974  te(decode(commit",
            "+00008160: 2e6d 6573 7361 6765 2920 2b20 225c 6e22  .message) + \"\\n\"",
            "+00008170: 290a 2020 2020 2020 2020 6f75 7473 7472  ).        outstr",
            "+00008180: 6561 6d2e 7772 6974 6528 225c 6e22 290a  eam.write(\"\\n\").",
            "+00008190: 0a0a 6465 6620 7072 696e 745f 7461 6728  ..def print_tag(",
            "+000081a0: 7461 672c 2064 6563 6f64 652c 206f 7574  tag, decode, out",
            "+000081b0: 7374 7265 616d 3d73 7973 2e73 7464 6f75  stream=sys.stdou",
            "+000081c0: 7429 202d 3e20 4e6f 6e65 3a0a 2020 2020  t) -> None:.    ",
            "+000081d0: 2222 2257 7269 7465 2061 2068 756d 616e  \"\"\"Write a human",
            "+000081e0: 2d72 6561 6461 626c 6520 7461 672e 0a0a  -readable tag...",
            "+000081f0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+00008200: 7461 673a 2041 2060 5461 6760 206f 626a  tag: A `Tag` obj",
            "+00008210: 6563 740a 2020 2020 2020 6465 636f 6465  ect.      decode",
            "+00008220: 3a20 4675 6e63 7469 6f6e 2066 6f72 2064  : Function for d",
            "+00008230: 6563 6f64 696e 6720 6279 7465 7320 746f  ecoding bytes to",
            "+00008240: 2075 6e69 636f 6465 2073 7472 696e 670a   unicode string.",
            "+00008250: 2020 2020 2020 6f75 7473 7472 6561 6d3a        outstream:",
            "+00008260: 2041 2073 7472 6561 6d20 746f 2077 7269   A stream to wri",
            "+00008270: 7465 2074 6f0a 2020 2020 2222 220a 2020  te to.    \"\"\".  ",
            "+00008280: 2020 6f75 7473 7472 6561 6d2e 7772 6974    outstream.writ",
            "+00008290: 6528 2254 6167 6765 723a 2022 202b 2064  e(\"Tagger: \" + d",
            "+000082a0: 6563 6f64 6528 7461 672e 7461 6767 6572  ecode(tag.tagger",
            "+000082b0: 2920 2b20 225c 6e22 290a 2020 2020 7469  ) + \"\\n\").    ti",
            "+000082c0: 6d65 5f74 7570 6c65 203d 2074 696d 652e  me_tuple = time.",
            "+000082d0: 676d 7469 6d65 2874 6167 2e74 6167 5f74  gmtime(tag.tag_t",
            "+000082e0: 696d 6520 2b20 7461 672e 7461 675f 7469  ime + tag.tag_ti",
            "+000082f0: 6d65 7a6f 6e65 290a 2020 2020 7469 6d65  mezone).    time",
            "+00008300: 5f73 7472 203d 2074 696d 652e 7374 7266  _str = time.strf",
            "+00008310: 7469 6d65 2822 2561 2025 6220 2564 2025  time(\"%a %b %d %",
            "+00008320: 5920 2548 3a25 4d3a 2553 222c 2074 696d  Y %H:%M:%S\", tim",
            "+00008330: 655f 7475 706c 6529 0a20 2020 2074 696d  e_tuple).    tim",
            "+00008340: 657a 6f6e 655f 7374 7220 3d20 666f 726d  ezone_str = form",
            "+00008350: 6174 5f74 696d 657a 6f6e 6528 7461 672e  at_timezone(tag.",
            "+00008360: 7461 675f 7469 6d65 7a6f 6e65 292e 6465  tag_timezone).de",
            "+00008370: 636f 6465 2822 6173 6369 6922 290a 2020  code(\"ascii\").  ",
            "+00008380: 2020 6f75 7473 7472 6561 6d2e 7772 6974    outstream.writ",
            "+00008390: 6528 2244 6174 653a 2020 2022 202b 2074  e(\"Date:   \" + t",
            "+000083a0: 696d 655f 7374 7220 2b20 2220 2220 2b20  ime_str + \" \" + ",
            "+000083b0: 7469 6d65 7a6f 6e65 5f73 7472 202b 2022  timezone_str + \"",
            "+000083c0: 5c6e 2229 0a20 2020 206f 7574 7374 7265  \\n\").    outstre",
            "+000083d0: 616d 2e77 7269 7465 2822 5c6e 2229 0a20  am.write(\"\\n\"). ",
            "+000083e0: 2020 206f 7574 7374 7265 616d 2e77 7269     outstream.wri",
            "+000083f0: 7465 2864 6563 6f64 6528 7461 672e 6d65  te(decode(tag.me",
            "+00008400: 7373 6167 6529 290a 2020 2020 6f75 7473  ssage)).    outs",
            "+00008410: 7472 6561 6d2e 7772 6974 6528 225c 6e22  tream.write(\"\\n\"",
            "+00008420: 290a 0a0a 6465 6620 7368 6f77 5f62 6c6f  )...def show_blo",
            "+00008430: 6228 7265 706f 2c20 626c 6f62 2c20 6465  b(repo, blob, de",
            "+00008440: 636f 6465 2c20 6f75 7473 7472 6561 6d3d  code, outstream=",
            "+00008450: 7379 732e 7374 646f 7574 2920 2d3e 204e  sys.stdout) -> N",
            "+00008460: 6f6e 653a 0a20 2020 2022 2222 5772 6974  one:.    \"\"\"Writ",
            "+00008470: 6520 6120 626c 6f62 2074 6f20 6120 7374  e a blob to a st",
            "+00008480: 7265 616d 2e0a 0a20 2020 2041 7267 733a  ream...    Args:",
            "+00008490: 0a20 2020 2020 2072 6570 6f3a 2041 2060  .      repo: A `",
            "+000084a0: 5265 706f 6020 6f62 6a65 6374 0a20 2020  Repo` object.   ",
            "+000084b0: 2020 2062 6c6f 623a 2041 2060 426c 6f62     blob: A `Blob",
            "+000084c0: 6020 6f62 6a65 6374 0a20 2020 2020 2064  ` object.      d",
            "+000084d0: 6563 6f64 653a 2046 756e 6374 696f 6e20  ecode: Function ",
            "+000084e0: 666f 7220 6465 636f 6469 6e67 2062 7974  for decoding byt",
            "+000084f0: 6573 2074 6f20 756e 6963 6f64 6520 7374  es to unicode st",
            "+00008500: 7269 6e67 0a20 2020 2020 206f 7574 7374  ring.      outst",
            "+00008510: 7265 616d 3a20 4120 7374 7265 616d 2066  ream: A stream f",
            "+00008520: 696c 6520 746f 2077 7269 7465 2074 6f0a  ile to write to.",
            "+00008530: 2020 2020 2222 220a 2020 2020 6f75 7473      \"\"\".    outs",
            "+00008540: 7472 6561 6d2e 7772 6974 6528 6465 636f  tream.write(deco",
            "+00008550: 6465 2862 6c6f 622e 6461 7461 2929 0a0a  de(blob.data))..",
            "+00008560: 0a64 6566 2073 686f 775f 636f 6d6d 6974  .def show_commit",
            "+00008570: 2872 6570 6f2c 2063 6f6d 6d69 742c 2064  (repo, commit, d",
            "+00008580: 6563 6f64 652c 206f 7574 7374 7265 616d  ecode, outstream",
            "+00008590: 3d73 7973 2e73 7464 6f75 7429 202d 3e20  =sys.stdout) -> ",
            "+000085a0: 4e6f 6e65 3a0a 2020 2020 2222 2253 686f  None:.    \"\"\"Sho",
            "+000085b0: 7720 6120 636f 6d6d 6974 2074 6f20 6120  w a commit to a ",
            "+000085c0: 7374 7265 616d 2e0a 0a20 2020 2041 7267  stream...    Arg",
            "+000085d0: 733a 0a20 2020 2020 2072 6570 6f3a 2041  s:.      repo: A",
            "+000085e0: 2060 5265 706f 6020 6f62 6a65 6374 0a20   `Repo` object. ",
            "+000085f0: 2020 2020 2063 6f6d 6d69 743a 2041 2060       commit: A `",
            "+00008600: 436f 6d6d 6974 6020 6f62 6a65 6374 0a20  Commit` object. ",
            "+00008610: 2020 2020 2064 6563 6f64 653a 2046 756e       decode: Fun",
            "+00008620: 6374 696f 6e20 666f 7220 6465 636f 6469  ction for decodi",
            "+00008630: 6e67 2062 7974 6573 2074 6f20 756e 6963  ng bytes to unic",
            "+00008640: 6f64 6520 7374 7269 6e67 0a20 2020 2020  ode string.     ",
            "+00008650: 206f 7574 7374 7265 616d 3a20 5374 7265   outstream: Stre",
            "+00008660: 616d 2074 6f20 7772 6974 6520 746f 0a20  am to write to. ",
            "+00008670: 2020 2022 2222 0a20 2020 2070 7269 6e74     \"\"\".    print",
            "+00008680: 5f63 6f6d 6d69 7428 636f 6d6d 6974 2c20  _commit(commit, ",
            "+00008690: 6465 636f 6465 3d64 6563 6f64 652c 206f  decode=decode, o",
            "+000086a0: 7574 7374 7265 616d 3d6f 7574 7374 7265  utstream=outstre",
            "+000086b0: 616d 290a 2020 2020 6966 2063 6f6d 6d69  am).    if commi",
            "+000086c0: 742e 7061 7265 6e74 733a 0a20 2020 2020  t.parents:.     ",
            "+000086d0: 2020 2070 6172 656e 745f 636f 6d6d 6974     parent_commit",
            "+000086e0: 203d 2072 6570 6f5b 636f 6d6d 6974 2e70   = repo[commit.p",
            "+000086f0: 6172 656e 7473 5b30 5d5d 0a20 2020 2020  arents[0]].     ",
            "+00008700: 2020 2062 6173 655f 7472 6565 203d 2070     base_tree = p",
            "+00008710: 6172 656e 745f 636f 6d6d 6974 2e74 7265  arent_commit.tre",
            "+00008720: 650a 2020 2020 656c 7365 3a0a 2020 2020  e.    else:.    ",
            "+00008730: 2020 2020 6261 7365 5f74 7265 6520 3d20      base_tree = ",
            "+00008740: 4e6f 6e65 0a20 2020 2064 6966 6673 7472  None.    diffstr",
            "+00008750: 6561 6d20 3d20 4279 7465 7349 4f28 290a  eam = BytesIO().",
            "+00008760: 2020 2020 7772 6974 655f 7472 6565 5f64      write_tree_d",
            "+00008770: 6966 6628 6469 6666 7374 7265 616d 2c20  iff(diffstream, ",
            "+00008780: 7265 706f 2e6f 626a 6563 745f 7374 6f72  repo.object_stor",
            "+00008790: 652c 2062 6173 655f 7472 6565 2c20 636f  e, base_tree, co",
            "+000087a0: 6d6d 6974 2e74 7265 6529 0a20 2020 2064  mmit.tree).    d",
            "+000087b0: 6966 6673 7472 6561 6d2e 7365 656b 2830  iffstream.seek(0",
            "+000087c0: 290a 2020 2020 6f75 7473 7472 6561 6d2e  ).    outstream.",
            "+000087d0: 7772 6974 6528 636f 6d6d 6974 5f64 6563  write(commit_dec",
            "+000087e0: 6f64 6528 636f 6d6d 6974 2c20 6469 6666  ode(commit, diff",
            "+000087f0: 7374 7265 616d 2e67 6574 7661 6c75 6528  stream.getvalue(",
            "+00008800: 2929 290a 0a0a 6465 6620 7368 6f77 5f74  )))...def show_t",
            "+00008810: 7265 6528 7265 706f 2c20 7472 6565 2c20  ree(repo, tree, ",
            "+00008820: 6465 636f 6465 2c20 6f75 7473 7472 6561  decode, outstrea",
            "+00008830: 6d3d 7379 732e 7374 646f 7574 2920 2d3e  m=sys.stdout) ->",
            "+00008840: 204e 6f6e 653a 0a20 2020 2022 2222 5072   None:.    \"\"\"Pr",
            "+00008850: 696e 7420 6120 7472 6565 2074 6f20 6120  int a tree to a ",
            "+00008860: 7374 7265 616d 2e0a 0a20 2020 2041 7267  stream...    Arg",
            "+00008870: 733a 0a20 2020 2020 2072 6570 6f3a 2041  s:.      repo: A",
            "+00008880: 2060 5265 706f 6020 6f62 6a65 6374 0a20   `Repo` object. ",
            "+00008890: 2020 2020 2074 7265 653a 2041 2060 5472       tree: A `Tr",
            "+000088a0: 6565 6020 6f62 6a65 6374 0a20 2020 2020  ee` object.     ",
            "+000088b0: 2064 6563 6f64 653a 2046 756e 6374 696f   decode: Functio",
            "+000088c0: 6e20 666f 7220 6465 636f 6469 6e67 2062  n for decoding b",
            "+000088d0: 7974 6573 2074 6f20 756e 6963 6f64 6520  ytes to unicode ",
            "+000088e0: 7374 7269 6e67 0a20 2020 2020 206f 7574  string.      out",
            "+000088f0: 7374 7265 616d 3a20 5374 7265 616d 2074  stream: Stream t",
            "+00008900: 6f20 7772 6974 6520 746f 0a20 2020 2022  o write to.    \"",
            "+00008910: 2222 0a20 2020 2066 6f72 206e 2069 6e20  \"\".    for n in ",
            "+00008920: 7472 6565 3a0a 2020 2020 2020 2020 6f75  tree:.        ou",
            "+00008930: 7473 7472 6561 6d2e 7772 6974 6528 6465  tstream.write(de",
            "+00008940: 636f 6465 286e 2920 2b20 225c 6e22 290a  code(n) + \"\\n\").",
            "+00008950: 0a0a 6465 6620 7368 6f77 5f74 6167 2872  ..def show_tag(r",
            "+00008960: 6570 6f2c 2074 6167 2c20 6465 636f 6465  epo, tag, decode",
            "+00008970: 2c20 6f75 7473 7472 6561 6d3d 7379 732e  , outstream=sys.",
            "+00008980: 7374 646f 7574 2920 2d3e 204e 6f6e 653a  stdout) -> None:",
            "+00008990: 0a20 2020 2022 2222 5072 696e 7420 6120  .    \"\"\"Print a ",
            "+000089a0: 7461 6720 746f 2061 2073 7472 6561 6d2e  tag to a stream.",
            "+000089b0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            "+000089c0: 2020 7265 706f 3a20 4120 6052 6570 6f60    repo: A `Repo`",
            "+000089d0: 206f 626a 6563 740a 2020 2020 2020 7461   object.      ta",
            "+000089e0: 673a 2041 2060 5461 6760 206f 626a 6563  g: A `Tag` objec",
            "+000089f0: 740a 2020 2020 2020 6465 636f 6465 3a20  t.      decode: ",
            "+00008a00: 4675 6e63 7469 6f6e 2066 6f72 2064 6563  Function for dec",
            "+00008a10: 6f64 696e 6720 6279 7465 7320 746f 2075  oding bytes to u",
            "+00008a20: 6e69 636f 6465 2073 7472 696e 670a 2020  nicode string.  ",
            "+00008a30: 2020 2020 6f75 7473 7472 6561 6d3a 2053      outstream: S",
            "+00008a40: 7472 6561 6d20 746f 2077 7269 7465 2074  tream to write t",
            "+00008a50: 6f0a 2020 2020 2222 220a 2020 2020 7072  o.    \"\"\".    pr",
            "+00008a60: 696e 745f 7461 6728 7461 672c 2064 6563  int_tag(tag, dec",
            "+00008a70: 6f64 652c 206f 7574 7374 7265 616d 290a  ode, outstream).",
            "+00008a80: 2020 2020 7368 6f77 5f6f 626a 6563 7428      show_object(",
            "+00008a90: 7265 706f 2c20 7265 706f 5b74 6167 2e6f  repo, repo[tag.o",
            "+00008aa0: 626a 6563 745b 315d 5d2c 2064 6563 6f64  bject[1]], decod",
            "+00008ab0: 652c 206f 7574 7374 7265 616d 290a 0a0a  e, outstream)...",
            "+00008ac0: 6465 6620 7368 6f77 5f6f 626a 6563 7428  def show_object(",
            "+00008ad0: 7265 706f 2c20 6f62 6a2c 2064 6563 6f64  repo, obj, decod",
            "+00008ae0: 652c 206f 7574 7374 7265 616d 293a 0a20  e, outstream):. ",
            "+00008af0: 2020 2072 6574 7572 6e20 7b0a 2020 2020     return {.    ",
            "+00008b00: 2020 2020 6222 7472 6565 223a 2073 686f      b\"tree\": sho",
            "+00008b10: 775f 7472 6565 2c0a 2020 2020 2020 2020  w_tree,.        ",
            "+00008b20: 6222 626c 6f62 223a 2073 686f 775f 626c  b\"blob\": show_bl",
            "+00008b30: 6f62 2c0a 2020 2020 2020 2020 6222 636f  ob,.        b\"co",
            "+00008b40: 6d6d 6974 223a 2073 686f 775f 636f 6d6d  mmit\": show_comm",
            "+00008b50: 6974 2c0a 2020 2020 2020 2020 6222 7461  it,.        b\"ta",
            "+00008b60: 6722 3a20 7368 6f77 5f74 6167 2c0a 2020  g\": show_tag,.  ",
            "+00008b70: 2020 7d5b 6f62 6a2e 7479 7065 5f6e 616d    }[obj.type_nam",
            "+00008b80: 655d 2872 6570 6f2c 206f 626a 2c20 6465  e](repo, obj, de",
            "+00008b90: 636f 6465 2c20 6f75 7473 7472 6561 6d29  code, outstream)",
            "+00008ba0: 0a0a 0a64 6566 2070 7269 6e74 5f6e 616d  ...def print_nam",
            "+00008bb0: 655f 7374 6174 7573 2863 6861 6e67 6573  e_status(changes",
            "+00008bc0: 293a 0a20 2020 2022 2222 5072 696e 7420  ):.    \"\"\"Print ",
            "+00008bd0: 6120 7369 6d70 6c65 2073 7461 7475 7320  a simple status ",
            "+00008be0: 7375 6d6d 6172 792c 206c 6973 7469 6e67  summary, listing",
            "+00008bf0: 2063 6861 6e67 6564 2066 696c 6573 2e22   changed files.\"",
            "+00008c00: 2222 0a20 2020 2066 6f72 2063 6861 6e67  \"\".    for chang",
            "+00008c10: 6520 696e 2063 6861 6e67 6573 3a0a 2020  e in changes:.  ",
            "+00008c20: 2020 2020 2020 6966 206e 6f74 2063 6861        if not cha",
            "+00008c30: 6e67 653a 0a20 2020 2020 2020 2020 2020  nge:.           ",
            "+00008c40: 2063 6f6e 7469 6e75 650a 2020 2020 2020   continue.      ",
            "+00008c50: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(",
            "+00008c60: 6368 616e 6765 2c20 6c69 7374 293a 0a20  change, list):. ",
            "+00008c70: 2020 2020 2020 2020 2020 2063 6861 6e67             chang",
            "+00008c80: 6520 3d20 6368 616e 6765 5b30 5d0a 2020  e = change[0].  ",
            "+00008c90: 2020 2020 2020 6966 2063 6861 6e67 652e        if change.",
            "+00008ca0: 7479 7065 203d 3d20 4348 414e 4745 5f41  type == CHANGE_A",
            "+00008cb0: 4444 3a0a 2020 2020 2020 2020 2020 2020  DD:.            ",
            "+00008cc0: 7061 7468 3120 3d20 6368 616e 6765 2e6e  path1 = change.n",
            "+00008cd0: 6577 2e70 6174 680a 2020 2020 2020 2020  ew.path.        ",
            "+00008ce0: 2020 2020 7061 7468 3220 3d20 2222 0a20      path2 = \"\". ",
            "+00008cf0: 2020 2020 2020 2020 2020 206b 696e 6420             kind ",
            "+00008d00: 3d20 2241 220a 2020 2020 2020 2020 656c  = \"A\".        el",
            "+00008d10: 6966 2063 6861 6e67 652e 7479 7065 203d  if change.type =",
            "+00008d20: 3d20 4348 414e 4745 5f44 454c 4554 453a  = CHANGE_DELETE:",
            "+00008d30: 0a20 2020 2020 2020 2020 2020 2070 6174  .            pat",
            "+00008d40: 6831 203d 2063 6861 6e67 652e 6f6c 642e  h1 = change.old.",
            "+00008d50: 7061 7468 0a20 2020 2020 2020 2020 2020  path.           ",
            "+00008d60: 2070 6174 6832 203d 2022 220a 2020 2020   path2 = \"\".    ",
            "+00008d70: 2020 2020 2020 2020 6b69 6e64 203d 2022          kind = \"",
            "+00008d80: 4422 0a20 2020 2020 2020 2065 6c69 6620  D\".        elif ",
            "+00008d90: 6368 616e 6765 2e74 7970 6520 3d3d 2043  change.type == C",
            "+00008da0: 4841 4e47 455f 4d4f 4449 4659 3a0a 2020  HANGE_MODIFY:.  ",
            "+00008db0: 2020 2020 2020 2020 2020 7061 7468 3120            path1 ",
            "+00008dc0: 3d20 6368 616e 6765 2e6e 6577 2e70 6174  = change.new.pat",
            "+00008dd0: 680a 2020 2020 2020 2020 2020 2020 7061  h.            pa",
            "+00008de0: 7468 3220 3d20 2222 0a20 2020 2020 2020  th2 = \"\".       ",
            "+00008df0: 2020 2020 206b 696e 6420 3d20 224d 220a       kind = \"M\".",
            "+00008e00: 2020 2020 2020 2020 656c 6966 2063 6861          elif cha",
            "+00008e10: 6e67 652e 7479 7065 2069 6e20 5245 4e41  nge.type in RENA",
            "+00008e20: 4d45 5f43 4841 4e47 455f 5459 5045 533a  ME_CHANGE_TYPES:",
            "+00008e30: 0a20 2020 2020 2020 2020 2020 2070 6174  .            pat",
            "+00008e40: 6831 203d 2063 6861 6e67 652e 6f6c 642e  h1 = change.old.",
            "+00008e50: 7061 7468 0a20 2020 2020 2020 2020 2020  path.           ",
            "+00008e60: 2070 6174 6832 203d 2063 6861 6e67 652e   path2 = change.",
            "+00008e70: 6e65 772e 7061 7468 0a20 2020 2020 2020  new.path.       ",
            "+00008e80: 2020 2020 2069 6620 6368 616e 6765 2e74       if change.t",
            "+00008e90: 7970 6520 3d3d 2043 4841 4e47 455f 5245  ype == CHANGE_RE",
            "+00008ea0: 4e41 4d45 3a0a 2020 2020 2020 2020 2020  NAME:.          ",
            "+00008eb0: 2020 2020 2020 6b69 6e64 203d 2022 5222        kind = \"R\"",
            "+00008ec0: 0a20 2020 2020 2020 2020 2020 2065 6c69  .            eli",
            "+00008ed0: 6620 6368 616e 6765 2e74 7970 6520 3d3d  f change.type ==",
            "+00008ee0: 2043 4841 4e47 455f 434f 5059 3a0a 2020   CHANGE_COPY:.  ",
            "+00008ef0: 2020 2020 2020 2020 2020 2020 2020 6b69                ki",
            "+00008f00: 6e64 203d 2022 4322 0a20 2020 2020 2020  nd = \"C\".       ",
            "+00008f10: 2079 6965 6c64 2022 252d 3873 252d 3230   yield \"%-8s%-20",
            "+00008f20: 7325 2d32 3073 2220 2520 286b 696e 642c  s%-20s\" % (kind,",
            "+00008f30: 2070 6174 6831 2c20 7061 7468 3229 2020   path1, path2)  ",
            "+00008f40: 2320 6e6f 7161 3a20 5550 3033 310a 0a0a  # noqa: UP031...",
            "+00008f50: 6465 6620 6c6f 6728 0a20 2020 2072 6570  def log(.    rep",
            "+00008f60: 6f3d 222e 222c 0a20 2020 2070 6174 6873  o=\".\",.    paths",
            "+00008f70: 3d4e 6f6e 652c 0a20 2020 206f 7574 7374  =None,.    outst",
            "+00008f80: 7265 616d 3d73 7973 2e73 7464 6f75 742c  ream=sys.stdout,",
            "+00008f90: 0a20 2020 206d 6178 5f65 6e74 7269 6573  .    max_entries",
            "+00008fa0: 3d4e 6f6e 652c 0a20 2020 2072 6576 6572  =None,.    rever",
            "+00008fb0: 7365 3d46 616c 7365 2c0a 2020 2020 6e61  se=False,.    na",
            "+00008fc0: 6d65 5f73 7461 7475 733d 4661 6c73 652c  me_status=False,",
            "+00008fd0: 0a29 202d 3e20 4e6f 6e65 3a0a 2020 2020  .) -> None:.    ",
            "+00008fe0: 2222 2257 7269 7465 2063 6f6d 6d69 7420  \"\"\"Write commit ",
            "+00008ff0: 6c6f 6773 2e0a 0a20 2020 2041 7267 733a  logs...    Args:",
            "+00009000: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "+00009010: 6820 746f 2072 6570 6f73 6974 6f72 790a  h to repository.",
            "+00009020: 2020 2020 2020 7061 7468 733a 204f 7074        paths: Opt",
            "+00009030: 696f 6e61 6c20 7365 7420 6f66 2073 7065  ional set of spe",
            "+00009040: 6369 6669 6320 7061 7468 7320 746f 2070  cific paths to p",
            "+00009050: 7269 6e74 2065 6e74 7269 6573 2066 6f72  rint entries for",
            "+00009060: 0a20 2020 2020 206f 7574 7374 7265 616d  .      outstream",
            "+00009070: 3a20 5374 7265 616d 2074 6f20 7772 6974  : Stream to writ",
            "+00009080: 6520 6c6f 6720 6f75 7470 7574 2074 6f0a  e log output to.",
            "+00009090: 2020 2020 2020 7265 7665 7273 653a 2052        reverse: R",
            "+000090a0: 6576 6572 7365 206f 7264 6572 2069 6e20  everse order in ",
            "+000090b0: 7768 6963 6820 656e 7472 6965 7320 6172  which entries ar",
            "+000090c0: 6520 7072 696e 7465 640a 2020 2020 2020  e printed.      ",
            "+000090d0: 6e61 6d65 5f73 7461 7475 733a 2050 7269  name_status: Pri",
            "+000090e0: 6e74 206e 616d 6520 7374 6174 7573 0a20  nt name status. ",
            "+000090f0: 2020 2020 206d 6178 5f65 6e74 7269 6573       max_entries",
            "+00009100: 3a20 4f70 7469 6f6e 616c 206d 6178 696d  : Optional maxim",
            "+00009110: 756d 206e 756d 6265 7220 6f66 2065 6e74  um number of ent",
            "+00009120: 7269 6573 2074 6f20 6469 7370 6c61 790a  ries to display.",
            "+00009130: 2020 2020 2222 220a 2020 2020 7769 7468      \"\"\".    with",
            "+00009140: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "+00009150: 6e67 2872 6570 6f29 2061 7320 723a 0a20  ng(repo) as r:. ",
            "+00009160: 2020 2020 2020 2074 7279 3a0a 2020 2020         try:.    ",
            "+00009170: 2020 2020 2020 2020 696e 636c 7564 6520          include ",
            "+00009180: 3d20 5b72 2e68 6561 6428 295d 0a20 2020  = [r.head()].   ",
            "+00009190: 2020 2020 2065 7863 6570 7420 4b65 7945       except KeyE",
            "+000091a0: 7272 6f72 3a0a 2020 2020 2020 2020 2020  rror:.          ",
            "+000091b0: 2020 696e 636c 7564 6520 3d20 5b5d 0a20    include = []. ",
            "+000091c0: 2020 2020 2020 2077 616c 6b65 7220 3d20         walker = ",
            "+000091d0: 722e 6765 745f 7761 6c6b 6572 280a 2020  r.get_walker(.  ",
            "+000091e0: 2020 2020 2020 2020 2020 696e 636c 7564            includ",
            "+000091f0: 653d 696e 636c 7564 652c 206d 6178 5f65  e=include, max_e",
            "+00009200: 6e74 7269 6573 3d6d 6178 5f65 6e74 7269  ntries=max_entri",
            "+00009210: 6573 2c20 7061 7468 733d 7061 7468 732c  es, paths=paths,",
            "+00009220: 2072 6576 6572 7365 3d72 6576 6572 7365   reverse=reverse",
            "+00009230: 0a20 2020 2020 2020 2029 0a20 2020 2020  .        ).     ",
            "+00009240: 2020 2066 6f72 2065 6e74 7279 2069 6e20     for entry in ",
            "+00009250: 7761 6c6b 6572 3a0a 0a20 2020 2020 2020  walker:..       ",
            "+00009260: 2020 2020 2064 6566 2064 6563 6f64 6528       def decode(",
            "+00009270: 7829 3a0a 2020 2020 2020 2020 2020 2020  x):.            ",
            "+00009280: 2020 2020 7265 7475 726e 2063 6f6d 6d69      return commi",
            "+00009290: 745f 6465 636f 6465 2865 6e74 7279 2e63  t_decode(entry.c",
            "+000092a0: 6f6d 6d69 742c 2078 290a 0a20 2020 2020  ommit, x)..     ",
            "+000092b0: 2020 2020 2020 2070 7269 6e74 5f63 6f6d         print_com",
            "+000092c0: 6d69 7428 656e 7472 792e 636f 6d6d 6974  mit(entry.commit",
            "+000092d0: 2c20 6465 636f 6465 2c20 6f75 7473 7472  , decode, outstr",
            "+000092e0: 6561 6d29 0a20 2020 2020 2020 2020 2020  eam).           ",
            "+000092f0: 2069 6620 6e61 6d65 5f73 7461 7475 733a   if name_status:",
            "+00009300: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00009310: 206f 7574 7374 7265 616d 2e77 7269 7465   outstream.write",
            "+00009320: 6c69 6e65 7328 0a20 2020 2020 2020 2020  lines(.         ",
            "+00009330: 2020 2020 2020 2020 2020 205b 6c69 6e65             [line",
            "+00009340: 202b 2022 5c6e 2220 666f 7220 6c69 6e65   + \"\\n\" for line",
            "+00009350: 2069 6e20 7072 696e 745f 6e61 6d65 5f73   in print_name_s",
            "+00009360: 7461 7475 7328 656e 7472 792e 6368 616e  tatus(entry.chan",
            "+00009370: 6765 7328 2929 5d0a 2020 2020 2020 2020  ges())].        ",
            "+00009380: 2020 2020 2020 2020 290a 0a0a 2320 544f          )...# TO",
            "+00009390: 444f 286a 656c 6d65 7229 3a20 6265 7474  DO(jelmer): bett",
            "+000093a0: 6572 2064 6566 6175 6c74 2066 6f72 2065  er default for e",
            "+000093b0: 6e63 6f64 696e 673f 0a64 6566 2073 686f  ncoding?.def sho",
            "+000093c0: 7728 0a20 2020 2072 6570 6f3d 222e 222c  w(.    repo=\".\",",
            "+000093d0: 0a20 2020 206f 626a 6563 7473 3d4e 6f6e  .    objects=Non",
            "+000093e0: 652c 0a20 2020 206f 7574 7374 7265 616d  e,.    outstream",
            "+000093f0: 3d73 7973 2e73 7464 6f75 742c 0a20 2020  =sys.stdout,.   ",
            "+00009400: 2064 6566 6175 6c74 5f65 6e63 6f64 696e   default_encodin",
            "+00009410: 673d 4445 4641 554c 545f 454e 434f 4449  g=DEFAULT_ENCODI",
            "+00009420: 4e47 2c0a 2920 2d3e 204e 6f6e 653a 0a20  NG,.) -> None:. ",
            "+00009430: 2020 2022 2222 5072 696e 7420 7468 6520     \"\"\"Print the ",
            "+00009440: 6368 616e 6765 7320 696e 2061 2063 6f6d  changes in a com",
            "+00009450: 6d69 742e 0a0a 2020 2020 4172 6773 3a0a  mit...    Args:.",
            "+00009460: 2020 2020 2020 7265 706f 3a20 5061 7468        repo: Path",
            "+00009470: 2074 6f20 7265 706f 7369 746f 7279 0a20   to repository. ",
            "+00009480: 2020 2020 206f 626a 6563 7473 3a20 4f62       objects: Ob",
            "+00009490: 6a65 6374 7320 746f 2073 686f 7720 2864  jects to show (d",
            "+000094a0: 6566 6175 6c74 7320 746f 205b 4845 4144  efaults to [HEAD",
            "+000094b0: 5d29 0a20 2020 2020 206f 7574 7374 7265  ]).      outstre",
            "+000094c0: 616d 3a20 5374 7265 616d 2074 6f20 7772  am: Stream to wr",
            "+000094d0: 6974 6520 746f 0a20 2020 2020 2064 6566  ite to.      def",
            "+000094e0: 6175 6c74 5f65 6e63 6f64 696e 673a 2044  ault_encoding: D",
            "+000094f0: 6566 6175 6c74 2065 6e63 6f64 696e 6720  efault encoding ",
            "+00009500: 746f 2075 7365 2069 6620 6e6f 6e65 2069  to use if none i",
            "+00009510: 7320 7365 7420 696e 2074 6865 0a20 2020  s set in the.   ",
            "+00009520: 2020 2020 2063 6f6d 6d69 740a 2020 2020       commit.    ",
            "+00009530: 2222 220a 2020 2020 6966 206f 626a 6563  \"\"\".    if objec",
            "+00009540: 7473 2069 7320 4e6f 6e65 3a0a 2020 2020  ts is None:.    ",
            "+00009550: 2020 2020 6f62 6a65 6374 7320 3d20 5b22      objects = [\"",
            "+00009560: 4845 4144 225d 0a20 2020 2069 6620 6e6f  HEAD\"].    if no",
            "+00009570: 7420 6973 696e 7374 616e 6365 286f 626a  t isinstance(obj",
            "+00009580: 6563 7473 2c20 6c69 7374 293a 0a20 2020  ects, list):.   ",
            "+00009590: 2020 2020 206f 626a 6563 7473 203d 205b       objects = [",
            "+000095a0: 6f62 6a65 6374 735d 0a20 2020 2077 6974  objects].    wit",
            "+000095b0: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "+000095c0: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "+000095d0: 2020 2020 2020 2020 666f 7220 6f62 6a65          for obje",
            "+000095e0: 6374 6973 6820 696e 206f 626a 6563 7473  ctish in objects",
            "+000095f0: 3a0a 2020 2020 2020 2020 2020 2020 6f20  :.            o ",
            "+00009600: 3d20 7061 7273 655f 6f62 6a65 6374 2872  = parse_object(r",
            "+00009610: 2c20 6f62 6a65 6374 6973 6829 0a20 2020  , objectish).   ",
            "+00009620: 2020 2020 2020 2020 2069 6620 6973 696e           if isin",
            "+00009630: 7374 616e 6365 286f 2c20 436f 6d6d 6974  stance(o, Commit",
            "+00009640: 293a 0a0a 2020 2020 2020 2020 2020 2020  ):..            ",
            "+00009650: 2020 2020 6465 6620 6465 636f 6465 2878      def decode(x",
            "+00009660: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             ",
            "+00009670: 2020 2020 2020 2072 6574 7572 6e20 636f         return co",
            "+00009680: 6d6d 6974 5f64 6563 6f64 6528 6f2c 2078  mmit_decode(o, x",
            "+00009690: 2c20 6465 6661 756c 745f 656e 636f 6469  , default_encodi",
            "+000096a0: 6e67 290a 0a20 2020 2020 2020 2020 2020  ng)..           ",
            "+000096b0: 2065 6c73 653a 0a0a 2020 2020 2020 2020   else:..        ",
            "+000096c0: 2020 2020 2020 2020 6465 6620 6465 636f          def deco",
            "+000096d0: 6465 2878 293a 0a20 2020 2020 2020 2020  de(x):.         ",
            "+000096e0: 2020 2020 2020 2020 2020 2072 6574 7572             retur",
            "+000096f0: 6e20 782e 6465 636f 6465 2864 6566 6175  n x.decode(defau",
            "+00009700: 6c74 5f65 6e63 6f64 696e 6729 0a0a 2020  lt_encoding)..  ",
            "+00009710: 2020 2020 2020 2020 2020 7368 6f77 5f6f            show_o",
            "+00009720: 626a 6563 7428 722c 206f 2c20 6465 636f  bject(r, o, deco",
            "+00009730: 6465 2c20 6f75 7473 7472 6561 6d29 0a0a  de, outstream)..",
            "+00009740: 0a64 6566 2064 6966 665f 7472 6565 2872  .def diff_tree(r",
            "+00009750: 6570 6f2c 206f 6c64 5f74 7265 652c 206e  epo, old_tree, n",
            "+00009760: 6577 5f74 7265 652c 206f 7574 7374 7265  ew_tree, outstre",
            "+00009770: 616d 3d64 6566 6175 6c74 5f62 7974 6573  am=default_bytes",
            "+00009780: 5f6f 7574 5f73 7472 6561 6d29 202d 3e20  _out_stream) -> ",
            "+00009790: 4e6f 6e65 3a0a 2020 2020 2222 2243 6f6d  None:.    \"\"\"Com",
            "+000097a0: 7061 7265 7320 7468 6520 636f 6e74 656e  pares the conten",
            "+000097b0: 7420 616e 6420 6d6f 6465 206f 6620 626c  t and mode of bl",
            "+000097c0: 6f62 7320 666f 756e 6420 7669 6120 7477  obs found via tw",
            "+000097d0: 6f20 7472 6565 206f 626a 6563 7473 2e0a  o tree objects..",
            "+000097e0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "+000097f0: 2072 6570 6f3a 2050 6174 6820 746f 2072   repo: Path to r",
            "+00009800: 6570 6f73 6974 6f72 790a 2020 2020 2020  epository.      ",
            "+00009810: 6f6c 645f 7472 6565 3a20 4964 206f 6620  old_tree: Id of ",
            "+00009820: 6f6c 6420 7472 6565 0a20 2020 2020 206e  old tree.      n",
            "+00009830: 6577 5f74 7265 653a 2049 6420 6f66 206e  ew_tree: Id of n",
            "+00009840: 6577 2074 7265 650a 2020 2020 2020 6f75  ew tree.      ou",
            "+00009850: 7473 7472 6561 6d3a 2053 7472 6561 6d20  tstream: Stream ",
            "+00009860: 746f 2077 7269 7465 2074 6f0a 2020 2020  to write to.    ",
            "+00009870: 2222 220a 2020 2020 7769 7468 206f 7065  \"\"\".    with ope",
            "+00009880: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "+00009890: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "+000098a0: 2020 2077 7269 7465 5f74 7265 655f 6469     write_tree_di",
            "+000098b0: 6666 286f 7574 7374 7265 616d 2c20 722e  ff(outstream, r.",
            "+000098c0: 6f62 6a65 6374 5f73 746f 7265 2c20 6f6c  object_store, ol",
            "+000098d0: 645f 7472 6565 2c20 6e65 775f 7472 6565  d_tree, new_tree",
            "+000098e0: 290a 0a0a 6465 6620 7265 765f 6c69 7374  )...def rev_list",
            "+000098f0: 2872 6570 6f2c 2063 6f6d 6d69 7473 2c20  (repo, commits, ",
            "+00009900: 6f75 7473 7472 6561 6d3d 7379 732e 7374  outstream=sys.st",
            "+00009910: 646f 7574 2920 2d3e 204e 6f6e 653a 0a20  dout) -> None:. ",
            "+00009920: 2020 2022 2222 4c69 7374 7320 636f 6d6d     \"\"\"Lists comm",
            "+00009930: 6974 206f 626a 6563 7473 2069 6e20 7265  it objects in re",
            "+00009940: 7665 7273 6520 6368 726f 6e6f 6c6f 6769  verse chronologi",
            "+00009950: 6361 6c20 6f72 6465 722e 0a0a 2020 2020  cal order...    ",
            "+00009960: 4172 6773 3a0a 2020 2020 2020 7265 706f  Args:.      repo",
            "+00009970: 3a20 5061 7468 2074 6f20 7265 706f 7369  : Path to reposi",
            "+00009980: 746f 7279 0a20 2020 2020 2063 6f6d 6d69  tory.      commi",
            "+00009990: 7473 3a20 436f 6d6d 6974 7320 6f76 6572  ts: Commits over",
            "+000099a0: 2077 6869 6368 2074 6f20 6974 6572 6174   which to iterat",
            "+000099b0: 650a 2020 2020 2020 6f75 7473 7472 6561  e.      outstrea",
            "+000099c0: 6d3a 2053 7472 6561 6d20 746f 2077 7269  m: Stream to wri",
            "+000099d0: 7465 2074 6f0a 2020 2020 2222 220a 2020  te to.    \"\"\".  ",
            "+000099e0: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+000099f0: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+00009a00: 7320 723a 0a20 2020 2020 2020 2066 6f72  s r:.        for",
            "+00009a10: 2065 6e74 7279 2069 6e20 722e 6765 745f   entry in r.get_",
            "+00009a20: 7761 6c6b 6572 2869 6e63 6c75 6465 3d5b  walker(include=[",
            "+00009a30: 725b 635d 2e69 6420 666f 7220 6320 696e  r[c].id for c in",
            "+00009a40: 2063 6f6d 6d69 7473 5d29 3a0a 2020 2020   commits]):.    ",
            "+00009a50: 2020 2020 2020 2020 6f75 7473 7472 6561          outstrea",
            "+00009a60: 6d2e 7772 6974 6528 656e 7472 792e 636f  m.write(entry.co",
            "+00009a70: 6d6d 6974 2e69 6420 2b20 6222 5c6e 2229  mmit.id + b\"\\n\")",
            "+00009a80: 0a0a 0a64 6566 205f 6361 6e6f 6e69 6361  ...def _canonica",
            "+00009a90: 6c5f 7061 7274 2875 726c 3a20 7374 7229  l_part(url: str)",
            "+00009aa0: 202d 3e20 7374 723a 0a20 2020 206e 616d   -> str:.    nam",
            "+00009ab0: 6520 3d20 7572 6c2e 7273 706c 6974 2822  e = url.rsplit(\"",
            "+00009ac0: 2f22 2c20 3129 5b2d 315d 0a20 2020 2069  /\", 1)[-1].    i",
            "+00009ad0: 6620 6e61 6d65 2e65 6e64 7377 6974 6828  f name.endswith(",
            "+00009ae0: 222e 6769 7422 293a 0a20 2020 2020 2020  \".git\"):.       ",
            "+00009af0: 206e 616d 6520 3d20 6e61 6d65 5b3a 2d34   name = name[:-4",
            "+00009b00: 5d0a 2020 2020 7265 7475 726e 206e 616d  ].    return nam",
            "+00009b10: 650a 0a0a 6465 6620 7375 626d 6f64 756c  e...def submodul",
            "+00009b20: 655f 6164 6428 7265 706f 2c20 7572 6c2c  e_add(repo, url,",
            "+00009b30: 2070 6174 683d 4e6f 6e65 2c20 6e61 6d65   path=None, name",
            "+00009b40: 3d4e 6f6e 6529 202d 3e20 4e6f 6e65 3a0a  =None) -> None:.",
            "+00009b50: 2020 2020 2222 2241 6464 2061 206e 6577      \"\"\"Add a new",
            "+00009b60: 2073 7562 6d6f 6475 6c65 2e0a 0a20 2020   submodule...   ",
            "+00009b70: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "+00009b80: 6f3a 2050 6174 6820 746f 2072 6570 6f73  o: Path to repos",
            "+00009b90: 6974 6f72 790a 2020 2020 2020 7572 6c3a  itory.      url:",
            "+00009ba0: 2055 524c 206f 6620 7265 706f 7369 746f   URL of reposito",
            "+00009bb0: 7279 2074 6f20 6164 6420 6173 2073 7562  ry to add as sub",
            "+00009bc0: 6d6f 6475 6c65 0a20 2020 2020 2070 6174  module.      pat",
            "+00009bd0: 683a 2050 6174 6820 7768 6572 6520 7375  h: Path where su",
            "+00009be0: 626d 6f64 756c 6520 7368 6f75 6c64 206c  bmodule should l",
            "+00009bf0: 6976 650a 2020 2020 2020 6e61 6d65 3a20  ive.      name: ",
            "+00009c00: 4e61 6d65 2066 6f72 2074 6865 2073 7562  Name for the sub",
            "+00009c10: 6d6f 6475 6c65 0a20 2020 2022 2222 0a20  module.    \"\"\". ",
            "+00009c20: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "+00009c30: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "+00009c40: 6173 2072 3a0a 2020 2020 2020 2020 6966  as r:.        if",
            "+00009c50: 2070 6174 6820 6973 204e 6f6e 653a 0a20   path is None:. ",
            "+00009c60: 2020 2020 2020 2020 2020 2070 6174 6820             path ",
            "+00009c70: 3d20 6f73 2e70 6174 682e 7265 6c70 6174  = os.path.relpat",
            "+00009c80: 6828 5f63 616e 6f6e 6963 616c 5f70 6172  h(_canonical_par",
            "+00009c90: 7428 7572 6c29 2c20 722e 7061 7468 290a  t(url), r.path).",
            "+00009ca0: 2020 2020 2020 2020 6966 206e 616d 6520          if name ",
            "+00009cb0: 6973 204e 6f6e 653a 0a20 2020 2020 2020  is None:.       ",
            "+00009cc0: 2020 2020 206e 616d 6520 3d20 7061 7468       name = path",
            "+00009cd0: 0a0a 2020 2020 2020 2020 2320 544f 444f  ..        # TODO",
            "+00009ce0: 286a 656c 6d65 7229 3a20 4d6f 7665 2074  (jelmer): Move t",
            "+00009cf0: 6869 7320 6c6f 6769 6320 746f 2064 756c  his logic to dul",
            "+00009d00: 7769 6368 2e73 7562 6d6f 6475 6c65 0a20  wich.submodule. ",
            "+00009d10: 2020 2020 2020 2067 6974 6d6f 6475 6c65         gitmodule",
            "+00009d20: 735f 7061 7468 203d 206f 732e 7061 7468  s_path = os.path",
            "+00009d30: 2e6a 6f69 6e28 722e 7061 7468 2c20 222e  .join(r.path, \".",
            "+00009d40: 6769 746d 6f64 756c 6573 2229 0a20 2020  gitmodules\").   ",
            "+00009d50: 2020 2020 2074 7279 3a0a 2020 2020 2020       try:.      ",
            "+00009d60: 2020 2020 2020 636f 6e66 6967 203d 2043        config = C",
            "+00009d70: 6f6e 6669 6746 696c 652e 6672 6f6d 5f70  onfigFile.from_p",
            "+00009d80: 6174 6828 6769 746d 6f64 756c 6573 5f70  ath(gitmodules_p",
            "+00009d90: 6174 6829 0a20 2020 2020 2020 2065 7863  ath).        exc",
            "+00009da0: 6570 7420 4669 6c65 4e6f 7446 6f75 6e64  ept FileNotFound",
            "+00009db0: 4572 726f 723a 0a20 2020 2020 2020 2020  Error:.         ",
            "+00009dc0: 2020 2063 6f6e 6669 6720 3d20 436f 6e66     config = Conf",
            "+00009dd0: 6967 4669 6c65 2829 0a20 2020 2020 2020  igFile().       ",
            "+00009de0: 2020 2020 2063 6f6e 6669 672e 7061 7468       config.path",
            "+00009df0: 203d 2067 6974 6d6f 6475 6c65 735f 7061   = gitmodules_pa",
            "+00009e00: 7468 0a20 2020 2020 2020 2063 6f6e 6669  th.        confi",
            "+00009e10: 672e 7365 7428 2822 7375 626d 6f64 756c  g.set((\"submodul",
            "+00009e20: 6522 2c20 6e61 6d65 292c 2022 7572 6c22  e\", name), \"url\"",
            "+00009e30: 2c20 7572 6c29 0a20 2020 2020 2020 2063  , url).        c",
            "+00009e40: 6f6e 6669 672e 7365 7428 2822 7375 626d  onfig.set((\"subm",
            "+00009e50: 6f64 756c 6522 2c20 6e61 6d65 292c 2022  odule\", name), \"",
            "+00009e60: 7061 7468 222c 2070 6174 6829 0a20 2020  path\", path).   ",
            "+00009e70: 2020 2020 2063 6f6e 6669 672e 7772 6974       config.writ",
            "+00009e80: 655f 746f 5f70 6174 6828 290a 0a0a 6465  e_to_path()...de",
            "+00009e90: 6620 7375 626d 6f64 756c 655f 696e 6974  f submodule_init",
            "+00009ea0: 2872 6570 6f29 202d 3e20 4e6f 6e65 3a0a  (repo) -> None:.",
            "+00009eb0: 2020 2020 2222 2249 6e69 7469 616c 697a      \"\"\"Initializ",
            "+00009ec0: 6520 7375 626d 6f64 756c 6573 2e0a 0a20  e submodules... ",
            "+00009ed0: 2020 2041 7267 733a 0a20 2020 2020 2072     Args:.      r",
            "+00009ee0: 6570 6f3a 2050 6174 6820 746f 2072 6570  epo: Path to rep",
            "+00009ef0: 6f73 6974 6f72 790a 2020 2020 2222 220a  ository.    \"\"\".",
            "+00009f00: 2020 2020 7769 7468 206f 7065 6e5f 7265      with open_re",
            "+00009f10: 706f 5f63 6c6f 7369 6e67 2872 6570 6f29  po_closing(repo)",
            "+00009f20: 2061 7320 723a 0a20 2020 2020 2020 2063   as r:.        c",
            "+00009f30: 6f6e 6669 6720 3d20 722e 6765 745f 636f  onfig = r.get_co",
            "+00009f40: 6e66 6967 2829 0a20 2020 2020 2020 2067  nfig().        g",
            "+00009f50: 6974 6d6f 6475 6c65 735f 7061 7468 203d  itmodules_path =",
            "+00009f60: 206f 732e 7061 7468 2e6a 6f69 6e28 722e   os.path.join(r.",
            "+00009f70: 7061 7468 2c20 222e 6769 746d 6f64 756c  path, \".gitmodul",
            "+00009f80: 6573 2229 0a20 2020 2020 2020 2066 6f72  es\").        for",
            "+00009f90: 2070 6174 682c 2075 726c 2c20 6e61 6d65   path, url, name",
            "+00009fa0: 2069 6e20 7265 6164 5f73 7562 6d6f 6475   in read_submodu",
            "+00009fb0: 6c65 7328 6769 746d 6f64 756c 6573 5f70  les(gitmodules_p",
            "+00009fc0: 6174 6829 3a0a 2020 2020 2020 2020 2020  ath):.          ",
            "+00009fd0: 2020 636f 6e66 6967 2e73 6574 2828 6222    config.set((b\"",
            "+00009fe0: 7375 626d 6f64 756c 6522 2c20 6e61 6d65  submodule\", name",
            "+00009ff0: 292c 2062 2261 6374 6976 6522 2c20 5472  ), b\"active\", Tr",
            "+0000a000: 7565 290a 2020 2020 2020 2020 2020 2020  ue).            ",
            "+0000a010: 636f 6e66 6967 2e73 6574 2828 6222 7375  config.set((b\"su",
            "+0000a020: 626d 6f64 756c 6522 2c20 6e61 6d65 292c  bmodule\", name),",
            "+0000a030: 2062 2275 726c 222c 2075 726c 290a 2020   b\"url\", url).  ",
            "+0000a040: 2020 2020 2020 636f 6e66 6967 2e77 7269        config.wri",
            "+0000a050: 7465 5f74 6f5f 7061 7468 2829 0a0a 0a64  te_to_path()...d",
            "+0000a060: 6566 2073 7562 6d6f 6475 6c65 5f6c 6973  ef submodule_lis",
            "+0000a070: 7428 7265 706f 293a 0a20 2020 2022 2222  t(repo):.    \"\"\"",
            "+0000a080: 4c69 7374 2073 7562 6d6f 6475 6c65 732e  List submodules.",
            " 0000a090: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            " 0000a0a0: 2020 7265 706f 3a20 5061 7468 2074 6f20    repo: Path to ",
            "-0000a0b0: 7265 706f 7369 746f 7279 206f 7220 7265  repository or re",
            "-0000a0c0: 706f 7369 746f 7279 206f 626a 6563 740a  pository object.",
            "-0000a0d0: 2020 2020 2020 6967 6e6f 7265 643a 2057        ignored: W",
            "-0000a0e0: 6865 7468 6572 2074 6f20 696e 636c 7564  hether to includ",
            "-0000a0f0: 6520 6967 6e6f 7265 6420 6669 6c65 7320  e ignored files ",
            "-0000a100: 696e 2075 6e74 7261 636b 6564 0a20 2020  in untracked.   ",
            "-0000a110: 2020 2075 6e74 7261 636b 6564 5f66 696c     untracked_fil",
            "-0000a120: 6573 3a20 486f 7720 746f 2068 616e 646c  es: How to handl",
            "-0000a130: 6520 756e 7472 6163 6b65 6420 6669 6c65  e untracked file",
            "-0000a140: 732c 2064 6566 6175 6c74 7320 746f 2022  s, defaults to \"",
            "-0000a150: 616c 6c22 3a0a 2020 2020 2020 2020 2020  all\":.          ",
            "-0000a160: 226e 6f22 3a20 646f 206e 6f74 2072 6574  \"no\": do not ret",
            "-0000a170: 7572 6e20 756e 7472 6163 6b65 6420 6669  urn untracked fi",
            "-0000a180: 6c65 730a 2020 2020 2020 2020 2020 2261  les.          \"a",
            "-0000a190: 6c6c 223a 2069 6e63 6c75 6465 2061 6c6c  ll\": include all",
            "-0000a1a0: 2066 696c 6573 2069 6e20 756e 7472 6163   files in untrac",
            "-0000a1b0: 6b65 6420 6469 7265 6374 6f72 6965 730a  ked directories.",
            "-0000a1c0: 2020 2020 2020 2020 5573 696e 6720 756e          Using un",
            "-0000a1d0: 7472 6163 6b65 645f 6669 6c65 733d 226e  tracked_files=\"n",
            "-0000a1e0: 6f22 2063 616e 2062 6520 6661 7374 6572  o\" can be faster",
            "-0000a1f0: 2074 6861 6e20 2261 6c6c 2220 7768 656e   than \"all\" when",
            "-0000a200: 2074 6865 2077 6f72 6b74 7265 6565 0a20   the worktreee. ",
            "-0000a210: 2020 2020 2020 2020 2063 6f6e 7461 696e           contain",
            "-0000a220: 7320 6d61 6e79 2075 6e74 7261 636b 6564  s many untracked",
            "-0000a230: 2066 696c 6573 2f64 6972 6563 746f 7269   files/directori",
            "-0000a240: 6573 2e0a 0a20 2020 204e 6f74 653a 2075  es...    Note: u",
            "-0000a250: 6e74 7261 636b 6564 5f66 696c 6573 3d22  ntracked_files=\"",
            "-0000a260: 6e6f 726d 616c 2220 2867 6974 2773 2064  normal\" (git's d",
            "-0000a270: 6566 6175 6c74 2920 6973 206e 6f74 2069  efault) is not i",
            "-0000a280: 6d70 6c65 6d65 6e74 6564 2e0a 0a20 2020  mplemented...   ",
            "-0000a290: 2052 6574 7572 6e73 3a20 4769 7453 7461   Returns: GitSta",
            "-0000a2a0: 7475 7320 7475 706c 652c 0a20 2020 2020  tus tuple,.     ",
            "-0000a2b0: 2020 2073 7461 6765 6420 2d20 2064 6963     staged -  dic",
            "-0000a2c0: 7420 7769 7468 206c 6973 7473 206f 6620  t with lists of ",
            "-0000a2d0: 7374 6167 6564 2070 6174 6873 2028 6469  staged paths (di",
            "-0000a2e0: 6666 2069 6e64 6578 2f48 4541 4429 0a20  ff index/HEAD). ",
            "-0000a2f0: 2020 2020 2020 2075 6e73 7461 6765 6420         unstaged ",
            "-0000a300: 2d20 206c 6973 7420 6f66 2075 6e73 7461  -  list of unsta",
            "-0000a310: 6765 6420 7061 7468 7320 2864 6966 6620  ged paths (diff ",
            "-0000a320: 696e 6465 782f 776f 726b 696e 672d 7472  index/working-tr",
            "-0000a330: 6565 290a 2020 2020 2020 2020 756e 7472  ee).        untr",
            "-0000a340: 6163 6b65 6420 2d20 6c69 7374 206f 6620  acked - list of ",
            "-0000a350: 756e 7472 6163 6b65 642c 2075 6e2d 6967  untracked, un-ig",
            "-0000a360: 6e6f 7265 6420 2620 6e6f 6e2d 2e67 6974  nored & non-.git",
            "-0000a370: 2070 6174 6873 0a20 2020 2022 2222 0a20   paths.    \"\"\". ",
            "-0000a380: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "-0000a390: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "-0000a3a0: 6173 2072 3a0a 2020 2020 2020 2020 2320  as r:.        # ",
            "-0000a3b0: 312e 2047 6574 2073 7461 7475 7320 6f66  1. Get status of",
            "-0000a3c0: 2073 7461 6765 640a 2020 2020 2020 2020   staged.        ",
            "-0000a3d0: 7472 6163 6b65 645f 6368 616e 6765 7320  tracked_changes ",
            "-0000a3e0: 3d20 6765 745f 7472 6565 5f63 6861 6e67  = get_tree_chang",
            "-0000a3f0: 6573 2872 290a 2020 2020 2020 2020 2320  es(r).        # ",
            "-0000a400: 322e 2047 6574 2073 7461 7475 7320 6f66  2. Get status of",
            "-0000a410: 2075 6e73 7461 6765 640a 2020 2020 2020   unstaged.      ",
            "-0000a420: 2020 696e 6465 7820 3d20 722e 6f70 656e    index = r.open",
            "-0000a430: 5f69 6e64 6578 2829 0a20 2020 2020 2020  _index().       ",
            "-0000a440: 206e 6f72 6d61 6c69 7a65 7220 3d20 722e   normalizer = r.",
            "-0000a450: 6765 745f 626c 6f62 5f6e 6f72 6d61 6c69  get_blob_normali",
            "-0000a460: 7a65 7228 290a 2020 2020 2020 2020 6669  zer().        fi",
            "-0000a470: 6c74 6572 5f63 616c 6c62 6163 6b20 3d20  lter_callback = ",
            "-0000a480: 6e6f 726d 616c 697a 6572 2e63 6865 636b  normalizer.check",
            "-0000a490: 696e 5f6e 6f72 6d61 6c69 7a65 0a20 2020  in_normalize.   ",
            "-0000a4a0: 2020 2020 2075 6e73 7461 6765 645f 6368       unstaged_ch",
            "-0000a4b0: 616e 6765 7320 3d20 6c69 7374 2867 6574  anges = list(get",
            "-0000a4c0: 5f75 6e73 7461 6765 645f 6368 616e 6765  _unstaged_change",
            "-0000a4d0: 7328 696e 6465 782c 2072 2e70 6174 682c  s(index, r.path,",
            "-0000a4e0: 2066 696c 7465 725f 6361 6c6c 6261 636b   filter_callback",
            "-0000a4f0: 2929 0a0a 2020 2020 2020 2020 756e 7472  ))..        untr",
            "-0000a500: 6163 6b65 645f 7061 7468 7320 3d20 6765  acked_paths = ge",
            "-0000a510: 745f 756e 7472 6163 6b65 645f 7061 7468  t_untracked_path",
            "-0000a520: 7328 0a20 2020 2020 2020 2020 2020 2072  s(.            r",
            "-0000a530: 2e70 6174 682c 0a20 2020 2020 2020 2020  .path,.         ",
            "-0000a540: 2020 2072 2e70 6174 682c 0a20 2020 2020     r.path,.     ",
            "-0000a550: 2020 2020 2020 2069 6e64 6578 2c0a 2020         index,.  ",
            "-0000a560: 2020 2020 2020 2020 2020 6578 636c 7564            exclud",
            "-0000a570: 655f 6967 6e6f 7265 643d 6e6f 7420 6967  e_ignored=not ig",
            "-0000a580: 6e6f 7265 642c 0a20 2020 2020 2020 2020  nored,.         ",
            "-0000a590: 2020 2075 6e74 7261 636b 6564 5f66 696c     untracked_fil",
            "-0000a5a0: 6573 3d75 6e74 7261 636b 6564 5f66 696c  es=untracked_fil",
            "-0000a5b0: 6573 2c0a 2020 2020 2020 2020 290a 2020  es,.        ).  ",
            "-0000a5c0: 2020 2020 2020 6966 2073 7973 2e70 6c61        if sys.pla",
            "-0000a5d0: 7466 6f72 6d20 3d3d 2022 7769 6e33 3222  tform == \"win32\"",
            "-0000a5e0: 3a0a 2020 2020 2020 2020 2020 2020 756e  :.            un",
            "-0000a5f0: 7472 6163 6b65 645f 6368 616e 6765 7320  tracked_changes ",
            "-0000a600: 3d20 5b0a 2020 2020 2020 2020 2020 2020  = [.            ",
            "-0000a610: 2020 2020 7061 7468 2e72 6570 6c61 6365      path.replace",
            "-0000a620: 286f 732e 7061 7468 2e73 6570 2c20 222f  (os.path.sep, \"/",
            "-0000a630: 2229 2066 6f72 2070 6174 6820 696e 2075  \") for path in u",
            "-0000a640: 6e74 7261 636b 6564 5f70 6174 6873 0a20  ntracked_paths. ",
            "-0000a650: 2020 2020 2020 2020 2020 205d 0a20 2020             ].   ",
            "-0000a660: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     ",
            "-0000a670: 2020 2020 2020 2075 6e74 7261 636b 6564         untracked",
            "-0000a680: 5f63 6861 6e67 6573 203d 206c 6973 7428  _changes = list(",
            "-0000a690: 756e 7472 6163 6b65 645f 7061 7468 7329  untracked_paths)",
            "-0000a6a0: 0a0a 2020 2020 2020 2020 7265 7475 726e  ..        return",
            "-0000a6b0: 2047 6974 5374 6174 7573 2874 7261 636b   GitStatus(track",
            "-0000a6c0: 6564 5f63 6861 6e67 6573 2c20 756e 7374  ed_changes, unst",
            "-0000a6d0: 6167 6564 5f63 6861 6e67 6573 2c20 756e  aged_changes, un",
            "-0000a6e0: 7472 6163 6b65 645f 6368 616e 6765 7329  tracked_changes)",
            "-0000a6f0: 0a0a 0a64 6566 205f 7761 6c6b 5f77 6f72  ...def _walk_wor",
            "-0000a700: 6b69 6e67 5f64 6972 5f70 6174 6873 2866  king_dir_paths(f",
            "-0000a710: 726f 6d70 6174 682c 2062 6173 6570 6174  rompath, basepat",
            "-0000a720: 682c 2070 7275 6e65 5f64 6972 6e61 6d65  h, prune_dirname",
            "-0000a730: 733d 4e6f 6e65 293a 0a20 2020 2022 2222  s=None):.    \"\"\"",
            "-0000a740: 4765 7420 7061 7468 2c20 6973 5f64 6972  Get path, is_dir",
            "-0000a750: 2066 6f72 2066 696c 6573 2069 6e20 776f   for files in wo",
            "-0000a760: 726b 696e 6720 6469 7220 6672 6f6d 2066  rking dir from f",
            "-0000a770: 726f 6d70 6174 682e 0a0a 2020 2020 4172  rompath...    Ar",
            "-0000a780: 6773 3a0a 2020 2020 2020 6672 6f6d 7061  gs:.      frompa",
            "-0000a790: 7468 3a20 5061 7468 2074 6f20 6265 6769  th: Path to begi",
            "-0000a7a0: 6e20 7761 6c6b 0a20 2020 2020 2062 6173  n walk.      bas",
            "-0000a7b0: 6570 6174 683a 2050 6174 6820 746f 2063  epath: Path to c",
            "-0000a7c0: 6f6d 7061 7265 2074 6f0a 2020 2020 2020  ompare to.      ",
            "-0000a7d0: 7072 756e 655f 6469 726e 616d 6573 3a20  prune_dirnames: ",
            "-0000a7e0: 4f70 7469 6f6e 616c 2063 616c 6c62 6163  Optional callbac",
            "-0000a7f0: 6b20 746f 2070 7275 6e65 2064 6972 6e61  k to prune dirna",
            "-0000a800: 6d65 7320 6475 7269 6e67 206f 732e 7761  mes during os.wa",
            "-0000a810: 6c6b 0a20 2020 2020 2020 2064 6972 6e61  lk.        dirna",
            "-0000a820: 6d65 7320 7769 6c6c 2062 6520 7365 7420  mes will be set ",
            "-0000a830: 746f 2072 6573 756c 7420 6f66 2070 7275  to result of pru",
            "-0000a840: 6e65 5f64 6972 6e61 6d65 7328 6469 7270  ne_dirnames(dirp",
            "-0000a850: 6174 682c 2064 6972 6e61 6d65 7329 0a20  ath, dirnames). ",
            "-0000a860: 2020 2022 2222 0a20 2020 2066 6f72 2064     \"\"\".    for d",
            "-0000a870: 6972 7061 7468 2c20 6469 726e 616d 6573  irpath, dirnames",
            "-0000a880: 2c20 6669 6c65 6e61 6d65 7320 696e 206f  , filenames in o",
            "-0000a890: 732e 7761 6c6b 2866 726f 6d70 6174 6829  s.walk(frompath)",
            "-0000a8a0: 3a0a 2020 2020 2020 2020 2320 536b 6970  :.        # Skip",
            "-0000a8b0: 202e 6769 7420 616e 6420 6265 6c6f 772e   .git and below.",
            "-0000a8c0: 0a20 2020 2020 2020 2069 6620 222e 6769  .        if \".gi",
            "-0000a8d0: 7422 2069 6e20 6469 726e 616d 6573 3a0a  t\" in dirnames:.",
            "-0000a8e0: 2020 2020 2020 2020 2020 2020 6469 726e              dirn",
            "-0000a8f0: 616d 6573 2e72 656d 6f76 6528 222e 6769  ames.remove(\".gi",
            "-0000a900: 7422 290a 2020 2020 2020 2020 2020 2020  t\").            ",
            "-0000a910: 6966 2064 6972 7061 7468 2021 3d20 6261  if dirpath != ba",
            "-0000a920: 7365 7061 7468 3a0a 2020 2020 2020 2020  sepath:.        ",
            "-0000a930: 2020 2020 2020 2020 636f 6e74 696e 7565          continue",
            "-0000a940: 0a0a 2020 2020 2020 2020 6966 2022 2e67  ..        if \".g",
            "-0000a950: 6974 2220 696e 2066 696c 656e 616d 6573  it\" in filenames",
            "-0000a960: 3a0a 2020 2020 2020 2020 2020 2020 6669  :.            fi",
            "-0000a970: 6c65 6e61 6d65 732e 7265 6d6f 7665 2822  lenames.remove(\"",
            "-0000a980: 2e67 6974 2229 0a20 2020 2020 2020 2020  .git\").         ",
            "-0000a990: 2020 2069 6620 6469 7270 6174 6820 213d     if dirpath !=",
            "-0000a9a0: 2062 6173 6570 6174 683a 0a20 2020 2020   basepath:.     ",
            "-0000a9b0: 2020 2020 2020 2020 2020 2063 6f6e 7469             conti",
            "-0000a9c0: 6e75 650a 0a20 2020 2020 2020 2069 6620  nue..        if ",
            "-0000a9d0: 6469 7270 6174 6820 213d 2066 726f 6d70  dirpath != fromp",
            "-0000a9e0: 6174 683a 0a20 2020 2020 2020 2020 2020  ath:.           ",
            "-0000a9f0: 2079 6965 6c64 2064 6972 7061 7468 2c20   yield dirpath, ",
            "-0000aa00: 5472 7565 0a0a 2020 2020 2020 2020 666f  True..        fo",
            "-0000aa10: 7220 6669 6c65 6e61 6d65 2069 6e20 6669  r filename in fi",
            "-0000aa20: 6c65 6e61 6d65 733a 0a20 2020 2020 2020  lenames:.       ",
            "-0000aa30: 2020 2020 2066 696c 6570 6174 6820 3d20       filepath = ",
            "-0000aa40: 6f73 2e70 6174 682e 6a6f 696e 2864 6972  os.path.join(dir",
            "-0000aa50: 7061 7468 2c20 6669 6c65 6e61 6d65 290a  path, filename).",
            "-0000aa60: 2020 2020 2020 2020 2020 2020 7969 656c              yiel",
            "-0000aa70: 6420 6669 6c65 7061 7468 2c20 4661 6c73  d filepath, Fals",
            "-0000aa80: 650a 0a20 2020 2020 2020 2069 6620 7072  e..        if pr",
            "-0000aa90: 756e 655f 6469 726e 616d 6573 3a0a 2020  une_dirnames:.  ",
            "-0000aaa0: 2020 2020 2020 2020 2020 6469 726e 616d            dirnam",
            "-0000aab0: 6573 5b3a 5d20 3d20 7072 756e 655f 6469  es[:] = prune_di",
            "-0000aac0: 726e 616d 6573 2864 6972 7061 7468 2c20  rnames(dirpath, ",
            "-0000aad0: 6469 726e 616d 6573 290a 0a0a 6465 6620  dirnames)...def ",
            "-0000aae0: 6765 745f 756e 7472 6163 6b65 645f 7061  get_untracked_pa",
            "-0000aaf0: 7468 7328 0a20 2020 2066 726f 6d70 6174  ths(.    frompat",
            "-0000ab00: 682c 2062 6173 6570 6174 682c 2069 6e64  h, basepath, ind",
            "-0000ab10: 6578 2c20 6578 636c 7564 655f 6967 6e6f  ex, exclude_igno",
            "-0000ab20: 7265 643d 4661 6c73 652c 2075 6e74 7261  red=False, untra",
            "-0000ab30: 636b 6564 5f66 696c 6573 3d22 616c 6c22  cked_files=\"all\"",
            "-0000ab40: 0a29 3a0a 2020 2020 2222 2247 6574 2075  .):.    \"\"\"Get u",
            "-0000ab50: 6e74 7261 636b 6564 2070 6174 6873 2e0a  ntracked paths..",
            "-0000ab60: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "-0000ab70: 2066 726f 6d70 6174 683a 2050 6174 6820   frompath: Path ",
            "-0000ab80: 746f 2077 616c 6b0a 2020 2020 2020 6261  to walk.      ba",
            "-0000ab90: 7365 7061 7468 3a20 5061 7468 2074 6f20  sepath: Path to ",
            "-0000aba0: 636f 6d70 6172 6520 746f 0a20 2020 2020  compare to.     ",
            "-0000abb0: 2069 6e64 6578 3a20 496e 6465 7820 746f   index: Index to",
            "-0000abc0: 2063 6865 636b 2061 6761 696e 7374 0a20   check against. ",
            "-0000abd0: 2020 2020 2065 7863 6c75 6465 5f69 676e       exclude_ign",
            "-0000abe0: 6f72 6564 3a20 5768 6574 6865 7220 746f  ored: Whether to",
            "-0000abf0: 2065 7863 6c75 6465 2069 676e 6f72 6564   exclude ignored",
            "-0000ac00: 2070 6174 6873 0a20 2020 2020 2075 6e74   paths.      unt",
            "-0000ac10: 7261 636b 6564 5f66 696c 6573 3a20 486f  racked_files: Ho",
            "-0000ac20: 7720 746f 2068 616e 646c 6520 756e 7472  w to handle untr",
            "-0000ac30: 6163 6b65 6420 6669 6c65 733a 0a20 2020  acked files:.   ",
            "-0000ac40: 2020 2020 202d 2022 6e6f 223a 2072 6574       - \"no\": ret",
            "-0000ac50: 7572 6e20 616e 2065 6d70 7479 206c 6973  urn an empty lis",
            "-0000ac60: 740a 2020 2020 2020 2020 2d20 2261 6c6c  t.        - \"all",
            "-0000ac70: 223a 2072 6574 7572 6e20 616c 6c20 6669  \": return all fi",
            "-0000ac80: 6c65 7320 696e 2075 6e74 7261 636b 6564  les in untracked",
            "-0000ac90: 2064 6972 6563 746f 7269 6573 0a20 2020   directories.   ",
            "-0000aca0: 2020 2020 202d 2022 6e6f 726d 616c 223a       - \"normal\":",
            "-0000acb0: 204e 6f74 2069 6d70 6c65 6d65 6e74 6564   Not implemented",
            "-0000acc0: 0a0a 2020 2020 4e6f 7465 3a20 6967 6e6f  ..    Note: igno",
            "-0000acd0: 7265 6420 6469 7265 6374 6f72 6965 7320  red directories ",
            "-0000ace0: 7769 6c6c 206e 6576 6572 2062 6520 7761  will never be wa",
            "-0000acf0: 6c6b 6564 2066 6f72 2070 6572 666f 726d  lked for perform",
            "-0000ad00: 616e 6365 2072 6561 736f 6e73 2e0a 2020  ance reasons..  ",
            "-0000ad10: 2020 2020 4966 2065 7863 6c75 6465 5f69      If exclude_i",
            "-0000ad20: 676e 6f72 6564 2069 7320 4661 6c73 652c  gnored is False,",
            "-0000ad30: 206f 6e6c 7920 7468 6520 7061 7468 2074   only the path t",
            "-0000ad40: 6f20 616e 2069 676e 6f72 6564 2064 6972  o an ignored dir",
            "-0000ad50: 6563 746f 7279 2077 696c 6c0a 2020 2020  ectory will.    ",
            "-0000ad60: 2020 6265 2079 6965 6c64 6564 2c20 6e6f    be yielded, no",
            "-0000ad70: 2066 696c 6573 2069 6e73 6964 6520 7468   files inside th",
            "-0000ad80: 6520 6469 7265 6374 6f72 7920 7769 6c6c  e directory will",
            "-0000ad90: 2062 6520 7265 7475 726e 6564 0a20 2020   be returned.   ",
            "-0000ada0: 2022 2222 0a20 2020 2069 6620 756e 7472   \"\"\".    if untr",
            "-0000adb0: 6163 6b65 645f 6669 6c65 7320 3d3d 2022  acked_files == \"",
            "-0000adc0: 6e6f 726d 616c 223a 0a20 2020 2020 2020  normal\":.       ",
            "-0000add0: 2072 6169 7365 204e 6f74 496d 706c 656d   raise NotImplem",
            "-0000ade0: 656e 7465 6445 7272 6f72 2822 6e6f 726d  entedError(\"norm",
            "-0000adf0: 616c 2069 7320 6e6f 7420 7965 7420 7375  al is not yet su",
            "-0000ae00: 7070 6f72 7465 6422 290a 0a20 2020 2069  pported\")..    i",
            "-0000ae10: 6620 756e 7472 6163 6b65 645f 6669 6c65  f untracked_file",
            "-0000ae20: 7320 6e6f 7420 696e 2028 226e 6f22 2c20  s not in (\"no\", ",
            "-0000ae30: 2261 6c6c 2229 3a0a 2020 2020 2020 2020  \"all\"):.        ",
            "-0000ae40: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError",
            "-0000ae50: 2822 756e 7472 6163 6b65 645f 6669 6c65  (\"untracked_file",
            "-0000ae60: 7320 6d75 7374 2062 6520 6f6e 6520 6f66  s must be one of",
            "-0000ae70: 2028 6e6f 2c20 616c 6c29 2229 0a0a 2020   (no, all)\")..  ",
            "-0000ae80: 2020 6966 2075 6e74 7261 636b 6564 5f66    if untracked_f",
            "-0000ae90: 696c 6573 203d 3d20 226e 6f22 3a0a 2020  iles == \"no\":.  ",
            "-0000aea0: 2020 2020 2020 7265 7475 726e 0a0a 2020        return..  ",
            "-0000aeb0: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "-0000aec0: 5f63 6c6f 7369 6e67 2862 6173 6570 6174  _closing(basepat",
            "-0000aed0: 6829 2061 7320 723a 0a20 2020 2020 2020  h) as r:.       ",
            "-0000aee0: 2069 676e 6f72 655f 6d61 6e61 6765 7220   ignore_manager ",
            "-0000aef0: 3d20 4967 6e6f 7265 4669 6c74 6572 4d61  = IgnoreFilterMa",
            "-0000af00: 6e61 6765 722e 6672 6f6d 5f72 6570 6f28  nager.from_repo(",
            "-0000af10: 7229 0a0a 2020 2020 6967 6e6f 7265 645f  r)..    ignored_",
            "-0000af20: 6469 7273 203d 205b 5d0a 0a20 2020 2064  dirs = []..    d",
            "-0000af30: 6566 2070 7275 6e65 5f64 6972 6e61 6d65  ef prune_dirname",
            "-0000af40: 7328 6469 7270 6174 682c 2064 6972 6e61  s(dirpath, dirna",
            "-0000af50: 6d65 7329 3a0a 2020 2020 2020 2020 666f  mes):.        fo",
            "-0000af60: 7220 6920 696e 2072 616e 6765 286c 656e  r i in range(len",
            "-0000af70: 2864 6972 6e61 6d65 7329 202d 2031 2c20  (dirnames) - 1, ",
            "-0000af80: 2d31 2c20 2d31 293a 0a20 2020 2020 2020  -1, -1):.       ",
            "-0000af90: 2020 2020 2070 6174 6820 3d20 6f73 2e70       path = os.p",
            "-0000afa0: 6174 682e 6a6f 696e 2864 6972 7061 7468  ath.join(dirpath",
            "-0000afb0: 2c20 6469 726e 616d 6573 5b69 5d29 0a20  , dirnames[i]). ",
            "-0000afc0: 2020 2020 2020 2020 2020 2069 7020 3d20             ip = ",
            "-0000afd0: 6f73 2e70 6174 682e 6a6f 696e 286f 732e  os.path.join(os.",
            "-0000afe0: 7061 7468 2e72 656c 7061 7468 2870 6174  path.relpath(pat",
            "-0000aff0: 682c 2062 6173 6570 6174 6829 2c20 2222  h, basepath), \"\"",
            "-0000b000: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if",
            "-0000b010: 2069 676e 6f72 655f 6d61 6e61 6765 722e   ignore_manager.",
            "-0000b020: 6973 5f69 676e 6f72 6564 2869 7029 3a0a  is_ignored(ip):.",
            "-0000b030: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-0000b040: 6966 206e 6f74 2065 7863 6c75 6465 5f69  if not exclude_i",
            "-0000b050: 676e 6f72 6564 3a0a 2020 2020 2020 2020  gnored:.        ",
            "-0000b060: 2020 2020 2020 2020 2020 2020 6967 6e6f              igno",
            "-0000b070: 7265 645f 6469 7273 2e61 7070 656e 6428  red_dirs.append(",
            "-0000b080: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "-0000b090: 2020 2020 2020 2020 206f 732e 7061 7468           os.path",
            "-0000b0a0: 2e6a 6f69 6e28 6f73 2e70 6174 682e 7265  .join(os.path.re",
            "-0000b0b0: 6c70 6174 6828 7061 7468 2c20 6672 6f6d  lpath(path, from",
            "-0000b0c0: 7061 7468 292c 2022 2229 0a20 2020 2020  path), \"\").     ",
            "-0000b0d0: 2020 2020 2020 2020 2020 2020 2020 2029                 )",
            "-0000b0e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "-0000b0f0: 2064 656c 2064 6972 6e61 6d65 735b 695d   del dirnames[i]",
            "-0000b100: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return ",
            "-0000b110: 6469 726e 616d 6573 0a0a 2020 2020 666f  dirnames..    fo",
            "-0000b120: 7220 6170 2c20 6973 5f64 6972 2069 6e20  r ap, is_dir in ",
            "-0000b130: 5f77 616c 6b5f 776f 726b 696e 675f 6469  _walk_working_di",
            "-0000b140: 725f 7061 7468 7328 0a20 2020 2020 2020  r_paths(.       ",
            "-0000b150: 2066 726f 6d70 6174 682c 2062 6173 6570   frompath, basep",
            "-0000b160: 6174 682c 2070 7275 6e65 5f64 6972 6e61  ath, prune_dirna",
            "-0000b170: 6d65 733d 7072 756e 655f 6469 726e 616d  mes=prune_dirnam",
            "-0000b180: 6573 0a20 2020 2029 3a0a 2020 2020 2020  es.    ):.      ",
            "-0000b190: 2020 6966 206e 6f74 2069 735f 6469 723a    if not is_dir:",
            "-0000b1a0: 0a20 2020 2020 2020 2020 2020 2069 7020  .            ip ",
            "-0000b1b0: 3d20 7061 7468 5f74 6f5f 7472 6565 5f70  = path_to_tree_p",
            "-0000b1c0: 6174 6828 6261 7365 7061 7468 2c20 6170  ath(basepath, ap",
            "-0000b1d0: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if",
            "-0000b1e0: 2069 7020 6e6f 7420 696e 2069 6e64 6578   ip not in index",
            "-0000b1f0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "-0000b200: 2020 6966 206e 6f74 2065 7863 6c75 6465    if not exclude",
            "-0000b210: 5f69 676e 6f72 6564 206f 7220 6e6f 7420  _ignored or not ",
            "-0000b220: 6967 6e6f 7265 5f6d 616e 6167 6572 2e69  ignore_manager.i",
            "-0000b230: 735f 6967 6e6f 7265 6428 0a20 2020 2020  s_ignored(.     ",
            "-0000b240: 2020 2020 2020 2020 2020 2020 2020 206f                 o",
            "-0000b250: 732e 7061 7468 2e72 656c 7061 7468 2861  s.path.relpath(a",
            "-0000b260: 702c 2062 6173 6570 6174 6829 0a20 2020  p, basepath).   ",
            "-0000b270: 2020 2020 2020 2020 2020 2020 2029 3a0a               ):.",
            "-0000b280: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-0000b290: 2020 2020 7969 656c 6420 6f73 2e70 6174      yield os.pat",
            "-0000b2a0: 682e 7265 6c70 6174 6828 6170 2c20 6672  h.relpath(ap, fr",
            "-0000b2b0: 6f6d 7061 7468 290a 0a20 2020 2079 6965  ompath)..    yie",
            "-0000b2c0: 6c64 2066 726f 6d20 6967 6e6f 7265 645f  ld from ignored_",
            "-0000b2d0: 6469 7273 0a0a 0a64 6566 2067 6574 5f74  dirs...def get_t",
            "-0000b2e0: 7265 655f 6368 616e 6765 7328 7265 706f  ree_changes(repo",
            "-0000b2f0: 293a 0a20 2020 2022 2222 5265 7475 726e  ):.    \"\"\"Return",
            "-0000b300: 2061 6464 2f64 656c 6574 652f 6d6f 6469   add/delete/modi",
            "-0000b310: 6679 2063 6861 6e67 6573 2074 6f20 7472  fy changes to tr",
            "-0000b320: 6565 2062 7920 636f 6d70 6172 696e 6720  ee by comparing ",
            "-0000b330: 696e 6465 7820 746f 2048 4541 442e 0a0a  index to HEAD...",
            "-0000b340: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "-0000b350: 7265 706f 3a20 7265 706f 2070 6174 6820  repo: repo path ",
            "-0000b360: 6f72 206f 626a 6563 740a 2020 2020 5265  or object.    Re",
            "-0000b370: 7475 726e 733a 2064 6963 7420 7769 7468  turns: dict with",
            "-0000b380: 206c 6973 7473 2066 6f72 2065 6163 6820   lists for each ",
            "-0000b390: 7479 7065 206f 6620 6368 616e 6765 0a20  type of change. ",
            "-0000b3a0: 2020 2022 2222 0a20 2020 2077 6974 6820     \"\"\".    with ",
            "-0000b3b0: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "-0000b3c0: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "-0000b3d0: 2020 2020 2020 696e 6465 7820 3d20 722e        index = r.",
            "-0000b3e0: 6f70 656e 5f69 6e64 6578 2829 0a0a 2020  open_index()..  ",
            "-0000b3f0: 2020 2020 2020 2320 436f 6d70 6172 6573        # Compares",
            "-0000b400: 2074 6865 2049 6e64 6578 2074 6f20 7468   the Index to th",
            "-0000b410: 6520 4845 4144 2026 2064 6574 6572 6d69  e HEAD & determi",
            "-0000b420: 6e65 7320 6368 616e 6765 730a 2020 2020  nes changes.    ",
            "-0000b430: 2020 2020 2320 4974 6572 6174 6520 7468      # Iterate th",
            "-0000b440: 726f 7567 6820 7468 6520 6368 616e 6765  rough the change",
            "-0000b450: 7320 616e 6420 7265 706f 7274 2061 6464  s and report add",
            "-0000b460: 2f64 656c 6574 652f 6d6f 6469 6679 0a20  /delete/modify. ",
            "-0000b470: 2020 2020 2020 2023 2054 4f44 4f3a 2063         # TODO: c",
            "-0000b480: 616c 6c20 6f75 7420 746f 2064 756c 7769  all out to dulwi",
            "-0000b490: 6368 2e64 6966 665f 7472 6565 2073 6f6d  ch.diff_tree som",
            "-0000b4a0: 6568 6f77 2e0a 2020 2020 2020 2020 7472  ehow..        tr",
            "-0000b4b0: 6163 6b65 645f 6368 616e 6765 7320 3d20  acked_changes = ",
            "-0000b4c0: 7b0a 2020 2020 2020 2020 2020 2020 2261  {.            \"a",
            "-0000b4d0: 6464 223a 205b 5d2c 0a20 2020 2020 2020  dd\": [],.       ",
            "-0000b4e0: 2020 2020 2022 6465 6c65 7465 223a 205b       \"delete\": [",
            "-0000b4f0: 5d2c 0a20 2020 2020 2020 2020 2020 2022  ],.            \"",
            "-0000b500: 6d6f 6469 6679 223a 205b 5d2c 0a20 2020  modify\": [],.   ",
            "-0000b510: 2020 2020 207d 0a20 2020 2020 2020 2074       }.        t",
            "-0000b520: 7279 3a0a 2020 2020 2020 2020 2020 2020  ry:.            ",
            "-0000b530: 7472 6565 5f69 6420 3d20 725b 6222 4845  tree_id = r[b\"HE",
            "-0000b540: 4144 225d 2e74 7265 650a 2020 2020 2020  AD\"].tree.      ",
            "-0000b550: 2020 6578 6365 7074 204b 6579 4572 726f    except KeyErro",
            "-0000b560: 723a 0a20 2020 2020 2020 2020 2020 2074  r:.            t",
            "-0000b570: 7265 655f 6964 203d 204e 6f6e 650a 0a20  ree_id = None.. ",
            "-0000b580: 2020 2020 2020 2066 6f72 2063 6861 6e67         for chang",
            "-0000b590: 6520 696e 2069 6e64 6578 2e63 6861 6e67  e in index.chang",
            "-0000b5a0: 6573 5f66 726f 6d5f 7472 6565 2872 2e6f  es_from_tree(r.o",
            "-0000b5b0: 626a 6563 745f 7374 6f72 652c 2074 7265  bject_store, tre",
            "-0000b5c0: 655f 6964 293a 0a20 2020 2020 2020 2020  e_id):.         ",
            "-0000b5d0: 2020 2069 6620 6e6f 7420 6368 616e 6765     if not change",
            "-0000b5e0: 5b30 5d5b 305d 3a0a 2020 2020 2020 2020  [0][0]:.        ",
            "-0000b5f0: 2020 2020 2020 2020 7472 6163 6b65 645f          tracked_",
            "-0000b600: 6368 616e 6765 735b 2261 6464 225d 2e61  changes[\"add\"].a",
            "-0000b610: 7070 656e 6428 6368 616e 6765 5b30 5d5b  ppend(change[0][",
            "-0000b620: 315d 290a 2020 2020 2020 2020 2020 2020  1]).            ",
            "-0000b630: 656c 6966 206e 6f74 2063 6861 6e67 655b  elif not change[",
            "-0000b640: 305d 5b31 5d3a 0a20 2020 2020 2020 2020  0][1]:.         ",
            "-0000b650: 2020 2020 2020 2074 7261 636b 6564 5f63         tracked_c",
            "-0000b660: 6861 6e67 6573 5b22 6465 6c65 7465 225d  hanges[\"delete\"]",
            "-0000b670: 2e61 7070 656e 6428 6368 616e 6765 5b30  .append(change[0",
            "-0000b680: 5d5b 305d 290a 2020 2020 2020 2020 2020  ][0]).          ",
            "-0000b690: 2020 656c 6966 2063 6861 6e67 655b 305d    elif change[0]",
            "-0000b6a0: 5b30 5d20 3d3d 2063 6861 6e67 655b 305d  [0] == change[0]",
            "-0000b6b0: 5b31 5d3a 0a20 2020 2020 2020 2020 2020  [1]:.           ",
            "-0000b6c0: 2020 2020 2074 7261 636b 6564 5f63 6861       tracked_cha",
            "-0000b6d0: 6e67 6573 5b22 6d6f 6469 6679 225d 2e61  nges[\"modify\"].a",
            "-0000b6e0: 7070 656e 6428 6368 616e 6765 5b30 5d5b  ppend(change[0][",
            "-0000b6f0: 305d 290a 2020 2020 2020 2020 2020 2020  0]).            ",
            "-0000b700: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          ",
            "-0000b710: 2020 2020 2020 7261 6973 6520 4e6f 7449        raise NotI",
            "-0000b720: 6d70 6c65 6d65 6e74 6564 4572 726f 7228  mplementedError(",
            "-0000b730: 2267 6974 206d 7620 6f70 7320 6e6f 7420  \"git mv ops not ",
            "-0000b740: 7965 7420 7375 7070 6f72 7465 6422 290a  yet supported\").",
            "-0000b750: 2020 2020 2020 2020 7265 7475 726e 2074          return t",
            "-0000b760: 7261 636b 6564 5f63 6861 6e67 6573 0a0a  racked_changes..",
            "-0000b770: 0a64 6566 2064 6165 6d6f 6e28 7061 7468  .def daemon(path",
            "-0000b780: 3d22 2e22 2c20 6164 6472 6573 733d 4e6f  =\".\", address=No",
            "-0000b790: 6e65 2c20 706f 7274 3d4e 6f6e 6529 202d  ne, port=None) -",
            "-0000b7a0: 3e20 4e6f 6e65 3a0a 2020 2020 2222 2252  > None:.    \"\"\"R",
            "-0000b7b0: 756e 2061 2064 6165 6d6f 6e20 7365 7276  un a daemon serv",
            "-0000b7c0: 696e 6720 4769 7420 7265 7175 6573 7473  ing Git requests",
            "-0000b7d0: 206f 7665 7220 5443 502f 4950 2e0a 0a20   over TCP/IP... ",
            "-0000b7e0: 2020 2041 7267 733a 0a20 2020 2020 2070     Args:.      p",
            "-0000b7f0: 6174 683a 2050 6174 6820 746f 2074 6865  ath: Path to the",
            "-0000b800: 2064 6972 6563 746f 7279 2074 6f20 7365   directory to se",
            "-0000b810: 7276 652e 0a20 2020 2020 2061 6464 7265  rve..      addre",
            "-0000b820: 7373 3a20 4f70 7469 6f6e 616c 2061 6464  ss: Optional add",
            "-0000b830: 7265 7373 2074 6f20 6c69 7374 656e 206f  ress to listen o",
            "-0000b840: 6e20 2864 6566 6175 6c74 7320 746f 203a  n (defaults to :",
            "-0000b850: 3a29 0a20 2020 2020 2070 6f72 743a 204f  :).      port: O",
            "-0000b860: 7074 696f 6e61 6c20 706f 7274 2074 6f20  ptional port to ",
            "-0000b870: 6c69 7374 656e 206f 6e20 2864 6566 6175  listen on (defau",
            "-0000b880: 6c74 7320 746f 2054 4350 5f47 4954 5f50  lts to TCP_GIT_P",
            "-0000b890: 4f52 5429 0a20 2020 2022 2222 0a20 2020  ORT).    \"\"\".   ",
            "-0000b8a0: 2023 2054 4f44 4f28 6a65 6c6d 6572 293a   # TODO(jelmer):",
            "-0000b8b0: 2053 7570 706f 7274 2067 6974 2d64 6165   Support git-dae",
            "-0000b8c0: 6d6f 6e2d 6578 706f 7274 2d6f 6b20 616e  mon-export-ok an",
            "-0000b8d0: 6420 2d2d 6578 706f 7274 2d61 6c6c 2e0a  d --export-all..",
            "-0000b8e0: 2020 2020 6261 636b 656e 6420 3d20 4669      backend = Fi",
            "-0000b8f0: 6c65 5379 7374 656d 4261 636b 656e 6428  leSystemBackend(",
            "-0000b900: 7061 7468 290a 2020 2020 7365 7276 6572  path).    server",
            "-0000b910: 203d 2054 4350 4769 7453 6572 7665 7228   = TCPGitServer(",
            "-0000b920: 6261 636b 656e 642c 2061 6464 7265 7373  backend, address",
            "-0000b930: 2c20 706f 7274 290a 2020 2020 7365 7276  , port).    serv",
            "-0000b940: 6572 2e73 6572 7665 5f66 6f72 6576 6572  er.serve_forever",
            "-0000b950: 2829 0a0a 0a64 6566 2077 6562 5f64 6165  ()...def web_dae",
            "-0000b960: 6d6f 6e28 7061 7468 3d22 2e22 2c20 6164  mon(path=\".\", ad",
            "-0000b970: 6472 6573 733d 4e6f 6e65 2c20 706f 7274  dress=None, port",
            "-0000b980: 3d4e 6f6e 6529 202d 3e20 4e6f 6e65 3a0a  =None) -> None:.",
            "-0000b990: 2020 2020 2222 2252 756e 2061 2064 6165      \"\"\"Run a dae",
            "-0000b9a0: 6d6f 6e20 7365 7276 696e 6720 4769 7420  mon serving Git ",
            "-0000b9b0: 7265 7175 6573 7473 206f 7665 7220 4854  requests over HT",
            "-0000b9c0: 5450 2e0a 0a20 2020 2041 7267 733a 0a20  TP...    Args:. ",
            "-0000b9d0: 2020 2020 2070 6174 683a 2050 6174 6820       path: Path ",
            "-0000b9e0: 746f 2074 6865 2064 6972 6563 746f 7279  to the directory",
            "-0000b9f0: 2074 6f20 7365 7276 650a 2020 2020 2020   to serve.      ",
            "-0000ba00: 6164 6472 6573 733a 204f 7074 696f 6e61  address: Optiona",
            "-0000ba10: 6c20 6164 6472 6573 7320 746f 206c 6973  l address to lis",
            "-0000ba20: 7465 6e20 6f6e 2028 6465 6661 756c 7473  ten on (defaults",
            "-0000ba30: 2074 6f20 3a3a 290a 2020 2020 2020 706f   to ::).      po",
            "-0000ba40: 7274 3a20 4f70 7469 6f6e 616c 2070 6f72  rt: Optional por",
            "-0000ba50: 7420 746f 206c 6973 7465 6e20 6f6e 2028  t to listen on (",
            "-0000ba60: 6465 6661 756c 7473 2074 6f20 3830 290a  defaults to 80).",
            "-0000ba70: 2020 2020 2222 220a 2020 2020 6672 6f6d      \"\"\".    from",
            "-0000ba80: 202e 7765 6220 696d 706f 7274 2028 0a20   .web import (. ",
            "-0000ba90: 2020 2020 2020 2057 5347 4952 6571 7565         WSGIReque",
            "-0000baa0: 7374 4861 6e64 6c65 724c 6f67 6765 722c  stHandlerLogger,",
            "-0000bab0: 0a20 2020 2020 2020 2057 5347 4953 6572  .        WSGISer",
            "-0000bac0: 7665 724c 6f67 6765 722c 0a20 2020 2020  verLogger,.     ",
            "-0000bad0: 2020 206d 616b 655f 7365 7276 6572 2c0a     make_server,.",
            "-0000bae0: 2020 2020 2020 2020 6d61 6b65 5f77 7367          make_wsg",
            "-0000baf0: 695f 6368 6169 6e2c 0a20 2020 2029 0a0a  i_chain,.    )..",
            "-0000bb00: 2020 2020 6261 636b 656e 6420 3d20 4669      backend = Fi",
            "-0000bb10: 6c65 5379 7374 656d 4261 636b 656e 6428  leSystemBackend(",
            "-0000bb20: 7061 7468 290a 2020 2020 6170 7020 3d20  path).    app = ",
            "-0000bb30: 6d61 6b65 5f77 7367 695f 6368 6169 6e28  make_wsgi_chain(",
            "-0000bb40: 6261 636b 656e 6429 0a20 2020 2073 6572  backend).    ser",
            "-0000bb50: 7665 7220 3d20 6d61 6b65 5f73 6572 7665  ver = make_serve",
            "-0000bb60: 7228 0a20 2020 2020 2020 2061 6464 7265  r(.        addre",
            "-0000bb70: 7373 2c0a 2020 2020 2020 2020 706f 7274  ss,.        port",
            "-0000bb80: 2c0a 2020 2020 2020 2020 6170 702c 0a20  ,.        app,. ",
            "-0000bb90: 2020 2020 2020 2068 616e 646c 6572 5f63         handler_c",
            "-0000bba0: 6c61 7373 3d57 5347 4952 6571 7565 7374  lass=WSGIRequest",
            "-0000bbb0: 4861 6e64 6c65 724c 6f67 6765 722c 0a20  HandlerLogger,. ",
            "-0000bbc0: 2020 2020 2020 2073 6572 7665 725f 636c         server_cl",
            "-0000bbd0: 6173 733d 5753 4749 5365 7276 6572 4c6f  ass=WSGIServerLo",
            "-0000bbe0: 6767 6572 2c0a 2020 2020 290a 2020 2020  gger,.    ).    ",
            "-0000bbf0: 7365 7276 6572 2e73 6572 7665 5f66 6f72  server.serve_for",
            "-0000bc00: 6576 6572 2829 0a0a 0a64 6566 2075 706c  ever()...def upl",
            "-0000bc10: 6f61 645f 7061 636b 2870 6174 683d 222e  oad_pack(path=\".",
            "-0000bc20: 222c 2069 6e66 3d4e 6f6e 652c 206f 7574  \", inf=None, out",
            "-0000bc30: 663d 4e6f 6e65 2920 2d3e 2069 6e74 3a0a  f=None) -> int:.",
            "-0000bc40: 2020 2020 2222 2255 706c 6f61 6420 6120      \"\"\"Upload a ",
            "-0000bc50: 7061 636b 2066 696c 6520 6166 7465 7220  pack file after ",
            "-0000bc60: 6e65 676f 7469 6174 696e 6720 6974 7320  negotiating its ",
            "-0000bc70: 636f 6e74 656e 7473 2075 7369 6e67 2073  contents using s",
            "-0000bc80: 6d61 7274 2070 726f 746f 636f 6c2e 0a0a  mart protocol...",
            "-0000bc90: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "-0000bca0: 7061 7468 3a20 5061 7468 2074 6f20 7468  path: Path to th",
            "-0000bcb0: 6520 7265 706f 7369 746f 7279 0a20 2020  e repository.   ",
            "-0000bcc0: 2020 2069 6e66 3a20 496e 7075 7420 7374     inf: Input st",
            "-0000bcd0: 7265 616d 2074 6f20 636f 6d6d 756e 6963  ream to communic",
            "-0000bce0: 6174 6520 7769 7468 2063 6c69 656e 740a  ate with client.",
            "-0000bcf0: 2020 2020 2020 6f75 7466 3a20 4f75 7470        outf: Outp",
            "-0000bd00: 7574 2073 7472 6561 6d20 746f 2063 6f6d  ut stream to com",
            "-0000bd10: 6d75 6e69 6361 7465 2077 6974 6820 636c  municate with cl",
            "-0000bd20: 6965 6e74 0a20 2020 2022 2222 0a20 2020  ient.    \"\"\".   ",
            "-0000bd30: 2069 6620 6f75 7466 2069 7320 4e6f 6e65   if outf is None",
            "-0000bd40: 3a0a 2020 2020 2020 2020 6f75 7466 203d  :.        outf =",
            "-0000bd50: 2067 6574 6174 7472 2873 7973 2e73 7464   getattr(sys.std",
            "-0000bd60: 6f75 742c 2022 6275 6666 6572 222c 2073  out, \"buffer\", s",
            "-0000bd70: 7973 2e73 7464 6f75 7429 0a20 2020 2069  ys.stdout).    i",
            "-0000bd80: 6620 696e 6620 6973 204e 6f6e 653a 0a20  f inf is None:. ",
            "-0000bd90: 2020 2020 2020 2069 6e66 203d 2067 6574         inf = get",
            "-0000bda0: 6174 7472 2873 7973 2e73 7464 696e 2c20  attr(sys.stdin, ",
            "-0000bdb0: 2262 7566 6665 7222 2c20 7379 732e 7374  \"buffer\", sys.st",
            "-0000bdc0: 6469 6e29 0a20 2020 2070 6174 6820 3d20  din).    path = ",
            "-0000bdd0: 6f73 2e70 6174 682e 6578 7061 6e64 7573  os.path.expandus",
            "-0000bde0: 6572 2870 6174 6829 0a20 2020 2062 6163  er(path).    bac",
            "-0000bdf0: 6b65 6e64 203d 2046 696c 6553 7973 7465  kend = FileSyste",
            "-0000be00: 6d42 6163 6b65 6e64 2870 6174 6829 0a0a  mBackend(path)..",
            "-0000be10: 2020 2020 6465 6620 7365 6e64 5f66 6e28      def send_fn(",
            "-0000be20: 6461 7461 2920 2d3e 204e 6f6e 653a 0a20  data) -> None:. ",
            "-0000be30: 2020 2020 2020 206f 7574 662e 7772 6974         outf.writ",
            "-0000be40: 6528 6461 7461 290a 2020 2020 2020 2020  e(data).        ",
            "-0000be50: 6f75 7466 2e66 6c75 7368 2829 0a0a 2020  outf.flush()..  ",
            "-0000be60: 2020 7072 6f74 6f20 3d20 5072 6f74 6f63    proto = Protoc",
            "-0000be70: 6f6c 2869 6e66 2e72 6561 642c 2073 656e  ol(inf.read, sen",
            "-0000be80: 645f 666e 290a 2020 2020 6861 6e64 6c65  d_fn).    handle",
            "-0000be90: 7220 3d20 5570 6c6f 6164 5061 636b 4861  r = UploadPackHa",
            "-0000bea0: 6e64 6c65 7228 6261 636b 656e 642c 205b  ndler(backend, [",
            "-0000beb0: 7061 7468 5d2c 2070 726f 746f 290a 2020  path], proto).  ",
            "-0000bec0: 2020 2320 4649 584d 453a 2043 6174 6368    # FIXME: Catch",
            "-0000bed0: 2065 7863 6570 7469 6f6e 7320 616e 6420   exceptions and ",
            "-0000bee0: 7772 6974 6520 6120 7369 6e67 6c65 2d6c  write a single-l",
            "-0000bef0: 696e 6520 7375 6d6d 6172 7920 746f 206f  ine summary to o",
            "-0000bf00: 7574 662e 0a20 2020 2068 616e 646c 6572  utf..    handler",
            "-0000bf10: 2e68 616e 646c 6528 290a 2020 2020 7265  .handle().    re",
            "-0000bf20: 7475 726e 2030 0a0a 0a64 6566 2072 6563  turn 0...def rec",
            "-0000bf30: 6569 7665 5f70 6163 6b28 7061 7468 3d22  eive_pack(path=\"",
            "-0000bf40: 2e22 2c20 696e 663d 4e6f 6e65 2c20 6f75  .\", inf=None, ou",
            "-0000bf50: 7466 3d4e 6f6e 6529 202d 3e20 696e 743a  tf=None) -> int:",
            "-0000bf60: 0a20 2020 2022 2222 5265 6365 6976 6520  .    \"\"\"Receive ",
            "-0000bf70: 6120 7061 636b 2066 696c 6520 6166 7465  a pack file afte",
            "-0000bf80: 7220 6e65 676f 7469 6174 696e 6720 6974  r negotiating it",
            "-0000bf90: 7320 636f 6e74 656e 7473 2075 7369 6e67  s contents using",
            "-0000bfa0: 2073 6d61 7274 2070 726f 746f 636f 6c2e   smart protocol.",
            "-0000bfb0: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            "-0000bfc0: 2020 7061 7468 3a20 5061 7468 2074 6f20    path: Path to ",
            "-0000bfd0: 7468 6520 7265 706f 7369 746f 7279 0a20  the repository. ",
            "-0000bfe0: 2020 2020 2069 6e66 3a20 496e 7075 7420       inf: Input ",
            "-0000bff0: 7374 7265 616d 2074 6f20 636f 6d6d 756e  stream to commun",
            "-0000c000: 6963 6174 6520 7769 7468 2063 6c69 656e  icate with clien",
            "-0000c010: 740a 2020 2020 2020 6f75 7466 3a20 4f75  t.      outf: Ou",
            "-0000c020: 7470 7574 2073 7472 6561 6d20 746f 2063  tput stream to c",
            "-0000c030: 6f6d 6d75 6e69 6361 7465 2077 6974 6820  ommunicate with ",
            "-0000c040: 636c 6965 6e74 0a20 2020 2022 2222 0a20  client.    \"\"\". ",
            "-0000c050: 2020 2069 6620 6f75 7466 2069 7320 4e6f     if outf is No",
            "-0000c060: 6e65 3a0a 2020 2020 2020 2020 6f75 7466  ne:.        outf",
            "-0000c070: 203d 2067 6574 6174 7472 2873 7973 2e73   = getattr(sys.s",
            "-0000c080: 7464 6f75 742c 2022 6275 6666 6572 222c  tdout, \"buffer\",",
            "-0000c090: 2073 7973 2e73 7464 6f75 7429 0a20 2020   sys.stdout).   ",
            "-0000c0a0: 2069 6620 696e 6620 6973 204e 6f6e 653a   if inf is None:",
            "-0000c0b0: 0a20 2020 2020 2020 2069 6e66 203d 2067  .        inf = g",
            "-0000c0c0: 6574 6174 7472 2873 7973 2e73 7464 696e  etattr(sys.stdin",
            "-0000c0d0: 2c20 2262 7566 6665 7222 2c20 7379 732e  , \"buffer\", sys.",
            "-0000c0e0: 7374 6469 6e29 0a20 2020 2070 6174 6820  stdin).    path ",
            "-0000c0f0: 3d20 6f73 2e70 6174 682e 6578 7061 6e64  = os.path.expand",
            "-0000c100: 7573 6572 2870 6174 6829 0a20 2020 2062  user(path).    b",
            "-0000c110: 6163 6b65 6e64 203d 2046 696c 6553 7973  ackend = FileSys",
            "-0000c120: 7465 6d42 6163 6b65 6e64 2870 6174 6829  temBackend(path)",
            "-0000c130: 0a0a 2020 2020 6465 6620 7365 6e64 5f66  ..    def send_f",
            "-0000c140: 6e28 6461 7461 2920 2d3e 204e 6f6e 653a  n(data) -> None:",
            "-0000c150: 0a20 2020 2020 2020 206f 7574 662e 7772  .        outf.wr",
            "-0000c160: 6974 6528 6461 7461 290a 2020 2020 2020  ite(data).      ",
            "-0000c170: 2020 6f75 7466 2e66 6c75 7368 2829 0a0a    outf.flush()..",
            "-0000c180: 2020 2020 7072 6f74 6f20 3d20 5072 6f74      proto = Prot",
            "-0000c190: 6f63 6f6c 2869 6e66 2e72 6561 642c 2073  ocol(inf.read, s",
            "-0000c1a0: 656e 645f 666e 290a 2020 2020 6861 6e64  end_fn).    hand",
            "-0000c1b0: 6c65 7220 3d20 5265 6365 6976 6550 6163  ler = ReceivePac",
            "-0000c1c0: 6b48 616e 646c 6572 2862 6163 6b65 6e64  kHandler(backend",
            "-0000c1d0: 2c20 5b70 6174 685d 2c20 7072 6f74 6f29  , [path], proto)",
            "-0000c1e0: 0a20 2020 2023 2046 4958 4d45 3a20 4361  .    # FIXME: Ca",
            "-0000c1f0: 7463 6820 6578 6365 7074 696f 6e73 2061  tch exceptions a",
            "-0000c200: 6e64 2077 7269 7465 2061 2073 696e 676c  nd write a singl",
            "-0000c210: 652d 6c69 6e65 2073 756d 6d61 7279 2074  e-line summary t",
            "-0000c220: 6f20 6f75 7466 2e0a 2020 2020 6861 6e64  o outf..    hand",
            "-0000c230: 6c65 722e 6861 6e64 6c65 2829 0a20 2020  ler.handle().   ",
            "-0000c240: 2072 6574 7572 6e20 300a 0a0a 6465 6620   return 0...def ",
            "-0000c250: 5f6d 616b 655f 6272 616e 6368 5f72 6566  _make_branch_ref",
            "-0000c260: 286e 616d 6529 3a0a 2020 2020 6966 2067  (name):.    if g",
            "-0000c270: 6574 6174 7472 286e 616d 652c 2022 656e  etattr(name, \"en",
            "-0000c280: 636f 6465 222c 204e 6f6e 6529 3a0a 2020  code\", None):.  ",
            "-0000c290: 2020 2020 2020 6e61 6d65 203d 206e 616d        name = nam",
            "-0000c2a0: 652e 656e 636f 6465 2844 4546 4155 4c54  e.encode(DEFAULT",
            "-0000c2b0: 5f45 4e43 4f44 494e 4729 0a20 2020 2072  _ENCODING).    r",
            "-0000c2c0: 6574 7572 6e20 4c4f 4341 4c5f 4252 414e  eturn LOCAL_BRAN",
            "-0000c2d0: 4348 5f50 5245 4649 5820 2b20 6e61 6d65  CH_PREFIX + name",
            "-0000c2e0: 0a0a 0a64 6566 205f 6d61 6b65 5f74 6167  ...def _make_tag",
            "-0000c2f0: 5f72 6566 286e 616d 6529 3a0a 2020 2020  _ref(name):.    ",
            "-0000c300: 6966 2067 6574 6174 7472 286e 616d 652c  if getattr(name,",
            "-0000c310: 2022 656e 636f 6465 222c 204e 6f6e 6529   \"encode\", None)",
            "-0000c320: 3a0a 2020 2020 2020 2020 6e61 6d65 203d  :.        name =",
            "-0000c330: 206e 616d 652e 656e 636f 6465 2844 4546   name.encode(DEF",
            "-0000c340: 4155 4c54 5f45 4e43 4f44 494e 4729 0a20  AULT_ENCODING). ",
            "-0000c350: 2020 2072 6574 7572 6e20 4c4f 4341 4c5f     return LOCAL_",
            "-0000c360: 5441 475f 5052 4546 4958 202b 206e 616d  TAG_PREFIX + nam",
            "-0000c370: 650a 0a0a 6465 6620 6272 616e 6368 5f64  e...def branch_d",
            "-0000c380: 656c 6574 6528 7265 706f 2c20 6e61 6d65  elete(repo, name",
            "-0000c390: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    \"",
            "-0000c3a0: 2222 4465 6c65 7465 2061 2062 7261 6e63  \"\"Delete a branc",
            "-0000c3b0: 682e 0a0a 2020 2020 4172 6773 3a0a 2020  h...    Args:.  ",
            "-0000c3c0: 2020 2020 7265 706f 3a20 5061 7468 2074      repo: Path t",
            "-0000c3d0: 6f20 7468 6520 7265 706f 7369 746f 7279  o the repository",
            "-0000c3e0: 0a20 2020 2020 206e 616d 653a 204e 616d  .      name: Nam",
            "-0000c3f0: 6520 6f66 2074 6865 2062 7261 6e63 680a  e of the branch.",
            "-0000c400: 2020 2020 2222 220a 2020 2020 7769 7468      \"\"\".    with",
            "-0000c410: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "-0000c420: 6e67 2872 6570 6f29 2061 7320 723a 0a20  ng(repo) as r:. ",
            "-0000c430: 2020 2020 2020 2069 6620 6973 696e 7374         if isinst",
            "-0000c440: 616e 6365 286e 616d 652c 206c 6973 7429  ance(name, list)",
            "-0000c450: 3a0a 2020 2020 2020 2020 2020 2020 6e61  :.            na",
            "-0000c460: 6d65 7320 3d20 6e61 6d65 0a20 2020 2020  mes = name.     ",
            "-0000c470: 2020 2065 6c73 653a 0a20 2020 2020 2020     else:.       ",
            "-0000c480: 2020 2020 206e 616d 6573 203d 205b 6e61       names = [na",
            "-0000c490: 6d65 5d0a 2020 2020 2020 2020 666f 7220  me].        for ",
            "-0000c4a0: 6e61 6d65 2069 6e20 6e61 6d65 733a 0a20  name in names:. ",
            "-0000c4b0: 2020 2020 2020 2020 2020 2064 656c 2072             del r",
            "-0000c4c0: 2e72 6566 735b 5f6d 616b 655f 6272 616e  .refs[_make_bran",
            "-0000c4d0: 6368 5f72 6566 286e 616d 6529 5d0a 0a0a  ch_ref(name)]...",
            "-0000c4e0: 6465 6620 6272 616e 6368 5f63 7265 6174  def branch_creat",
            "-0000c4f0: 6528 7265 706f 2c20 6e61 6d65 2c20 6f62  e(repo, name, ob",
            "-0000c500: 6a65 6374 6973 683d 4e6f 6e65 2c20 666f  jectish=None, fo",
            "-0000c510: 7263 653d 4661 6c73 6529 202d 3e20 4e6f  rce=False) -> No",
            "-0000c520: 6e65 3a0a 2020 2020 2222 2243 7265 6174  ne:.    \"\"\"Creat",
            "-0000c530: 6520 6120 6272 616e 6368 2e0a 0a20 2020  e a branch...   ",
            "-0000c540: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "-0000c550: 6f3a 2050 6174 6820 746f 2074 6865 2072  o: Path to the r",
            "-0000c560: 6570 6f73 6974 6f72 790a 2020 2020 2020  epository.      ",
            "-0000c570: 6e61 6d65 3a20 4e61 6d65 206f 6620 7468  name: Name of th",
            "-0000c580: 6520 6e65 7720 6272 616e 6368 0a20 2020  e new branch.   ",
            "-0000c590: 2020 206f 626a 6563 7469 7368 3a20 5461     objectish: Ta",
            "-0000c5a0: 7267 6574 206f 626a 6563 7420 746f 2070  rget object to p",
            "-0000c5b0: 6f69 6e74 206e 6577 2062 7261 6e63 6820  oint new branch ",
            "-0000c5c0: 6174 2028 6465 6661 756c 7473 2074 6f20  at (defaults to ",
            "-0000c5d0: 4845 4144 290a 2020 2020 2020 666f 7263  HEAD).      forc",
            "-0000c5e0: 653a 2046 6f72 6365 2063 7265 6174 696f  e: Force creatio",
            "-0000c5f0: 6e20 6f66 2062 7261 6e63 682c 2065 7665  n of branch, eve",
            "-0000c600: 6e20 6966 2069 7420 616c 7265 6164 7920  n if it already ",
            "-0000c610: 6578 6973 7473 0a20 2020 2022 2222 0a20  exists.    \"\"\". ",
            "-0000c620: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "-0000c630: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "-0000c640: 6173 2072 3a0a 2020 2020 2020 2020 6966  as r:.        if",
            "-0000c650: 206f 626a 6563 7469 7368 2069 7320 4e6f   objectish is No",
            "-0000c660: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            ",
            "-0000c670: 6f62 6a65 6374 6973 6820 3d20 2248 4541  objectish = \"HEA",
            "-0000c680: 4422 0a20 2020 2020 2020 206f 626a 6563  D\".        objec",
            "-0000c690: 7420 3d20 7061 7273 655f 6f62 6a65 6374  t = parse_object",
            "-0000c6a0: 2872 2c20 6f62 6a65 6374 6973 6829 0a20  (r, objectish). ",
            "-0000c6b0: 2020 2020 2020 2072 6566 6e61 6d65 203d         refname =",
            "-0000c6c0: 205f 6d61 6b65 5f62 7261 6e63 685f 7265   _make_branch_re",
            "-0000c6d0: 6628 6e61 6d65 290a 2020 2020 2020 2020  f(name).        ",
            "-0000c6e0: 7265 665f 6d65 7373 6167 6520 3d20 6222  ref_message = b\"",
            "-0000c6f0: 6272 616e 6368 3a20 4372 6561 7465 6420  branch: Created ",
            "-0000c700: 6672 6f6d 2022 202b 206f 626a 6563 7469  from \" + objecti",
            "-0000c710: 7368 2e65 6e63 6f64 6528 4445 4641 554c  sh.encode(DEFAUL",
            "-0000c720: 545f 454e 434f 4449 4e47 290a 2020 2020  T_ENCODING).    ",
            "-0000c730: 2020 2020 6966 2066 6f72 6365 3a0a 2020      if force:.  ",
            "-0000c740: 2020 2020 2020 2020 2020 722e 7265 6673            r.refs",
            "-0000c750: 2e73 6574 5f69 665f 6571 7561 6c73 2872  .set_if_equals(r",
            "-0000c760: 6566 6e61 6d65 2c20 4e6f 6e65 2c20 6f62  efname, None, ob",
            "-0000c770: 6a65 6374 2e69 642c 206d 6573 7361 6765  ject.id, message",
            "-0000c780: 3d72 6566 5f6d 6573 7361 6765 290a 2020  =ref_message).  ",
            "-0000c790: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    ",
            "-0000c7a0: 2020 2020 2020 2020 6966 206e 6f74 2072          if not r",
            "-0000c7b0: 2e72 6566 732e 6164 645f 6966 5f6e 6577  .refs.add_if_new",
            "-0000c7c0: 2872 6566 6e61 6d65 2c20 6f62 6a65 6374  (refname, object",
            "-0000c7d0: 2e69 642c 206d 6573 7361 6765 3d72 6566  .id, message=ref",
            "-0000c7e0: 5f6d 6573 7361 6765 293a 0a20 2020 2020  _message):.     ",
            "-0000c7f0: 2020 2020 2020 2020 2020 2072 6169 7365             raise",
            "-0000c800: 2045 7272 6f72 2866 2242 7261 6e63 6820   Error(f\"Branch ",
            "-0000c810: 7769 7468 206e 616d 6520 7b6e 616d 657d  with name {name}",
            "-0000c820: 2061 6c72 6561 6479 2065 7869 7374 732e   already exists.",
            "-0000c830: 2229 0a0a 0a64 6566 2062 7261 6e63 685f  \")...def branch_",
            "-0000c840: 6c69 7374 2872 6570 6f29 3a0a 2020 2020  list(repo):.    ",
            "-0000c850: 2222 224c 6973 7420 616c 6c20 6272 616e  \"\"\"List all bran",
            "-0000c860: 6368 6573 2e0a 0a20 2020 2041 7267 733a  ches...    Args:",
            "-0000c870: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "-0000c880: 6820 746f 2074 6865 2072 6570 6f73 6974  h to the reposit",
            "-0000c890: 6f72 790a 2020 2020 2222 220a 2020 2020  ory.    \"\"\".    ",
            "-0000c8a0: 7769 7468 206f 7065 6e5f 7265 706f 5f63  with open_repo_c",
            "-0000c8b0: 6c6f 7369 6e67 2872 6570 6f29 2061 7320  losing(repo) as ",
            "-0000c8c0: 723a 0a20 2020 2020 2020 2072 6574 7572  r:.        retur",
            "-0000c8d0: 6e20 722e 7265 6673 2e6b 6579 7328 6261  n r.refs.keys(ba",
            "-0000c8e0: 7365 3d4c 4f43 414c 5f42 5241 4e43 485f  se=LOCAL_BRANCH_",
            "-0000c8f0: 5052 4546 4958 290a 0a0a 6465 6620 6163  PREFIX)...def ac",
            "-0000c900: 7469 7665 5f62 7261 6e63 6828 7265 706f  tive_branch(repo",
            "-0000c910: 293a 0a20 2020 2022 2222 5265 7475 726e  ):.    \"\"\"Return",
            "-0000c920: 2074 6865 2061 6374 6976 6520 6272 616e   the active bran",
            "-0000c930: 6368 2069 6e20 7468 6520 7265 706f 7369  ch in the reposi",
            "-0000c940: 746f 7279 2c20 6966 2061 6e79 2e0a 0a20  tory, if any... ",
            "-0000c950: 2020 2041 7267 733a 0a20 2020 2020 2072     Args:.      r",
            "-0000c960: 6570 6f3a 2052 6570 6f73 6974 6f72 7920  epo: Repository ",
            "-0000c970: 746f 206f 7065 6e0a 2020 2020 5265 7475  to open.    Retu",
            "-0000c980: 726e 733a 0a20 2020 2020 2062 7261 6e63  rns:.      branc",
            "-0000c990: 6820 6e61 6d65 0a20 2020 2052 6169 7365  h name.    Raise",
            "-0000c9a0: 733a 0a20 2020 2020 204b 6579 4572 726f  s:.      KeyErro",
            "-0000c9b0: 723a 2069 6620 7468 6520 7265 706f 7369  r: if the reposi",
            "-0000c9c0: 746f 7279 2064 6f65 7320 6e6f 7420 6861  tory does not ha",
            "-0000c9d0: 7665 2061 2077 6f72 6b69 6e67 2074 7265  ve a working tre",
            "-0000c9e0: 650a 2020 2020 2020 496e 6465 7845 7272  e.      IndexErr",
            "-0000c9f0: 6f72 3a20 6966 2048 4541 4420 6973 2066  or: if HEAD is f",
            "-0000ca00: 6c6f 6174 696e 670a 2020 2020 2222 220a  loating.    \"\"\".",
            "-0000ca10: 2020 2020 7769 7468 206f 7065 6e5f 7265      with open_re",
            "-0000ca20: 706f 5f63 6c6f 7369 6e67 2872 6570 6f29  po_closing(repo)",
            "-0000ca30: 2061 7320 723a 0a20 2020 2020 2020 2061   as r:.        a",
            "-0000ca40: 6374 6976 655f 7265 6620 3d20 722e 7265  ctive_ref = r.re",
            "-0000ca50: 6673 2e66 6f6c 6c6f 7728 6222 4845 4144  fs.follow(b\"HEAD",
            "-0000ca60: 2229 5b30 5d5b 315d 0a20 2020 2020 2020  \")[0][1].       ",
            "-0000ca70: 2069 6620 6e6f 7420 6163 7469 7665 5f72   if not active_r",
            "-0000ca80: 6566 2e73 7461 7274 7377 6974 6828 4c4f  ef.startswith(LO",
            "-0000ca90: 4341 4c5f 4252 414e 4348 5f50 5245 4649  CAL_BRANCH_PREFI",
            "-0000caa0: 5829 3a0a 2020 2020 2020 2020 2020 2020  X):.            ",
            "-0000cab0: 7261 6973 6520 5661 6c75 6545 7272 6f72  raise ValueError",
            "-0000cac0: 2861 6374 6976 655f 7265 6629 0a20 2020  (active_ref).   ",
            "-0000cad0: 2020 2020 2072 6574 7572 6e20 6163 7469       return acti",
            "-0000cae0: 7665 5f72 6566 5b6c 656e 284c 4f43 414c  ve_ref[len(LOCAL",
            "-0000caf0: 5f42 5241 4e43 485f 5052 4546 4958 2920  _BRANCH_PREFIX) ",
            "-0000cb00: 3a5d 0a0a 0a64 6566 2067 6574 5f62 7261  :]...def get_bra",
            "-0000cb10: 6e63 685f 7265 6d6f 7465 2872 6570 6f29  nch_remote(repo)",
            "-0000cb20: 3a0a 2020 2020 2222 2252 6574 7572 6e20  :.    \"\"\"Return ",
            "-0000cb30: 7468 6520 6163 7469 7665 2062 7261 6e63  the active branc",
            "-0000cb40: 6827 7320 7265 6d6f 7465 206e 616d 652c  h's remote name,",
            "-0000cb50: 2069 6620 616e 792e 0a0a 2020 2020 4172   if any...    Ar",
            "-0000cb60: 6773 3a0a 2020 2020 2020 7265 706f 3a20  gs:.      repo: ",
            "-0000cb70: 5265 706f 7369 746f 7279 2074 6f20 6f70  Repository to op",
            "-0000cb80: 656e 0a20 2020 2052 6574 7572 6e73 3a0a  en.    Returns:.",
            "-0000cb90: 2020 2020 2020 7265 6d6f 7465 206e 616d        remote nam",
            "-0000cba0: 650a 2020 2020 5261 6973 6573 3a0a 2020  e.    Raises:.  ",
            "-0000cbb0: 2020 2020 4b65 7945 7272 6f72 3a20 6966      KeyError: if",
            "-0000cbc0: 2074 6865 2072 6570 6f73 6974 6f72 7920   the repository ",
            "-0000cbd0: 646f 6573 206e 6f74 2068 6176 6520 6120  does not have a ",
            "-0000cbe0: 776f 726b 696e 6720 7472 6565 0a20 2020  working tree.   ",
            "-0000cbf0: 2022 2222 0a20 2020 2077 6974 6820 6f70   \"\"\".    with op",
            "-0000cc00: 656e 5f72 6570 6f5f 636c 6f73 696e 6728  en_repo_closing(",
            "-0000cc10: 7265 706f 2920 6173 2072 3a0a 2020 2020  repo) as r:.    ",
            "-0000cc20: 2020 2020 6272 616e 6368 5f6e 616d 6520      branch_name ",
            "-0000cc30: 3d20 6163 7469 7665 5f62 7261 6e63 6828  = active_branch(",
            "-0000cc40: 722e 7061 7468 290a 2020 2020 2020 2020  r.path).        ",
            "-0000cc50: 636f 6e66 6967 203d 2072 2e67 6574 5f63  config = r.get_c",
            "-0000cc60: 6f6e 6669 6728 290a 2020 2020 2020 2020  onfig().        ",
            "-0000cc70: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           ",
            "-0000cc80: 2072 656d 6f74 655f 6e61 6d65 203d 2063   remote_name = c",
            "-0000cc90: 6f6e 6669 672e 6765 7428 2862 2262 7261  onfig.get((b\"bra",
            "-0000cca0: 6e63 6822 2c20 6272 616e 6368 5f6e 616d  nch\", branch_nam",
            "-0000ccb0: 6529 2c20 6222 7265 6d6f 7465 2229 0a20  e), b\"remote\"). ",
            "-0000ccc0: 2020 2020 2020 2065 7863 6570 7420 4b65         except Ke",
            "-0000ccd0: 7945 7272 6f72 3a0a 2020 2020 2020 2020  yError:.        ",
            "-0000cce0: 2020 2020 7265 6d6f 7465 5f6e 616d 6520      remote_name ",
            "-0000ccf0: 3d20 6222 6f72 6967 696e 220a 2020 2020  = b\"origin\".    ",
            "-0000cd00: 7265 7475 726e 2072 656d 6f74 655f 6e61  return remote_na",
            "-0000cd10: 6d65 0a0a 0a64 6566 2066 6574 6368 280a  me...def fetch(.",
            "-0000cd20: 2020 2020 7265 706f 2c0a 2020 2020 7265      repo,.    re",
            "-0000cd30: 6d6f 7465 5f6c 6f63 6174 696f 6e3d 4e6f  mote_location=No",
            "-0000cd40: 6e65 2c0a 2020 2020 6f75 7473 7472 6561  ne,.    outstrea",
            "-0000cd50: 6d3d 7379 732e 7374 646f 7574 2c0a 2020  m=sys.stdout,.  ",
            "-0000cd60: 2020 6572 7273 7472 6561 6d3d 6465 6661    errstream=defa",
            "-0000cd70: 756c 745f 6279 7465 735f 6572 725f 7374  ult_bytes_err_st",
            "-0000cd80: 7265 616d 2c0a 2020 2020 6d65 7373 6167  ream,.    messag",
            "-0000cd90: 653d 4e6f 6e65 2c0a 2020 2020 6465 7074  e=None,.    dept",
            "-0000cda0: 683d 4e6f 6e65 2c0a 2020 2020 7072 756e  h=None,.    prun",
            "-0000cdb0: 653d 4661 6c73 652c 0a20 2020 2070 7275  e=False,.    pru",
            "-0000cdc0: 6e65 5f74 6167 733d 4661 6c73 652c 0a20  ne_tags=False,. ",
            "-0000cdd0: 2020 2066 6f72 6365 3d46 616c 7365 2c0a     force=False,.",
            "-0000cde0: 2020 2020 2a2a 6b77 6172 6773 2c0a 293a      **kwargs,.):",
            "-0000cdf0: 0a20 2020 2022 2222 4665 7463 6820 6f62  .    \"\"\"Fetch ob",
            "-0000ce00: 6a65 6374 7320 6672 6f6d 2061 2072 656d  jects from a rem",
            "-0000ce10: 6f74 6520 7365 7276 6572 2e0a 0a20 2020  ote server...   ",
            "-0000ce20: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "-0000ce30: 6f3a 2050 6174 6820 746f 2074 6865 2072  o: Path to the r",
            "-0000ce40: 6570 6f73 6974 6f72 790a 2020 2020 2020  epository.      ",
            "-0000ce50: 7265 6d6f 7465 5f6c 6f63 6174 696f 6e3a  remote_location:",
            "-0000ce60: 2053 7472 696e 6720 6964 656e 7469 6679   String identify",
            "-0000ce70: 696e 6720 6120 7265 6d6f 7465 2073 6572  ing a remote ser",
            "-0000ce80: 7665 720a 2020 2020 2020 6f75 7473 7472  ver.      outstr",
            "-0000ce90: 6561 6d3a 204f 7574 7075 7420 7374 7265  eam: Output stre",
            "-0000cea0: 616d 2028 6465 6661 756c 7473 2074 6f20  am (defaults to ",
            "-0000ceb0: 7374 646f 7574 290a 2020 2020 2020 6572  stdout).      er",
            "-0000cec0: 7273 7472 6561 6d3a 2045 7272 6f72 2073  rstream: Error s",
            "-0000ced0: 7472 6561 6d20 2864 6566 6175 6c74 7320  tream (defaults ",
            "-0000cee0: 746f 2073 7464 6572 7229 0a20 2020 2020  to stderr).     ",
            "-0000cef0: 206d 6573 7361 6765 3a20 5265 666c 6f67   message: Reflog",
            "-0000cf00: 206d 6573 7361 6765 2028 6465 6661 756c   message (defaul",
            "-0000cf10: 7473 2074 6f20 6222 6665 7463 683a 2066  ts to b\"fetch: f",
            "-0000cf20: 726f 6d20 3c72 656d 6f74 655f 6e61 6d65  rom <remote_name",
            "-0000cf30: 3e22 290a 2020 2020 2020 6465 7074 683a  >\").      depth:",
            "-0000cf40: 2044 6570 7468 2074 6f20 6665 7463 6820   Depth to fetch ",
            "-0000cf50: 6174 0a20 2020 2020 2070 7275 6e65 3a20  at.      prune: ",
            "-0000cf60: 5072 756e 6520 7265 6d6f 7465 2072 656d  Prune remote rem",
            "-0000cf70: 6f76 6564 2072 6566 730a 2020 2020 2020  oved refs.      ",
            "-0000cf80: 7072 756e 655f 7461 6773 3a20 5072 756e  prune_tags: Prun",
            "-0000cf90: 6520 7265 6f6d 7465 2072 656d 6f76 6564  e reomte removed",
            "-0000cfa0: 2074 6167 730a 2020 2020 5265 7475 726e   tags.    Return",
            "-0000cfb0: 733a 0a20 2020 2020 2044 6963 7469 6f6e  s:.      Diction",
            "-0000cfc0: 6172 7920 7769 7468 2072 6566 7320 6f6e  ary with refs on",
            "-0000cfd0: 2074 6865 2072 656d 6f74 650a 2020 2020   the remote.    ",
            "-0000cfe0: 2222 220a 2020 2020 7769 7468 206f 7065  \"\"\".    with ope",
            "-0000cff0: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "-0000d000: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "-0000d010: 2020 2028 7265 6d6f 7465 5f6e 616d 652c     (remote_name,",
            "-0000d020: 2072 656d 6f74 655f 6c6f 6361 7469 6f6e   remote_location",
            "-0000d030: 2920 3d20 6765 745f 7265 6d6f 7465 5f72  ) = get_remote_r",
            "-0000d040: 6570 6f28 722c 2072 656d 6f74 655f 6c6f  epo(r, remote_lo",
            "-0000d050: 6361 7469 6f6e 290a 2020 2020 2020 2020  cation).        ",
            "-0000d060: 6966 206d 6573 7361 6765 2069 7320 4e6f  if message is No",
            "-0000d070: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            ",
            "-0000d080: 6d65 7373 6167 6520 3d20 6222 6665 7463  message = b\"fetc",
            "-0000d090: 683a 2066 726f 6d20 2220 2b20 7265 6d6f  h: from \" + remo",
            "-0000d0a0: 7465 5f6c 6f63 6174 696f 6e2e 656e 636f  te_location.enco",
            "-0000d0b0: 6465 2844 4546 4155 4c54 5f45 4e43 4f44  de(DEFAULT_ENCOD",
            "-0000d0c0: 494e 4729 0a20 2020 2020 2020 2063 6c69  ING).        cli",
            "-0000d0d0: 656e 742c 2070 6174 6820 3d20 6765 745f  ent, path = get_",
            "-0000d0e0: 7472 616e 7370 6f72 745f 616e 645f 7061  transport_and_pa",
            "-0000d0f0: 7468 280a 2020 2020 2020 2020 2020 2020  th(.            ",
            "-0000d100: 7265 6d6f 7465 5f6c 6f63 6174 696f 6e2c  remote_location,",
            "-0000d110: 2063 6f6e 6669 673d 722e 6765 745f 636f   config=r.get_co",
            "-0000d120: 6e66 6967 5f73 7461 636b 2829 2c20 2a2a  nfig_stack(), **",
            "-0000d130: 6b77 6172 6773 0a20 2020 2020 2020 2029  kwargs.        )",
            "-0000d140: 0a20 2020 2020 2020 2066 6574 6368 5f72  .        fetch_r",
            "-0000d150: 6573 756c 7420 3d20 636c 6965 6e74 2e66  esult = client.f",
            "-0000d160: 6574 6368 2870 6174 682c 2072 2c20 7072  etch(path, r, pr",
            "-0000d170: 6f67 7265 7373 3d65 7272 7374 7265 616d  ogress=errstream",
            "-0000d180: 2e77 7269 7465 2c20 6465 7074 683d 6465  .write, depth=de",
            "-0000d190: 7074 6829 0a20 2020 2020 2020 2069 6620  pth).        if ",
            "-0000d1a0: 7265 6d6f 7465 5f6e 616d 6520 6973 206e  remote_name is n",
            "-0000d1b0: 6f74 204e 6f6e 653a 0a20 2020 2020 2020  ot None:.       ",
            "-0000d1c0: 2020 2020 205f 696d 706f 7274 5f72 656d       _import_rem",
            "-0000d1d0: 6f74 655f 7265 6673 280a 2020 2020 2020  ote_refs(.      ",
            "-0000d1e0: 2020 2020 2020 2020 2020 722e 7265 6673            r.refs",
            "-0000d1f0: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              ",
            "-0000d200: 2020 7265 6d6f 7465 5f6e 616d 652c 0a20    remote_name,. ",
            "-0000d210: 2020 2020 2020 2020 2020 2020 2020 2066                 f",
            "-0000d220: 6574 6368 5f72 6573 756c 742e 7265 6673  etch_result.refs",
            "-0000d230: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              ",
            "-0000d240: 2020 6d65 7373 6167 652c 0a20 2020 2020    message,.     ",
            "-0000d250: 2020 2020 2020 2020 2020 2070 7275 6e65             prune",
            "-0000d260: 3d70 7275 6e65 2c0a 2020 2020 2020 2020  =prune,.        ",
            "-0000d270: 2020 2020 2020 2020 7072 756e 655f 7461          prune_ta",
            "-0000d280: 6773 3d70 7275 6e65 5f74 6167 732c 0a20  gs=prune_tags,. ",
            "-0000d290: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   ",
            "-0000d2a0: 2072 6574 7572 6e20 6665 7463 685f 7265   return fetch_re",
            "-0000d2b0: 7375 6c74 0a0a 0a64 6566 2066 6f72 5f65  sult...def for_e",
            "-0000d2c0: 6163 685f 7265 6628 0a20 2020 2072 6570  ach_ref(.    rep",
            "-0000d2d0: 6f3a 2055 6e69 6f6e 5b52 6570 6f2c 2073  o: Union[Repo, s",
            "-0000d2e0: 7472 5d20 3d20 222e 222c 0a20 2020 2070  tr] = \".\",.    p",
            "-0000d2f0: 6174 7465 726e 3a20 4f70 7469 6f6e 616c  attern: Optional",
            "-0000d300: 5b55 6e69 6f6e 5b73 7472 2c20 6279 7465  [Union[str, byte",
            "-0000d310: 735d 5d20 3d20 4e6f 6e65 2c0a 2920 2d3e  s]] = None,.) ->",
            "-0000d320: 206c 6973 745b 7475 706c 655b 6279 7465   list[tuple[byte",
            "-0000d330: 732c 2062 7974 6573 2c20 6279 7465 735d  s, bytes, bytes]",
            "-0000d340: 5d3a 0a20 2020 2022 2222 4974 6572 6174  ]:.    \"\"\"Iterat",
            "-0000d350: 6520 6f76 6572 2061 6c6c 2072 6566 7320  e over all refs ",
            "-0000d360: 7468 6174 206d 6174 6368 2074 6865 2028  that match the (",
            "-0000d370: 6f70 7469 6f6e 616c 2920 7061 7474 6572  optional) patter",
            "-0000d380: 6e2e 0a0a 2020 2020 4172 6773 3a0a 2020  n...    Args:.  ",
            "-0000d390: 2020 2020 7265 706f 3a20 5061 7468 2074      repo: Path t",
            "-0000d3a0: 6f20 7468 6520 7265 706f 7369 746f 7279  o the repository",
            "-0000d3b0: 0a20 2020 2020 2070 6174 7465 726e 3a20  .      pattern: ",
            "-0000d3c0: 4f70 7469 6f6e 616c 2067 6c6f 6220 2837  Optional glob (7",
            "-0000d3d0: 2920 7061 7474 6572 6e73 2074 6f20 6669  ) patterns to fi",
            "-0000d3e0: 6c74 6572 2074 6865 2072 6566 7320 7769  lter the refs wi",
            "-0000d3f0: 7468 0a20 2020 2052 6574 7572 6e73 3a0a  th.    Returns:.",
            "-0000d400: 2020 2020 2020 4c69 7374 206f 6620 6279        List of by",
            "-0000d410: 7465 7320 7475 706c 6573 2077 6974 683a  tes tuples with:",
            "-0000d420: 2028 7368 612c 206f 626a 6563 745f 7479   (sha, object_ty",
            "-0000d430: 7065 2c20 7265 665f 6e61 6d65 290a 2020  pe, ref_name).  ",
            "-0000d440: 2020 2222 220a 2020 2020 6966 2069 7369    \"\"\".    if isi",
            "-0000d450: 6e73 7461 6e63 6528 7061 7474 6572 6e2c  nstance(pattern,",
            "-0000d460: 2073 7472 293a 0a20 2020 2020 2020 2070   str):.        p",
            "-0000d470: 6174 7465 726e 203d 206f 732e 6673 656e  attern = os.fsen",
            "-0000d480: 636f 6465 2870 6174 7465 726e 290a 0a20  code(pattern).. ",
            "-0000d490: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "-0000d4a0: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "-0000d4b0: 6173 2072 3a0a 2020 2020 2020 2020 7265  as r:.        re",
            "-0000d4c0: 6673 203d 2072 2e67 6574 5f72 6566 7328  fs = r.get_refs(",
            "-0000d4d0: 290a 0a20 2020 2069 6620 7061 7474 6572  )..    if patter",
            "-0000d4e0: 6e3a 0a20 2020 2020 2020 206d 6174 6368  n:.        match",
            "-0000d4f0: 696e 675f 7265 6673 3a20 6469 6374 5b62  ing_refs: dict[b",
            "-0000d500: 7974 6573 2c20 6279 7465 735d 203d 207b  ytes, bytes] = {",
            "-0000d510: 7d0a 2020 2020 2020 2020 7061 7474 6572  }.        patter",
            "-0000d520: 6e5f 7061 7274 7320 3d20 7061 7474 6572  n_parts = patter",
            "-0000d530: 6e2e 7370 6c69 7428 6222 2f22 290a 2020  n.split(b\"/\").  ",
            "-0000d540: 2020 2020 2020 666f 7220 7265 662c 2073        for ref, s",
            "-0000d550: 6861 2069 6e20 7265 6673 2e69 7465 6d73  ha in refs.items",
            "-0000d560: 2829 3a0a 2020 2020 2020 2020 2020 2020  ():.            ",
            "-0000d570: 6d61 7463 6865 7320 3d20 4661 6c73 650a  matches = False.",
            "-0000d580: 0a20 2020 2020 2020 2020 2020 2023 2067  .            # g",
            "-0000d590: 6974 2066 6f72 2d65 6163 682d 7265 6620  it for-each-ref ",
            "-0000d5a0: 7573 6573 2067 6c6f 6220 2837 2920 7374  uses glob (7) st",
            "-0000d5b0: 796c 6520 7061 7474 6572 6e73 2c20 6275  yle patterns, bu",
            "-0000d5c0: 7420 666e 6d61 7463 680a 2020 2020 2020  t fnmatch.      ",
            "-0000d5d0: 2020 2020 2020 2320 6973 2067 7265 6564        # is greed",
            "-0000d5e0: 7920 616e 6420 616c 736f 206d 6174 6368  y and also match",
            "-0000d5f0: 6573 2073 6c61 7368 6573 2c20 756e 6c69  es slashes, unli",
            "-0000d600: 6b65 2067 6c6f 622e 676c 6f62 2e0a 2020  ke glob.glob..  ",
            "-0000d610: 2020 2020 2020 2020 2020 2320 5765 2068            # We h",
            "-0000d620: 6176 6520 746f 2063 6865 636b 2070 6172  ave to check par",
            "-0000d630: 7473 206f 6620 7468 6520 7061 7474 6572  ts of the patter",
            "-0000d640: 6e20 696e 6469 7669 6475 616c 6c79 2e0a  n individually..",
            "-0000d650: 2020 2020 2020 2020 2020 2020 2320 5365              # Se",
            "-0000d660: 6520 6874 7470 733a 2f2f 6769 7468 7562  e https://github",
            "-0000d670: 2e63 6f6d 2f70 7974 686f 6e2f 6370 7974  .com/python/cpyt",
            "-0000d680: 686f 6e2f 6973 7375 6573 2f37 3239 3034  hon/issues/72904",
            "-0000d690: 0a20 2020 2020 2020 2020 2020 2072 6566  .            ref",
            "-0000d6a0: 5f70 6172 7473 203d 2072 6566 2e73 706c  _parts = ref.spl",
            "-0000d6b0: 6974 2862 222f 2229 0a20 2020 2020 2020  it(b\"/\").       ",
            "-0000d6c0: 2020 2020 2069 6620 6c65 6e28 7265 665f       if len(ref_",
            "-0000d6d0: 7061 7274 7329 203e 206c 656e 2870 6174  parts) > len(pat",
            "-0000d6e0: 7465 726e 5f70 6172 7473 293a 0a20 2020  tern_parts):.   ",
            "-0000d6f0: 2020 2020 2020 2020 2020 2020 2063 6f6e               con",
            "-0000d700: 7469 6e75 650a 0a20 2020 2020 2020 2020  tinue..         ",
            "-0000d710: 2020 2066 6f72 2070 6174 2c20 7265 665f     for pat, ref_",
            "-0000d720: 7061 7274 2069 6e20 7a69 7028 7061 7474  part in zip(patt",
            "-0000d730: 6572 6e5f 7061 7274 732c 2072 6566 5f70  ern_parts, ref_p",
            "-0000d740: 6172 7473 293a 0a20 2020 2020 2020 2020  arts):.         ",
            "-0000d750: 2020 2020 2020 206d 6174 6368 6573 203d         matches =",
            "-0000d760: 2066 6e6d 6174 6368 2e66 6e6d 6174 6368   fnmatch.fnmatch",
            "-0000d770: 6361 7365 2872 6566 5f70 6172 742c 2070  case(ref_part, p",
            "-0000d780: 6174 290a 2020 2020 2020 2020 2020 2020  at).            ",
            "-0000d790: 2020 2020 6966 206e 6f74 206d 6174 6368      if not match",
            "-0000d7a0: 6573 3a0a 2020 2020 2020 2020 2020 2020  es:.            ",
            "-0000d7b0: 2020 2020 2020 2020 6272 6561 6b0a 0a20          break.. ",
            "-0000d7c0: 2020 2020 2020 2020 2020 2069 6620 6d61             if ma",
            "-0000d7d0: 7463 6865 733a 0a20 2020 2020 2020 2020  tches:.         ",
            "-0000d7e0: 2020 2020 2020 206d 6174 6368 696e 675f         matching_",
            "-0000d7f0: 7265 6673 5b72 6566 5d20 3d20 7368 610a  refs[ref] = sha.",
            "-0000d800: 0a20 2020 2020 2020 2072 6566 7320 3d20  .        refs = ",
            "-0000d810: 6d61 7463 6869 6e67 5f72 6566 730a 0a20  matching_refs.. ",
            "-0000d820: 2020 2072 6574 3a20 6c69 7374 5b74 7570     ret: list[tup",
            "-0000d830: 6c65 5b62 7974 6573 2c20 6279 7465 732c  le[bytes, bytes,",
            "-0000d840: 2062 7974 6573 5d5d 203d 205b 0a20 2020   bytes]] = [.   ",
            "-0000d850: 2020 2020 2028 7368 612c 2072 2e67 6574       (sha, r.get",
            "-0000d860: 5f6f 626a 6563 7428 7368 6129 2e74 7970  _object(sha).typ",
            "-0000d870: 655f 6e61 6d65 2c20 7265 6629 0a20 2020  e_name, ref).   ",
            "-0000d880: 2020 2020 2066 6f72 2072 6566 2c20 7368       for ref, sh",
            "-0000d890: 6120 696e 2073 6f72 7465 6428 0a20 2020  a in sorted(.   ",
            "-0000d8a0: 2020 2020 2020 2020 2072 6566 732e 6974           refs.it",
            "-0000d8b0: 656d 7328 292c 0a20 2020 2020 2020 2020  ems(),.         ",
            "-0000d8c0: 2020 206b 6579 3d6c 616d 6264 6120 7265     key=lambda re",
            "-0000d8d0: 665f 7368 613a 2072 6566 5f73 6861 5b30  f_sha: ref_sha[0",
            "-0000d8e0: 5d2c 0a20 2020 2020 2020 2029 0a20 2020  ],.        ).   ",
            "-0000d8f0: 2020 2020 2069 6620 7265 6620 213d 2062       if ref != b",
            "-0000d900: 2248 4541 4422 0a20 2020 205d 0a0a 2020  \"HEAD\".    ]..  ",
            "-0000d910: 2020 7265 7475 726e 2072 6574 0a0a 0a64    return ret...d",
            "-0000d920: 6566 206c 735f 7265 6d6f 7465 2872 656d  ef ls_remote(rem",
            "-0000d930: 6f74 652c 2063 6f6e 6669 673a 204f 7074  ote, config: Opt",
            "-0000d940: 696f 6e61 6c5b 436f 6e66 6967 5d20 3d20  ional[Config] = ",
            "-0000d950: 4e6f 6e65 2c20 2a2a 6b77 6172 6773 293a  None, **kwargs):",
            "-0000d960: 0a20 2020 2022 2222 4c69 7374 2074 6865  .    \"\"\"List the",
            "-0000d970: 2072 6566 7320 696e 2061 2072 656d 6f74   refs in a remot",
            "-0000d980: 652e 0a0a 2020 2020 4172 6773 3a0a 2020  e...    Args:.  ",
            "-0000d990: 2020 2020 7265 6d6f 7465 3a20 5265 6d6f      remote: Remo",
            "-0000d9a0: 7465 2072 6570 6f73 6974 6f72 7920 6c6f  te repository lo",
            "-0000d9b0: 6361 7469 6f6e 0a20 2020 2020 2063 6f6e  cation.      con",
            "-0000d9c0: 6669 673a 2043 6f6e 6669 6775 7261 7469  fig: Configurati",
            "-0000d9d0: 6f6e 2074 6f20 7573 650a 2020 2020 5265  on to use.    Re",
            "-0000d9e0: 7475 726e 733a 0a20 2020 2020 2044 6963  turns:.      Dic",
            "-0000d9f0: 7469 6f6e 6172 7920 7769 7468 2072 656d  tionary with rem",
            "-0000da00: 6f74 6520 7265 6673 0a20 2020 2022 2222  ote refs.    \"\"\"",
            "-0000da10: 0a20 2020 2069 6620 636f 6e66 6967 2069  .    if config i",
            "-0000da20: 7320 4e6f 6e65 3a0a 2020 2020 2020 2020  s None:.        ",
            "-0000da30: 636f 6e66 6967 203d 2053 7461 636b 6564  config = Stacked",
            "-0000da40: 436f 6e66 6967 2e64 6566 6175 6c74 2829  Config.default()",
            "-0000da50: 0a20 2020 2063 6c69 656e 742c 2068 6f73  .    client, hos",
            "-0000da60: 745f 7061 7468 203d 2067 6574 5f74 7261  t_path = get_tra",
            "-0000da70: 6e73 706f 7274 5f61 6e64 5f70 6174 6828  nsport_and_path(",
            "-0000da80: 7265 6d6f 7465 2c20 636f 6e66 6967 3d63  remote, config=c",
            "-0000da90: 6f6e 6669 672c 202a 2a6b 7761 7267 7329  onfig, **kwargs)",
            "-0000daa0: 0a20 2020 2072 6574 7572 6e20 636c 6965  .    return clie",
            "-0000dab0: 6e74 2e67 6574 5f72 6566 7328 686f 7374  nt.get_refs(host",
            "-0000dac0: 5f70 6174 6829 0a0a 0a64 6566 2072 6570  _path)...def rep",
            "-0000dad0: 6163 6b28 7265 706f 2920 2d3e 204e 6f6e  ack(repo) -> Non",
            "-0000dae0: 653a 0a20 2020 2022 2222 5265 7061 636b  e:.    \"\"\"Repack",
            "-0000daf0: 206c 6f6f 7365 2066 696c 6573 2069 6e20   loose files in ",
            "-0000db00: 6120 7265 706f 7369 746f 7279 2e0a 0a20  a repository... ",
            "-0000db10: 2020 2043 7572 7265 6e74 6c79 2074 6869     Currently thi",
            "-0000db20: 7320 6f6e 6c79 2070 6163 6b73 206c 6f6f  s only packs loo",
            "-0000db30: 7365 206f 626a 6563 7473 2e0a 0a20 2020  se objects...   ",
            "-0000db40: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "-0000db50: 6f3a 2050 6174 6820 746f 2074 6865 2072  o: Path to the r",
            "-0000db60: 6570 6f73 6974 6f72 790a 2020 2020 2222  epository.    \"\"",
            "-0000db70: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "-0000db80: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "-0000db90: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "-0000dba0: 2072 2e6f 626a 6563 745f 7374 6f72 652e   r.object_store.",
            "-0000dbb0: 7061 636b 5f6c 6f6f 7365 5f6f 626a 6563  pack_loose_objec",
            "-0000dbc0: 7473 2829 0a0a 0a64 6566 2070 6163 6b5f  ts()...def pack_",
            "-0000dbd0: 6f62 6a65 6374 7328 0a20 2020 2072 6570  objects(.    rep",
            "-0000dbe0: 6f2c 0a20 2020 206f 626a 6563 745f 6964  o,.    object_id",
            "-0000dbf0: 732c 0a20 2020 2070 6163 6b66 2c0a 2020  s,.    packf,.  ",
            "-0000dc00: 2020 6964 7866 2c0a 2020 2020 6465 6c74    idxf,.    delt",
            "-0000dc10: 615f 7769 6e64 6f77 5f73 697a 653d 4e6f  a_window_size=No",
            "-0000dc20: 6e65 2c0a 2020 2020 6465 6c74 6966 793d  ne,.    deltify=",
            "-0000dc30: 4e6f 6e65 2c0a 2020 2020 7265 7573 655f  None,.    reuse_",
            "-0000dc40: 6465 6c74 6173 3d54 7275 652c 0a29 202d  deltas=True,.) -",
            "-0000dc50: 3e20 4e6f 6e65 3a0a 2020 2020 2222 2250  > None:.    \"\"\"P",
            "-0000dc60: 6163 6b20 6f62 6a65 6374 7320 696e 746f  ack objects into",
            "-0000dc70: 2061 2066 696c 652e 0a0a 2020 2020 4172   a file...    Ar",
            "-0000dc80: 6773 3a0a 2020 2020 2020 7265 706f 3a20  gs:.      repo: ",
            "-0000dc90: 5061 7468 2074 6f20 7468 6520 7265 706f  Path to the repo",
            "-0000dca0: 7369 746f 7279 0a20 2020 2020 206f 626a  sitory.      obj",
            "-0000dcb0: 6563 745f 6964 733a 204c 6973 7420 6f66  ect_ids: List of",
            "-0000dcc0: 206f 626a 6563 7420 6964 7320 746f 2077   object ids to w",
            "-0000dcd0: 7269 7465 0a20 2020 2020 2070 6163 6b66  rite.      packf",
            "-0000dce0: 3a20 4669 6c65 2d6c 696b 6520 6f62 6a65  : File-like obje",
            "-0000dcf0: 6374 2074 6f20 7772 6974 6520 746f 0a20  ct to write to. ",
            "-0000dd00: 2020 2020 2069 6478 663a 2046 696c 652d       idxf: File-",
            "-0000dd10: 6c69 6b65 206f 626a 6563 7420 746f 2077  like object to w",
            "-0000dd20: 7269 7465 2074 6f20 2863 616e 2062 6520  rite to (can be ",
            "-0000dd30: 4e6f 6e65 290a 2020 2020 2020 6465 6c74  None).      delt",
            "-0000dd40: 615f 7769 6e64 6f77 5f73 697a 653a 2053  a_window_size: S",
            "-0000dd50: 6c69 6469 6e67 2077 696e 646f 7720 7369  liding window si",
            "-0000dd60: 7a65 2066 6f72 2073 6561 7263 6869 6e67  ze for searching",
            "-0000dd70: 2066 6f72 2064 656c 7461 733b 0a20 2020   for deltas;.   ",
            "-0000dd80: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-0000dd90: 2020 2020 2020 5365 7420 746f 204e 6f6e        Set to Non",
            "-0000dda0: 6520 666f 7220 6465 6661 756c 7420 7769  e for default wi",
            "-0000ddb0: 6e64 6f77 2073 697a 652e 0a20 2020 2020  ndow size..     ",
            "-0000ddc0: 2064 656c 7469 6679 3a20 5768 6574 6865   deltify: Whethe",
            "-0000ddd0: 7220 746f 2064 656c 7469 6679 206f 626a  r to deltify obj",
            "-0000dde0: 6563 7473 0a20 2020 2020 2072 6575 7365  ects.      reuse",
            "-0000ddf0: 5f64 656c 7461 733a 2041 6c6c 6f77 2072  _deltas: Allow r",
            "-0000de00: 6575 7365 206f 6620 6578 6973 7469 6e67  euse of existing",
            "-0000de10: 2064 656c 7461 7320 7768 696c 6520 6465   deltas while de",
            "-0000de20: 6c74 6966 7969 6e67 0a20 2020 2022 2222  ltifying.    \"\"\"",
            "-0000de30: 0a20 2020 2077 6974 6820 6f70 656e 5f72  .    with open_r",
            "-0000de40: 6570 6f5f 636c 6f73 696e 6728 7265 706f  epo_closing(repo",
            "-0000de50: 2920 6173 2072 3a0a 2020 2020 2020 2020  ) as r:.        ",
            "-0000de60: 656e 7472 6965 732c 2064 6174 615f 7375  entries, data_su",
            "-0000de70: 6d20 3d20 7772 6974 655f 7061 636b 5f66  m = write_pack_f",
            "-0000de80: 726f 6d5f 636f 6e74 6169 6e65 7228 0a20  rom_container(. ",
            "-0000de90: 2020 2020 2020 2020 2020 2070 6163 6b66             packf",
            "-0000dea0: 2e77 7269 7465 2c0a 2020 2020 2020 2020  .write,.        ",
            "-0000deb0: 2020 2020 722e 6f62 6a65 6374 5f73 746f      r.object_sto",
            "-0000dec0: 7265 2c0a 2020 2020 2020 2020 2020 2020  re,.            ",
            "-0000ded0: 5b28 6f69 642c 204e 6f6e 6529 2066 6f72  [(oid, None) for",
            "-0000dee0: 206f 6964 2069 6e20 6f62 6a65 6374 5f69   oid in object_i",
            "-0000def0: 6473 5d2c 0a20 2020 2020 2020 2020 2020  ds],.           ",
            "-0000df00: 2064 656c 7469 6679 3d64 656c 7469 6679   deltify=deltify",
            "-0000df10: 2c0a 2020 2020 2020 2020 2020 2020 6465  ,.            de",
            "-0000df20: 6c74 615f 7769 6e64 6f77 5f73 697a 653d  lta_window_size=",
            "-0000df30: 6465 6c74 615f 7769 6e64 6f77 5f73 697a  delta_window_siz",
            "-0000df40: 652c 0a20 2020 2020 2020 2020 2020 2072  e,.            r",
            "-0000df50: 6575 7365 5f64 656c 7461 733d 7265 7573  euse_deltas=reus",
            "-0000df60: 655f 6465 6c74 6173 2c0a 2020 2020 2020  e_deltas,.      ",
            "-0000df70: 2020 290a 2020 2020 6966 2069 6478 6620    ).    if idxf ",
            "-0000df80: 6973 206e 6f74 204e 6f6e 653a 0a20 2020  is not None:.   ",
            "-0000df90: 2020 2020 2065 6e74 7269 6573 203d 2073       entries = s",
            "-0000dfa0: 6f72 7465 6428 5b28 6b2c 2076 5b30 5d2c  orted([(k, v[0],",
            "-0000dfb0: 2076 5b31 5d29 2066 6f72 2028 6b2c 2076   v[1]) for (k, v",
            "-0000dfc0: 2920 696e 2065 6e74 7269 6573 2e69 7465  ) in entries.ite",
            "-0000dfd0: 6d73 2829 5d29 0a20 2020 2020 2020 2077  ms()]).        w",
            "-0000dfe0: 7269 7465 5f70 6163 6b5f 696e 6465 7828  rite_pack_index(",
            "-0000dff0: 6964 7866 2c20 656e 7472 6965 732c 2064  idxf, entries, d",
            "-0000e000: 6174 615f 7375 6d29 0a0a 0a64 6566 206c  ata_sum)...def l",
            "-0000e010: 735f 7472 6565 280a 2020 2020 7265 706f  s_tree(.    repo",
            "-0000e020: 2c0a 2020 2020 7472 6565 6973 683d 6222  ,.    treeish=b\"",
            "-0000e030: 4845 4144 222c 0a20 2020 206f 7574 7374  HEAD\",.    outst",
            "-0000e040: 7265 616d 3d73 7973 2e73 7464 6f75 742c  ream=sys.stdout,",
            "-0000e050: 0a20 2020 2072 6563 7572 7369 7665 3d46  .    recursive=F",
            "-0000e060: 616c 7365 2c0a 2020 2020 6e61 6d65 5f6f  alse,.    name_o",
            "-0000e070: 6e6c 793d 4661 6c73 652c 0a29 202d 3e20  nly=False,.) -> ",
            "-0000e080: 4e6f 6e65 3a0a 2020 2020 2222 224c 6973  None:.    \"\"\"Lis",
            "-0000e090: 7420 636f 6e74 656e 7473 206f 6620 6120  t contents of a ",
            "-0000e0a0: 7472 6565 2e0a 0a20 2020 2041 7267 733a  tree...    Args:",
            "-0000e0b0: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "-0000e0c0: 6820 746f 2074 6865 2072 6570 6f73 6974  h to the reposit",
            "-0000e0d0: 6f72 790a 2020 2020 2020 7472 6565 6973  ory.      treeis",
            "-0000e0e0: 683a 2054 7265 6520 6964 2074 6f20 6c69  h: Tree id to li",
            "-0000e0f0: 7374 0a20 2020 2020 206f 7574 7374 7265  st.      outstre",
            "-0000e100: 616d 3a20 4f75 7470 7574 2073 7472 6561  am: Output strea",
            "-0000e110: 6d20 2864 6566 6175 6c74 7320 746f 2073  m (defaults to s",
            "-0000e120: 7464 6f75 7429 0a20 2020 2020 2072 6563  tdout).      rec",
            "-0000e130: 7572 7369 7665 3a20 5768 6574 6865 7220  ursive: Whether ",
            "-0000e140: 746f 2072 6563 7572 7369 7665 6c79 206c  to recursively l",
            "-0000e150: 6973 7420 6669 6c65 730a 2020 2020 2020  ist files.      ",
            "-0000e160: 6e61 6d65 5f6f 6e6c 793a 204f 6e6c 7920  name_only: Only ",
            "-0000e170: 7072 696e 7420 6974 656d 206e 616d 650a  print item name.",
            "-0000e180: 2020 2020 2222 220a 0a20 2020 2064 6566      \"\"\"..    def",
            "-0000e190: 206c 6973 745f 7472 6565 2873 746f 7265   list_tree(store",
            "-0000e1a0: 2c20 7472 6565 6964 2c20 6261 7365 2920  , treeid, base) ",
            "-0000e1b0: 2d3e 204e 6f6e 653a 0a20 2020 2020 2020  -> None:.       ",
            "-0000e1c0: 2066 6f72 206e 616d 652c 206d 6f64 652c   for name, mode,",
            "-0000e1d0: 2073 6861 2069 6e20 7374 6f72 655b 7472   sha in store[tr",
            "-0000e1e0: 6565 6964 5d2e 6974 6572 6974 656d 7328  eeid].iteritems(",
            "-0000e1f0: 293a 0a20 2020 2020 2020 2020 2020 2069  ):.            i",
            "-0000e200: 6620 6261 7365 3a0a 2020 2020 2020 2020  f base:.        ",
            "-0000e210: 2020 2020 2020 2020 6e61 6d65 203d 2070          name = p",
            "-0000e220: 6f73 6978 7061 7468 2e6a 6f69 6e28 6261  osixpath.join(ba",
            "-0000e230: 7365 2c20 6e61 6d65 290a 2020 2020 2020  se, name).      ",
            "-0000e240: 2020 2020 2020 6966 206e 616d 655f 6f6e        if name_on",
            "-0000e250: 6c79 3a0a 2020 2020 2020 2020 2020 2020  ly:.            ",
            "-0000e260: 2020 2020 6f75 7473 7472 6561 6d2e 7772      outstream.wr",
            "-0000e270: 6974 6528 6e61 6d65 202b 2062 225c 6e22  ite(name + b\"\\n\"",
            "-0000e280: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el",
            "-0000e290: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            ",
            "-0000e2a0: 2020 2020 6f75 7473 7472 6561 6d2e 7772      outstream.wr",
            "-0000e2b0: 6974 6528 7072 6574 7479 5f66 6f72 6d61  ite(pretty_forma",
            "-0000e2c0: 745f 7472 6565 5f65 6e74 7279 286e 616d  t_tree_entry(nam",
            "-0000e2d0: 652c 206d 6f64 652c 2073 6861 2929 0a20  e, mode, sha)). ",
            "-0000e2e0: 2020 2020 2020 2020 2020 2069 6620 7374             if st",
            "-0000e2f0: 6174 2e53 5f49 5344 4952 286d 6f64 6529  at.S_ISDIR(mode)",
            "-0000e300: 2061 6e64 2072 6563 7572 7369 7665 3a0a   and recursive:.",
            "-0000e310: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-0000e320: 6c69 7374 5f74 7265 6528 7374 6f72 652c  list_tree(store,",
            "-0000e330: 2073 6861 2c20 6e61 6d65 290a 0a20 2020   sha, name)..   ",
            "-0000e340: 2077 6974 6820 6f70 656e 5f72 6570 6f5f   with open_repo_",
            "-0000e350: 636c 6f73 696e 6728 7265 706f 2920 6173  closing(repo) as",
            "-0000e360: 2072 3a0a 2020 2020 2020 2020 7472 6565   r:.        tree",
            "-0000e370: 203d 2070 6172 7365 5f74 7265 6528 722c   = parse_tree(r,",
            "-0000e380: 2074 7265 6569 7368 290a 2020 2020 2020   treeish).      ",
            "-0000e390: 2020 6c69 7374 5f74 7265 6528 722e 6f62    list_tree(r.ob",
            "-0000e3a0: 6a65 6374 5f73 746f 7265 2c20 7472 6565  ject_store, tree",
            "-0000e3b0: 2e69 642c 2022 2229 0a0a 0a64 6566 2072  .id, \"\")...def r",
            "-0000e3c0: 656d 6f74 655f 6164 6428 7265 706f 2c20  emote_add(repo, ",
            "-0000e3d0: 6e61 6d65 3a20 556e 696f 6e5b 6279 7465  name: Union[byte",
            "-0000e3e0: 732c 2073 7472 5d2c 2075 726c 3a20 556e  s, str], url: Un",
            "-0000e3f0: 696f 6e5b 6279 7465 732c 2073 7472 5d29  ion[bytes, str])",
            "-0000e400: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2222   -> None:.    \"\"",
            "-0000e410: 2241 6464 2061 2072 656d 6f74 652e 0a0a  \"Add a remote...",
            "-0000e420: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "-0000e430: 7265 706f 3a20 5061 7468 2074 6f20 7468  repo: Path to th",
            "-0000e440: 6520 7265 706f 7369 746f 7279 0a20 2020  e repository.   ",
            "-0000e450: 2020 206e 616d 653a 2052 656d 6f74 6520     name: Remote ",
            "-0000e460: 6e61 6d65 0a20 2020 2020 2075 726c 3a20  name.      url: ",
            "-0000e470: 5265 6d6f 7465 2055 524c 0a20 2020 2022  Remote URL.    \"",
            "-0000e480: 2222 0a20 2020 2069 6620 6e6f 7420 6973  \"\".    if not is",
            "-0000e490: 696e 7374 616e 6365 286e 616d 652c 2062  instance(name, b",
            "-0000e4a0: 7974 6573 293a 0a20 2020 2020 2020 206e  ytes):.        n",
            "-0000e4b0: 616d 6520 3d20 6e61 6d65 2e65 6e63 6f64  ame = name.encod",
            "-0000e4c0: 6528 4445 4641 554c 545f 454e 434f 4449  e(DEFAULT_ENCODI",
            "-0000e4d0: 4e47 290a 2020 2020 6966 206e 6f74 2069  NG).    if not i",
            "-0000e4e0: 7369 6e73 7461 6e63 6528 7572 6c2c 2062  sinstance(url, b",
            "-0000e4f0: 7974 6573 293a 0a20 2020 2020 2020 2075  ytes):.        u",
            "-0000e500: 726c 203d 2075 726c 2e65 6e63 6f64 6528  rl = url.encode(",
            "-0000e510: 4445 4641 554c 545f 454e 434f 4449 4e47  DEFAULT_ENCODING",
            "-0000e520: 290a 2020 2020 7769 7468 206f 7065 6e5f  ).    with open_",
            "-0000e530: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "-0000e540: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "-0000e550: 2063 203d 2072 2e67 6574 5f63 6f6e 6669   c = r.get_confi",
            "-0000e560: 6728 290a 2020 2020 2020 2020 7365 6374  g().        sect",
            "-0000e570: 696f 6e20 3d20 2862 2272 656d 6f74 6522  ion = (b\"remote\"",
            "-0000e580: 2c20 6e61 6d65 290a 2020 2020 2020 2020  , name).        ",
            "-0000e590: 6966 2063 2e68 6173 5f73 6563 7469 6f6e  if c.has_section",
            "-0000e5a0: 2873 6563 7469 6f6e 293a 0a20 2020 2020  (section):.     ",
            "-0000e5b0: 2020 2020 2020 2072 6169 7365 2052 656d         raise Rem",
            "-0000e5c0: 6f74 6545 7869 7374 7328 7365 6374 696f  oteExists(sectio",
            "-0000e5d0: 6e29 0a20 2020 2020 2020 2063 2e73 6574  n).        c.set",
            "-0000e5e0: 2873 6563 7469 6f6e 2c20 6222 7572 6c22  (section, b\"url\"",
            "-0000e5f0: 2c20 7572 6c29 0a20 2020 2020 2020 2063  , url).        c",
            "-0000e600: 2e77 7269 7465 5f74 6f5f 7061 7468 2829  .write_to_path()",
            "-0000e610: 0a0a 0a64 6566 2072 656d 6f74 655f 7265  ...def remote_re",
            "-0000e620: 6d6f 7665 2872 6570 6f3a 2052 6570 6f2c  move(repo: Repo,",
            "-0000e630: 206e 616d 653a 2055 6e69 6f6e 5b62 7974   name: Union[byt",
            "-0000e640: 6573 2c20 7374 725d 2920 2d3e 204e 6f6e  es, str]) -> Non",
            "-0000e650: 653a 0a20 2020 2022 2222 5265 6d6f 7665  e:.    \"\"\"Remove",
            "-0000e660: 2061 2072 656d 6f74 652e 0a0a 2020 2020   a remote...    ",
            "-0000e670: 4172 6773 3a0a 2020 2020 2020 7265 706f  Args:.      repo",
            "-0000e680: 3a20 5061 7468 2074 6f20 7468 6520 7265  : Path to the re",
            "-0000e690: 706f 7369 746f 7279 0a20 2020 2020 206e  pository.      n",
            "-0000e6a0: 616d 653a 2052 656d 6f74 6520 6e61 6d65  ame: Remote name",
            "-0000e6b0: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    \"\"\".    if ",
            "-0000e6c0: 6e6f 7420 6973 696e 7374 616e 6365 286e  not isinstance(n",
            "-0000e6d0: 616d 652c 2062 7974 6573 293a 0a20 2020  ame, bytes):.   ",
            "-0000e6e0: 2020 2020 206e 616d 6520 3d20 6e61 6d65       name = name",
            "-0000e6f0: 2e65 6e63 6f64 6528 4445 4641 554c 545f  .encode(DEFAULT_",
            "-0000e700: 454e 434f 4449 4e47 290a 2020 2020 7769  ENCODING).    wi",
            "-0000e710: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "-0000e720: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "-0000e730: 0a20 2020 2020 2020 2063 203d 2072 2e67  .        c = r.g",
            "-0000e740: 6574 5f63 6f6e 6669 6728 290a 2020 2020  et_config().    ",
            "-0000e750: 2020 2020 7365 6374 696f 6e20 3d20 2862      section = (b",
            "-0000e760: 2272 656d 6f74 6522 2c20 6e61 6d65 290a  \"remote\", name).",
            "-0000e770: 2020 2020 2020 2020 6465 6c20 635b 7365          del c[se",
            "-0000e780: 6374 696f 6e5d 0a20 2020 2020 2020 2063  ction].        c",
            "-0000e790: 2e77 7269 7465 5f74 6f5f 7061 7468 2829  .write_to_path()",
            "-0000e7a0: 0a0a 0a64 6566 2063 6865 636b 5f69 676e  ...def check_ign",
            "-0000e7b0: 6f72 6528 7265 706f 2c20 7061 7468 732c  ore(repo, paths,",
            "-0000e7c0: 206e 6f5f 696e 6465 783d 4661 6c73 6529   no_index=False)",
            "-0000e7d0: 3a0a 2020 2020 2222 2244 6562 7567 2067  :.    \"\"\"Debug g",
            "-0000e7e0: 6974 6967 6e6f 7265 2066 696c 6573 2e0a  itignore files..",
            "-0000e7f0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "-0000e800: 2072 6570 6f3a 2050 6174 6820 746f 2074   repo: Path to t",
            "-0000e810: 6865 2072 6570 6f73 6974 6f72 790a 2020  he repository.  ",
            "-0000e820: 2020 2020 7061 7468 733a 204c 6973 7420      paths: List ",
            "-0000e830: 6f66 2070 6174 6873 2074 6f20 6368 6563  of paths to chec",
            "-0000e840: 6b20 666f 720a 2020 2020 2020 6e6f 5f69  k for.      no_i",
            "-0000e850: 6e64 6578 3a20 446f 6e27 7420 6368 6563  ndex: Don't chec",
            "-0000e860: 6b20 696e 6465 780a 2020 2020 5265 7475  k index.    Retu",
            "-0000e870: 726e 733a 204c 6973 7420 6f66 2069 676e  rns: List of ign",
            "-0000e880: 6f72 6564 2066 696c 6573 0a20 2020 2022  ored files.    \"",
            "-0000e890: 2222 0a20 2020 2077 6974 6820 6f70 656e  \"\".    with open",
            "-0000e8a0: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "-0000e8b0: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "-0000e8c0: 2020 696e 6465 7820 3d20 722e 6f70 656e    index = r.open",
            "-0000e8d0: 5f69 6e64 6578 2829 0a20 2020 2020 2020  _index().       ",
            "-0000e8e0: 2069 676e 6f72 655f 6d61 6e61 6765 7220   ignore_manager ",
            "-0000e8f0: 3d20 4967 6e6f 7265 4669 6c74 6572 4d61  = IgnoreFilterMa",
            "-0000e900: 6e61 6765 722e 6672 6f6d 5f72 6570 6f28  nager.from_repo(",
            "-0000e910: 7229 0a20 2020 2020 2020 2066 6f72 2070  r).        for p",
            "-0000e920: 6174 6820 696e 2070 6174 6873 3a0a 2020  ath in paths:.  ",
            "-0000e930: 2020 2020 2020 2020 2020 6966 206e 6f74            if not",
            "-0000e940: 206e 6f5f 696e 6465 7820 616e 6420 7061   no_index and pa",
            "-0000e950: 7468 5f74 6f5f 7472 6565 5f70 6174 6828  th_to_tree_path(",
            "-0000e960: 722e 7061 7468 2c20 7061 7468 2920 696e  r.path, path) in",
            "-0000e970: 2069 6e64 6578 3a0a 2020 2020 2020 2020   index:.        ",
            "-0000e980: 2020 2020 2020 2020 636f 6e74 696e 7565          continue",
            "-0000e990: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if ",
            "-0000e9a0: 6f73 2e70 6174 682e 6973 6162 7328 7061  os.path.isabs(pa",
            "-0000e9b0: 7468 293a 0a20 2020 2020 2020 2020 2020  th):.           ",
            "-0000e9c0: 2020 2020 2070 6174 6820 3d20 6f73 2e70       path = os.p",
            "-0000e9d0: 6174 682e 7265 6c70 6174 6828 7061 7468  ath.relpath(path",
            "-0000e9e0: 2c20 722e 7061 7468 290a 2020 2020 2020  , r.path).      ",
            "-0000e9f0: 2020 2020 2020 6966 2069 676e 6f72 655f        if ignore_",
            "-0000ea00: 6d61 6e61 6765 722e 6973 5f69 676e 6f72  manager.is_ignor",
            "-0000ea10: 6564 2870 6174 6829 3a0a 2020 2020 2020  ed(path):.      ",
            "-0000ea20: 2020 2020 2020 2020 2020 7969 656c 6420            yield ",
            "-0000ea30: 7061 7468 0a0a 0a64 6566 2075 7064 6174  path...def updat",
            "-0000ea40: 655f 6865 6164 2872 6570 6f2c 2074 6172  e_head(repo, tar",
            "-0000ea50: 6765 742c 2064 6574 6163 6865 643d 4661  get, detached=Fa",
            "-0000ea60: 6c73 652c 206e 6577 5f62 7261 6e63 683d  lse, new_branch=",
            "-0000ea70: 4e6f 6e65 2920 2d3e 204e 6f6e 653a 0a20  None) -> None:. ",
            "-0000ea80: 2020 2022 2222 5570 6461 7465 2048 4541     \"\"\"Update HEA",
            "-0000ea90: 4420 746f 2070 6f69 6e74 2061 7420 6120  D to point at a ",
            "-0000eaa0: 6e65 7720 6272 616e 6368 2f63 6f6d 6d69  new branch/commi",
            "-0000eab0: 742e 0a0a 2020 2020 4e6f 7465 2074 6861  t...    Note tha",
            "-0000eac0: 7420 7468 6973 2064 6f65 7320 6e6f 7420  t this does not ",
            "-0000ead0: 6163 7475 616c 6c79 2075 7064 6174 6520  actually update ",
            "-0000eae0: 7468 6520 776f 726b 696e 6720 7472 6565  the working tree",
            "-0000eaf0: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "-0000eb00: 2020 2072 6570 6f3a 2050 6174 6820 746f     repo: Path to",
            "-0000eb10: 2074 6865 2072 6570 6f73 6974 6f72 790a   the repository.",
            "-0000eb20: 2020 2020 2020 6465 7461 6368 6564 3a20        detached: ",
            "-0000eb30: 4372 6561 7465 2061 2064 6574 6163 6865  Create a detache",
            "-0000eb40: 6420 6865 6164 0a20 2020 2020 2074 6172  d head.      tar",
            "-0000eb50: 6765 743a 2042 7261 6e63 6820 6f72 2063  get: Branch or c",
            "-0000eb60: 6f6d 6d69 7474 6973 6820 746f 2073 7769  ommittish to swi",
            "-0000eb70: 7463 6820 746f 0a20 2020 2020 206e 6577  tch to.      new",
            "-0000eb80: 5f62 7261 6e63 683a 204e 6577 2062 7261  _branch: New bra",
            "-0000eb90: 6e63 6820 746f 2063 7265 6174 650a 2020  nch to create.  ",
            "-0000eba0: 2020 2222 220a 2020 2020 7769 7468 206f    \"\"\".    with o",
            "-0000ebb0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "-0000ebc0: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "-0000ebd0: 2020 2020 2069 6620 6e65 775f 6272 616e       if new_bran",
            "-0000ebe0: 6368 2069 7320 6e6f 7420 4e6f 6e65 3a0a  ch is not None:.",
            "-0000ebf0: 2020 2020 2020 2020 2020 2020 746f 5f73              to_s",
            "-0000ec00: 6574 203d 205f 6d61 6b65 5f62 7261 6e63  et = _make_branc",
            "-0000ec10: 685f 7265 6628 6e65 775f 6272 616e 6368  h_ref(new_branch",
            "-0000ec20: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.",
            "-0000ec30: 2020 2020 2020 2020 2020 2020 746f 5f73              to_s",
            "-0000ec40: 6574 203d 2062 2248 4541 4422 0a20 2020  et = b\"HEAD\".   ",
            "-0000ec50: 2020 2020 2069 6620 6465 7461 6368 6564       if detached",
            "-0000ec60: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # ",
            "-0000ec70: 544f 444f 286a 656c 6d65 7229 3a20 5072  TODO(jelmer): Pr",
            "-0000ec80: 6f76 6964 6520 736f 6d65 2077 6179 2073  ovide some way s",
            "-0000ec90: 6f20 7468 6174 2074 6865 2061 6374 7561  o that the actua",
            "-0000eca0: 6c20 7265 6620 6765 7473 0a20 2020 2020  l ref gets.     ",
            "-0000ecb0: 2020 2020 2020 2023 2075 7064 6174 6564         # updated",
            "-0000ecc0: 2072 6174 6865 7220 7468 616e 2077 6861   rather than wha",
            "-0000ecd0: 7420 6974 2070 6f69 6e74 7320 746f 2c20  t it points to, ",
            "-0000ece0: 736f 2074 6865 2064 656c 6574 6520 6973  so the delete is",
            "-0000ecf0: 6e27 740a 2020 2020 2020 2020 2020 2020  n't.            ",
            "-0000ed00: 2320 6e65 6365 7373 6172 792e 0a20 2020  # necessary..   ",
            "-0000ed10: 2020 2020 2020 2020 2064 656c 2072 2e72           del r.r",
            "-0000ed20: 6566 735b 746f 5f73 6574 5d0a 2020 2020  efs[to_set].    ",
            "-0000ed30: 2020 2020 2020 2020 722e 7265 6673 5b74          r.refs[t",
            "-0000ed40: 6f5f 7365 745d 203d 2070 6172 7365 5f63  o_set] = parse_c",
            "-0000ed50: 6f6d 6d69 7428 722c 2074 6172 6765 7429  ommit(r, target)",
            "-0000ed60: 2e69 640a 2020 2020 2020 2020 656c 7365  .id.        else",
            "-0000ed70: 3a0a 2020 2020 2020 2020 2020 2020 722e  :.            r.",
            "-0000ed80: 7265 6673 2e73 6574 5f73 796d 626f 6c69  refs.set_symboli",
            "-0000ed90: 635f 7265 6628 746f 5f73 6574 2c20 7061  c_ref(to_set, pa",
            "-0000eda0: 7273 655f 7265 6628 722c 2074 6172 6765  rse_ref(r, targe",
            "-0000edb0: 7429 290a 2020 2020 2020 2020 6966 206e  t)).        if n",
            "-0000edc0: 6577 5f62 7261 6e63 6820 6973 206e 6f74  ew_branch is not",
            "-0000edd0: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         ",
            "-0000ede0: 2020 2072 2e72 6566 732e 7365 745f 7379     r.refs.set_sy",
            "-0000edf0: 6d62 6f6c 6963 5f72 6566 2862 2248 4541  mbolic_ref(b\"HEA",
            "-0000ee00: 4422 2c20 746f 5f73 6574 290a 0a0a 6465  D\", to_set)...de",
            "-0000ee10: 6620 7265 7365 745f 6669 6c65 2872 6570  f reset_file(rep",
            "-0000ee20: 6f2c 2066 696c 655f 7061 7468 3a20 7374  o, file_path: st",
            "-0000ee30: 722c 2074 6172 6765 743a 2062 7974 6573  r, target: bytes",
            "-0000ee40: 203d 2062 2248 4541 4422 2c20 7379 6d6c   = b\"HEAD\", syml",
            "-0000ee50: 696e 6b5f 666e 3d4e 6f6e 6529 202d 3e20  ink_fn=None) -> ",
            "-0000ee60: 4e6f 6e65 3a0a 2020 2020 2222 2252 6573  None:.    \"\"\"Res",
            "-0000ee70: 6574 2074 6865 2066 696c 6520 746f 2073  et the file to s",
            "-0000ee80: 7065 6369 6669 6320 636f 6d6d 6974 206f  pecific commit o",
            "-0000ee90: 7220 6272 616e 6368 2e0a 0a20 2020 2041  r branch...    A",
            "-0000eea0: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "-0000eeb0: 2064 756c 7769 6368 2052 6570 6f20 6f62   dulwich Repo ob",
            "-0000eec0: 6a65 6374 0a20 2020 2020 2066 696c 655f  ject.      file_",
            "-0000eed0: 7061 7468 3a20 6669 6c65 2074 6f20 7265  path: file to re",
            "-0000eee0: 7365 742c 2072 656c 6174 6976 6520 746f  set, relative to",
            "-0000eef0: 2074 6865 2072 6570 6f73 6974 6f72 7920   the repository ",
            "-0000ef00: 7061 7468 0a20 2020 2020 2074 6172 6765  path.      targe",
            "-0000ef10: 743a 2062 7261 6e63 6820 6f72 2063 6f6d  t: branch or com",
            "-0000ef20: 6d69 7420 6f72 2062 2748 4541 4427 2074  mit or b'HEAD' t",
            "-0000ef30: 6f20 7265 7365 740a 2020 2020 2222 220a  o reset.    \"\"\".",
            "-0000ef40: 2020 2020 7472 6565 203d 2070 6172 7365      tree = parse",
            "-0000ef50: 5f74 7265 6528 7265 706f 2c20 7472 6565  _tree(repo, tree",
            "-0000ef60: 6973 683d 7461 7267 6574 290a 2020 2020  ish=target).    ",
            "-0000ef70: 7472 6565 5f70 6174 6820 3d20 5f66 735f  tree_path = _fs_",
            "-0000ef80: 746f 5f74 7265 655f 7061 7468 2866 696c  to_tree_path(fil",
            "-0000ef90: 655f 7061 7468 290a 0a20 2020 2066 696c  e_path)..    fil",
            "-0000efa0: 655f 656e 7472 7920 3d20 7472 6565 2e6c  e_entry = tree.l",
            "-0000efb0: 6f6f 6b75 705f 7061 7468 2872 6570 6f2e  ookup_path(repo.",
            "-0000efc0: 6f62 6a65 6374 5f73 746f 7265 2e5f 5f67  object_store.__g",
            "-0000efd0: 6574 6974 656d 5f5f 2c20 7472 6565 5f70  etitem__, tree_p",
            "-0000efe0: 6174 6829 0a20 2020 2066 756c 6c5f 7061  ath).    full_pa",
            "-0000eff0: 7468 203d 206f 732e 7061 7468 2e6a 6f69  th = os.path.joi",
            "-0000f000: 6e28 6f73 2e66 7365 6e63 6f64 6528 7265  n(os.fsencode(re",
            "-0000f010: 706f 2e70 6174 6829 2c20 7472 6565 5f70  po.path), tree_p",
            "-0000f020: 6174 6829 0a20 2020 2062 6c6f 6220 3d20  ath).    blob = ",
            "-0000f030: 7265 706f 2e6f 626a 6563 745f 7374 6f72  repo.object_stor",
            "-0000f040: 655b 6669 6c65 5f65 6e74 7279 5b31 5d5d  e[file_entry[1]]",
            "-0000f050: 0a20 2020 206d 6f64 6520 3d20 6669 6c65  .    mode = file",
            "-0000f060: 5f65 6e74 7279 5b30 5d0a 2020 2020 6275  _entry[0].    bu",
            "-0000f070: 696c 645f 6669 6c65 5f66 726f 6d5f 626c  ild_file_from_bl",
            "-0000f080: 6f62 2862 6c6f 622c 206d 6f64 652c 2066  ob(blob, mode, f",
            "-0000f090: 756c 6c5f 7061 7468 2c20 7379 6d6c 696e  ull_path, symlin",
            "-0000f0a0: 6b5f 666e 3d73 796d 6c69 6e6b 5f66 6e29  k_fn=symlink_fn)",
            "-0000f0b0: 0a0a 0a64 6566 205f 7570 6461 7465 5f68  ...def _update_h",
            "-0000f0c0: 6561 645f 6475 7269 6e67 5f63 6865 636b  ead_during_check",
            "-0000f0d0: 6f75 745f 6272 616e 6368 2872 6570 6f2c  out_branch(repo,",
            "-0000f0e0: 2074 6172 6765 7429 3a0a 2020 2020 6368   target):.    ch",
            "-0000f0f0: 6563 6b6f 7574 5f74 6172 6765 7420 3d20  eckout_target = ",
            "-0000f100: 4e6f 6e65 0a20 2020 2069 6620 7461 7267  None.    if targ",
            "-0000f110: 6574 203d 3d20 6222 4845 4144 223a 2020  et == b\"HEAD\":  ",
            "-0000f120: 2320 446f 206e 6f74 2075 7064 6174 6520  # Do not update ",
            "-0000f130: 6865 6164 2077 6869 6c65 2074 7279 696e  head while tryin",
            "-0000f140: 6720 746f 2063 6865 636b 6f75 7420 746f  g to checkout to",
            "-0000f150: 2048 4541 442e 0a20 2020 2020 2020 2070   HEAD..        p",
            "-0000f160: 6173 730a 2020 2020 656c 6966 2074 6172  ass.    elif tar",
            "-0000f170: 6765 7420 696e 2072 6570 6f2e 7265 6673  get in repo.refs",
            "-0000f180: 2e6b 6579 7328 6261 7365 3d4c 4f43 414c  .keys(base=LOCAL",
            "-0000f190: 5f42 5241 4e43 485f 5052 4546 4958 293a  _BRANCH_PREFIX):",
            "-0000f1a0: 0a20 2020 2020 2020 2075 7064 6174 655f  .        update_",
            "-0000f1b0: 6865 6164 2872 6570 6f2c 2074 6172 6765  head(repo, targe",
            "-0000f1c0: 7429 0a20 2020 2065 6c73 653a 0a20 2020  t).    else:.   ",
            "-0000f1d0: 2020 2020 2023 2049 6620 6368 6563 6b69       # If checki",
            "-0000f1e0: 6e67 206f 7574 2061 2072 656d 6f74 6520  ng out a remote ",
            "-0000f1f0: 6272 616e 6368 2c20 6372 6561 7465 2061  branch, create a",
            "-0000f200: 206c 6f63 616c 206f 6e65 2077 6974 686f   local one witho",
            "-0000f210: 7574 2074 6865 2072 656d 6f74 6520 6e61  ut the remote na",
            "-0000f220: 6d65 2070 7265 6669 782e 0a20 2020 2020  me prefix..     ",
            "-0000f230: 2020 2063 6f6e 6669 6720 3d20 7265 706f     config = repo",
            "-0000f240: 2e67 6574 5f63 6f6e 6669 6728 290a 2020  .get_config().  ",
            "-0000f250: 2020 2020 2020 6e61 6d65 203d 2074 6172        name = tar",
            "-0000f260: 6765 742e 7370 6c69 7428 6222 2f22 295b  get.split(b\"/\")[",
            "-0000f270: 305d 0a20 2020 2020 2020 2073 6563 7469  0].        secti",
            "-0000f280: 6f6e 203d 2028 6222 7265 6d6f 7465 222c  on = (b\"remote\",",
            "-0000f290: 206e 616d 6529 0a20 2020 2020 2020 2069   name).        i",
            "-0000f2a0: 6620 636f 6e66 6967 2e68 6173 5f73 6563  f config.has_sec",
            "-0000f2b0: 7469 6f6e 2873 6563 7469 6f6e 293a 0a20  tion(section):. ",
            "-0000f2c0: 2020 2020 2020 2020 2020 2063 6865 636b             check",
            "-0000f2d0: 6f75 745f 7461 7267 6574 203d 2074 6172  out_target = tar",
            "-0000f2e0: 6765 742e 7265 706c 6163 6528 6e61 6d65  get.replace(name",
            "-0000f2f0: 202b 2062 222f 222c 2062 2222 290a 2020   + b\"/\", b\"\").  ",
            "-0000f300: 2020 2020 2020 2020 2020 7472 793a 0a20            try:. ",
            "-0000f310: 2020 2020 2020 2020 2020 2020 2020 2062                 b",
            "-0000f320: 7261 6e63 685f 6372 6561 7465 280a 2020  ranch_create(.  ",
            "-0000f330: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-0000f340: 2020 7265 706f 2c20 6368 6563 6b6f 7574    repo, checkout",
            "-0000f350: 5f74 6172 6765 742c 2028 4c4f 4341 4c5f  _target, (LOCAL_",
            "-0000f360: 5245 4d4f 5445 5f50 5245 4649 5820 2b20  REMOTE_PREFIX + ",
            "-0000f370: 7461 7267 6574 292e 6465 636f 6465 2829  target).decode()",
            "+0000a0b0: 7265 706f 7369 746f 7279 0a20 2020 2022  repository.    \"",
            "+0000a0c0: 2222 0a20 2020 2066 726f 6d20 2e73 7562  \"\".    from .sub",
            "+0000a0d0: 6d6f 6475 6c65 2069 6d70 6f72 7420 6974  module import it",
            "+0000a0e0: 6572 5f63 6163 6865 645f 7375 626d 6f64  er_cached_submod",
            "+0000a0f0: 756c 6573 0a0a 2020 2020 7769 7468 206f  ules..    with o",
            "+0000a100: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+0000a110: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+0000a120: 2020 2020 2066 6f72 2070 6174 682c 2073       for path, s",
            "+0000a130: 6861 2069 6e20 6974 6572 5f63 6163 6865  ha in iter_cache",
            "+0000a140: 645f 7375 626d 6f64 756c 6573 2872 2e6f  d_submodules(r.o",
            "+0000a150: 626a 6563 745f 7374 6f72 652c 2072 5b72  bject_store, r[r",
            "+0000a160: 2e68 6561 6428 295d 2e74 7265 6529 3a0a  .head()].tree):.",
            "+0000a170: 2020 2020 2020 2020 2020 2020 7969 656c              yiel",
            "+0000a180: 6420 7061 7468 2c20 7368 612e 6465 636f  d path, sha.deco",
            "+0000a190: 6465 2844 4546 4155 4c54 5f45 4e43 4f44  de(DEFAULT_ENCOD",
            "+0000a1a0: 494e 4729 0a0a 0a64 6566 2073 7562 6d6f  ING)...def submo",
            "+0000a1b0: 6475 6c65 5f75 7064 6174 6528 7265 706f  dule_update(repo",
            "+0000a1c0: 2c20 7061 7468 733d 4e6f 6e65 2c20 696e  , paths=None, in",
            "+0000a1d0: 6974 3d46 616c 7365 2c20 666f 7263 653d  it=False, force=",
            "+0000a1e0: 4661 6c73 652c 2065 7272 7374 7265 616d  False, errstream",
            "+0000a1f0: 3d4e 6f6e 6529 202d 3e20 4e6f 6e65 3a0a  =None) -> None:.",
            "+0000a200: 2020 2020 2222 2255 7064 6174 6520 7375      \"\"\"Update su",
            "+0000a210: 626d 6f64 756c 6573 2e0a 0a20 2020 2041  bmodules...    A",
            "+0000a220: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "+0000a230: 2050 6174 6820 746f 2072 6570 6f73 6974   Path to reposit",
            "+0000a240: 6f72 790a 2020 2020 2020 7061 7468 733a  ory.      paths:",
            "+0000a250: 204f 7074 696f 6e61 6c20 6c69 7374 206f   Optional list o",
            "+0000a260: 6620 7370 6563 6966 6963 2073 7562 6d6f  f specific submo",
            "+0000a270: 6475 6c65 2070 6174 6873 2074 6f20 7570  dule paths to up",
            "+0000a280: 6461 7465 2e20 4966 204e 6f6e 652c 2075  date. If None, u",
            "+0000a290: 7064 6174 6573 2061 6c6c 2e0a 2020 2020  pdates all..    ",
            "+0000a2a0: 2020 696e 6974 3a20 4966 2054 7275 652c    init: If True,",
            "+0000a2b0: 2069 6e69 7469 616c 697a 6520 7375 626d   initialize subm",
            "+0000a2c0: 6f64 756c 6573 2066 6972 7374 0a20 2020  odules first.   ",
            "+0000a2d0: 2020 2066 6f72 6365 3a20 466f 7263 6520     force: Force ",
            "+0000a2e0: 7570 6461 7465 2065 7665 6e20 6966 206c  update even if l",
            "+0000a2f0: 6f63 616c 2063 6861 6e67 6573 2065 7869  ocal changes exi",
            "+0000a300: 7374 0a20 2020 2022 2222 0a20 2020 2066  st.    \"\"\".    f",
            "+0000a310: 726f 6d20 2e63 6c69 656e 7420 696d 706f  rom .client impo",
            "+0000a320: 7274 2067 6574 5f74 7261 6e73 706f 7274  rt get_transport",
            "+0000a330: 5f61 6e64 5f70 6174 680a 2020 2020 6672  _and_path.    fr",
            "+0000a340: 6f6d 202e 696e 6465 7820 696d 706f 7274  om .index import",
            "+0000a350: 2062 7569 6c64 5f69 6e64 6578 5f66 726f   build_index_fro",
            "+0000a360: 6d5f 7472 6565 0a20 2020 2066 726f 6d20  m_tree.    from ",
            "+0000a370: 2e73 7562 6d6f 6475 6c65 2069 6d70 6f72  .submodule impor",
            "+0000a380: 7420 6974 6572 5f63 6163 6865 645f 7375  t iter_cached_su",
            "+0000a390: 626d 6f64 756c 6573 0a0a 2020 2020 7769  bmodules..    wi",
            "+0000a3a0: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "+0000a3b0: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "+0000a3c0: 0a20 2020 2020 2020 2069 6620 696e 6974  .        if init",
            "+0000a3d0: 3a0a 2020 2020 2020 2020 2020 2020 7375  :.            su",
            "+0000a3e0: 626d 6f64 756c 655f 696e 6974 2872 290a  bmodule_init(r).",
            "+0000a3f0: 0a20 2020 2020 2020 2063 6f6e 6669 6720  .        config ",
            "+0000a400: 3d20 722e 6765 745f 636f 6e66 6967 2829  = r.get_config()",
            "+0000a410: 0a20 2020 2020 2020 2067 6974 6d6f 6475  .        gitmodu",
            "+0000a420: 6c65 735f 7061 7468 203d 206f 732e 7061  les_path = os.pa",
            "+0000a430: 7468 2e6a 6f69 6e28 722e 7061 7468 2c20  th.join(r.path, ",
            "+0000a440: 222e 6769 746d 6f64 756c 6573 2229 0a0a  \".gitmodules\")..",
            "+0000a450: 2020 2020 2020 2020 2320 4765 7420 6c69          # Get li",
            "+0000a460: 7374 206f 6620 7375 626d 6f64 756c 6573  st of submodules",
            "+0000a470: 2074 6f20 7570 6461 7465 0a20 2020 2020   to update.     ",
            "+0000a480: 2020 2073 7562 6d6f 6475 6c65 735f 746f     submodules_to",
            "+0000a490: 5f75 7064 6174 6520 3d20 5b5d 0a20 2020  _update = [].   ",
            "+0000a4a0: 2020 2020 2066 6f72 2070 6174 682c 2073       for path, s",
            "+0000a4b0: 6861 2069 6e20 6974 6572 5f63 6163 6865  ha in iter_cache",
            "+0000a4c0: 645f 7375 626d 6f64 756c 6573 2872 2e6f  d_submodules(r.o",
            "+0000a4d0: 626a 6563 745f 7374 6f72 652c 2072 5b72  bject_store, r[r",
            "+0000a4e0: 2e68 6561 6428 295d 2e74 7265 6529 3a0a  .head()].tree):.",
            "+0000a4f0: 2020 2020 2020 2020 2020 2020 7061 7468              path",
            "+0000a500: 5f73 7472 203d 2028 0a20 2020 2020 2020  _str = (.       ",
            "+0000a510: 2020 2020 2020 2020 2070 6174 682e 6465           path.de",
            "+0000a520: 636f 6465 2844 4546 4155 4c54 5f45 4e43  code(DEFAULT_ENC",
            "+0000a530: 4f44 494e 4729 2069 6620 6973 696e 7374  ODING) if isinst",
            "+0000a540: 616e 6365 2870 6174 682c 2062 7974 6573  ance(path, bytes",
            "+0000a550: 2920 656c 7365 2070 6174 680a 2020 2020  ) else path.    ",
            "+0000a560: 2020 2020 2020 2020 290a 2020 2020 2020          ).      ",
            "+0000a570: 2020 2020 2020 6966 2070 6174 6873 2069        if paths i",
            "+0000a580: 7320 4e6f 6e65 206f 7220 7061 7468 5f73  s None or path_s",
            "+0000a590: 7472 2069 6e20 7061 7468 733a 0a20 2020  tr in paths:.   ",
            "+0000a5a0: 2020 2020 2020 2020 2020 2020 2073 7562               sub",
            "+0000a5b0: 6d6f 6475 6c65 735f 746f 5f75 7064 6174  modules_to_updat",
            "+0000a5c0: 652e 6170 7065 6e64 2828 7061 7468 2c20  e.append((path, ",
            "+0000a5d0: 7368 6129 290a 0a20 2020 2020 2020 2023  sha))..        #",
            "+0000a5e0: 2052 6561 6420 7375 626d 6f64 756c 6520   Read submodule ",
            "+0000a5f0: 636f 6e66 6967 7572 6174 696f 6e0a 2020  configuration.  ",
            "+0000a600: 2020 2020 2020 666f 7220 7061 7468 2c20        for path, ",
            "+0000a610: 7461 7267 6574 5f73 6861 2069 6e20 7375  target_sha in su",
            "+0000a620: 626d 6f64 756c 6573 5f74 6f5f 7570 6461  bmodules_to_upda",
            "+0000a630: 7465 3a0a 2020 2020 2020 2020 2020 2020  te:.            ",
            "+0000a640: 7061 7468 5f73 7472 203d 2028 0a20 2020  path_str = (.   ",
            "+0000a650: 2020 2020 2020 2020 2020 2020 2070 6174               pat",
            "+0000a660: 682e 6465 636f 6465 2844 4546 4155 4c54  h.decode(DEFAULT",
            "+0000a670: 5f45 4e43 4f44 494e 4729 2069 6620 6973  _ENCODING) if is",
            "+0000a680: 696e 7374 616e 6365 2870 6174 682c 2062  instance(path, b",
            "+0000a690: 7974 6573 2920 656c 7365 2070 6174 680a  ytes) else path.",
            "+0000a6a0: 2020 2020 2020 2020 2020 2020 290a 0a20              ).. ",
            "+0000a6b0: 2020 2020 2020 2020 2020 2023 2046 696e             # Fin",
            "+0000a6c0: 6420 7468 6520 7375 626d 6f64 756c 6520  d the submodule ",
            "+0000a6d0: 6e61 6d65 2066 726f 6d20 2e67 6974 6d6f  name from .gitmo",
            "+0000a6e0: 6475 6c65 730a 2020 2020 2020 2020 2020  dules.          ",
            "+0000a6f0: 2020 7375 626d 6f64 756c 655f 6e61 6d65    submodule_name",
            "+0000a700: 203d 204e 6f6e 650a 2020 2020 2020 2020   = None.        ",
            "+0000a710: 2020 2020 666f 7220 736d 5f70 6174 682c      for sm_path,",
            "+0000a720: 2073 6d5f 7572 6c2c 2073 6d5f 6e61 6d65   sm_url, sm_name",
            "+0000a730: 2069 6e20 7265 6164 5f73 7562 6d6f 6475   in read_submodu",
            "+0000a740: 6c65 7328 6769 746d 6f64 756c 6573 5f70  les(gitmodules_p",
            "+0000a750: 6174 6829 3a0a 2020 2020 2020 2020 2020  ath):.          ",
            "+0000a760: 2020 2020 2020 6966 2073 6d5f 7061 7468        if sm_path",
            "+0000a770: 203d 3d20 7061 7468 3a0a 2020 2020 2020   == path:.      ",
            "+0000a780: 2020 2020 2020 2020 2020 2020 2020 7375                su",
            "+0000a790: 626d 6f64 756c 655f 6e61 6d65 203d 2073  bmodule_name = s",
            "+0000a7a0: 6d5f 6e61 6d65 0a20 2020 2020 2020 2020  m_name.         ",
            "+0000a7b0: 2020 2020 2020 2020 2020 2062 7265 616b             break",
            "+0000a7c0: 0a0a 2020 2020 2020 2020 2020 2020 6966  ..            if",
            "+0000a7d0: 206e 6f74 2073 7562 6d6f 6475 6c65 5f6e   not submodule_n",
            "+0000a7e0: 616d 653a 0a20 2020 2020 2020 2020 2020  ame:.           ",
            "+0000a7f0: 2020 2020 2063 6f6e 7469 6e75 650a 0a20       continue.. ",
            "+0000a800: 2020 2020 2020 2020 2020 2023 2047 6574             # Get",
            "+0000a810: 2074 6865 2055 524c 2066 726f 6d20 636f   the URL from co",
            "+0000a820: 6e66 6967 0a20 2020 2020 2020 2020 2020  nfig.           ",
            "+0000a830: 2073 6563 7469 6f6e 203d 2028 0a20 2020   section = (.   ",
            "+0000a840: 2020 2020 2020 2020 2020 2020 2062 2273               b\"s",
            "+0000a850: 7562 6d6f 6475 6c65 222c 0a20 2020 2020  ubmodule\",.     ",
            "+0000a860: 2020 2020 2020 2020 2020 2073 7562 6d6f             submo",
            "+0000a870: 6475 6c65 5f6e 616d 650a 2020 2020 2020  dule_name.      ",
            "+0000a880: 2020 2020 2020 2020 2020 6966 2069 7369            if isi",
            "+0000a890: 6e73 7461 6e63 6528 7375 626d 6f64 756c  nstance(submodul",
            "+0000a8a0: 655f 6e61 6d65 2c20 6279 7465 7329 0a20  e_name, bytes). ",
            "+0000a8b0: 2020 2020 2020 2020 2020 2020 2020 2065                 e",
            "+0000a8c0: 6c73 6520 7375 626d 6f64 756c 655f 6e61  lse submodule_na",
            "+0000a8d0: 6d65 2e65 6e63 6f64 6528 292c 0a20 2020  me.encode(),.   ",
            "+0000a8e0: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     ",
            "+0000a8f0: 2020 2020 2020 2074 7279 3a0a 2020 2020         try:.    ",
            "+0000a900: 2020 2020 2020 2020 2020 2020 7572 6c20              url ",
            "+0000a910: 3d20 636f 6e66 6967 2e67 6574 2873 6563  = config.get(sec",
            "+0000a920: 7469 6f6e 2c20 6222 7572 6c22 290a 2020  tion, b\"url\").  ",
            "+0000a930: 2020 2020 2020 2020 2020 2020 2020 6966                if",
            "+0000a940: 2069 7369 6e73 7461 6e63 6528 7572 6c2c   isinstance(url,",
            "+0000a950: 2062 7974 6573 293a 0a20 2020 2020 2020   bytes):.       ",
            "+0000a960: 2020 2020 2020 2020 2020 2020 2075 726c               url",
            "+0000a970: 203d 2075 726c 2e64 6563 6f64 6528 4445   = url.decode(DE",
            "+0000a980: 4641 554c 545f 454e 434f 4449 4e47 290a  FAULT_ENCODING).",
            "+0000a990: 2020 2020 2020 2020 2020 2020 6578 6365              exce",
            "+0000a9a0: 7074 204b 6579 4572 726f 723a 0a20 2020  pt KeyError:.   ",
            "+0000a9b0: 2020 2020 2020 2020 2020 2020 2023 2055               # U",
            "+0000a9c0: 524c 206e 6f74 2069 6e20 636f 6e66 6967  RL not in config",
            "+0000a9d0: 2c20 736b 6970 2074 6869 7320 7375 626d  , skip this subm",
            "+0000a9e0: 6f64 756c 650a 2020 2020 2020 2020 2020  odule.          ",
            "+0000a9f0: 2020 2020 2020 636f 6e74 696e 7565 0a0a        continue..",
            "+0000aa00: 2020 2020 2020 2020 2020 2020 2320 4765              # Ge",
            "+0000aa10: 7420 6f72 2063 7265 6174 6520 7468 6520  t or create the ",
            "+0000aa20: 7375 626d 6f64 756c 6520 7265 706f 7369  submodule reposi",
            "+0000aa30: 746f 7279 2070 6174 6873 0a20 2020 2020  tory paths.     ",
            "+0000aa40: 2020 2020 2020 2073 7562 6d6f 6475 6c65         submodule",
            "+0000aa50: 5f70 6174 6820 3d20 6f73 2e70 6174 682e  _path = os.path.",
            "+0000aa60: 6a6f 696e 2872 2e70 6174 682c 2070 6174  join(r.path, pat",
            "+0000aa70: 685f 7374 7229 0a20 2020 2020 2020 2020  h_str).         ",
            "+0000aa80: 2020 2073 7562 6d6f 6475 6c65 5f67 6974     submodule_git",
            "+0000aa90: 5f64 6972 203d 206f 732e 7061 7468 2e6a  _dir = os.path.j",
            "+0000aaa0: 6f69 6e28 722e 7061 7468 2c20 222e 6769  oin(r.path, \".gi",
            "+0000aab0: 7422 2c20 226d 6f64 756c 6573 222c 2070  t\", \"modules\", p",
            "+0000aac0: 6174 685f 7374 7229 0a0a 2020 2020 2020  ath_str)..      ",
            "+0000aad0: 2020 2020 2020 2320 436c 6f6e 6520 6f72        # Clone or",
            "+0000aae0: 2066 6574 6368 2074 6865 2073 7562 6d6f   fetch the submo",
            "+0000aaf0: 6475 6c65 0a20 2020 2020 2020 2020 2020  dule.           ",
            "+0000ab00: 2069 6620 6e6f 7420 6f73 2e70 6174 682e   if not os.path.",
            "+0000ab10: 6578 6973 7473 2873 7562 6d6f 6475 6c65  exists(submodule",
            "+0000ab20: 5f67 6974 5f64 6972 293a 0a20 2020 2020  _git_dir):.     ",
            "+0000ab30: 2020 2020 2020 2020 2020 2023 2043 6c6f             # Clo",
            "+0000ab40: 6e65 2074 6865 2073 7562 6d6f 6475 6c65  ne the submodule",
            "+0000ab50: 2061 7320 6261 7265 2072 6570 6f73 6974   as bare reposit",
            "+0000ab60: 6f72 790a 2020 2020 2020 2020 2020 2020  ory.            ",
            "+0000ab70: 2020 2020 6f73 2e6d 616b 6564 6972 7328      os.makedirs(",
            "+0000ab80: 6f73 2e70 6174 682e 6469 726e 616d 6528  os.path.dirname(",
            "+0000ab90: 7375 626d 6f64 756c 655f 6769 745f 6469  submodule_git_di",
            "+0000aba0: 7229 2c20 6578 6973 745f 6f6b 3d54 7275  r), exist_ok=Tru",
            "+0000abb0: 6529 0a0a 2020 2020 2020 2020 2020 2020  e)..            ",
            "+0000abc0: 2020 2020 2320 436c 6f6e 6520 746f 2074      # Clone to t",
            "+0000abd0: 6865 2067 6974 2064 6972 6563 746f 7279  he git directory",
            "+0000abe0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000abf0: 2073 7562 5f72 6570 6f20 3d20 636c 6f6e   sub_repo = clon",
            "+0000ac00: 6528 7572 6c2c 2073 7562 6d6f 6475 6c65  e(url, submodule",
            "+0000ac10: 5f67 6974 5f64 6972 2c20 6261 7265 3d54  _git_dir, bare=T",
            "+0000ac20: 7275 652c 2063 6865 636b 6f75 743d 4661  rue, checkout=Fa",
            "+0000ac30: 6c73 6529 0a20 2020 2020 2020 2020 2020  lse).           ",
            "+0000ac40: 2020 2020 2073 7562 5f72 6570 6f2e 636c       sub_repo.cl",
            "+0000ac50: 6f73 6528 290a 0a20 2020 2020 2020 2020  ose()..         ",
            "+0000ac60: 2020 2020 2020 2023 2043 7265 6174 6520         # Create ",
            "+0000ac70: 7468 6520 7375 626d 6f64 756c 6520 6469  the submodule di",
            "+0000ac80: 7265 6374 6f72 7920 6966 2069 7420 646f  rectory if it do",
            "+0000ac90: 6573 6e27 7420 6578 6973 740a 2020 2020  esn't exist.    ",
            "+0000aca0: 2020 2020 2020 2020 2020 2020 6966 206e              if n",
            "+0000acb0: 6f74 206f 732e 7061 7468 2e65 7869 7374  ot os.path.exist",
            "+0000acc0: 7328 7375 626d 6f64 756c 655f 7061 7468  s(submodule_path",
            "+0000acd0: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             ",
            "+0000ace0: 2020 2020 2020 206f 732e 6d61 6b65 6469         os.makedi",
            "+0000acf0: 7273 2873 7562 6d6f 6475 6c65 5f70 6174  rs(submodule_pat",
            "+0000ad00: 6829 0a0a 2020 2020 2020 2020 2020 2020  h)..            ",
            "+0000ad10: 2020 2020 2320 4372 6561 7465 202e 6769      # Create .gi",
            "+0000ad20: 7420 6669 6c65 2069 6e20 7468 6520 7375  t file in the su",
            "+0000ad30: 626d 6f64 756c 6520 6469 7265 6374 6f72  bmodule director",
            "+0000ad40: 790a 2020 2020 2020 2020 2020 2020 2020  y.              ",
            "+0000ad50: 2020 6465 7074 6820 3d20 7061 7468 5f73    depth = path_s",
            "+0000ad60: 7472 2e63 6f75 6e74 2822 2f22 2920 2b20  tr.count(\"/\") + ",
            "+0000ad70: 310a 2020 2020 2020 2020 2020 2020 2020  1.              ",
            "+0000ad80: 2020 7265 6c61 7469 7665 5f67 6974 5f64    relative_git_d",
            "+0000ad90: 6972 203d 2022 2e2e 2f22 202a 2064 6570  ir = \"../\" * dep",
            "+0000ada0: 7468 202b 2022 2e67 6974 2f6d 6f64 756c  th + \".git/modul",
            "+0000adb0: 6573 2f22 202b 2070 6174 685f 7374 720a  es/\" + path_str.",
            "+0000adc0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000add0: 6769 745f 6669 6c65 5f70 6174 6820 3d20  git_file_path = ",
            "+0000ade0: 6f73 2e70 6174 682e 6a6f 696e 2873 7562  os.path.join(sub",
            "+0000adf0: 6d6f 6475 6c65 5f70 6174 682c 2022 2e67  module_path, \".g",
            "+0000ae00: 6974 2229 0a20 2020 2020 2020 2020 2020  it\").           ",
            "+0000ae10: 2020 2020 2077 6974 6820 6f70 656e 2867       with open(g",
            "+0000ae20: 6974 5f66 696c 655f 7061 7468 2c20 2277  it_file_path, \"w",
            "+0000ae30: 2229 2061 7320 663a 0a20 2020 2020 2020  \") as f:.       ",
            "+0000ae40: 2020 2020 2020 2020 2020 2020 2066 2e77               f.w",
            "+0000ae50: 7269 7465 2866 2267 6974 6469 723a 207b  rite(f\"gitdir: {",
            "+0000ae60: 7265 6c61 7469 7665 5f67 6974 5f64 6972  relative_git_dir",
            "+0000ae70: 7d5c 6e22 290a 0a20 2020 2020 2020 2020  }\\n\")..         ",
            "+0000ae80: 2020 2020 2020 2023 2053 6574 2075 7020         # Set up ",
            "+0000ae90: 776f 726b 696e 6720 6469 7265 6374 6f72  working director",
            "+0000aea0: 7920 636f 6e66 6967 7572 6174 696f 6e0a  y configuration.",
            "+0000aeb0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000aec0: 7769 7468 206f 7065 6e5f 7265 706f 5f63  with open_repo_c",
            "+0000aed0: 6c6f 7369 6e67 2873 7562 6d6f 6475 6c65  losing(submodule",
            "+0000aee0: 5f67 6974 5f64 6972 2920 6173 2073 7562  _git_dir) as sub",
            "+0000aef0: 5f72 6570 6f3a 0a20 2020 2020 2020 2020  _repo:.         ",
            "+0000af00: 2020 2020 2020 2020 2020 2073 7562 5f63             sub_c",
            "+0000af10: 6f6e 6669 6720 3d20 7375 625f 7265 706f  onfig = sub_repo",
            "+0000af20: 2e67 6574 5f63 6f6e 6669 6728 290a 2020  .get_config().  ",
            "+0000af30: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000af40: 2020 7375 625f 636f 6e66 6967 2e73 6574    sub_config.set",
            "+0000af50: 280a 2020 2020 2020 2020 2020 2020 2020  (.              ",
            "+0000af60: 2020 2020 2020 2020 2020 2862 2263 6f72            (b\"cor",
            "+0000af70: 6522 2c29 2c0a 2020 2020 2020 2020 2020  e\",),.          ",
            "+0000af80: 2020 2020 2020 2020 2020 2020 2020 6222                b\"",
            "+0000af90: 776f 726b 7472 6565 222c 0a20 2020 2020  worktree\",.     ",
            "+0000afa0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000afb0: 2020 206f 732e 7061 7468 2e61 6273 7061     os.path.abspa",
            "+0000afc0: 7468 2873 7562 6d6f 6475 6c65 5f70 6174  th(submodule_pat",
            "+0000afd0: 6829 2e65 6e63 6f64 6528 292c 0a20 2020  h).encode(),.   ",
            "+0000afe0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000aff0: 2029 0a20 2020 2020 2020 2020 2020 2020   ).             ",
            "+0000b000: 2020 2020 2020 2073 7562 5f63 6f6e 6669         sub_confi",
            "+0000b010: 672e 7772 6974 655f 746f 5f70 6174 6828  g.write_to_path(",
            "+0000b020: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             ",
            "+0000b030: 2020 2020 2020 2023 2043 6865 636b 6f75         # Checkou",
            "+0000b040: 7420 7468 6520 7461 7267 6574 2063 6f6d  t the target com",
            "+0000b050: 6d69 740a 2020 2020 2020 2020 2020 2020  mit.            ",
            "+0000b060: 2020 2020 2020 2020 7375 625f 7265 706f          sub_repo",
            "+0000b070: 2e72 6566 735b 6222 4845 4144 225d 203d  .refs[b\"HEAD\"] =",
            "+0000b080: 2074 6172 6765 745f 7368 610a 0a20 2020   target_sha..   ",
            "+0000b090: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b0a0: 2023 2042 7569 6c64 2074 6865 2069 6e64   # Build the ind",
            "+0000b0b0: 6578 2061 6e64 2063 6865 636b 6f75 7420  ex and checkout ",
            "+0000b0c0: 6669 6c65 730a 2020 2020 2020 2020 2020  files.          ",
            "+0000b0d0: 2020 2020 2020 2020 2020 7472 6565 203d            tree =",
            "+0000b0e0: 2073 7562 5f72 6570 6f5b 7461 7267 6574   sub_repo[target",
            "+0000b0f0: 5f73 6861 5d0a 2020 2020 2020 2020 2020  _sha].          ",
            "+0000b100: 2020 2020 2020 2020 2020 6966 2068 6173            if has",
            "+0000b110: 6174 7472 2874 7265 652c 2022 7472 6565  attr(tree, \"tree",
            "+0000b120: 2229 3a20 2023 2049 6620 6974 2773 2061  \"):  # If it's a",
            "+0000b130: 2063 6f6d 6d69 742c 2067 6574 2074 6865   commit, get the",
            "+0000b140: 2074 7265 650a 2020 2020 2020 2020 2020   tree.          ",
            "+0000b150: 2020 2020 2020 2020 2020 2020 2020 7472                tr",
            "+0000b160: 6565 5f69 6420 3d20 7472 6565 2e74 7265  ee_id = tree.tre",
            "+0000b170: 650a 2020 2020 2020 2020 2020 2020 2020  e.              ",
            "+0000b180: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    ",
            "+0000b190: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b1a0: 2020 2020 7472 6565 5f69 6420 3d20 7461      tree_id = ta",
            "+0000b1b0: 7267 6574 5f73 6861 0a0a 2020 2020 2020  rget_sha..      ",
            "+0000b1c0: 2020 2020 2020 2020 2020 2020 2020 6275                bu",
            "+0000b1d0: 696c 645f 696e 6465 785f 6672 6f6d 5f74  ild_index_from_t",
            "+0000b1e0: 7265 6528 0a20 2020 2020 2020 2020 2020  ree(.           ",
            "+0000b1f0: 2020 2020 2020 2020 2020 2020 2073 7562               sub",
            "+0000b200: 6d6f 6475 6c65 5f70 6174 682c 0a20 2020  module_path,.   ",
            "+0000b210: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b220: 2020 2020 2073 7562 5f72 6570 6f2e 696e       sub_repo.in",
            "+0000b230: 6465 785f 7061 7468 2829 2c0a 2020 2020  dex_path(),.    ",
            "+0000b240: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b250: 2020 2020 7375 625f 7265 706f 2e6f 626a      sub_repo.obj",
            "+0000b260: 6563 745f 7374 6f72 652c 0a20 2020 2020  ect_store,.     ",
            "+0000b270: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b280: 2020 2074 7265 655f 6964 2c0a 2020 2020     tree_id,.    ",
            "+0000b290: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b2a0: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el",
            "+0000b2b0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            ",
            "+0000b2c0: 2020 2020 2320 4665 7463 6820 616e 6420      # Fetch and ",
            "+0000b2d0: 6368 6563 6b6f 7574 2069 6e20 6578 6973  checkout in exis",
            "+0000b2e0: 7469 6e67 2073 7562 6d6f 6475 6c65 0a20  ting submodule. ",
            "+0000b2f0: 2020 2020 2020 2020 2020 2020 2020 2077                 w",
            "+0000b300: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "+0000b310: 6f73 696e 6728 7375 626d 6f64 756c 655f  osing(submodule_",
            "+0000b320: 6769 745f 6469 7229 2061 7320 7375 625f  git_dir) as sub_",
            "+0000b330: 7265 706f 3a0a 2020 2020 2020 2020 2020  repo:.          ",
            "+0000b340: 2020 2020 2020 2020 2020 2320 4665 7463            # Fetc",
            "+0000b350: 6820 6672 6f6d 2072 656d 6f74 650a 2020  h from remote.  ",
            "+0000b360: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b370: 2020 636c 6965 6e74 2c20 7061 7468 5f73    client, path_s",
            "+0000b380: 6567 6d65 6e74 7320 3d20 6765 745f 7472  egments = get_tr",
            "+0000b390: 616e 7370 6f72 745f 616e 645f 7061 7468  ansport_and_path",
            "+0000b3a0: 2875 726c 290a 2020 2020 2020 2020 2020  (url).          ",
            "+0000b3b0: 2020 2020 2020 2020 2020 636c 6965 6e74            client",
            "+0000b3c0: 2e66 6574 6368 2870 6174 685f 7365 676d  .fetch(path_segm",
            "+0000b3d0: 656e 7473 2c20 7375 625f 7265 706f 290a  ents, sub_repo).",
            "+0000b3e0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000b3f0: 2020 2020 2023 2055 7064 6174 6520 746f       # Update to",
            "+0000b400: 2074 6865 2074 6172 6765 7420 636f 6d6d   the target comm",
            "+0000b410: 6974 0a20 2020 2020 2020 2020 2020 2020  it.             ",
            "+0000b420: 2020 2020 2020 2073 7562 5f72 6570 6f2e         sub_repo.",
            "+0000b430: 7265 6673 5b62 2248 4541 4422 5d20 3d20  refs[b\"HEAD\"] = ",
            "+0000b440: 7461 7267 6574 5f73 6861 0a0a 2020 2020  target_sha..    ",
            "+0000b450: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b460: 2320 5265 7365 7420 7468 6520 776f 726b  # Reset the work",
            "+0000b470: 696e 6720 6469 7265 6374 6f72 790a 2020  ing directory.  ",
            "+0000b480: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000b490: 2020 7265 7365 7428 7375 625f 7265 706f    reset(sub_repo",
            "+0000b4a0: 2c20 2268 6172 6422 2c20 7461 7267 6574  , \"hard\", target",
            "+0000b4b0: 5f73 6861 290a 0a0a 6465 6620 7461 675f  _sha)...def tag_",
            "+0000b4c0: 6372 6561 7465 280a 2020 2020 7265 706f  create(.    repo",
            "+0000b4d0: 2c0a 2020 2020 7461 673a 2055 6e69 6f6e  ,.    tag: Union",
            "+0000b4e0: 5b73 7472 2c20 6279 7465 735d 2c0a 2020  [str, bytes],.  ",
            "+0000b4f0: 2020 6175 7468 6f72 3a20 4f70 7469 6f6e    author: Option",
            "+0000b500: 616c 5b55 6e69 6f6e 5b73 7472 2c20 6279  al[Union[str, by",
            "+0000b510: 7465 735d 5d20 3d20 4e6f 6e65 2c0a 2020  tes]] = None,.  ",
            "+0000b520: 2020 6d65 7373 6167 653a 204f 7074 696f    message: Optio",
            "+0000b530: 6e61 6c5b 556e 696f 6e5b 7374 722c 2062  nal[Union[str, b",
            "+0000b540: 7974 6573 5d5d 203d 204e 6f6e 652c 0a20  ytes]] = None,. ",
            "+0000b550: 2020 2061 6e6e 6f74 6174 6564 3d46 616c     annotated=Fal",
            "+0000b560: 7365 2c0a 2020 2020 6f62 6a65 6374 6973  se,.    objectis",
            "+0000b570: 683a 2055 6e69 6f6e 5b73 7472 2c20 6279  h: Union[str, by",
            "+0000b580: 7465 735d 203d 2022 4845 4144 222c 0a20  tes] = \"HEAD\",. ",
            "+0000b590: 2020 2074 6167 5f74 696d 653d 4e6f 6e65     tag_time=None",
            "+0000b5a0: 2c0a 2020 2020 7461 675f 7469 6d65 7a6f  ,.    tag_timezo",
            "+0000b5b0: 6e65 3d4e 6f6e 652c 0a20 2020 2073 6967  ne=None,.    sig",
            "+0000b5c0: 6e3a 2062 6f6f 6c20 3d20 4661 6c73 652c  n: bool = False,",
            "+0000b5d0: 0a20 2020 2065 6e63 6f64 696e 673a 2073  .    encoding: s",
            "+0000b5e0: 7472 203d 2044 4546 4155 4c54 5f45 4e43  tr = DEFAULT_ENC",
            "+0000b5f0: 4f44 494e 472c 0a29 202d 3e20 4e6f 6e65  ODING,.) -> None",
            "+0000b600: 3a0a 2020 2020 2222 2243 7265 6174 6573  :.    \"\"\"Creates",
            "+0000b610: 2061 2074 6167 2069 6e20 6769 7420 7669   a tag in git vi",
            "+0000b620: 6120 6475 6c77 6963 6820 6361 6c6c 732e  a dulwich calls.",
            "+0000b630: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            "+0000b640: 2020 7265 706f 3a20 5061 7468 2074 6f20    repo: Path to ",
            "+0000b650: 7265 706f 7369 746f 7279 0a20 2020 2020  repository.     ",
            "+0000b660: 2074 6167 3a20 7461 6720 7374 7269 6e67   tag: tag string",
            "+0000b670: 0a20 2020 2020 2061 7574 686f 723a 2074  .      author: t",
            "+0000b680: 6167 2061 7574 686f 7220 286f 7074 696f  ag author (optio",
            "+0000b690: 6e61 6c2c 2069 6620 616e 6e6f 7461 7465  nal, if annotate",
            "+0000b6a0: 6420 6973 2073 6574 290a 2020 2020 2020  d is set).      ",
            "+0000b6b0: 6d65 7373 6167 653a 2074 6167 206d 6573  message: tag mes",
            "+0000b6c0: 7361 6765 2028 6f70 7469 6f6e 616c 290a  sage (optional).",
            "+0000b6d0: 2020 2020 2020 616e 6e6f 7461 7465 643a        annotated:",
            "+0000b6e0: 2077 6865 7468 6572 2074 6f20 6372 6561   whether to crea",
            "+0000b6f0: 7465 2061 6e20 616e 6e6f 7461 7465 6420  te an annotated ",
            "+0000b700: 7461 670a 2020 2020 2020 6f62 6a65 6374  tag.      object",
            "+0000b710: 6973 683a 206f 626a 6563 7420 7468 6520  ish: object the ",
            "+0000b720: 7461 6720 7368 6f75 6c64 2070 6f69 6e74  tag should point",
            "+0000b730: 2061 742c 2064 6566 6175 6c74 7320 746f   at, defaults to",
            "+0000b740: 2048 4541 440a 2020 2020 2020 7461 675f   HEAD.      tag_",
            "+0000b750: 7469 6d65 3a20 4f70 7469 6f6e 616c 2074  time: Optional t",
            "+0000b760: 696d 6520 666f 7220 616e 6e6f 7461 7465  ime for annotate",
            "+0000b770: 6420 7461 670a 2020 2020 2020 7461 675f  d tag.      tag_",
            "+0000b780: 7469 6d65 7a6f 6e65 3a20 4f70 7469 6f6e  timezone: Option",
            "+0000b790: 616c 2074 696d 657a 6f6e 6520 666f 7220  al timezone for ",
            "+0000b7a0: 616e 6e6f 7461 7465 6420 7461 670a 2020  annotated tag.  ",
            "+0000b7b0: 2020 2020 7369 676e 3a20 4750 4720 5369      sign: GPG Si",
            "+0000b7c0: 676e 2074 6865 2074 6167 2028 626f 6f6c  gn the tag (bool",
            "+0000b7d0: 2c20 6465 6661 756c 7473 2074 6f20 4661  , defaults to Fa",
            "+0000b7e0: 6c73 652c 0a20 2020 2020 2020 2070 6173  lse,.        pas",
            "+0000b7f0: 7320 5472 7565 2074 6f20 7573 6520 6465  s True to use de",
            "+0000b800: 6661 756c 7420 4750 4720 6b65 792c 0a20  fault GPG key,. ",
            "+0000b810: 2020 2020 2020 2070 6173 7320 6120 7374         pass a st",
            "+0000b820: 7220 636f 6e74 6169 6e69 6e67 204b 6579  r containing Key",
            "+0000b830: 2049 4420 746f 2075 7365 2061 2073 7065   ID to use a spe",
            "+0000b840: 6369 6669 6320 4750 4720 6b65 7929 0a20  cific GPG key). ",
            "+0000b850: 2020 2022 2222 0a20 2020 2077 6974 6820     \"\"\".    with ",
            "+0000b860: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "+0000b870: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "+0000b880: 2020 2020 2020 6f62 6a65 6374 203d 2070        object = p",
            "+0000b890: 6172 7365 5f6f 626a 6563 7428 722c 206f  arse_object(r, o",
            "+0000b8a0: 626a 6563 7469 7368 290a 0a20 2020 2020  bjectish)..     ",
            "+0000b8b0: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance",
            "+0000b8c0: 2874 6167 2c20 7374 7229 3a0a 2020 2020  (tag, str):.    ",
            "+0000b8d0: 2020 2020 2020 2020 7461 6720 3d20 7461          tag = ta",
            "+0000b8e0: 672e 656e 636f 6465 2865 6e63 6f64 696e  g.encode(encodin",
            "+0000b8f0: 6729 0a0a 2020 2020 2020 2020 6966 2061  g)..        if a",
            "+0000b900: 6e6e 6f74 6174 6564 3a0a 2020 2020 2020  nnotated:.      ",
            "+0000b910: 2020 2020 2020 2320 4372 6561 7465 2074        # Create t",
            "+0000b920: 6865 2074 6167 206f 626a 6563 740a 2020  he tag object.  ",
            "+0000b930: 2020 2020 2020 2020 2020 7461 675f 6f62            tag_ob",
            "+0000b940: 6a20 3d20 5461 6728 290a 2020 2020 2020  j = Tag().      ",
            "+0000b950: 2020 2020 2020 6966 2061 7574 686f 7220        if author ",
            "+0000b960: 6973 204e 6f6e 653a 0a20 2020 2020 2020  is None:.       ",
            "+0000b970: 2020 2020 2020 2020 2061 7574 686f 7220           author ",
            "+0000b980: 3d20 6765 745f 7573 6572 5f69 6465 6e74  = get_user_ident",
            "+0000b990: 6974 7928 722e 6765 745f 636f 6e66 6967  ity(r.get_config",
            "+0000b9a0: 5f73 7461 636b 2829 290a 2020 2020 2020  _stack()).      ",
            "+0000b9b0: 2020 2020 2020 656c 6966 2069 7369 6e73        elif isins",
            "+0000b9c0: 7461 6e63 6528 6175 7468 6f72 2c20 7374  tance(author, st",
            "+0000b9d0: 7229 3a0a 2020 2020 2020 2020 2020 2020  r):.            ",
            "+0000b9e0: 2020 2020 6175 7468 6f72 203d 2061 7574      author = aut",
            "+0000b9f0: 686f 722e 656e 636f 6465 2865 6e63 6f64  hor.encode(encod",
            "+0000ba00: 696e 6729 0a20 2020 2020 2020 2020 2020  ing).           ",
            "+0000ba10: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         ",
            "+0000ba20: 2020 2020 2020 2061 7373 6572 7420 6973         assert is",
            "+0000ba30: 696e 7374 616e 6365 2861 7574 686f 722c  instance(author,",
            "+0000ba40: 2062 7974 6573 290a 2020 2020 2020 2020   bytes).        ",
            "+0000ba50: 2020 2020 7461 675f 6f62 6a2e 7461 6767      tag_obj.tagg",
            "+0000ba60: 6572 203d 2061 7574 686f 720a 2020 2020  er = author.    ",
            "+0000ba70: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins",
            "+0000ba80: 7461 6e63 6528 6d65 7373 6167 652c 2073  tance(message, s",
            "+0000ba90: 7472 293a 0a20 2020 2020 2020 2020 2020  tr):.           ",
            "+0000baa0: 2020 2020 206d 6573 7361 6765 203d 206d       message = m",
            "+0000bab0: 6573 7361 6765 2e65 6e63 6f64 6528 656e  essage.encode(en",
            "+0000bac0: 636f 6469 6e67 290a 2020 2020 2020 2020  coding).        ",
            "+0000bad0: 2020 2020 656c 6966 2069 7369 6e73 7461      elif isinsta",
            "+0000bae0: 6e63 6528 6d65 7373 6167 652c 2062 7974  nce(message, byt",
            "+0000baf0: 6573 293a 0a20 2020 2020 2020 2020 2020  es):.           ",
            "+0000bb00: 2020 2020 2070 6173 730a 2020 2020 2020       pass.      ",
            "+0000bb10: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    ",
            "+0000bb20: 2020 2020 2020 2020 2020 2020 6d65 7373              mess",
            "+0000bb30: 6167 6520 3d20 6222 220a 2020 2020 2020  age = b\"\".      ",
            "+0000bb40: 2020 2020 2020 7461 675f 6f62 6a2e 6d65        tag_obj.me",
            "+0000bb50: 7373 6167 6520 3d20 6d65 7373 6167 6520  ssage = message ",
            "+0000bb60: 2b20 225c 6e22 2e65 6e63 6f64 6528 656e  + \"\\n\".encode(en",
            "+0000bb70: 636f 6469 6e67 290a 2020 2020 2020 2020  coding).        ",
            "+0000bb80: 2020 2020 7461 675f 6f62 6a2e 6e61 6d65      tag_obj.name",
            "+0000bb90: 203d 2074 6167 0a20 2020 2020 2020 2020   = tag.         ",
            "+0000bba0: 2020 2074 6167 5f6f 626a 2e6f 626a 6563     tag_obj.objec",
            "+0000bbb0: 7420 3d20 2874 7970 6528 6f62 6a65 6374  t = (type(object",
            "+0000bbc0: 292c 206f 626a 6563 742e 6964 290a 2020  ), object.id).  ",
            "+0000bbd0: 2020 2020 2020 2020 2020 6966 2074 6167            if tag",
            "+0000bbe0: 5f74 696d 6520 6973 204e 6f6e 653a 0a20  _time is None:. ",
            "+0000bbf0: 2020 2020 2020 2020 2020 2020 2020 2074                 t",
            "+0000bc00: 6167 5f74 696d 6520 3d20 696e 7428 7469  ag_time = int(ti",
            "+0000bc10: 6d65 2e74 696d 6528 2929 0a20 2020 2020  me.time()).     ",
            "+0000bc20: 2020 2020 2020 2074 6167 5f6f 626a 2e74         tag_obj.t",
            "+0000bc30: 6167 5f74 696d 6520 3d20 7461 675f 7469  ag_time = tag_ti",
            "+0000bc40: 6d65 0a20 2020 2020 2020 2020 2020 2069  me.            i",
            "+0000bc50: 6620 7461 675f 7469 6d65 7a6f 6e65 2069  f tag_timezone i",
            "+0000bc60: 7320 4e6f 6e65 3a0a 2020 2020 2020 2020  s None:.        ",
            "+0000bc70: 2020 2020 2020 2020 7461 675f 7469 6d65          tag_time",
            "+0000bc80: 7a6f 6e65 203d 2067 6574 5f75 7365 725f  zone = get_user_",
            "+0000bc90: 7469 6d65 7a6f 6e65 7328 295b 315d 0a20  timezones()[1]. ",
            "+0000bca0: 2020 2020 2020 2020 2020 2065 6c69 6620             elif ",
            "+0000bcb0: 6973 696e 7374 616e 6365 2874 6167 5f74  isinstance(tag_t",
            "+0000bcc0: 696d 657a 6f6e 652c 2073 7472 293a 0a20  imezone, str):. ",
            "+0000bcd0: 2020 2020 2020 2020 2020 2020 2020 2074                 t",
            "+0000bce0: 6167 5f74 696d 657a 6f6e 6520 3d20 7061  ag_timezone = pa",
            "+0000bcf0: 7273 655f 7469 6d65 7a6f 6e65 2874 6167  rse_timezone(tag",
            "+0000bd00: 5f74 696d 657a 6f6e 652e 656e 636f 6465  _timezone.encode",
            "+0000bd10: 2829 290a 2020 2020 2020 2020 2020 2020  ()).            ",
            "+0000bd20: 7461 675f 6f62 6a2e 7461 675f 7469 6d65  tag_obj.tag_time",
            "+0000bd30: 7a6f 6e65 203d 2074 6167 5f74 696d 657a  zone = tag_timez",
            "+0000bd40: 6f6e 650a 0a20 2020 2020 2020 2020 2020  one..           ",
            "+0000bd50: 2023 2043 6865 636b 2069 6620 7765 2073   # Check if we s",
            "+0000bd60: 686f 756c 6420 7369 676e 2074 6865 2074  hould sign the t",
            "+0000bd70: 6167 0a20 2020 2020 2020 2020 2020 2073  ag.            s",
            "+0000bd80: 686f 756c 645f 7369 676e 203d 2073 6967  hould_sign = sig",
            "+0000bd90: 6e0a 2020 2020 2020 2020 2020 2020 6966  n.            if",
            "+0000bda0: 2073 6967 6e20 6973 204e 6f6e 653a 0a20   sign is None:. ",
            "+0000bdb0: 2020 2020 2020 2020 2020 2020 2020 2023                 #",
            "+0000bdc0: 2043 6865 636b 2074 6167 2e67 7067 5369   Check tag.gpgSi",
            "+0000bdd0: 676e 2063 6f6e 6669 6775 7261 7469 6f6e  gn configuration",
            "+0000bde0: 2077 6865 6e20 7369 676e 2069 7320 6e6f   when sign is no",
            "+0000bdf0: 7420 6578 706c 6963 6974 6c79 2073 6574  t explicitly set",
            "+0000be00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000be10: 2063 6f6e 6669 6720 3d20 722e 6765 745f   config = r.get_",
            "+0000be20: 636f 6e66 6967 5f73 7461 636b 2829 0a20  config_stack(). ",
            "+0000be30: 2020 2020 2020 2020 2020 2020 2020 2074                 t",
            "+0000be40: 7279 3a0a 2020 2020 2020 2020 2020 2020  ry:.            ",
            "+0000be50: 2020 2020 2020 2020 7368 6f75 6c64 5f73          should_s",
            "+0000be60: 6967 6e20 3d20 636f 6e66 6967 2e67 6574  ign = config.get",
            "+0000be70: 5f62 6f6f 6c65 616e 2828 6222 7461 6722  _boolean((b\"tag\"",
            "+0000be80: 2c29 2c20 6222 6770 6753 6967 6e22 290a  ,), b\"gpgSign\").",
            "+0000be90: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000bea0: 6578 6365 7074 204b 6579 4572 726f 723a  except KeyError:",
            "+0000beb0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000bec0: 2020 2020 2073 686f 756c 645f 7369 676e       should_sign",
            "+0000bed0: 203d 2046 616c 7365 2020 2320 4465 6661   = False  # Defa",
            "+0000bee0: 756c 7420 746f 206e 6f74 2073 6967 6e69  ult to not signi",
            "+0000bef0: 6e67 2069 6620 6e6f 2063 6f6e 6669 670a  ng if no config.",
            "+0000bf00: 2020 2020 2020 2020 2020 2020 6966 2073              if s",
            "+0000bf10: 686f 756c 645f 7369 676e 3a0a 2020 2020  hould_sign:.    ",
            "+0000bf20: 2020 2020 2020 2020 2020 2020 6b65 7969              keyi",
            "+0000bf30: 6420 3d20 7369 676e 2069 6620 6973 696e  d = sign if isin",
            "+0000bf40: 7374 616e 6365 2873 6967 6e2c 2073 7472  stance(sign, str",
            "+0000bf50: 2920 656c 7365 204e 6f6e 650a 2020 2020  ) else None.    ",
            "+0000bf60: 2020 2020 2020 2020 2020 2020 2320 4966              # If",
            "+0000bf70: 2073 6967 6e20 6973 2054 7275 6520 6275   sign is True bu",
            "+0000bf80: 7420 6e6f 206b 6579 6964 2073 7065 6369  t no keyid speci",
            "+0000bf90: 6669 6564 2c20 6368 6563 6b20 7573 6572  fied, check user",
            "+0000bfa0: 2e73 6967 6e69 6e67 4b65 7920 636f 6e66  .signingKey conf",
            "+0000bfb0: 6967 0a20 2020 2020 2020 2020 2020 2020  ig.             ",
            "+0000bfc0: 2020 2069 6620 7368 6f75 6c64 5f73 6967     if should_sig",
            "+0000bfd0: 6e20 6973 2054 7275 6520 616e 6420 6b65  n is True and ke",
            "+0000bfe0: 7969 6420 6973 204e 6f6e 653a 0a20 2020  yid is None:.   ",
            "+0000bff0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000c000: 2063 6f6e 6669 6720 3d20 722e 6765 745f   config = r.get_",
            "+0000c010: 636f 6e66 6967 5f73 7461 636b 2829 0a20  config_stack(). ",
            "+0000c020: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000c030: 2020 2074 7279 3a0a 2020 2020 2020 2020     try:.        ",
            "+0000c040: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000c050: 6b65 7969 6420 3d20 636f 6e66 6967 2e67  keyid = config.g",
            "+0000c060: 6574 2828 6222 7573 6572 222c 292c 2062  et((b\"user\",), b",
            "+0000c070: 2273 6967 6e69 6e67 4b65 7922 292e 6465  \"signingKey\").de",
            "+0000c080: 636f 6465 2822 6173 6369 6922 290a 2020  code(\"ascii\").  ",
            "+0000c090: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000c0a0: 2020 6578 6365 7074 204b 6579 4572 726f    except KeyErro",
            "+0000c0b0: 723a 0a20 2020 2020 2020 2020 2020 2020  r:.             ",
            "+0000c0c0: 2020 2020 2020 2020 2020 2023 204e 6f20             # No ",
            "+0000c0d0: 7573 6572 2e73 6967 6e69 6e67 4b65 7920  user.signingKey ",
            "+0000c0e0: 636f 6e66 6967 7572 6564 2c20 7769 6c6c  configured, will",
            "+0000c0f0: 2075 7365 2064 6566 6175 6c74 2047 5047   use default GPG",
            "+0000c100: 206b 6579 0a20 2020 2020 2020 2020 2020   key.           ",
            "+0000c110: 2020 2020 2020 2020 2020 2020 2070 6173               pas",
            "+0000c120: 730a 2020 2020 2020 2020 2020 2020 2020  s.              ",
            "+0000c130: 2020 7461 675f 6f62 6a2e 7369 676e 286b    tag_obj.sign(k",
            "+0000c140: 6579 6964 290a 0a20 2020 2020 2020 2020  eyid)..         ",
            "+0000c150: 2020 2072 2e6f 626a 6563 745f 7374 6f72     r.object_stor",
            "+0000c160: 652e 6164 645f 6f62 6a65 6374 2874 6167  e.add_object(tag",
            "+0000c170: 5f6f 626a 290a 2020 2020 2020 2020 2020  _obj).          ",
            "+0000c180: 2020 7461 675f 6964 203d 2074 6167 5f6f    tag_id = tag_o",
            "+0000c190: 626a 2e69 640a 2020 2020 2020 2020 656c  bj.id.        el",
            "+0000c1a0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            ",
            "+0000c1b0: 7461 675f 6964 203d 206f 626a 6563 742e  tag_id = object.",
            "+0000c1c0: 6964 0a0a 2020 2020 2020 2020 722e 7265  id..        r.re",
            "+0000c1d0: 6673 5b5f 6d61 6b65 5f74 6167 5f72 6566  fs[_make_tag_ref",
            "+0000c1e0: 2874 6167 295d 203d 2074 6167 5f69 640a  (tag)] = tag_id.",
            "+0000c1f0: 0a0a 6465 6620 7461 675f 6c69 7374 2872  ..def tag_list(r",
            "+0000c200: 6570 6f2c 206f 7574 7374 7265 616d 3d73  epo, outstream=s",
            "+0000c210: 7973 2e73 7464 6f75 7429 3a0a 2020 2020  ys.stdout):.    ",
            "+0000c220: 2222 224c 6973 7420 616c 6c20 7461 6773  \"\"\"List all tags",
            "+0000c230: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "+0000c240: 2020 2072 6570 6f3a 2050 6174 6820 746f     repo: Path to",
            "+0000c250: 2072 6570 6f73 6974 6f72 790a 2020 2020   repository.    ",
            "+0000c260: 2020 6f75 7473 7472 6561 6d3a 2053 7472    outstream: Str",
            "+0000c270: 6561 6d20 746f 2077 7269 7465 2074 6167  eam to write tag",
            "+0000c280: 7320 746f 0a20 2020 2022 2222 0a20 2020  s to.    \"\"\".   ",
            "+0000c290: 2077 6974 6820 6f70 656e 5f72 6570 6f5f   with open_repo_",
            "+0000c2a0: 636c 6f73 696e 6728 7265 706f 2920 6173  closing(repo) as",
            "+0000c2b0: 2072 3a0a 2020 2020 2020 2020 7461 6773   r:.        tags",
            "+0000c2c0: 203d 2073 6f72 7465 6428 722e 7265 6673   = sorted(r.refs",
            "+0000c2d0: 2e61 735f 6469 6374 2862 2272 6566 732f  .as_dict(b\"refs/",
            "+0000c2e0: 7461 6773 2229 290a 2020 2020 2020 2020  tags\")).        ",
            "+0000c2f0: 7265 7475 726e 2074 6167 730a 0a0a 6465  return tags...de",
            "+0000c300: 6620 7461 675f 6465 6c65 7465 2872 6570  f tag_delete(rep",
            "+0000c310: 6f2c 206e 616d 6529 202d 3e20 4e6f 6e65  o, name) -> None",
            "+0000c320: 3a0a 2020 2020 2222 2252 656d 6f76 6520  :.    \"\"\"Remove ",
            "+0000c330: 6120 7461 672e 0a0a 2020 2020 4172 6773  a tag...    Args",
            "+0000c340: 3a0a 2020 2020 2020 7265 706f 3a20 5061  :.      repo: Pa",
            "+0000c350: 7468 2074 6f20 7265 706f 7369 746f 7279  th to repository",
            "+0000c360: 0a20 2020 2020 206e 616d 653a 204e 616d  .      name: Nam",
            "+0000c370: 6520 6f66 2074 6167 2074 6f20 7265 6d6f  e of tag to remo",
            "+0000c380: 7665 0a20 2020 2022 2222 0a20 2020 2077  ve.    \"\"\".    w",
            "+0000c390: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "+0000c3a0: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "+0000c3b0: 3a0a 2020 2020 2020 2020 6966 2069 7369  :.        if isi",
            "+0000c3c0: 6e73 7461 6e63 6528 6e61 6d65 2c20 6279  nstance(name, by",
            "+0000c3d0: 7465 7329 3a0a 2020 2020 2020 2020 2020  tes):.          ",
            "+0000c3e0: 2020 6e61 6d65 7320 3d20 5b6e 616d 655d    names = [name]",
            "+0000c3f0: 0a20 2020 2020 2020 2065 6c69 6620 6973  .        elif is",
            "+0000c400: 696e 7374 616e 6365 286e 616d 652c 206c  instance(name, l",
            "+0000c410: 6973 7429 3a0a 2020 2020 2020 2020 2020  ist):.          ",
            "+0000c420: 2020 6e61 6d65 7320 3d20 6e61 6d65 0a20    names = name. ",
            "+0000c430: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   ",
            "+0000c440: 2020 2020 2020 2020 2072 6169 7365 2045           raise E",
            "+0000c450: 7272 6f72 2866 2255 6e65 7870 6563 7465  rror(f\"Unexpecte",
            "+0000c460: 6420 7461 6720 6e61 6d65 2074 7970 6520  d tag name type ",
            "+0000c470: 7b6e 616d 6521 727d 2229 0a20 2020 2020  {name!r}\").     ",
            "+0000c480: 2020 2066 6f72 206e 616d 6520 696e 206e     for name in n",
            "+0000c490: 616d 6573 3a0a 2020 2020 2020 2020 2020  ames:.          ",
            "+0000c4a0: 2020 6465 6c20 722e 7265 6673 5b5f 6d61    del r.refs[_ma",
            "+0000c4b0: 6b65 5f74 6167 5f72 6566 286e 616d 6529  ke_tag_ref(name)",
            "+0000c4c0: 5d0a 0a0a 6465 6620 5f6d 616b 655f 6e6f  ]...def _make_no",
            "+0000c4d0: 7465 735f 7265 6628 6e61 6d65 3a20 6279  tes_ref(name: by",
            "+0000c4e0: 7465 7329 202d 3e20 6279 7465 733a 0a20  tes) -> bytes:. ",
            "+0000c4f0: 2020 2022 2222 4d61 6b65 2061 206e 6f74     \"\"\"Make a not",
            "+0000c500: 6573 2072 6566 206e 616d 652e 2222 220a  es ref name.\"\"\".",
            "+0000c510: 2020 2020 6966 206e 616d 652e 7374 6172      if name.star",
            "+0000c520: 7473 7769 7468 2862 2272 6566 732f 6e6f  tswith(b\"refs/no",
            "+0000c530: 7465 732f 2229 3a0a 2020 2020 2020 2020  tes/\"):.        ",
            "+0000c540: 7265 7475 726e 206e 616d 650a 2020 2020  return name.    ",
            "+0000c550: 7265 7475 726e 204c 4f43 414c 5f4e 4f54  return LOCAL_NOT",
            "+0000c560: 4553 5f50 5245 4649 5820 2b20 6e61 6d65  ES_PREFIX + name",
            "+0000c570: 0a0a 0a64 6566 206e 6f74 6573 5f61 6464  ...def notes_add",
            "+0000c580: 280a 2020 2020 7265 706f 2c20 6f62 6a65  (.    repo, obje",
            "+0000c590: 6374 5f73 6861 2c20 6e6f 7465 2c20 7265  ct_sha, note, re",
            "+0000c5a0: 663d 6222 636f 6d6d 6974 7322 2c20 6175  f=b\"commits\", au",
            "+0000c5b0: 7468 6f72 3d4e 6f6e 652c 2063 6f6d 6d69  thor=None, commi",
            "+0000c5c0: 7474 6572 3d4e 6f6e 652c 206d 6573 7361  tter=None, messa",
            "+0000c5d0: 6765 3d4e 6f6e 650a 293a 0a20 2020 2022  ge=None.):.    \"",
            "+0000c5e0: 2222 4164 6420 6f72 2075 7064 6174 6520  \"\"Add or update ",
            "+0000c5f0: 6120 6e6f 7465 2066 6f72 2061 6e20 6f62  a note for an ob",
            "+0000c600: 6a65 6374 2e0a 0a20 2020 2041 7267 733a  ject...    Args:",
            "+0000c610: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "+0000c620: 6820 746f 2072 6570 6f73 6974 6f72 790a  h to repository.",
            "+0000c630: 2020 2020 2020 6f62 6a65 6374 5f73 6861        object_sha",
            "+0000c640: 3a20 5348 4120 6f66 2074 6865 206f 626a  : SHA of the obj",
            "+0000c650: 6563 7420 746f 2061 6e6e 6f74 6174 650a  ect to annotate.",
            "+0000c660: 2020 2020 2020 6e6f 7465 3a20 4e6f 7465        note: Note",
            "+0000c670: 2063 6f6e 7465 6e74 0a20 2020 2020 2072   content.      r",
            "+0000c680: 6566 3a20 4e6f 7465 7320 7265 6620 746f  ef: Notes ref to",
            "+0000c690: 2075 7365 2028 6465 6661 756c 7473 2074   use (defaults t",
            "+0000c6a0: 6f20 2263 6f6d 6d69 7473 2220 666f 7220  o \"commits\" for ",
            "+0000c6b0: 7265 6673 2f6e 6f74 6573 2f63 6f6d 6d69  refs/notes/commi",
            "+0000c6c0: 7473 290a 2020 2020 2020 6175 7468 6f72  ts).      author",
            "+0000c6d0: 3a20 4175 7468 6f72 2069 6465 6e74 6974  : Author identit",
            "+0000c6e0: 7920 2864 6566 6175 6c74 7320 746f 2063  y (defaults to c",
            "+0000c6f0: 6f6d 6d69 7474 6572 290a 2020 2020 2020  ommitter).      ",
            "+0000c700: 636f 6d6d 6974 7465 723a 2043 6f6d 6d69  committer: Commi",
            "+0000c710: 7474 6572 2069 6465 6e74 6974 7920 2864  tter identity (d",
            "+0000c720: 6566 6175 6c74 7320 746f 2063 6f6e 6669  efaults to confi",
            "+0000c730: 6729 0a20 2020 2020 206d 6573 7361 6765  g).      message",
            "+0000c740: 3a20 436f 6d6d 6974 206d 6573 7361 6765  : Commit message",
            "+0000c750: 2066 6f72 2074 6865 206e 6f74 6573 2075   for the notes u",
            "+0000c760: 7064 6174 650a 0a20 2020 2052 6574 7572  pdate..    Retur",
            "+0000c770: 6e73 3a0a 2020 2020 2020 5348 4120 6f66  ns:.      SHA of",
            "+0000c780: 2074 6865 206e 6577 206e 6f74 6573 2063   the new notes c",
            "+0000c790: 6f6d 6d69 740a 2020 2020 2222 220a 2020  ommit.    \"\"\".  ",
            "+0000c7a0: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+0000c7b0: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+0000c7c0: 7320 723a 0a20 2020 2020 2020 2023 2050  s r:.        # P",
            "+0000c7d0: 6172 7365 2074 6865 206f 626a 6563 7420  arse the object ",
            "+0000c7e0: 746f 2067 6574 2069 7473 2053 4841 0a20  to get its SHA. ",
            "+0000c7f0: 2020 2020 2020 206f 626a 203d 2070 6172         obj = par",
            "+0000c800: 7365 5f6f 626a 6563 7428 722c 206f 626a  se_object(r, obj",
            "+0000c810: 6563 745f 7368 6129 0a20 2020 2020 2020  ect_sha).       ",
            "+0000c820: 206f 626a 6563 745f 7368 6120 3d20 6f62   object_sha = ob",
            "+0000c830: 6a2e 6964 0a0a 2020 2020 2020 2020 6966  j.id..        if",
            "+0000c840: 2069 7369 6e73 7461 6e63 6528 6e6f 7465   isinstance(note",
            "+0000c850: 2c20 7374 7229 3a0a 2020 2020 2020 2020  , str):.        ",
            "+0000c860: 2020 2020 6e6f 7465 203d 206e 6f74 652e      note = note.",
            "+0000c870: 656e 636f 6465 2844 4546 4155 4c54 5f45  encode(DEFAULT_E",
            "+0000c880: 4e43 4f44 494e 4729 0a20 2020 2020 2020  NCODING).       ",
            "+0000c890: 2069 6620 6973 696e 7374 616e 6365 2872   if isinstance(r",
            "+0000c8a0: 6566 2c20 7374 7229 3a0a 2020 2020 2020  ef, str):.      ",
            "+0000c8b0: 2020 2020 2020 7265 6620 3d20 7265 662e        ref = ref.",
            "+0000c8c0: 656e 636f 6465 2844 4546 4155 4c54 5f45  encode(DEFAULT_E",
            "+0000c8d0: 4e43 4f44 494e 4729 0a0a 2020 2020 2020  NCODING)..      ",
            "+0000c8e0: 2020 6e6f 7465 735f 7265 6620 3d20 5f6d    notes_ref = _m",
            "+0000c8f0: 616b 655f 6e6f 7465 735f 7265 6628 7265  ake_notes_ref(re",
            "+0000c900: 6629 0a20 2020 2020 2020 2063 6f6e 6669  f).        confi",
            "+0000c910: 6720 3d20 722e 6765 745f 636f 6e66 6967  g = r.get_config",
            "+0000c920: 5f73 7461 636b 2829 0a0a 2020 2020 2020  _stack()..      ",
            "+0000c930: 2020 7265 7475 726e 2072 2e6e 6f74 6573    return r.notes",
            "+0000c940: 2e73 6574 5f6e 6f74 6528 0a20 2020 2020  .set_note(.     ",
            "+0000c950: 2020 2020 2020 206f 626a 6563 745f 7368         object_sh",
            "+0000c960: 612c 0a20 2020 2020 2020 2020 2020 206e  a,.            n",
            "+0000c970: 6f74 652c 0a20 2020 2020 2020 2020 2020  ote,.           ",
            "+0000c980: 206e 6f74 6573 5f72 6566 2c0a 2020 2020   notes_ref,.    ",
            "+0000c990: 2020 2020 2020 2020 6175 7468 6f72 3d61          author=a",
            "+0000c9a0: 7574 686f 722c 0a20 2020 2020 2020 2020  uthor,.         ",
            "+0000c9b0: 2020 2063 6f6d 6d69 7474 6572 3d63 6f6d     committer=com",
            "+0000c9c0: 6d69 7474 6572 2c0a 2020 2020 2020 2020  mitter,.        ",
            "+0000c9d0: 2020 2020 6d65 7373 6167 653d 6d65 7373      message=mess",
            "+0000c9e0: 6167 652c 0a20 2020 2020 2020 2020 2020  age,.           ",
            "+0000c9f0: 2063 6f6e 6669 673d 636f 6e66 6967 2c0a   config=config,.",
            "+0000ca00: 2020 2020 2020 2020 290a 0a0a 6465 6620          )...def ",
            "+0000ca10: 6e6f 7465 735f 7265 6d6f 7665 280a 2020  notes_remove(.  ",
            "+0000ca20: 2020 7265 706f 2c20 6f62 6a65 6374 5f73    repo, object_s",
            "+0000ca30: 6861 2c20 7265 663d 6222 636f 6d6d 6974  ha, ref=b\"commit",
            "+0000ca40: 7322 2c20 6175 7468 6f72 3d4e 6f6e 652c  s\", author=None,",
            "+0000ca50: 2063 6f6d 6d69 7474 6572 3d4e 6f6e 652c   committer=None,",
            "+0000ca60: 206d 6573 7361 6765 3d4e 6f6e 650a 293a   message=None.):",
            "+0000ca70: 0a20 2020 2022 2222 5265 6d6f 7665 2061  .    \"\"\"Remove a",
            "+0000ca80: 206e 6f74 6520 666f 7220 616e 206f 626a   note for an obj",
            "+0000ca90: 6563 742e 0a0a 2020 2020 4172 6773 3a0a  ect...    Args:.",
            "+0000caa0: 2020 2020 2020 7265 706f 3a20 5061 7468        repo: Path",
            "+0000cab0: 2074 6f20 7265 706f 7369 746f 7279 0a20   to repository. ",
            "+0000cac0: 2020 2020 206f 626a 6563 745f 7368 613a       object_sha:",
            "+0000cad0: 2053 4841 206f 6620 7468 6520 6f62 6a65   SHA of the obje",
            "+0000cae0: 6374 2074 6f20 7265 6d6f 7665 206e 6f74  ct to remove not",
            "+0000caf0: 6573 2066 726f 6d0a 2020 2020 2020 7265  es from.      re",
            "+0000cb00: 663a 204e 6f74 6573 2072 6566 2074 6f20  f: Notes ref to ",
            "+0000cb10: 7573 6520 2864 6566 6175 6c74 7320 746f  use (defaults to",
            "+0000cb20: 2022 636f 6d6d 6974 7322 2066 6f72 2072   \"commits\" for r",
            "+0000cb30: 6566 732f 6e6f 7465 732f 636f 6d6d 6974  efs/notes/commit",
            "+0000cb40: 7329 0a20 2020 2020 2061 7574 686f 723a  s).      author:",
            "+0000cb50: 2041 7574 686f 7220 6964 656e 7469 7479   Author identity",
            "+0000cb60: 2028 6465 6661 756c 7473 2074 6f20 636f   (defaults to co",
            "+0000cb70: 6d6d 6974 7465 7229 0a20 2020 2020 2063  mmitter).      c",
            "+0000cb80: 6f6d 6d69 7474 6572 3a20 436f 6d6d 6974  ommitter: Commit",
            "+0000cb90: 7465 7220 6964 656e 7469 7479 2028 6465  ter identity (de",
            "+0000cba0: 6661 756c 7473 2074 6f20 636f 6e66 6967  faults to config",
            "+0000cbb0: 290a 2020 2020 2020 6d65 7373 6167 653a  ).      message:",
            "+0000cbc0: 2043 6f6d 6d69 7420 6d65 7373 6167 6520   Commit message ",
            "+0000cbd0: 666f 7220 7468 6520 6e6f 7465 7320 7265  for the notes re",
            "+0000cbe0: 6d6f 7661 6c0a 0a20 2020 2052 6574 7572  moval..    Retur",
            "+0000cbf0: 6e73 3a0a 2020 2020 2020 5348 4120 6f66  ns:.      SHA of",
            "+0000cc00: 2074 6865 206e 6577 206e 6f74 6573 2063   the new notes c",
            "+0000cc10: 6f6d 6d69 742c 206f 7220 4e6f 6e65 2069  ommit, or None i",
            "+0000cc20: 6620 6e6f 206e 6f74 6520 6578 6973 7465  f no note existe",
            "+0000cc30: 640a 2020 2020 2222 220a 2020 2020 7769  d.    \"\"\".    wi",
            "+0000cc40: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "+0000cc50: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "+0000cc60: 0a20 2020 2020 2020 2023 2050 6172 7365  .        # Parse",
            "+0000cc70: 2074 6865 206f 626a 6563 7420 746f 2067   the object to g",
            "+0000cc80: 6574 2069 7473 2053 4841 0a20 2020 2020  et its SHA.     ",
            "+0000cc90: 2020 206f 626a 203d 2070 6172 7365 5f6f     obj = parse_o",
            "+0000cca0: 626a 6563 7428 722c 206f 626a 6563 745f  bject(r, object_",
            "+0000ccb0: 7368 6129 0a20 2020 2020 2020 206f 626a  sha).        obj",
            "+0000ccc0: 6563 745f 7368 6120 3d20 6f62 6a2e 6964  ect_sha = obj.id",
            "+0000ccd0: 0a0a 2020 2020 2020 2020 6966 2069 7369  ..        if isi",
            "+0000cce0: 6e73 7461 6e63 6528 7265 662c 2073 7472  nstance(ref, str",
            "+0000ccf0: 293a 0a20 2020 2020 2020 2020 2020 2072  ):.            r",
            "+0000cd00: 6566 203d 2072 6566 2e65 6e63 6f64 6528  ef = ref.encode(",
            "+0000cd10: 4445 4641 554c 545f 454e 434f 4449 4e47  DEFAULT_ENCODING",
            "+0000cd20: 290a 0a20 2020 2020 2020 206e 6f74 6573  )..        notes",
            "+0000cd30: 5f72 6566 203d 205f 6d61 6b65 5f6e 6f74  _ref = _make_not",
            "+0000cd40: 6573 5f72 6566 2872 6566 290a 2020 2020  es_ref(ref).    ",
            "+0000cd50: 2020 2020 636f 6e66 6967 203d 2072 2e67      config = r.g",
            "+0000cd60: 6574 5f63 6f6e 6669 675f 7374 6163 6b28  et_config_stack(",
            "+0000cd70: 290a 0a20 2020 2020 2020 2072 6574 7572  )..        retur",
            "+0000cd80: 6e20 722e 6e6f 7465 732e 7265 6d6f 7665  n r.notes.remove",
            "+0000cd90: 5f6e 6f74 6528 0a20 2020 2020 2020 2020  _note(.         ",
            "+0000cda0: 2020 206f 626a 6563 745f 7368 612c 0a20     object_sha,. ",
            "+0000cdb0: 2020 2020 2020 2020 2020 206e 6f74 6573             notes",
            "+0000cdc0: 5f72 6566 2c0a 2020 2020 2020 2020 2020  _ref,.          ",
            "+0000cdd0: 2020 6175 7468 6f72 3d61 7574 686f 722c    author=author,",
            "+0000cde0: 0a20 2020 2020 2020 2020 2020 2063 6f6d  .            com",
            "+0000cdf0: 6d69 7474 6572 3d63 6f6d 6d69 7474 6572  mitter=committer",
            "+0000ce00: 2c0a 2020 2020 2020 2020 2020 2020 6d65  ,.            me",
            "+0000ce10: 7373 6167 653d 6d65 7373 6167 652c 0a20  ssage=message,. ",
            "+0000ce20: 2020 2020 2020 2020 2020 2063 6f6e 6669             confi",
            "+0000ce30: 673d 636f 6e66 6967 2c0a 2020 2020 2020  g=config,.      ",
            "+0000ce40: 2020 290a 0a0a 6465 6620 6e6f 7465 735f    )...def notes_",
            "+0000ce50: 7368 6f77 2872 6570 6f2c 206f 626a 6563  show(repo, objec",
            "+0000ce60: 745f 7368 612c 2072 6566 3d62 2263 6f6d  t_sha, ref=b\"com",
            "+0000ce70: 6d69 7473 2229 3a0a 2020 2020 2222 2253  mits\"):.    \"\"\"S",
            "+0000ce80: 686f 7720 7468 6520 6e6f 7465 2066 6f72  how the note for",
            "+0000ce90: 2061 6e20 6f62 6a65 6374 2e0a 0a20 2020   an object...   ",
            "+0000cea0: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "+0000ceb0: 6f3a 2050 6174 6820 746f 2072 6570 6f73  o: Path to repos",
            "+0000cec0: 6974 6f72 790a 2020 2020 2020 6f62 6a65  itory.      obje",
            "+0000ced0: 6374 5f73 6861 3a20 5348 4120 6f66 2074  ct_sha: SHA of t",
            "+0000cee0: 6865 206f 626a 6563 740a 2020 2020 2020  he object.      ",
            "+0000cef0: 7265 663a 204e 6f74 6573 2072 6566 2074  ref: Notes ref t",
            "+0000cf00: 6f20 7573 6520 2864 6566 6175 6c74 7320  o use (defaults ",
            "+0000cf10: 746f 2022 636f 6d6d 6974 7322 2066 6f72  to \"commits\" for",
            "+0000cf20: 2072 6566 732f 6e6f 7465 732f 636f 6d6d   refs/notes/comm",
            "+0000cf30: 6974 7329 0a0a 2020 2020 5265 7475 726e  its)..    Return",
            "+0000cf40: 733a 0a20 2020 2020 204e 6f74 6520 636f  s:.      Note co",
            "+0000cf50: 6e74 656e 7420 6173 2062 7974 6573 2c20  ntent as bytes, ",
            "+0000cf60: 6f72 204e 6f6e 6520 6966 206e 6f20 6e6f  or None if no no",
            "+0000cf70: 7465 2065 7869 7374 730a 2020 2020 2222  te exists.    \"\"",
            "+0000cf80: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "+0000cf90: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+0000cfa0: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "+0000cfb0: 2023 2050 6172 7365 2074 6865 206f 626a   # Parse the obj",
            "+0000cfc0: 6563 7420 746f 2067 6574 2069 7473 2053  ect to get its S",
            "+0000cfd0: 4841 0a20 2020 2020 2020 206f 626a 203d  HA.        obj =",
            "+0000cfe0: 2070 6172 7365 5f6f 626a 6563 7428 722c   parse_object(r,",
            "+0000cff0: 206f 626a 6563 745f 7368 6129 0a20 2020   object_sha).   ",
            "+0000d000: 2020 2020 206f 626a 6563 745f 7368 6120       object_sha ",
            "+0000d010: 3d20 6f62 6a2e 6964 0a0a 2020 2020 2020  = obj.id..      ",
            "+0000d020: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(",
            "+0000d030: 7265 662c 2073 7472 293a 0a20 2020 2020  ref, str):.     ",
            "+0000d040: 2020 2020 2020 2072 6566 203d 2072 6566         ref = ref",
            "+0000d050: 2e65 6e63 6f64 6528 4445 4641 554c 545f  .encode(DEFAULT_",
            "+0000d060: 454e 434f 4449 4e47 290a 0a20 2020 2020  ENCODING)..     ",
            "+0000d070: 2020 206e 6f74 6573 5f72 6566 203d 205f     notes_ref = _",
            "+0000d080: 6d61 6b65 5f6e 6f74 6573 5f72 6566 2872  make_notes_ref(r",
            "+0000d090: 6566 290a 2020 2020 2020 2020 636f 6e66  ef).        conf",
            "+0000d0a0: 6967 203d 2072 2e67 6574 5f63 6f6e 6669  ig = r.get_confi",
            "+0000d0b0: 675f 7374 6163 6b28 290a 0a20 2020 2020  g_stack()..     ",
            "+0000d0c0: 2020 2072 6574 7572 6e20 722e 6e6f 7465     return r.note",
            "+0000d0d0: 732e 6765 745f 6e6f 7465 286f 626a 6563  s.get_note(objec",
            "+0000d0e0: 745f 7368 612c 206e 6f74 6573 5f72 6566  t_sha, notes_ref",
            "+0000d0f0: 2c20 636f 6e66 6967 3d63 6f6e 6669 6729  , config=config)",
            "+0000d100: 0a0a 0a64 6566 206e 6f74 6573 5f6c 6973  ...def notes_lis",
            "+0000d110: 7428 7265 706f 2c20 7265 663d 6222 636f  t(repo, ref=b\"co",
            "+0000d120: 6d6d 6974 7322 293a 0a20 2020 2022 2222  mmits\"):.    \"\"\"",
            "+0000d130: 4c69 7374 2061 6c6c 206e 6f74 6573 2069  List all notes i",
            "+0000d140: 6e20 6120 6e6f 7465 7320 7265 662e 0a0a  n a notes ref...",
            "+0000d150: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+0000d160: 7265 706f 3a20 5061 7468 2074 6f20 7265  repo: Path to re",
            "+0000d170: 706f 7369 746f 7279 0a20 2020 2020 2072  pository.      r",
            "+0000d180: 6566 3a20 4e6f 7465 7320 7265 6620 746f  ef: Notes ref to",
            "+0000d190: 2075 7365 2028 6465 6661 756c 7473 2074   use (defaults t",
            "+0000d1a0: 6f20 2263 6f6d 6d69 7473 2220 666f 7220  o \"commits\" for ",
            "+0000d1b0: 7265 6673 2f6e 6f74 6573 2f63 6f6d 6d69  refs/notes/commi",
            "+0000d1c0: 7473 290a 0a20 2020 2052 6574 7572 6e73  ts)..    Returns",
            "+0000d1d0: 3a0a 2020 2020 2020 4c69 7374 206f 6620  :.      List of ",
            "+0000d1e0: 7475 706c 6573 206f 6620 286f 626a 6563  tuples of (objec",
            "+0000d1f0: 745f 7368 612c 206e 6f74 655f 636f 6e74  t_sha, note_cont",
            "+0000d200: 656e 7429 0a20 2020 2022 2222 0a20 2020  ent).    \"\"\".   ",
            "+0000d210: 2077 6974 6820 6f70 656e 5f72 6570 6f5f   with open_repo_",
            "+0000d220: 636c 6f73 696e 6728 7265 706f 2920 6173  closing(repo) as",
            "+0000d230: 2072 3a0a 2020 2020 2020 2020 6966 2069   r:.        if i",
            "+0000d240: 7369 6e73 7461 6e63 6528 7265 662c 2073  sinstance(ref, s",
            "+0000d250: 7472 293a 0a20 2020 2020 2020 2020 2020  tr):.           ",
            "+0000d260: 2072 6566 203d 2072 6566 2e65 6e63 6f64   ref = ref.encod",
            "+0000d270: 6528 4445 4641 554c 545f 454e 434f 4449  e(DEFAULT_ENCODI",
            "+0000d280: 4e47 290a 0a20 2020 2020 2020 206e 6f74  NG)..        not",
            "+0000d290: 6573 5f72 6566 203d 205f 6d61 6b65 5f6e  es_ref = _make_n",
            "+0000d2a0: 6f74 6573 5f72 6566 2872 6566 290a 2020  otes_ref(ref).  ",
            "+0000d2b0: 2020 2020 2020 636f 6e66 6967 203d 2072        config = r",
            "+0000d2c0: 2e67 6574 5f63 6f6e 6669 675f 7374 6163  .get_config_stac",
            "+0000d2d0: 6b28 290a 0a20 2020 2020 2020 2072 6574  k()..        ret",
            "+0000d2e0: 7572 6e20 722e 6e6f 7465 732e 6c69 7374  urn r.notes.list",
            "+0000d2f0: 5f6e 6f74 6573 286e 6f74 6573 5f72 6566  _notes(notes_ref",
            "+0000d300: 2c20 636f 6e66 6967 3d63 6f6e 6669 6729  , config=config)",
            "+0000d310: 0a0a 0a64 6566 2072 6573 6574 2872 6570  ...def reset(rep",
            "+0000d320: 6f2c 206d 6f64 652c 2074 7265 6569 7368  o, mode, treeish",
            "+0000d330: 3d22 4845 4144 2229 202d 3e20 4e6f 6e65  =\"HEAD\") -> None",
            "+0000d340: 3a0a 2020 2020 2222 2252 6573 6574 2063  :.    \"\"\"Reset c",
            "+0000d350: 7572 7265 6e74 2048 4541 4420 746f 2074  urrent HEAD to t",
            "+0000d360: 6865 2073 7065 6369 6669 6564 2073 7461  he specified sta",
            "+0000d370: 7465 2e0a 0a20 2020 2041 7267 733a 0a20  te...    Args:. ",
            "+0000d380: 2020 2020 2072 6570 6f3a 2050 6174 6820       repo: Path ",
            "+0000d390: 746f 2072 6570 6f73 6974 6f72 790a 2020  to repository.  ",
            "+0000d3a0: 2020 2020 6d6f 6465 3a20 4d6f 6465 2028      mode: Mode (",
            "+0000d3b0: 2268 6172 6422 2c20 2273 6f66 7422 2c20  \"hard\", \"soft\", ",
            "+0000d3c0: 226d 6978 6564 2229 0a20 2020 2020 2074  \"mixed\").      t",
            "+0000d3d0: 7265 6569 7368 3a20 5472 6565 6973 6820  reeish: Treeish ",
            "+0000d3e0: 746f 2072 6573 6574 2074 6f0a 2020 2020  to reset to.    ",
            "+0000d3f0: 2222 220a 2020 2020 7769 7468 206f 7065  \"\"\".    with ope",
            "+0000d400: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "+0000d410: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "+0000d420: 2020 2023 2050 6172 7365 2074 6865 2074     # Parse the t",
            "+0000d430: 6172 6765 7420 7472 6565 0a20 2020 2020  arget tree.     ",
            "+0000d440: 2020 2074 7265 6520 3d20 7061 7273 655f     tree = parse_",
            "+0000d450: 7472 6565 2872 2c20 7472 6565 6973 6829  tree(r, treeish)",
            "+0000d460: 0a20 2020 2020 2020 2074 6172 6765 745f  .        target_",
            "+0000d470: 636f 6d6d 6974 203d 2070 6172 7365 5f63  commit = parse_c",
            "+0000d480: 6f6d 6d69 7428 722c 2074 7265 6569 7368  ommit(r, treeish",
            "+0000d490: 290a 0a20 2020 2020 2020 2023 2055 7064  )..        # Upd",
            "+0000d4a0: 6174 6520 4845 4144 2074 6f20 706f 696e  ate HEAD to poin",
            "+0000d4b0: 7420 746f 2074 6865 2074 6172 6765 7420  t to the target ",
            "+0000d4c0: 636f 6d6d 6974 0a20 2020 2020 2020 2072  commit.        r",
            "+0000d4d0: 2e72 6566 735b 6222 4845 4144 225d 203d  .refs[b\"HEAD\"] =",
            "+0000d4e0: 2074 6172 6765 745f 636f 6d6d 6974 2e69   target_commit.i",
            "+0000d4f0: 640a 0a20 2020 2020 2020 2069 6620 6d6f  d..        if mo",
            "+0000d500: 6465 203d 3d20 2273 6f66 7422 3a0a 2020  de == \"soft\":.  ",
            "+0000d510: 2020 2020 2020 2020 2020 2320 536f 6674            # Soft",
            "+0000d520: 2072 6573 6574 3a20 6f6e 6c79 2075 7064   reset: only upd",
            "+0000d530: 6174 6520 4845 4144 2c20 6c65 6176 6520  ate HEAD, leave ",
            "+0000d540: 696e 6465 7820 616e 6420 776f 726b 696e  index and workin",
            "+0000d550: 6720 7472 6565 2075 6e63 6861 6e67 6564  g tree unchanged",
            "+0000d560: 0a20 2020 2020 2020 2020 2020 2072 6574  .            ret",
            "+0000d570: 7572 6e0a 0a20 2020 2020 2020 2065 6c69  urn..        eli",
            "+0000d580: 6620 6d6f 6465 203d 3d20 226d 6978 6564  f mode == \"mixed",
            "+0000d590: 223a 0a20 2020 2020 2020 2020 2020 2023  \":.            #",
            "+0000d5a0: 204d 6978 6564 2072 6573 6574 3a20 7570   Mixed reset: up",
            "+0000d5b0: 6461 7465 2048 4541 4420 616e 6420 696e  date HEAD and in",
            "+0000d5c0: 6465 782c 2062 7574 206c 6561 7665 2077  dex, but leave w",
            "+0000d5d0: 6f72 6b69 6e67 2074 7265 6520 756e 6368  orking tree unch",
            "+0000d5e0: 616e 6765 640a 2020 2020 2020 2020 2020  anged.          ",
            "+0000d5f0: 2020 6672 6f6d 202e 696e 6465 7820 696d    from .index im",
            "+0000d600: 706f 7274 2049 6e64 6578 456e 7472 790a  port IndexEntry.",
            "+0000d610: 2020 2020 2020 2020 2020 2020 6672 6f6d              from",
            "+0000d620: 202e 6f62 6a65 6374 5f73 746f 7265 2069   .object_store i",
            "+0000d630: 6d70 6f72 7420 6974 6572 5f74 7265 655f  mport iter_tree_",
            "+0000d640: 636f 6e74 656e 7473 0a0a 2020 2020 2020  contents..      ",
            "+0000d650: 2020 2020 2020 2320 4f70 656e 2074 6865        # Open the",
            "+0000d660: 2069 6e64 6578 0a20 2020 2020 2020 2020   index.         ",
            "+0000d670: 2020 2069 6e64 6578 203d 2072 2e6f 7065     index = r.ope",
            "+0000d680: 6e5f 696e 6465 7828 290a 0a20 2020 2020  n_index()..     ",
            "+0000d690: 2020 2020 2020 2023 2043 6c65 6172 2074         # Clear t",
            "+0000d6a0: 6865 2063 7572 7265 6e74 2069 6e64 6578  he current index",
            "+0000d6b0: 0a20 2020 2020 2020 2020 2020 2069 6e64  .            ind",
            "+0000d6c0: 6578 2e63 6c65 6172 2829 0a0a 2020 2020  ex.clear()..    ",
            "+0000d6d0: 2020 2020 2020 2020 2320 506f 7075 6c61          # Popula",
            "+0000d6e0: 7465 2069 6e64 6578 2066 726f 6d20 7468  te index from th",
            "+0000d6f0: 6520 7461 7267 6574 2074 7265 650a 2020  e target tree.  ",
            "+0000d700: 2020 2020 2020 2020 2020 666f 7220 656e            for en",
            "+0000d710: 7472 7920 696e 2069 7465 725f 7472 6565  try in iter_tree",
            "+0000d720: 5f63 6f6e 7465 6e74 7328 722e 6f62 6a65  _contents(r.obje",
            "+0000d730: 6374 5f73 746f 7265 2c20 7472 6565 2e69  ct_store, tree.i",
            "+0000d740: 6429 3a0a 2020 2020 2020 2020 2020 2020  d):.            ",
            "+0000d750: 2020 2020 2320 4372 6561 7465 2061 6e20      # Create an ",
            "+0000d760: 496e 6465 7845 6e74 7279 2066 726f 6d20  IndexEntry from ",
            "+0000d770: 7468 6520 7472 6565 2065 6e74 7279 0a20  the tree entry. ",
            "+0000d780: 2020 2020 2020 2020 2020 2020 2020 2023                 #",
            "+0000d790: 2055 7365 207a 6572 6f73 2066 6f72 2066   Use zeros for f",
            "+0000d7a0: 696c 6573 7973 7465 6d2d 7370 6563 6966  ilesystem-specif",
            "+0000d7b0: 6963 2066 6965 6c64 7320 7369 6e63 6520  ic fields since ",
            "+0000d7c0: 7765 2772 6520 6e6f 7420 746f 7563 6869  we're not touchi",
            "+0000d7d0: 6e67 2074 6865 2077 6f72 6b69 6e67 2074  ng the working t",
            "+0000d7e0: 7265 650a 2020 2020 2020 2020 2020 2020  ree.            ",
            "+0000d7f0: 2020 2020 696e 6465 785f 656e 7472 7920      index_entry ",
            "+0000d800: 3d20 496e 6465 7845 6e74 7279 280a 2020  = IndexEntry(.  ",
            "+0000d810: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000d820: 2020 6374 696d 653d 2830 2c20 3029 2c0a    ctime=(0, 0),.",
            "+0000d830: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000d840: 2020 2020 6d74 696d 653d 2830 2c20 3029      mtime=(0, 0)",
            "+0000d850: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              ",
            "+0000d860: 2020 2020 2020 6465 763d 302c 0a20 2020        dev=0,.   ",
            "+0000d870: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000d880: 2069 6e6f 3d30 2c0a 2020 2020 2020 2020   ino=0,.        ",
            "+0000d890: 2020 2020 2020 2020 2020 2020 6d6f 6465              mode",
            "+0000d8a0: 3d65 6e74 7279 2e6d 6f64 652c 0a20 2020  =entry.mode,.   ",
            "+0000d8b0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000d8c0: 2075 6964 3d30 2c0a 2020 2020 2020 2020   uid=0,.        ",
            "+0000d8d0: 2020 2020 2020 2020 2020 2020 6769 643d              gid=",
            "+0000d8e0: 302c 0a20 2020 2020 2020 2020 2020 2020  0,.             ",
            "+0000d8f0: 2020 2020 2020 2073 697a 653d 302c 2020         size=0,  ",
            "+0000d900: 2320 5369 7a65 2077 696c 6c20 6265 2030  # Size will be 0",
            "+0000d910: 2073 696e 6365 2077 6527 7265 206e 6f74   since we're not",
            "+0000d920: 2072 6561 6469 6e67 2066 726f 6d20 6469   reading from di",
            "+0000d930: 736b 0a20 2020 2020 2020 2020 2020 2020  sk.             ",
            "+0000d940: 2020 2020 2020 2073 6861 3d65 6e74 7279         sha=entry",
            "+0000d950: 2e73 6861 2c0a 2020 2020 2020 2020 2020  .sha,.          ",
            "+0000d960: 2020 2020 2020 2020 2020 666c 6167 733d            flags=",
            "+0000d970: 302c 0a20 2020 2020 2020 2020 2020 2020  0,.             ",
            "+0000d980: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           ",
            "+0000d990: 2020 2020 2069 6e64 6578 5b65 6e74 7279       index[entry",
            "+0000d9a0: 2e70 6174 685d 203d 2069 6e64 6578 5f65  .path] = index_e",
            "+0000d9b0: 6e74 7279 0a0a 2020 2020 2020 2020 2020  ntry..          ",
            "+0000d9c0: 2020 2320 5772 6974 6520 7468 6520 7570    # Write the up",
            "+0000d9d0: 6461 7465 6420 696e 6465 780a 2020 2020  dated index.    ",
            "+0000d9e0: 2020 2020 2020 2020 696e 6465 782e 7772          index.wr",
            "+0000d9f0: 6974 6528 290a 0a20 2020 2020 2020 2065  ite()..        e",
            "+0000da00: 6c69 6620 6d6f 6465 203d 3d20 2268 6172  lif mode == \"har",
            "+0000da10: 6422 3a0a 2020 2020 2020 2020 2020 2020  d\":.            ",
            "+0000da20: 2320 4861 7264 2072 6573 6574 3a20 7570  # Hard reset: up",
            "+0000da30: 6461 7465 2048 4541 442c 2069 6e64 6578  date HEAD, index",
            "+0000da40: 2c20 616e 6420 776f 726b 696e 6720 7472  , and working tr",
            "+0000da50: 6565 0a20 2020 2020 2020 2020 2020 2023  ee.            #",
            "+0000da60: 2047 6574 2063 7572 7265 6e74 2048 4541   Get current HEA",
            "+0000da70: 4420 7472 6565 2066 6f72 2063 6f6d 7061  D tree for compa",
            "+0000da80: 7269 736f 6e0a 2020 2020 2020 2020 2020  rison.          ",
            "+0000da90: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         ",
            "+0000daa0: 2020 2020 2020 2063 7572 7265 6e74 5f68         current_h",
            "+0000dab0: 6561 6420 3d20 722e 7265 6673 5b62 2248  ead = r.refs[b\"H",
            "+0000dac0: 4541 4422 5d0a 2020 2020 2020 2020 2020  EAD\"].          ",
            "+0000dad0: 2020 2020 2020 6375 7272 656e 745f 7472        current_tr",
            "+0000dae0: 6565 203d 2072 5b63 7572 7265 6e74 5f68  ee = r[current_h",
            "+0000daf0: 6561 645d 2e74 7265 650a 2020 2020 2020  ead].tree.      ",
            "+0000db00: 2020 2020 2020 6578 6365 7074 204b 6579        except Key",
            "+0000db10: 4572 726f 723a 0a20 2020 2020 2020 2020  Error:.         ",
            "+0000db20: 2020 2020 2020 2063 7572 7265 6e74 5f74         current_t",
            "+0000db30: 7265 6520 3d20 4e6f 6e65 0a0a 2020 2020  ree = None..    ",
            "+0000db40: 2020 2020 2020 2020 2320 4765 7420 636f          # Get co",
            "+0000db50: 6e66 6967 7572 6174 696f 6e20 666f 7220  nfiguration for ",
            "+0000db60: 776f 726b 696e 6720 6469 7265 6374 6f72  working director",
            "+0000db70: 7920 7570 6461 7465 0a20 2020 2020 2020  y update.       ",
            "+0000db80: 2020 2020 2063 6f6e 6669 6720 3d20 722e       config = r.",
            "+0000db90: 6765 745f 636f 6e66 6967 2829 0a20 2020  get_config().   ",
            "+0000dba0: 2020 2020 2020 2020 2068 6f6e 6f72 5f66           honor_f",
            "+0000dbb0: 696c 656d 6f64 6520 3d20 636f 6e66 6967  ilemode = config",
            "+0000dbc0: 2e67 6574 5f62 6f6f 6c65 616e 2862 2263  .get_boolean(b\"c",
            "+0000dbd0: 6f72 6522 2c20 6222 6669 6c65 6d6f 6465  ore\", b\"filemode",
            "+0000dbe0: 222c 206f 732e 6e61 6d65 2021 3d20 226e  \", os.name != \"n",
            "+0000dbf0: 7422 290a 0a20 2020 2020 2020 2020 2020  t\")..           ",
            "+0000dc00: 2023 2049 6d70 6f72 7420 7661 6c69 6461   # Import valida",
            "+0000dc10: 7469 6f6e 2066 756e 6374 696f 6e73 0a20  tion functions. ",
            "+0000dc20: 2020 2020 2020 2020 2020 2066 726f 6d20             from ",
            "+0000dc30: 2e69 6e64 6578 2069 6d70 6f72 7420 280a  .index import (.",
            "+0000dc40: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000dc50: 7661 6c69 6461 7465 5f70 6174 685f 656c  validate_path_el",
            "+0000dc60: 656d 656e 745f 6465 6661 756c 742c 0a20  ement_default,. ",
            "+0000dc70: 2020 2020 2020 2020 2020 2020 2020 2076                 v",
            "+0000dc80: 616c 6964 6174 655f 7061 7468 5f65 6c65  alidate_path_ele",
            "+0000dc90: 6d65 6e74 5f68 6673 2c0a 2020 2020 2020  ment_hfs,.      ",
            "+0000dca0: 2020 2020 2020 2020 2020 7661 6c69 6461            valida",
            "+0000dcb0: 7465 5f70 6174 685f 656c 656d 656e 745f  te_path_element_",
            "+0000dcc0: 6e74 6673 2c0a 2020 2020 2020 2020 2020  ntfs,.          ",
            "+0000dcd0: 2020 290a 0a20 2020 2020 2020 2020 2020    )..           ",
            "+0000dce0: 2069 6620 636f 6e66 6967 2e67 6574 5f62   if config.get_b",
            "+0000dcf0: 6f6f 6c65 616e 2862 2263 6f72 6522 2c20  oolean(b\"core\", ",
            "+0000dd00: 6222 636f 7265 2e70 726f 7465 6374 4e54  b\"core.protectNT",
            "+0000dd10: 4653 222c 206f 732e 6e61 6d65 203d 3d20  FS\", os.name == ",
            "+0000dd20: 226e 7422 293a 0a20 2020 2020 2020 2020  \"nt\"):.         ",
            "+0000dd30: 2020 2020 2020 2076 616c 6964 6174 655f         validate_",
            "+0000dd40: 7061 7468 5f65 6c65 6d65 6e74 203d 2076  path_element = v",
            "+0000dd50: 616c 6964 6174 655f 7061 7468 5f65 6c65  alidate_path_ele",
            "+0000dd60: 6d65 6e74 5f6e 7466 730a 2020 2020 2020  ment_ntfs.      ",
            "+0000dd70: 2020 2020 2020 656c 6966 2063 6f6e 6669        elif confi",
            "+0000dd80: 672e 6765 745f 626f 6f6c 6561 6e28 0a20  g.get_boolean(. ",
            "+0000dd90: 2020 2020 2020 2020 2020 2020 2020 2062                 b",
            "+0000dda0: 2263 6f72 6522 2c20 6222 636f 7265 2e70  \"core\", b\"core.p",
            "+0000ddb0: 726f 7465 6374 4846 5322 2c20 7379 732e  rotectHFS\", sys.",
            "+0000ddc0: 706c 6174 666f 726d 203d 3d20 2264 6172  platform == \"dar",
            "+0000ddd0: 7769 6e22 0a20 2020 2020 2020 2020 2020  win\".           ",
            "+0000dde0: 2029 3a0a 2020 2020 2020 2020 2020 2020   ):.            ",
            "+0000ddf0: 2020 2020 7661 6c69 6461 7465 5f70 6174      validate_pat",
            "+0000de00: 685f 656c 656d 656e 7420 3d20 7661 6c69  h_element = vali",
            "+0000de10: 6461 7465 5f70 6174 685f 656c 656d 656e  date_path_elemen",
            "+0000de20: 745f 6866 730a 2020 2020 2020 2020 2020  t_hfs.          ",
            "+0000de30: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        ",
            "+0000de40: 2020 2020 2020 2020 7661 6c69 6461 7465          validate",
            "+0000de50: 5f70 6174 685f 656c 656d 656e 7420 3d20  _path_element = ",
            "+0000de60: 7661 6c69 6461 7465 5f70 6174 685f 656c  validate_path_el",
            "+0000de70: 656d 656e 745f 6465 6661 756c 740a 0a20  ement_default.. ",
            "+0000de80: 2020 2020 2020 2020 2020 2069 6620 636f             if co",
            "+0000de90: 6e66 6967 2e67 6574 5f62 6f6f 6c65 616e  nfig.get_boolean",
            "+0000dea0: 2862 2263 6f72 6522 2c20 6222 7379 6d6c  (b\"core\", b\"syml",
            "+0000deb0: 696e 6b73 222c 2054 7275 6529 3a0a 2020  inks\", True):.  ",
            "+0000dec0: 2020 2020 2020 2020 2020 2020 2020 2320                # ",
            "+0000ded0: 496d 706f 7274 2073 796d 6c69 6e6b 2066  Import symlink f",
            "+0000dee0: 756e 6374 696f 6e0a 2020 2020 2020 2020  unction.        ",
            "+0000def0: 2020 2020 2020 2020 6672 6f6d 202e 696e          from .in",
            "+0000df00: 6465 7820 696d 706f 7274 2073 796d 6c69  dex import symli",
            "+0000df10: 6e6b 0a0a 2020 2020 2020 2020 2020 2020  nk..            ",
            "+0000df20: 2020 2020 7379 6d6c 696e 6b5f 666e 203d      symlink_fn =",
            "+0000df30: 2073 796d 6c69 6e6b 0a20 2020 2020 2020   symlink.       ",
            "+0000df40: 2020 2020 2065 6c73 653a 0a0a 2020 2020       else:..    ",
            "+0000df50: 2020 2020 2020 2020 2020 2020 6465 6620              def ",
            "+0000df60: 7379 6d6c 696e 6b5f 666e 2820 2023 2074  symlink_fn(  # t",
            "+0000df70: 7970 653a 2069 676e 6f72 650a 2020 2020  ype: ignore.    ",
            "+0000df80: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000df90: 736f 7572 6365 2c20 7461 7267 6574 2c20  source, target, ",
            "+0000dfa0: 7461 7267 6574 5f69 735f 6469 7265 6374  target_is_direct",
            "+0000dfb0: 6f72 793d 4661 6c73 652c 202a 2c20 6469  ory=False, *, di",
            "+0000dfc0: 725f 6664 3d4e 6f6e 650a 2020 2020 2020  r_fd=None.      ",
            "+0000dfd0: 2020 2020 2020 2020 2020 2920 2d3e 204e            ) -> N",
            "+0000dfe0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           ",
            "+0000dff0: 2020 2020 2020 2020 206d 6f64 6520 3d20           mode = ",
            "+0000e000: 2277 2220 2b20 2822 6222 2069 6620 6973  \"w\" + (\"b\" if is",
            "+0000e010: 696e 7374 616e 6365 2873 6f75 7263 652c  instance(source,",
            "+0000e020: 2062 7974 6573 2920 656c 7365 2022 2229   bytes) else \"\")",
            "+0000e030: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000e040: 2020 2020 2077 6974 6820 6f70 656e 2874       with open(t",
            "+0000e050: 6172 6765 742c 206d 6f64 6529 2061 7320  arget, mode) as ",
            "+0000e060: 663a 0a20 2020 2020 2020 2020 2020 2020  f:.             ",
            "+0000e070: 2020 2020 2020 2020 2020 2066 2e77 7269             f.wri",
            "+0000e080: 7465 2873 6f75 7263 6529 0a0a 2020 2020  te(source)..    ",
            "+0000e090: 2020 2020 2020 2020 2320 5570 6461 7465          # Update",
            "+0000e0a0: 2077 6f72 6b69 6e67 2074 7265 6520 616e   working tree an",
            "+0000e0b0: 6420 696e 6465 780a 2020 2020 2020 2020  d index.        ",
            "+0000e0c0: 2020 2020 626c 6f62 5f6e 6f72 6d61 6c69      blob_normali",
            "+0000e0d0: 7a65 7220 3d20 722e 6765 745f 626c 6f62  zer = r.get_blob",
            "+0000e0e0: 5f6e 6f72 6d61 6c69 7a65 7228 290a 2020  _normalizer().  ",
            "+0000e0f0: 2020 2020 2020 2020 2020 7570 6461 7465            update",
            "+0000e100: 5f77 6f72 6b69 6e67 5f74 7265 6528 0a20  _working_tree(. ",
            "+0000e110: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+0000e120: 2c0a 2020 2020 2020 2020 2020 2020 2020  ,.              ",
            "+0000e130: 2020 6375 7272 656e 745f 7472 6565 2c0a    current_tree,.",
            "+0000e140: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000e150: 7472 6565 2e69 642c 0a20 2020 2020 2020  tree.id,.       ",
            "+0000e160: 2020 2020 2020 2020 2068 6f6e 6f72 5f66           honor_f",
            "+0000e170: 696c 656d 6f64 653d 686f 6e6f 725f 6669  ilemode=honor_fi",
            "+0000e180: 6c65 6d6f 6465 2c0a 2020 2020 2020 2020  lemode,.        ",
            "+0000e190: 2020 2020 2020 2020 7661 6c69 6461 7465          validate",
            "+0000e1a0: 5f70 6174 685f 656c 656d 656e 743d 7661  _path_element=va",
            "+0000e1b0: 6c69 6461 7465 5f70 6174 685f 656c 656d  lidate_path_elem",
            "+0000e1c0: 656e 742c 0a20 2020 2020 2020 2020 2020  ent,.           ",
            "+0000e1d0: 2020 2020 2073 796d 6c69 6e6b 5f66 6e3d       symlink_fn=",
            "+0000e1e0: 7379 6d6c 696e 6b5f 666e 2c0a 2020 2020  symlink_fn,.    ",
            "+0000e1f0: 2020 2020 2020 2020 2020 2020 666f 7263              forc",
            "+0000e200: 655f 7265 6d6f 7665 5f75 6e74 7261 636b  e_remove_untrack",
            "+0000e210: 6564 3d54 7275 652c 0a20 2020 2020 2020  ed=True,.       ",
            "+0000e220: 2020 2020 2020 2020 2062 6c6f 625f 6e6f           blob_no",
            "+0000e230: 726d 616c 697a 6572 3d62 6c6f 625f 6e6f  rmalizer=blob_no",
            "+0000e240: 726d 616c 697a 6572 2c0a 2020 2020 2020  rmalizer,.      ",
            "+0000e250: 2020 2020 2020 290a 2020 2020 2020 2020        ).        ",
            "+0000e260: 656c 7365 3a0a 2020 2020 2020 2020 2020  else:.          ",
            "+0000e270: 2020 7261 6973 6520 4572 726f 7228 6622    raise Error(f\"",
            "+0000e280: 496e 7661 6c69 6420 7265 7365 7420 6d6f  Invalid reset mo",
            "+0000e290: 6465 3a20 7b6d 6f64 657d 2229 0a0a 0a64  de: {mode}\")...d",
            "+0000e2a0: 6566 2067 6574 5f72 656d 6f74 655f 7265  ef get_remote_re",
            "+0000e2b0: 706f 280a 2020 2020 7265 706f 3a20 5265  po(.    repo: Re",
            "+0000e2c0: 706f 2c20 7265 6d6f 7465 5f6c 6f63 6174  po, remote_locat",
            "+0000e2d0: 696f 6e3a 204f 7074 696f 6e61 6c5b 556e  ion: Optional[Un",
            "+0000e2e0: 696f 6e5b 7374 722c 2062 7974 6573 5d5d  ion[str, bytes]]",
            "+0000e2f0: 203d 204e 6f6e 650a 2920 2d3e 2074 7570   = None.) -> tup",
            "+0000e300: 6c65 5b4f 7074 696f 6e61 6c5b 7374 725d  le[Optional[str]",
            "+0000e310: 2c20 7374 725d 3a0a 2020 2020 636f 6e66  , str]:.    conf",
            "+0000e320: 6967 203d 2072 6570 6f2e 6765 745f 636f  ig = repo.get_co",
            "+0000e330: 6e66 6967 2829 0a20 2020 2069 6620 7265  nfig().    if re",
            "+0000e340: 6d6f 7465 5f6c 6f63 6174 696f 6e20 6973  mote_location is",
            "+0000e350: 204e 6f6e 653a 0a20 2020 2020 2020 2072   None:.        r",
            "+0000e360: 656d 6f74 655f 6c6f 6361 7469 6f6e 203d  emote_location =",
            "+0000e370: 2067 6574 5f62 7261 6e63 685f 7265 6d6f   get_branch_remo",
            "+0000e380: 7465 2872 6570 6f29 0a20 2020 2069 6620  te(repo).    if ",
            "+0000e390: 6973 696e 7374 616e 6365 2872 656d 6f74  isinstance(remot",
            "+0000e3a0: 655f 6c6f 6361 7469 6f6e 2c20 7374 7229  e_location, str)",
            "+0000e3b0: 3a0a 2020 2020 2020 2020 656e 636f 6465  :.        encode",
            "+0000e3c0: 645f 6c6f 6361 7469 6f6e 203d 2072 656d  d_location = rem",
            "+0000e3d0: 6f74 655f 6c6f 6361 7469 6f6e 2e65 6e63  ote_location.enc",
            "+0000e3e0: 6f64 6528 290a 2020 2020 656c 7365 3a0a  ode().    else:.",
            "+0000e3f0: 2020 2020 2020 2020 656e 636f 6465 645f          encoded_",
            "+0000e400: 6c6f 6361 7469 6f6e 203d 2072 656d 6f74  location = remot",
            "+0000e410: 655f 6c6f 6361 7469 6f6e 0a0a 2020 2020  e_location..    ",
            "+0000e420: 7365 6374 696f 6e20 3d20 2862 2272 656d  section = (b\"rem",
            "+0000e430: 6f74 6522 2c20 656e 636f 6465 645f 6c6f  ote\", encoded_lo",
            "+0000e440: 6361 7469 6f6e 290a 0a20 2020 2072 656d  cation)..    rem",
            "+0000e450: 6f74 655f 6e61 6d65 3a20 4f70 7469 6f6e  ote_name: Option",
            "+0000e460: 616c 5b73 7472 5d20 3d20 4e6f 6e65 0a0a  al[str] = None..",
            "+0000e470: 2020 2020 6966 2063 6f6e 6669 672e 6861      if config.ha",
            "+0000e480: 735f 7365 6374 696f 6e28 7365 6374 696f  s_section(sectio",
            "+0000e490: 6e29 3a0a 2020 2020 2020 2020 7265 6d6f  n):.        remo",
            "+0000e4a0: 7465 5f6e 616d 6520 3d20 656e 636f 6465  te_name = encode",
            "+0000e4b0: 645f 6c6f 6361 7469 6f6e 2e64 6563 6f64  d_location.decod",
            "+0000e4c0: 6528 290a 2020 2020 2020 2020 656e 636f  e().        enco",
            "+0000e4d0: 6465 645f 6c6f 6361 7469 6f6e 203d 2063  ded_location = c",
            "+0000e4e0: 6f6e 6669 672e 6765 7428 7365 6374 696f  onfig.get(sectio",
            "+0000e4f0: 6e2c 2022 7572 6c22 290a 2020 2020 656c  n, \"url\").    el",
            "+0000e500: 7365 3a0a 2020 2020 2020 2020 7265 6d6f  se:.        remo",
            "+0000e510: 7465 5f6e 616d 6520 3d20 4e6f 6e65 0a0a  te_name = None..",
            "+0000e520: 2020 2020 7265 7475 726e 2028 7265 6d6f      return (remo",
            "+0000e530: 7465 5f6e 616d 652c 2065 6e63 6f64 6564  te_name, encoded",
            "+0000e540: 5f6c 6f63 6174 696f 6e2e 6465 636f 6465  _location.decode",
            "+0000e550: 2829 290a 0a0a 6465 6620 7075 7368 280a  ())...def push(.",
            "+0000e560: 2020 2020 7265 706f 2c0a 2020 2020 7265      repo,.    re",
            "+0000e570: 6d6f 7465 5f6c 6f63 6174 696f 6e3d 4e6f  mote_location=No",
            "+0000e580: 6e65 2c0a 2020 2020 7265 6673 7065 6373  ne,.    refspecs",
            "+0000e590: 3d4e 6f6e 652c 0a20 2020 206f 7574 7374  =None,.    outst",
            "+0000e5a0: 7265 616d 3d64 6566 6175 6c74 5f62 7974  ream=default_byt",
            "+0000e5b0: 6573 5f6f 7574 5f73 7472 6561 6d2c 0a20  es_out_stream,. ",
            "+0000e5c0: 2020 2065 7272 7374 7265 616d 3d64 6566     errstream=def",
            "+0000e5d0: 6175 6c74 5f62 7974 6573 5f65 7272 5f73  ault_bytes_err_s",
            "+0000e5e0: 7472 6561 6d2c 0a20 2020 2066 6f72 6365  tream,.    force",
            "+0000e5f0: 3d46 616c 7365 2c0a 2020 2020 2a2a 6b77  =False,.    **kw",
            "+0000e600: 6172 6773 2c0a 293a 0a20 2020 2022 2222  args,.):.    \"\"\"",
            "+0000e610: 5265 6d6f 7465 2070 7573 6820 7769 7468  Remote push with",
            "+0000e620: 2064 756c 7769 6368 2076 6961 2064 756c   dulwich via dul",
            "+0000e630: 7769 6368 2e63 6c69 656e 742e 0a0a 2020  wich.client...  ",
            "+0000e640: 2020 4172 6773 3a0a 2020 2020 2020 7265    Args:.      re",
            "+0000e650: 706f 3a20 5061 7468 2074 6f20 7265 706f  po: Path to repo",
            "+0000e660: 7369 746f 7279 0a20 2020 2020 2072 656d  sitory.      rem",
            "+0000e670: 6f74 655f 6c6f 6361 7469 6f6e 3a20 4c6f  ote_location: Lo",
            "+0000e680: 6361 7469 6f6e 206f 6620 7468 6520 7265  cation of the re",
            "+0000e690: 6d6f 7465 0a20 2020 2020 2072 6566 7370  mote.      refsp",
            "+0000e6a0: 6563 733a 2052 6566 7320 746f 2070 7573  ecs: Refs to pus",
            "+0000e6b0: 6820 746f 2072 656d 6f74 650a 2020 2020  h to remote.    ",
            "+0000e6c0: 2020 6f75 7473 7472 6561 6d3a 2041 2073    outstream: A s",
            "+0000e6d0: 7472 6561 6d20 6669 6c65 2074 6f20 7772  tream file to wr",
            "+0000e6e0: 6974 6520 6f75 7470 7574 0a20 2020 2020  ite output.     ",
            "+0000e6f0: 2065 7272 7374 7265 616d 3a20 4120 7374   errstream: A st",
            "+0000e700: 7265 616d 2066 696c 6520 746f 2077 7269  ream file to wri",
            "+0000e710: 7465 2065 7272 6f72 730a 2020 2020 2020  te errors.      ",
            "+0000e720: 666f 7263 653a 2046 6f72 6365 206f 7665  force: Force ove",
            "+0000e730: 7277 7269 7469 6e67 2072 6566 730a 2020  rwriting refs.  ",
            "+0000e740: 2020 2222 220a 2020 2020 2320 4f70 656e    \"\"\".    # Open",
            "+0000e750: 2074 6865 2072 6570 6f0a 2020 2020 7769   the repo.    wi",
            "+0000e760: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "+0000e770: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "+0000e780: 0a20 2020 2020 2020 2028 7265 6d6f 7465  .        (remote",
            "+0000e790: 5f6e 616d 652c 2072 656d 6f74 655f 6c6f  _name, remote_lo",
            "+0000e7a0: 6361 7469 6f6e 2920 3d20 6765 745f 7265  cation) = get_re",
            "+0000e7b0: 6d6f 7465 5f72 6570 6f28 722c 2072 656d  mote_repo(r, rem",
            "+0000e7c0: 6f74 655f 6c6f 6361 7469 6f6e 290a 2020  ote_location).  ",
            "+0000e7d0: 2020 2020 2020 2320 4368 6563 6b20 6966        # Check if",
            "+0000e7e0: 206d 6972 726f 7220 6d6f 6465 2069 7320   mirror mode is ",
            "+0000e7f0: 656e 6162 6c65 640a 2020 2020 2020 2020  enabled.        ",
            "+0000e800: 6d69 7272 6f72 5f6d 6f64 6520 3d20 4661  mirror_mode = Fa",
            "+0000e810: 6c73 650a 2020 2020 2020 2020 6966 2072  lse.        if r",
            "+0000e820: 656d 6f74 655f 6e61 6d65 3a0a 2020 2020  emote_name:.    ",
            "+0000e830: 2020 2020 2020 2020 7472 793a 0a20 2020          try:.   ",
            "+0000e840: 2020 2020 2020 2020 2020 2020 206d 6972               mir",
            "+0000e850: 726f 725f 6d6f 6465 203d 2072 2e67 6574  ror_mode = r.get",
            "+0000e860: 5f63 6f6e 6669 675f 7374 6163 6b28 292e  _config_stack().",
            "+0000e870: 6765 745f 626f 6f6c 6561 6e28 0a20 2020  get_boolean(.   ",
            "+0000e880: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000e890: 2028 6222 7265 6d6f 7465 222c 2072 656d   (b\"remote\", rem",
            "+0000e8a0: 6f74 655f 6e61 6d65 2e65 6e63 6f64 6528  ote_name.encode(",
            "+0000e8b0: 2929 2c20 6222 6d69 7272 6f72 220a 2020  )), b\"mirror\".  ",
            "+0000e8c0: 2020 2020 2020 2020 2020 2020 2020 290a                ).",
            "+0000e8d0: 2020 2020 2020 2020 2020 2020 6578 6365              exce",
            "+0000e8e0: 7074 204b 6579 4572 726f 723a 0a20 2020  pt KeyError:.   ",
            "+0000e8f0: 2020 2020 2020 2020 2020 2020 2070 6173               pas",
            "+0000e900: 730a 0a20 2020 2020 2020 2069 6620 6d69  s..        if mi",
            "+0000e910: 7272 6f72 5f6d 6f64 653a 0a20 2020 2020  rror_mode:.     ",
            "+0000e920: 2020 2020 2020 2023 204d 6972 726f 7220         # Mirror ",
            "+0000e930: 6d6f 6465 3a20 7075 7368 2061 6c6c 2072  mode: push all r",
            "+0000e940: 6566 7320 616e 6420 6465 6c65 7465 206e  efs and delete n",
            "+0000e950: 6f6e 2d65 7869 7374 656e 7420 6f6e 6573  on-existent ones",
            "+0000e960: 0a20 2020 2020 2020 2020 2020 2072 6566  .            ref",
            "+0000e970: 7370 6563 7320 3d20 5b5d 0a20 2020 2020  specs = [].     ",
            "+0000e980: 2020 2020 2020 2066 6f72 2072 6566 2069         for ref i",
            "+0000e990: 6e20 722e 7265 6673 2e6b 6579 7328 293a  n r.refs.keys():",
            "+0000e9a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000e9b0: 2023 2050 7573 6820 616c 6c20 7265 6673   # Push all refs",
            "+0000e9c0: 2074 6f20 7468 6520 7361 6d65 206e 616d   to the same nam",
            "+0000e9d0: 6520 6f6e 2072 656d 6f74 650a 2020 2020  e on remote.    ",
            "+0000e9e0: 2020 2020 2020 2020 2020 2020 7265 6673              refs",
            "+0000e9f0: 7065 6373 2e61 7070 656e 6428 7265 6620  pecs.append(ref ",
            "+0000ea00: 2b20 6222 3a22 202b 2072 6566 290a 2020  + b\":\" + ref).  ",
            "+0000ea10: 2020 2020 2020 656c 6966 2072 6566 7370        elif refsp",
            "+0000ea20: 6563 7320 6973 204e 6f6e 653a 0a20 2020  ecs is None:.   ",
            "+0000ea30: 2020 2020 2020 2020 2072 6566 7370 6563           refspec",
            "+0000ea40: 7320 3d20 5b61 6374 6976 655f 6272 616e  s = [active_bran",
            "+0000ea50: 6368 2872 295d 0a0a 2020 2020 2020 2020  ch(r)]..        ",
            "+0000ea60: 2320 4765 7420 7468 6520 636c 6965 6e74  # Get the client",
            "+0000ea70: 2061 6e64 2070 6174 680a 2020 2020 2020   and path.      ",
            "+0000ea80: 2020 636c 6965 6e74 2c20 7061 7468 203d    client, path =",
            "+0000ea90: 2067 6574 5f74 7261 6e73 706f 7274 5f61   get_transport_a",
            "+0000eaa0: 6e64 5f70 6174 6828 0a20 2020 2020 2020  nd_path(.       ",
            "+0000eab0: 2020 2020 2072 656d 6f74 655f 6c6f 6361       remote_loca",
            "+0000eac0: 7469 6f6e 2c20 636f 6e66 6967 3d72 2e67  tion, config=r.g",
            "+0000ead0: 6574 5f63 6f6e 6669 675f 7374 6163 6b28  et_config_stack(",
            "+0000eae0: 292c 202a 2a6b 7761 7267 730a 2020 2020  ), **kwargs.    ",
            "+0000eaf0: 2020 2020 290a 0a20 2020 2020 2020 2073      )..        s",
            "+0000eb00: 656c 6563 7465 645f 7265 6673 203d 205b  elected_refs = [",
            "+0000eb10: 5d0a 2020 2020 2020 2020 7265 6d6f 7465  ].        remote",
            "+0000eb20: 5f63 6861 6e67 6564 5f72 6566 7320 3d20  _changed_refs = ",
            "+0000eb30: 7b7d 0a0a 2020 2020 2020 2020 6465 6620  {}..        def ",
            "+0000eb40: 7570 6461 7465 5f72 6566 7328 7265 6673  update_refs(refs",
            "+0000eb50: 293a 0a20 2020 2020 2020 2020 2020 2073  ):.            s",
            "+0000eb60: 656c 6563 7465 645f 7265 6673 2e65 7874  elected_refs.ext",
            "+0000eb70: 656e 6428 7061 7273 655f 7265 6674 7570  end(parse_reftup",
            "+0000eb80: 6c65 7328 722e 7265 6673 2c20 7265 6673  les(r.refs, refs",
            "+0000eb90: 2c20 7265 6673 7065 6373 2c20 666f 7263  , refspecs, forc",
            "+0000eba0: 653d 666f 7263 6529 290a 2020 2020 2020  e=force)).      ",
            "+0000ebb0: 2020 2020 2020 6e65 775f 7265 6673 203d        new_refs =",
            "+0000ebc0: 207b 7d0a 0a20 2020 2020 2020 2020 2020   {}..           ",
            "+0000ebd0: 2023 2049 6e20 6d69 7272 6f72 206d 6f64   # In mirror mod",
            "+0000ebe0: 652c 2064 656c 6574 6520 7265 6d6f 7465  e, delete remote",
            "+0000ebf0: 2072 6566 7320 7468 6174 2064 6f6e 2774   refs that don't",
            "+0000ec00: 2065 7869 7374 206c 6f63 616c 6c79 0a20   exist locally. ",
            "+0000ec10: 2020 2020 2020 2020 2020 2069 6620 6d69             if mi",
            "+0000ec20: 7272 6f72 5f6d 6f64 653a 0a20 2020 2020  rror_mode:.     ",
            "+0000ec30: 2020 2020 2020 2020 2020 206c 6f63 616c             local",
            "+0000ec40: 5f72 6566 7320 3d20 7365 7428 722e 7265  _refs = set(r.re",
            "+0000ec50: 6673 2e6b 6579 7328 2929 0a20 2020 2020  fs.keys()).     ",
            "+0000ec60: 2020 2020 2020 2020 2020 2066 6f72 2072             for r",
            "+0000ec70: 656d 6f74 655f 7265 6620 696e 2072 6566  emote_ref in ref",
            "+0000ec80: 732e 6b65 7973 2829 3a0a 2020 2020 2020  s.keys():.      ",
            "+0000ec90: 2020 2020 2020 2020 2020 2020 2020 6966                if",
            "+0000eca0: 2072 656d 6f74 655f 7265 6620 6e6f 7420   remote_ref not ",
            "+0000ecb0: 696e 206c 6f63 616c 5f72 6566 733a 0a20  in local_refs:. ",
            "+0000ecc0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000ecd0: 2020 2020 2020 206e 6577 5f72 6566 735b         new_refs[",
            "+0000ece0: 7265 6d6f 7465 5f72 6566 5d20 3d20 5a45  remote_ref] = ZE",
            "+0000ecf0: 524f 5f53 4841 0a20 2020 2020 2020 2020  RO_SHA.         ",
            "+0000ed00: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+0000ed10: 656d 6f74 655f 6368 616e 6765 645f 7265  emote_changed_re",
            "+0000ed20: 6673 5b72 656d 6f74 655f 7265 665d 203d  fs[remote_ref] =",
            "+0000ed30: 204e 6f6e 650a 2020 2020 2020 2020 2020   None.          ",
            "+0000ed40: 2020 2320 544f 444f 3a20 4861 6e64 6c65    # TODO: Handle",
            "+0000ed50: 2073 656c 6563 7465 645f 7265 6673 203d   selected_refs =",
            "+0000ed60: 3d20 7b4e 6f6e 653a 204e 6f6e 657d 0a20  = {None: None}. ",
            "+0000ed70: 2020 2020 2020 2020 2020 2066 6f72 206c             for l",
            "+0000ed80: 682c 2072 682c 2066 6f72 6365 5f72 6566  h, rh, force_ref",
            "+0000ed90: 2069 6e20 7365 6c65 6374 6564 5f72 6566   in selected_ref",
            "+0000eda0: 733a 0a20 2020 2020 2020 2020 2020 2020  s:.             ",
            "+0000edb0: 2020 2069 6620 6c68 2069 7320 4e6f 6e65     if lh is None",
            "+0000edc0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "+0000edd0: 2020 2020 2020 6e65 775f 7265 6673 5b72        new_refs[r",
            "+0000ede0: 685d 203d 205a 4552 4f5f 5348 410a 2020  h] = ZERO_SHA.  ",
            "+0000edf0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000ee00: 2020 7265 6d6f 7465 5f63 6861 6e67 6564    remote_changed",
            "+0000ee10: 5f72 6566 735b 7268 5d20 3d20 4e6f 6e65  _refs[rh] = None",
            "+0000ee20: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000ee30: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         ",
            "+0000ee40: 2020 2020 2020 2020 2020 2074 7279 3a0a             try:.",
            "+0000ee50: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000ee60: 2020 2020 2020 2020 6c6f 6361 6c73 6861          localsha",
            "+0000ee70: 203d 2072 2e72 6566 735b 6c68 5d0a 2020   = r.refs[lh].  ",
            "+0000ee80: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000ee90: 2020 6578 6365 7074 204b 6579 4572 726f    except KeyErro",
            "+0000eea0: 7220 6173 2065 7863 3a0a 2020 2020 2020  r as exc:.      ",
            "+0000eeb0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000eec0: 2020 7261 6973 6520 4572 726f 7228 6622    raise Error(f\"",
            "+0000eed0: 4e6f 2076 616c 6964 2072 6566 207b 6c68  No valid ref {lh",
            "+0000eee0: 7d20 696e 206c 6f63 616c 2072 6570 6f73  } in local repos",
            "+0000eef0: 6974 6f72 7922 2920 6672 6f6d 2065 7863  itory\") from exc",
            "+0000ef00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000ef10: 2020 2020 2069 6620 6e6f 7420 666f 7263       if not forc",
            "+0000ef20: 655f 7265 6620 616e 6420 7268 2069 6e20  e_ref and rh in ",
            "+0000ef30: 7265 6673 3a0a 2020 2020 2020 2020 2020  refs:.          ",
            "+0000ef40: 2020 2020 2020 2020 2020 2020 2020 6368                ch",
            "+0000ef50: 6563 6b5f 6469 7665 7267 6564 2872 2c20  eck_diverged(r, ",
            "+0000ef60: 7265 6673 5b72 685d 2c20 6c6f 6361 6c73  refs[rh], locals",
            "+0000ef70: 6861 290a 2020 2020 2020 2020 2020 2020  ha).            ",
            "+0000ef80: 2020 2020 2020 2020 6e65 775f 7265 6673          new_refs",
            "+0000ef90: 5b72 685d 203d 206c 6f63 616c 7368 610a  [rh] = localsha.",
            "+0000efa0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000efb0: 2020 2020 7265 6d6f 7465 5f63 6861 6e67      remote_chang",
            "+0000efc0: 6564 5f72 6566 735b 7268 5d20 3d20 6c6f  ed_refs[rh] = lo",
            "+0000efd0: 6361 6c73 6861 0a20 2020 2020 2020 2020  calsha.         ",
            "+0000efe0: 2020 2072 6574 7572 6e20 6e65 775f 7265     return new_re",
            "+0000eff0: 6673 0a0a 2020 2020 2020 2020 6572 725f  fs..        err_",
            "+0000f000: 656e 636f 6469 6e67 203d 2067 6574 6174  encoding = getat",
            "+0000f010: 7472 2865 7272 7374 7265 616d 2c20 2265  tr(errstream, \"e",
            "+0000f020: 6e63 6f64 696e 6722 2c20 4e6f 6e65 2920  ncoding\", None) ",
            "+0000f030: 6f72 2044 4546 4155 4c54 5f45 4e43 4f44  or DEFAULT_ENCOD",
            "+0000f040: 494e 470a 2020 2020 2020 2020 7265 6d6f  ING.        remo",
            "+0000f050: 7465 5f6c 6f63 6174 696f 6e20 3d20 636c  te_location = cl",
            "+0000f060: 6965 6e74 2e67 6574 5f75 726c 2870 6174  ient.get_url(pat",
            "+0000f070: 6829 0a20 2020 2020 2020 2074 7279 3a0a  h).        try:.",
            "+0000f080: 2020 2020 2020 2020 2020 2020 7265 7375              resu",
            "+0000f090: 6c74 203d 2063 6c69 656e 742e 7365 6e64  lt = client.send",
            "+0000f0a0: 5f70 6163 6b28 0a20 2020 2020 2020 2020  _pack(.         ",
            "+0000f0b0: 2020 2020 2020 2070 6174 682c 0a20 2020         path,.   ",
            "+0000f0c0: 2020 2020 2020 2020 2020 2020 2075 7064               upd",
            "+0000f0d0: 6174 655f 7265 6673 2c0a 2020 2020 2020  ate_refs,.      ",
            "+0000f0e0: 2020 2020 2020 2020 2020 6765 6e65 7261            genera",
            "+0000f0f0: 7465 5f70 6163 6b5f 6461 7461 3d72 2e67  te_pack_data=r.g",
            "+0000f100: 656e 6572 6174 655f 7061 636b 5f64 6174  enerate_pack_dat",
            "+0000f110: 612c 0a20 2020 2020 2020 2020 2020 2020  a,.             ",
            "+0000f120: 2020 2070 726f 6772 6573 733d 6572 7273     progress=errs",
            "+0000f130: 7472 6561 6d2e 7772 6974 652c 0a20 2020  tream.write,.   ",
            "+0000f140: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     ",
            "+0000f150: 2020 2065 7863 6570 7420 5365 6e64 5061     except SendPa",
            "+0000f160: 636b 4572 726f 7220 6173 2065 7863 3a0a  ckError as exc:.",
            "+0000f170: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+0000f180: 6520 4572 726f 7228 0a20 2020 2020 2020  e Error(.       ",
            "+0000f190: 2020 2020 2020 2020 2022 5075 7368 2074           \"Push t",
            "+0000f1a0: 6f20 2220 2b20 7265 6d6f 7465 5f6c 6f63  o \" + remote_loc",
            "+0000f1b0: 6174 696f 6e20 2b20 2220 6661 696c 6564  ation + \" failed",
            "+0000f1c0: 202d 3e20 2220 2b20 6578 632e 6172 6773   -> \" + exc.args",
            "+0000f1d0: 5b30 5d2e 6465 636f 6465 2829 2c0a 2020  [0].decode(),.  ",
            "+0000f1e0: 2020 2020 2020 2020 2020 2920 6672 6f6d            ) from",
            "+0000f1f0: 2065 7863 0a20 2020 2020 2020 2065 6c73   exc.        els",
            "+0000f200: 653a 0a20 2020 2020 2020 2020 2020 2065  e:.            e",
            "+0000f210: 7272 7374 7265 616d 2e77 7269 7465 280a  rrstream.write(.",
            "+0000f220: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000f230: 6222 5075 7368 2074 6f20 2220 2b20 7265  b\"Push to \" + re",
            "+0000f240: 6d6f 7465 5f6c 6f63 6174 696f 6e2e 656e  mote_location.en",
            "+0000f250: 636f 6465 2865 7272 5f65 6e63 6f64 696e  code(err_encodin",
            "+0000f260: 6729 202b 2062 2220 7375 6363 6573 7366  g) + b\" successf",
            "+0000f270: 756c 2e5c 6e22 0a20 2020 2020 2020 2020  ul.\\n\".         ",
            "+0000f280: 2020 2029 0a0a 2020 2020 2020 2020 666f     )..        fo",
            "+0000f290: 7220 7265 662c 2065 7272 6f72 2069 6e20  r ref, error in ",
            "+0000f2a0: 2872 6573 756c 742e 7265 665f 7374 6174  (result.ref_stat",
            "+0000f2b0: 7573 206f 7220 7b7d 292e 6974 656d 7328  us or {}).items(",
            "+0000f2c0: 293a 0a20 2020 2020 2020 2020 2020 2069  ):.            i",
            "+0000f2d0: 6620 6572 726f 7220 6973 206e 6f74 204e  f error is not N",
            "+0000f2e0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           ",
            "+0000f2f0: 2020 2020 2065 7272 7374 7265 616d 2e77       errstream.w",
            "+0000f300: 7269 7465 280a 2020 2020 2020 2020 2020  rite(.          ",
            "+0000f310: 2020 2020 2020 2020 2020 6222 5075 7368            b\"Push",
            "+0000f320: 206f 6620 7265 6620 2573 2066 6169 6c65   of ref %s faile",
            "+0000f330: 643a 2025 735c 6e22 2025 2028 7265 662c  d: %s\\n\" % (ref,",
            "+0000f340: 2065 7272 6f72 2e65 6e63 6f64 6528 6572   error.encode(er",
            "+0000f350: 725f 656e 636f 6469 6e67 2929 0a20 2020  r_encoding)).   ",
            "+0000f360: 2020 2020 2020 2020 2020 2020 2029 0a20               ). ",
            "+0000f370: 2020 2020 2020 2020 2020 2065 6c73 653a             else:",
            " 0000f380: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "-0000f390: 2029 0a20 2020 2020 2020 2020 2020 2065   ).            e",
            "-0000f3a0: 7863 6570 7420 4572 726f 723a 0a20 2020  xcept Error:.   ",
            "-0000f3b0: 2020 2020 2020 2020 2020 2020 2070 6173               pas",
            "-0000f3c0: 730a 2020 2020 2020 2020 2020 2020 7570  s.            up",
            "-0000f3d0: 6461 7465 5f68 6561 6428 7265 706f 2c20  date_head(repo, ",
            "-0000f3e0: 4c4f 4341 4c5f 4252 414e 4348 5f50 5245  LOCAL_BRANCH_PRE",
            "-0000f3f0: 4649 5820 2b20 6368 6563 6b6f 7574 5f74  FIX + checkout_t",
            "-0000f400: 6172 6765 7429 0a20 2020 2020 2020 2065  arget).        e",
            "-0000f410: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           ",
            "-0000f420: 2075 7064 6174 655f 6865 6164 2872 6570   update_head(rep",
            "-0000f430: 6f2c 2074 6172 6765 742c 2064 6574 6163  o, target, detac",
            "-0000f440: 6865 643d 5472 7565 290a 0a20 2020 2072  hed=True)..    r",
            "-0000f450: 6574 7572 6e20 6368 6563 6b6f 7574 5f74  eturn checkout_t",
            "-0000f460: 6172 6765 740a 0a0a 6465 6620 6368 6563  arget...def chec",
            "-0000f470: 6b6f 7574 5f62 7261 6e63 6828 7265 706f  kout_branch(repo",
            "-0000f480: 2c20 7461 7267 6574 3a20 556e 696f 6e5b  , target: Union[",
            "-0000f490: 6279 7465 732c 2073 7472 5d2c 2066 6f72  bytes, str], for",
            "-0000f4a0: 6365 3a20 626f 6f6c 203d 2046 616c 7365  ce: bool = False",
            "-0000f4b0: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    \"",
            "-0000f4c0: 2222 5377 6974 6368 2062 7261 6e63 6865  \"\"Switch branche",
            "-0000f4d0: 7320 6f72 2072 6573 746f 7265 2077 6f72  s or restore wor",
            "-0000f4e0: 6b69 6e67 2074 7265 6520 6669 6c65 732e  king tree files.",
            "-0000f4f0: 0a0a 2020 2020 5468 6520 696d 706c 656d  ..    The implem",
            "-0000f500: 656e 7461 7469 6f6e 206f 6620 7468 6973  entation of this",
            "-0000f510: 2066 756e 6374 696f 6e20 7769 6c6c 2070   function will p",
            "-0000f520: 726f 6261 626c 7920 6e6f 7420 7363 616c  robably not scal",
            "-0000f530: 6520 7765 6c6c 0a20 2020 2066 6f72 2062  e well.    for b",
            "-0000f540: 7261 6e63 6865 7320 7769 7468 206c 6f74  ranches with lot",
            "-0000f550: 7320 6f66 206c 6f63 616c 2063 6861 6e67  s of local chang",
            "-0000f560: 6573 2e0a 2020 2020 5468 6973 2069 7320  es..    This is ",
            "-0000f570: 6475 6520 746f 2074 6865 2061 6e61 6c79  due to the analy",
            "-0000f580: 7369 7320 6f66 2061 2064 6966 6620 6265  sis of a diff be",
            "-0000f590: 7477 6565 6e20 6272 616e 6368 6573 2062  tween branches b",
            "-0000f5a0: 6566 6f72 6520 616e 790a 2020 2020 6368  efore any.    ch",
            "-0000f5b0: 616e 6765 7320 6172 6520 6170 706c 6965  anges are applie",
            "-0000f5c0: 642e 0a0a 2020 2020 4172 6773 3a0a 2020  d...    Args:.  ",
            "-0000f5d0: 2020 2020 7265 706f 3a20 6475 6c77 6963      repo: dulwic",
            "-0000f5e0: 6820 5265 706f 206f 626a 6563 740a 2020  h Repo object.  ",
            "-0000f5f0: 2020 2020 7461 7267 6574 3a20 6272 616e      target: bran",
            "-0000f600: 6368 206e 616d 6520 6f72 2063 6f6d 6d69  ch name or commi",
            "-0000f610: 7420 7368 6120 746f 2063 6865 636b 6f75  t sha to checkou",
            "-0000f620: 740a 2020 2020 2020 666f 7263 653a 2074  t.      force: t",
            "-0000f630: 7275 6520 6f72 206e 6f74 2074 6f20 666f  rue or not to fo",
            "-0000f640: 7263 6520 6368 6563 6b6f 7574 0a20 2020  rce checkout.   ",
            "-0000f650: 2022 2222 0a20 2020 2074 6172 6765 7420   \"\"\".    target ",
            "-0000f660: 3d20 746f 5f62 7974 6573 2874 6172 6765  = to_bytes(targe",
            "-0000f670: 7429 0a0a 2020 2020 6375 7272 656e 745f  t)..    current_",
            "-0000f680: 7472 6565 203d 2070 6172 7365 5f74 7265  tree = parse_tre",
            "-0000f690: 6528 7265 706f 2c20 7265 706f 2e68 6561  e(repo, repo.hea",
            "-0000f6a0: 6428 2929 0a20 2020 2074 6172 6765 745f  d()).    target_",
            "-0000f6b0: 7472 6565 203d 2070 6172 7365 5f74 7265  tree = parse_tre",
            "-0000f6c0: 6528 7265 706f 2c20 7461 7267 6574 290a  e(repo, target).",
            "-0000f6d0: 0a20 2020 2069 6620 666f 7263 653a 0a20  .    if force:. ",
            "-0000f6e0: 2020 2020 2020 2072 6570 6f2e 7265 7365         repo.rese",
            "-0000f6f0: 745f 696e 6465 7828 7461 7267 6574 5f74  t_index(target_t",
            "-0000f700: 7265 652e 6964 290a 2020 2020 2020 2020  ree.id).        ",
            "-0000f710: 5f75 7064 6174 655f 6865 6164 5f64 7572  _update_head_dur",
            "-0000f720: 696e 675f 6368 6563 6b6f 7574 5f62 7261  ing_checkout_bra",
            "-0000f730: 6e63 6828 7265 706f 2c20 7461 7267 6574  nch(repo, target",
            "-0000f740: 290a 2020 2020 656c 7365 3a0a 2020 2020  ).    else:.    ",
            "-0000f750: 2020 2020 7374 6174 7573 5f72 6570 6f72      status_repor",
            "-0000f760: 7420 3d20 7374 6174 7573 2872 6570 6f29  t = status(repo)",
            "-0000f770: 0a20 2020 2020 2020 2063 6861 6e67 6573  .        changes",
            "-0000f780: 203d 206c 6973 7428 0a20 2020 2020 2020   = list(.       ",
            "-0000f790: 2020 2020 2073 6574 280a 2020 2020 2020       set(.      ",
            "-0000f7a0: 2020 2020 2020 2020 2020 7374 6174 7573            status",
            "-0000f7b0: 5f72 6570 6f72 745b 305d 5b22 6164 6422  _report[0][\"add\"",
            "-0000f7c0: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              ",
            "-0000f7d0: 2020 2b20 7374 6174 7573 5f72 6570 6f72    + status_repor",
            "-0000f7e0: 745b 305d 5b22 6465 6c65 7465 225d 0a20  t[0][\"delete\"]. ",
            "-0000f7f0: 2020 2020 2020 2020 2020 2020 2020 202b                 +",
            "-0000f800: 2073 7461 7475 735f 7265 706f 7274 5b30   status_report[0",
            "-0000f810: 5d5b 226d 6f64 6966 7922 5d0a 2020 2020  ][\"modify\"].    ",
            "-0000f820: 2020 2020 2020 2020 2020 2020 2b20 7374              + st",
            "-0000f830: 6174 7573 5f72 6570 6f72 745b 315d 0a20  atus_report[1]. ",
            "-0000f840: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   ",
            "-0000f850: 2020 2020 2029 0a20 2020 2020 2020 2069       ).        i",
            "-0000f860: 6e64 6578 203d 2030 0a20 2020 2020 2020  ndex = 0.       ",
            "-0000f870: 2077 6869 6c65 2069 6e64 6578 203c 206c   while index < l",
            "-0000f880: 656e 2863 6861 6e67 6573 293a 0a20 2020  en(changes):.   ",
            "-0000f890: 2020 2020 2020 2020 2063 6861 6e67 6520           change ",
            "-0000f8a0: 3d20 6368 616e 6765 735b 696e 6465 785d  = changes[index]",
            "-0000f8b0: 0a20 2020 2020 2020 2020 2020 2074 7279  .            try",
            "-0000f8c0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "-0000f8d0: 2020 6375 7272 656e 745f 7472 6565 2e6c    current_tree.l",
            "-0000f8e0: 6f6f 6b75 705f 7061 7468 2872 6570 6f2e  ookup_path(repo.",
            "-0000f8f0: 6f62 6a65 6374 5f73 746f 7265 2e5f 5f67  object_store.__g",
            "-0000f900: 6574 6974 656d 5f5f 2c20 6368 616e 6765  etitem__, change",
            "-0000f910: 290a 2020 2020 2020 2020 2020 2020 2020  ).              ",
            "-0000f920: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         ",
            "-0000f930: 2020 2020 2020 2020 2020 2074 6172 6765             targe",
            "-0000f940: 745f 7472 6565 2e6c 6f6f 6b75 705f 7061  t_tree.lookup_pa",
            "-0000f950: 7468 2872 6570 6f2e 6f62 6a65 6374 5f73  th(repo.object_s",
            "-0000f960: 746f 7265 2e5f 5f67 6574 6974 656d 5f5f  tore.__getitem__",
            "-0000f970: 2c20 6368 616e 6765 290a 2020 2020 2020  , change).      ",
            "-0000f980: 2020 2020 2020 2020 2020 2020 2020 696e                in",
            "-0000f990: 6465 7820 2b3d 2031 0a20 2020 2020 2020  dex += 1.       ",
            "-0000f9a0: 2020 2020 2020 2020 2065 7863 6570 7420           except ",
            "-0000f9b0: 4b65 7945 7272 6f72 3a0a 2020 2020 2020  KeyError:.      ",
            "-0000f9c0: 2020 2020 2020 2020 2020 2020 2020 7261                ra",
            "-0000f9d0: 6973 6520 4368 6563 6b6f 7574 4572 726f  ise CheckoutErro",
            "-0000f9e0: 7228 0a20 2020 2020 2020 2020 2020 2020  r(.             ",
            "-0000f9f0: 2020 2020 2020 2020 2020 2022 596f 7572             \"Your",
            "-0000fa00: 206c 6f63 616c 2063 6861 6e67 6573 2074   local changes t",
            "-0000fa10: 6f20 7468 6520 666f 6c6c 6f77 696e 6720  o the following ",
            "-0000fa20: 6669 6c65 7320 776f 756c 6420 6265 206f  files would be o",
            "-0000fa30: 7665 7277 7269 7474 656e 2062 7920 6368  verwritten by ch",
            "-0000fa40: 6563 6b6f 7574 3a20 220a 2020 2020 2020  eckout: \".      ",
            "-0000fa50: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-0000fa60: 2020 2b20 6368 616e 6765 2e64 6563 6f64    + change.decod",
            "-0000fa70: 6528 290a 2020 2020 2020 2020 2020 2020  e().            ",
            "-0000fa80: 2020 2020 2020 2020 290a 2020 2020 2020          ).      ",
            "-0000fa90: 2020 2020 2020 6578 6365 7074 204b 6579        except Key",
            "-0000faa0: 4572 726f 723a 0a20 2020 2020 2020 2020  Error:.         ",
            "-0000fab0: 2020 2020 2020 2063 6861 6e67 6573 2e70         changes.p",
            "-0000fac0: 6f70 2869 6e64 6578 290a 0a20 2020 2020  op(index)..     ",
            "-0000fad0: 2020 2023 2055 7064 6174 6520 6865 6164     # Update head",
            "-0000fae0: 2e0a 2020 2020 2020 2020 6368 6563 6b6f  ..        checko",
            "-0000faf0: 7574 5f74 6172 6765 7420 3d20 5f75 7064  ut_target = _upd",
            "-0000fb00: 6174 655f 6865 6164 5f64 7572 696e 675f  ate_head_during_",
            "-0000fb10: 6368 6563 6b6f 7574 5f62 7261 6e63 6828  checkout_branch(",
            "-0000fb20: 7265 706f 2c20 7461 7267 6574 290a 2020  repo, target).  ",
            "-0000fb30: 2020 2020 2020 6966 2063 6865 636b 6f75        if checkou",
            "-0000fb40: 745f 7461 7267 6574 2069 7320 6e6f 7420  t_target is not ",
            "-0000fb50: 4e6f 6e65 3a0a 2020 2020 2020 2020 2020  None:.          ",
            "-0000fb60: 2020 7461 7267 6574 5f74 7265 6520 3d20    target_tree = ",
            "-0000fb70: 7061 7273 655f 7472 6565 2872 6570 6f2c  parse_tree(repo,",
            "-0000fb80: 2063 6865 636b 6f75 745f 7461 7267 6574   checkout_target",
            "-0000fb90: 290a 0a20 2020 2020 2020 2064 6561 6c74  )..        dealt",
            "-0000fba0: 5f77 6974 6820 3d20 7365 7428 290a 2020  _with = set().  ",
            "-0000fbb0: 2020 2020 2020 7265 706f 5f69 6e64 6578        repo_index",
            "-0000fbc0: 203d 2072 6570 6f2e 6f70 656e 5f69 6e64   = repo.open_ind",
            "-0000fbd0: 6578 2829 0a20 2020 2020 2020 2066 6f72  ex().        for",
            "-0000fbe0: 2065 6e74 7279 2069 6e20 6974 6572 5f74   entry in iter_t",
            "-0000fbf0: 7265 655f 636f 6e74 656e 7473 2872 6570  ree_contents(rep",
            "-0000fc00: 6f2e 6f62 6a65 6374 5f73 746f 7265 2c20  o.object_store, ",
            "-0000fc10: 7461 7267 6574 5f74 7265 652e 6964 293a  target_tree.id):",
            "-0000fc20: 0a20 2020 2020 2020 2020 2020 2064 6561  .            dea",
            "-0000fc30: 6c74 5f77 6974 682e 6164 6428 656e 7472  lt_with.add(entr",
            "-0000fc40: 792e 7061 7468 290a 2020 2020 2020 2020  y.path).        ",
            "-0000fc50: 2020 2020 6966 2065 6e74 7279 2e70 6174      if entry.pat",
            "-0000fc60: 6820 696e 2063 6861 6e67 6573 3a0a 2020  h in changes:.  ",
            "-0000fc70: 2020 2020 2020 2020 2020 2020 2020 636f                co",
            "-0000fc80: 6e74 696e 7565 0a20 2020 2020 2020 2020  ntinue.         ",
            "-0000fc90: 2020 2066 756c 6c5f 7061 7468 203d 206f     full_path = o",
            "-0000fca0: 732e 7061 7468 2e6a 6f69 6e28 6f73 2e66  s.path.join(os.f",
            "-0000fcb0: 7365 6e63 6f64 6528 7265 706f 2e70 6174  sencode(repo.pat",
            "-0000fcc0: 6829 2c20 656e 7472 792e 7061 7468 290a  h), entry.path).",
            "-0000fcd0: 2020 2020 2020 2020 2020 2020 626c 6f62              blob",
            "-0000fce0: 203d 2072 6570 6f2e 6f62 6a65 6374 5f73   = repo.object_s",
            "-0000fcf0: 746f 7265 5b65 6e74 7279 2e73 6861 5d0a  tore[entry.sha].",
            "-0000fd00: 2020 2020 2020 2020 2020 2020 656e 7375              ensu",
            "-0000fd10: 7265 5f64 6972 5f65 7869 7374 7328 6f73  re_dir_exists(os",
            "-0000fd20: 2e70 6174 682e 6469 726e 616d 6528 6675  .path.dirname(fu",
            "-0000fd30: 6c6c 5f70 6174 6829 290a 2020 2020 2020  ll_path)).      ",
            "-0000fd40: 2020 2020 2020 7374 203d 2062 7569 6c64        st = build",
            "-0000fd50: 5f66 696c 655f 6672 6f6d 5f62 6c6f 6228  _file_from_blob(",
            "-0000fd60: 626c 6f62 2c20 656e 7472 792e 6d6f 6465  blob, entry.mode",
            "-0000fd70: 2c20 6675 6c6c 5f70 6174 6829 0a20 2020  , full_path).   ",
            "-0000fd80: 2020 2020 2020 2020 2072 6570 6f5f 696e           repo_in",
            "-0000fd90: 6465 785b 656e 7472 792e 7061 7468 5d20  dex[entry.path] ",
            "-0000fda0: 3d20 696e 6465 785f 656e 7472 795f 6672  = index_entry_fr",
            "-0000fdb0: 6f6d 5f73 7461 7428 7374 2c20 656e 7472  om_stat(st, entr",
            "-0000fdc0: 792e 7368 6129 0a0a 2020 2020 2020 2020  y.sha)..        ",
            "-0000fdd0: 7265 706f 5f69 6e64 6578 2e77 7269 7465  repo_index.write",
            "-0000fde0: 2829 0a0a 2020 2020 2020 2020 666f 7220  ()..        for ",
            "-0000fdf0: 656e 7472 7920 696e 2069 7465 725f 7472  entry in iter_tr",
            "-0000fe00: 6565 5f63 6f6e 7465 6e74 7328 7265 706f  ee_contents(repo",
            "-0000fe10: 2e6f 626a 6563 745f 7374 6f72 652c 2063  .object_store, c",
            "-0000fe20: 7572 7265 6e74 5f74 7265 652e 6964 293a  urrent_tree.id):",
            "-0000fe30: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if ",
            "-0000fe40: 656e 7472 792e 7061 7468 206e 6f74 2069  entry.path not i",
            "-0000fe50: 6e20 6465 616c 745f 7769 7468 3a0a 2020  n dealt_with:.  ",
            "-0000fe60: 2020 2020 2020 2020 2020 2020 2020 7265                re",
            "-0000fe70: 706f 2e75 6e73 7461 6765 285b 656e 7472  po.unstage([entr",
            "-0000fe80: 792e 7061 7468 5d29 0a0a 2020 2020 2320  y.path])..    # ",
            "-0000fe90: 5265 6d6f 7665 2074 6865 2075 6e74 7261  Remove the untra",
            "-0000fea0: 636b 6564 2066 696c 6573 2077 6869 6368  cked files which",
            "-0000feb0: 2061 7265 2069 6e20 7468 6520 6375 7272   are in the curr",
            "-0000fec0: 656e 745f 6669 6c65 5f73 6574 2e0a 2020  ent_file_set..  ",
            "-0000fed0: 2020 7265 706f 5f69 6e64 6578 203d 2072    repo_index = r",
            "-0000fee0: 6570 6f2e 6f70 656e 5f69 6e64 6578 2829  epo.open_index()",
            "-0000fef0: 0a20 2020 2066 6f72 2063 6861 6e67 6520  .    for change ",
            "-0000ff00: 696e 2072 6570 6f5f 696e 6465 782e 6368  in repo_index.ch",
            "-0000ff10: 616e 6765 735f 6672 6f6d 5f74 7265 6528  anges_from_tree(",
            "-0000ff20: 7265 706f 2e6f 626a 6563 745f 7374 6f72  repo.object_stor",
            "-0000ff30: 652c 2063 7572 7265 6e74 5f74 7265 652e  e, current_tree.",
            "-0000ff40: 6964 293a 0a20 2020 2020 2020 2070 6174  id):.        pat",
            "-0000ff50: 685f 6368 616e 6765 203d 2063 6861 6e67  h_change = chang",
            "-0000ff60: 655b 305d 0a20 2020 2020 2020 2069 6620  e[0].        if ",
            "-0000ff70: 7061 7468 5f63 6861 6e67 655b 315d 2069  path_change[1] i",
            "-0000ff80: 7320 4e6f 6e65 3a0a 2020 2020 2020 2020  s None:.        ",
            "-0000ff90: 2020 2020 6669 6c65 5f6e 616d 6520 3d20      file_name = ",
            "-0000ffa0: 7061 7468 5f63 6861 6e67 655b 305d 0a20  path_change[0]. ",
            "-0000ffb0: 2020 2020 2020 2020 2020 2066 756c 6c5f             full_",
            "-0000ffc0: 7061 7468 203d 206f 732e 7061 7468 2e6a  path = os.path.j",
            "-0000ffd0: 6f69 6e28 7265 706f 2e70 6174 682c 2066  oin(repo.path, f",
            "-0000ffe0: 696c 655f 6e61 6d65 2e64 6563 6f64 6528  ile_name.decode(",
            "-0000fff0: 2929 0a20 2020 2020 2020 2020 2020 2069  )).            i",
            "-00010000: 6620 6f73 2e70 6174 682e 6973 6669 6c65  f os.path.isfile",
            "-00010010: 2866 756c 6c5f 7061 7468 293a 0a20 2020  (full_path):.   ",
            "-00010020: 2020 2020 2020 2020 2020 2020 206f 732e               os.",
            "-00010030: 7265 6d6f 7665 2866 756c 6c5f 7061 7468  remove(full_path",
            "-00010040: 290a 2020 2020 2020 2020 2020 2020 6469  ).            di",
            "-00010050: 725f 7061 7468 203d 206f 732e 7061 7468  r_path = os.path",
            "-00010060: 2e64 6972 6e61 6d65 2866 756c 6c5f 7061  .dirname(full_pa",
            "-00010070: 7468 290a 2020 2020 2020 2020 2020 2020  th).            ",
            "-00010080: 7768 696c 6520 6469 725f 7061 7468 2021  while dir_path !",
            "-00010090: 3d20 7265 706f 2e70 6174 683a 0a20 2020  = repo.path:.   ",
            "-000100a0: 2020 2020 2020 2020 2020 2020 2069 735f               is_",
            "-000100b0: 656d 7074 7920 3d20 6c65 6e28 6f73 2e6c  empty = len(os.l",
            "-000100c0: 6973 7464 6972 2864 6972 5f70 6174 6829  istdir(dir_path)",
            "-000100d0: 2920 3d3d 2030 0a20 2020 2020 2020 2020  ) == 0.         ",
            "-000100e0: 2020 2020 2020 2069 6620 6973 5f65 6d70         if is_emp",
            "-000100f0: 7479 3a0a 2020 2020 2020 2020 2020 2020  ty:.            ",
            "-00010100: 2020 2020 2020 2020 6f73 2e72 6d64 6972          os.rmdir",
            "-00010110: 2864 6972 5f70 6174 6829 0a20 2020 2020  (dir_path).     ",
            "-00010120: 2020 2020 2020 2020 2020 2064 6972 5f70             dir_p",
            "-00010130: 6174 6820 3d20 6f73 2e70 6174 682e 6469  ath = os.path.di",
            "-00010140: 726e 616d 6528 6469 725f 7061 7468 290a  rname(dir_path).",
            "-00010150: 0a0a 6465 6620 6368 6563 6b5f 6d61 696c  ..def check_mail",
            "-00010160: 6d61 7028 7265 706f 2c20 636f 6e74 6163  map(repo, contac",
            "-00010170: 7429 3a0a 2020 2020 2222 2243 6865 636b  t):.    \"\"\"Check",
            "-00010180: 2063 616e 6f6e 6963 616c 206e 616d 6520   canonical name ",
            "-00010190: 616e 6420 656d 6169 6c20 6f66 2063 6f6e  and email of con",
            "-000101a0: 7461 6374 2e0a 0a20 2020 2041 7267 733a  tact...    Args:",
            "-000101b0: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "-000101c0: 6820 746f 2074 6865 2072 6570 6f73 6974  h to the reposit",
            "-000101d0: 6f72 790a 2020 2020 2020 636f 6e74 6163  ory.      contac",
            "-000101e0: 743a 2043 6f6e 7461 6374 206e 616d 6520  t: Contact name ",
            "-000101f0: 616e 642f 6f72 2065 6d61 696c 0a20 2020  and/or email.   ",
            "-00010200: 2052 6574 7572 6e73 3a20 4361 6e6f 6e69   Returns: Canoni",
            "-00010210: 6361 6c20 636f 6e74 6163 7420 6461 7461  cal contact data",
            "-00010220: 0a20 2020 2022 2222 0a20 2020 2077 6974  .    \"\"\".    wit",
            "-00010230: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "-00010240: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "-00010250: 2020 2020 2020 2020 6672 6f6d 202e 6d61          from .ma",
            "-00010260: 696c 6d61 7020 696d 706f 7274 204d 6169  ilmap import Mai",
            "-00010270: 6c6d 6170 0a0a 2020 2020 2020 2020 7472  lmap..        tr",
            "-00010280: 793a 0a20 2020 2020 2020 2020 2020 206d  y:.            m",
            "-00010290: 6169 6c6d 6170 203d 204d 6169 6c6d 6170  ailmap = Mailmap",
            "-000102a0: 2e66 726f 6d5f 7061 7468 286f 732e 7061  .from_path(os.pa",
            "-000102b0: 7468 2e6a 6f69 6e28 722e 7061 7468 2c20  th.join(r.path, ",
            "-000102c0: 222e 6d61 696c 6d61 7022 2929 0a20 2020  \".mailmap\")).   ",
            "-000102d0: 2020 2020 2065 7863 6570 7420 4669 6c65       except File",
            "-000102e0: 4e6f 7446 6f75 6e64 4572 726f 723a 0a20  NotFoundError:. ",
            "-000102f0: 2020 2020 2020 2020 2020 206d 6169 6c6d             mailm",
            "-00010300: 6170 203d 204d 6169 6c6d 6170 2829 0a20  ap = Mailmap(). ",
            "-00010310: 2020 2020 2020 2072 6574 7572 6e20 6d61         return ma",
            "-00010320: 696c 6d61 702e 6c6f 6f6b 7570 2863 6f6e  ilmap.lookup(con",
            "-00010330: 7461 6374 290a 0a0a 6465 6620 6673 636b  tact)...def fsck",
            "-00010340: 2872 6570 6f29 3a0a 2020 2020 2222 2243  (repo):.    \"\"\"C",
            "-00010350: 6865 636b 2061 2072 6570 6f73 6974 6f72  heck a repositor",
            "-00010360: 792e 0a0a 2020 2020 4172 6773 3a0a 2020  y...    Args:.  ",
            "-00010370: 2020 2020 7265 706f 3a20 4120 7061 7468      repo: A path",
            "-00010380: 2074 6f20 7468 6520 7265 706f 7369 746f   to the reposito",
            "-00010390: 7279 0a20 2020 2052 6574 7572 6e73 3a20  ry.    Returns: ",
            "-000103a0: 4974 6572 6174 6f72 206f 7665 7220 6572  Iterator over er",
            "-000103b0: 726f 7273 2f77 6172 6e69 6e67 730a 2020  rors/warnings.  ",
            "-000103c0: 2020 2222 220a 2020 2020 7769 7468 206f    \"\"\".    with o",
            "-000103d0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "-000103e0: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "-000103f0: 2020 2020 2023 2054 4f44 4f28 6a65 6c6d       # TODO(jelm",
            "-00010400: 6572 293a 2063 6865 636b 2070 6163 6b20  er): check pack ",
            "-00010410: 6669 6c65 730a 2020 2020 2020 2020 2320  files.        # ",
            "-00010420: 544f 444f 286a 656c 6d65 7229 3a20 6368  TODO(jelmer): ch",
            "-00010430: 6563 6b20 6772 6170 680a 2020 2020 2020  eck graph.      ",
            "-00010440: 2020 2320 544f 444f 286a 656c 6d65 7229    # TODO(jelmer)",
            "-00010450: 3a20 6368 6563 6b20 7265 6673 0a20 2020  : check refs.   ",
            "-00010460: 2020 2020 2066 6f72 2073 6861 2069 6e20       for sha in ",
            "-00010470: 722e 6f62 6a65 6374 5f73 746f 7265 3a0a  r.object_store:.",
            "-00010480: 2020 2020 2020 2020 2020 2020 6f20 3d20              o = ",
            "-00010490: 722e 6f62 6a65 6374 5f73 746f 7265 5b73  r.object_store[s",
            "-000104a0: 6861 5d0a 2020 2020 2020 2020 2020 2020  ha].            ",
            "-000104b0: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           ",
            "-000104c0: 2020 2020 206f 2e63 6865 636b 2829 0a20       o.check(). ",
            "-000104d0: 2020 2020 2020 2020 2020 2065 7863 6570             excep",
            "-000104e0: 7420 4578 6365 7074 696f 6e20 6173 2065  t Exception as e",
            "-000104f0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "-00010500: 2020 7969 656c 6420 2873 6861 2c20 6529    yield (sha, e)",
            "-00010510: 0a0a 0a64 6566 2073 7461 7368 5f6c 6973  ...def stash_lis",
            "-00010520: 7428 7265 706f 293a 0a20 2020 2022 2222  t(repo):.    \"\"\"",
            "-00010530: 4c69 7374 2061 6c6c 2073 7461 7368 6573  List all stashes",
            "-00010540: 2069 6e20 6120 7265 706f 7369 746f 7279   in a repository",
            "-00010550: 2e22 2222 0a20 2020 2077 6974 6820 6f70  .\"\"\".    with op",
            "-00010560: 656e 5f72 6570 6f5f 636c 6f73 696e 6728  en_repo_closing(",
            "-00010570: 7265 706f 2920 6173 2072 3a0a 2020 2020  repo) as r:.    ",
            "-00010580: 2020 2020 6672 6f6d 202e 7374 6173 6820      from .stash ",
            "-00010590: 696d 706f 7274 2053 7461 7368 0a0a 2020  import Stash..  ",
            "-000105a0: 2020 2020 2020 7374 6173 6820 3d20 5374        stash = St",
            "-000105b0: 6173 682e 6672 6f6d 5f72 6570 6f28 7229  ash.from_repo(r)",
            "-000105c0: 0a20 2020 2020 2020 2072 6574 7572 6e20  .        return ",
            "-000105d0: 656e 756d 6572 6174 6528 6c69 7374 2873  enumerate(list(s",
            "-000105e0: 7461 7368 2e73 7461 7368 6573 2829 2929  tash.stashes()))",
            "-000105f0: 0a0a 0a64 6566 2073 7461 7368 5f70 7573  ...def stash_pus",
            "-00010600: 6828 7265 706f 2920 2d3e 204e 6f6e 653a  h(repo) -> None:",
            "-00010610: 0a20 2020 2022 2222 5075 7368 2061 206e  .    \"\"\"Push a n",
            "-00010620: 6577 2073 7461 7368 206f 6e74 6f20 7468  ew stash onto th",
            "-00010630: 6520 7374 6163 6b2e 2222 220a 2020 2020  e stack.\"\"\".    ",
            "-00010640: 7769 7468 206f 7065 6e5f 7265 706f 5f63  with open_repo_c",
            "-00010650: 6c6f 7369 6e67 2872 6570 6f29 2061 7320  losing(repo) as ",
            "-00010660: 723a 0a20 2020 2020 2020 2066 726f 6d20  r:.        from ",
            "-00010670: 2e73 7461 7368 2069 6d70 6f72 7420 5374  .stash import St",
            "-00010680: 6173 680a 0a20 2020 2020 2020 2073 7461  ash..        sta",
            "-00010690: 7368 203d 2053 7461 7368 2e66 726f 6d5f  sh = Stash.from_",
            "-000106a0: 7265 706f 2872 290a 2020 2020 2020 2020  repo(r).        ",
            "-000106b0: 7374 6173 682e 7075 7368 2829 0a0a 0a64  stash.push()...d",
            "-000106c0: 6566 2073 7461 7368 5f70 6f70 2872 6570  ef stash_pop(rep",
            "-000106d0: 6f29 202d 3e20 4e6f 6e65 3a0a 2020 2020  o) -> None:.    ",
            "-000106e0: 2222 2250 6f70 2061 2073 7461 7368 2066  \"\"\"Pop a stash f",
            "-000106f0: 726f 6d20 7468 6520 7374 6163 6b2e 2222  rom the stack.\"\"",
            "-00010700: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "-00010710: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "-00010720: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "-00010730: 2066 726f 6d20 2e73 7461 7368 2069 6d70   from .stash imp",
            "-00010740: 6f72 7420 5374 6173 680a 0a20 2020 2020  ort Stash..     ",
            "-00010750: 2020 2073 7461 7368 203d 2053 7461 7368     stash = Stash",
            "-00010760: 2e66 726f 6d5f 7265 706f 2872 290a 2020  .from_repo(r).  ",
            "-00010770: 2020 2020 2020 7374 6173 682e 706f 7028        stash.pop(",
            "-00010780: 290a 0a0a 6465 6620 7374 6173 685f 6472  )...def stash_dr",
            "-00010790: 6f70 2872 6570 6f2c 2069 6e64 6578 2920  op(repo, index) ",
            "-000107a0: 2d3e 204e 6f6e 653a 0a20 2020 2022 2222  -> None:.    \"\"\"",
            "-000107b0: 4472 6f70 2061 2073 7461 7368 2066 726f  Drop a stash fro",
            "-000107c0: 6d20 7468 6520 7374 6163 6b2e 2222 220a  m the stack.\"\"\".",
            "-000107d0: 2020 2020 7769 7468 206f 7065 6e5f 7265      with open_re",
            "-000107e0: 706f 5f63 6c6f 7369 6e67 2872 6570 6f29  po_closing(repo)",
            "-000107f0: 2061 7320 723a 0a20 2020 2020 2020 2066   as r:.        f",
            "-00010800: 726f 6d20 2e73 7461 7368 2069 6d70 6f72  rom .stash impor",
            "-00010810: 7420 5374 6173 680a 0a20 2020 2020 2020  t Stash..       ",
            "-00010820: 2073 7461 7368 203d 2053 7461 7368 2e66   stash = Stash.f",
            "-00010830: 726f 6d5f 7265 706f 2872 290a 2020 2020  rom_repo(r).    ",
            "-00010840: 2020 2020 7374 6173 682e 6472 6f70 2869      stash.drop(i",
            "-00010850: 6e64 6578 290a 0a0a 6465 6620 6c73 5f66  ndex)...def ls_f",
            "-00010860: 696c 6573 2872 6570 6f29 3a0a 2020 2020  iles(repo):.    ",
            "-00010870: 2222 224c 6973 7420 616c 6c20 6669 6c65  \"\"\"List all file",
            "-00010880: 7320 696e 2061 6e20 696e 6465 782e 2222  s in an index.\"\"",
            "-00010890: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "-000108a0: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "-000108b0: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "-000108c0: 2072 6574 7572 6e20 736f 7274 6564 2872   return sorted(r",
            "-000108d0: 2e6f 7065 6e5f 696e 6465 7828 2929 0a0a  .open_index())..",
            "-000108e0: 0a64 6566 2066 696e 645f 756e 6971 7565  .def find_unique",
            "-000108f0: 5f61 6262 7265 7628 6f62 6a65 6374 5f73  _abbrev(object_s",
            "-00010900: 746f 7265 2c20 6f62 6a65 6374 5f69 6429  tore, object_id)",
            "-00010910: 3a0a 2020 2020 2222 2246 6f72 206e 6f77  :.    \"\"\"For now",
            "-00010920: 2c20 6a75 7374 2072 6574 7572 6e20 3720  , just return 7 ",
            "-00010930: 6368 6172 6163 7465 7273 2e22 2222 0a20  characters.\"\"\". ",
            "-00010940: 2020 2023 2054 4f44 4f28 6a65 6c6d 6572     # TODO(jelmer",
            "-00010950: 293a 2041 6464 2073 6f6d 6520 6c6f 6769  ): Add some logi",
            "-00010960: 6320 6865 7265 2074 6f20 7265 7475 726e  c here to return",
            "-00010970: 2061 206e 756d 6265 7220 6f66 2063 6861   a number of cha",
            "-00010980: 7261 6374 6572 7320 7468 6174 0a20 2020  racters that.   ",
            "-00010990: 2023 2073 6361 6c65 7320 7265 6c61 7469   # scales relati",
            "-000109a0: 7665 2077 6974 6820 7468 6520 7369 7a65  ve with the size",
            "-000109b0: 206f 6620 7468 6520 7265 706f 7369 746f   of the reposito",
            "-000109c0: 7279 0a20 2020 2072 6574 7572 6e20 6f62  ry.    return ob",
            "-000109d0: 6a65 6374 5f69 642e 6465 636f 6465 2822  ject_id.decode(\"",
            "-000109e0: 6173 6369 6922 295b 3a37 5d0a 0a0a 6465  ascii\")[:7]...de",
            "-000109f0: 6620 6465 7363 7269 6265 2872 6570 6f2c  f describe(repo,",
            "-00010a00: 2061 6262 7265 763d 3729 3a0a 2020 2020   abbrev=7):.    ",
            "-00010a10: 2222 2244 6573 6372 6962 6520 7468 6520  \"\"\"Describe the ",
            "-00010a20: 7265 706f 7369 746f 7279 2076 6572 7369  repository versi",
            "-00010a30: 6f6e 2e0a 0a20 2020 2041 7267 733a 0a20  on...    Args:. ",
            "-00010a40: 2020 2020 2072 6570 6f3a 2067 6974 2072       repo: git r",
            "-00010a50: 6570 6f73 6974 6f72 790a 2020 2020 2020  epository.      ",
            "-00010a60: 6162 6272 6576 3a20 6e75 6d62 6572 206f  abbrev: number o",
            "-00010a70: 6620 6368 6172 6163 7465 7273 206f 6620  f characters of ",
            "-00010a80: 636f 6d6d 6974 2074 6f20 7461 6b65 2c20  commit to take, ",
            "-00010a90: 6465 6661 756c 7420 6973 2037 0a20 2020  default is 7.   ",
            "-00010aa0: 2052 6574 7572 6e73 3a20 6120 7374 7269   Returns: a stri",
            "-00010ab0: 6e67 2064 6573 6372 6970 7469 6f6e 206f  ng description o",
            "-00010ac0: 6620 7468 6520 6375 7272 656e 7420 6769  f the current gi",
            "-00010ad0: 7420 7265 7669 7369 6f6e 0a0a 2020 2020  t revision..    ",
            "-00010ae0: 4578 616d 706c 6573 3a20 2267 6162 6364  Examples: \"gabcd",
            "-00010af0: 6566 6822 2c20 2276 302e 3122 206f 7220  efh\", \"v0.1\" or ",
            "-00010b00: 2276 302e 312d 352d 6761 6263 6465 6668  \"v0.1-5-gabcdefh",
            "-00010b10: 222e 0a20 2020 2022 2222 0a20 2020 2023  \"..    \"\"\".    #",
            "-00010b20: 2047 6574 2074 6865 2072 6570 6f73 6974   Get the reposit",
            "-00010b30: 6f72 790a 2020 2020 7769 7468 206f 7065  ory.    with ope",
            "-00010b40: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "-00010b50: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "-00010b60: 2020 2023 2047 6574 2061 206c 6973 7420     # Get a list ",
            "-00010b70: 6f66 2061 6c6c 2074 6167 730a 2020 2020  of all tags.    ",
            "-00010b80: 2020 2020 7265 6673 203d 2072 2e67 6574      refs = r.get",
            "-00010b90: 5f72 6566 7328 290a 2020 2020 2020 2020  _refs().        ",
            "-00010ba0: 7461 6773 203d 207b 7d0a 2020 2020 2020  tags = {}.      ",
            "-00010bb0: 2020 666f 7220 6b65 792c 2076 616c 7565    for key, value",
            "-00010bc0: 2069 6e20 7265 6673 2e69 7465 6d73 2829   in refs.items()",
            "-00010bd0: 3a0a 2020 2020 2020 2020 2020 2020 6b65  :.            ke",
            "-00010be0: 7920 3d20 6b65 792e 6465 636f 6465 2829  y = key.decode()",
            "-00010bf0: 0a20 2020 2020 2020 2020 2020 206f 626a  .            obj",
            "-00010c00: 203d 2072 2e67 6574 5f6f 626a 6563 7428   = r.get_object(",
            "-00010c10: 7661 6c75 6529 0a20 2020 2020 2020 2020  value).         ",
            "-00010c20: 2020 2069 6620 2274 6167 7322 206e 6f74     if \"tags\" not",
            "-00010c30: 2069 6e20 6b65 793a 0a20 2020 2020 2020   in key:.       ",
            "-00010c40: 2020 2020 2020 2020 2063 6f6e 7469 6e75           continu",
            "-00010c50: 650a 0a20 2020 2020 2020 2020 2020 205f  e..            _",
            "-00010c60: 2c20 7461 6720 3d20 6b65 792e 7273 706c  , tag = key.rspl",
            "-00010c70: 6974 2822 2f22 2c20 3129 0a0a 2020 2020  it(\"/\", 1)..    ",
            "-00010c80: 2020 2020 2020 2020 7472 793a 0a20 2020          try:.   ",
            "-00010c90: 2020 2020 2020 2020 2020 2020 2063 6f6d               com",
            "-00010ca0: 6d69 7420 3d20 6f62 6a2e 6f62 6a65 6374  mit = obj.object",
            "-00010cb0: 0a20 2020 2020 2020 2020 2020 2065 7863  .            exc",
            "-00010cc0: 6570 7420 4174 7472 6962 7574 6545 7272  ept AttributeErr",
            "-00010cd0: 6f72 3a0a 2020 2020 2020 2020 2020 2020  or:.            ",
            "-00010ce0: 2020 2020 636f 6e74 696e 7565 0a20 2020      continue.   ",
            "-00010cf0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. ",
            "-00010d00: 2020 2020 2020 2020 2020 2020 2020 2063                 c",
            "-00010d10: 6f6d 6d69 7420 3d20 722e 6765 745f 6f62  ommit = r.get_ob",
            "-00010d20: 6a65 6374 2863 6f6d 6d69 745b 315d 290a  ject(commit[1]).",
            "-00010d30: 2020 2020 2020 2020 2020 2020 7461 6773              tags",
            "-00010d40: 5b74 6167 5d20 3d20 5b0a 2020 2020 2020  [tag] = [.      ",
            "-00010d50: 2020 2020 2020 2020 2020 6461 7465 7469            dateti",
            "-00010d60: 6d65 2e64 6174 6574 696d 6528 2a74 696d  me.datetime(*tim",
            "-00010d70: 652e 676d 7469 6d65 2863 6f6d 6d69 742e  e.gmtime(commit.",
            "-00010d80: 636f 6d6d 6974 5f74 696d 6529 5b3a 365d  commit_time)[:6]",
            "-00010d90: 292c 0a20 2020 2020 2020 2020 2020 2020  ),.             ",
            "-00010da0: 2020 2063 6f6d 6d69 742e 6964 2e64 6563     commit.id.dec",
            "-00010db0: 6f64 6528 2261 7363 6969 2229 2c0a 2020  ode(\"ascii\"),.  ",
            "-00010dc0: 2020 2020 2020 2020 2020 5d0a 0a20 2020            ]..   ",
            "-00010dd0: 2020 2020 2073 6f72 7465 645f 7461 6773       sorted_tags",
            "-00010de0: 203d 2073 6f72 7465 6428 7461 6773 2e69   = sorted(tags.i",
            "-00010df0: 7465 6d73 2829 2c20 6b65 793d 6c61 6d62  tems(), key=lamb",
            "-00010e00: 6461 2074 6167 3a20 7461 675b 315d 5b30  da tag: tag[1][0",
            "-00010e10: 5d2c 2072 6576 6572 7365 3d54 7275 6529  ], reverse=True)",
            "-00010e20: 0a0a 2020 2020 2020 2020 2320 4966 2074  ..        # If t",
            "-00010e30: 6865 7265 2061 7265 206e 6f20 7461 6773  here are no tags",
            "-00010e40: 2c20 7265 7475 726e 2074 6865 2063 7572  , return the cur",
            "-00010e50: 7265 6e74 2063 6f6d 6d69 740a 2020 2020  rent commit.    ",
            "-00010e60: 2020 2020 6966 206c 656e 2873 6f72 7465      if len(sorte",
            "-00010e70: 645f 7461 6773 2920 3d3d 2030 3a0a 2020  d_tags) == 0:.  ",
            "-00010e80: 2020 2020 2020 2020 2020 7265 7475 726e            return",
            "-00010e90: 2066 2267 7b66 696e 645f 756e 6971 7565   f\"g{find_unique",
            "-00010ea0: 5f61 6262 7265 7628 722e 6f62 6a65 6374  _abbrev(r.object",
            "-00010eb0: 5f73 746f 7265 2c20 725b 722e 6865 6164  _store, r[r.head",
            "-00010ec0: 2829 5d2e 6964 297d 220a 0a20 2020 2020  ()].id)}\"..     ",
            "-00010ed0: 2020 2023 2057 6527 7265 206e 6f77 2030     # We're now 0",
            "-00010ee0: 2063 6f6d 6d69 7473 2066 726f 6d20 7468   commits from th",
            "-00010ef0: 6520 746f 700a 2020 2020 2020 2020 636f  e top.        co",
            "-00010f00: 6d6d 6974 5f63 6f75 6e74 203d 2030 0a0a  mmit_count = 0..",
            "-00010f10: 2020 2020 2020 2020 2320 4765 7420 7468          # Get th",
            "-00010f20: 6520 6c61 7465 7374 2063 6f6d 6d69 740a  e latest commit.",
            "-00010f30: 2020 2020 2020 2020 6c61 7465 7374 5f63          latest_c",
            "-00010f40: 6f6d 6d69 7420 3d20 725b 722e 6865 6164  ommit = r[r.head",
            "-00010f50: 2829 5d0a 0a20 2020 2020 2020 2023 2057  ()]..        # W",
            "-00010f60: 616c 6b20 7468 726f 7567 6820 616c 6c20  alk through all ",
            "-00010f70: 636f 6d6d 6974 730a 2020 2020 2020 2020  commits.        ",
            "-00010f80: 7761 6c6b 6572 203d 2072 2e67 6574 5f77  walker = r.get_w",
            "-00010f90: 616c 6b65 7228 290a 2020 2020 2020 2020  alker().        ",
            "-00010fa0: 666f 7220 656e 7472 7920 696e 2077 616c  for entry in wal",
            "-00010fb0: 6b65 723a 0a20 2020 2020 2020 2020 2020  ker:.           ",
            "-00010fc0: 2023 2043 6865 636b 2069 6620 7461 670a   # Check if tag.",
            "-00010fd0: 2020 2020 2020 2020 2020 2020 636f 6d6d              comm",
            "-00010fe0: 6974 5f69 6420 3d20 656e 7472 792e 636f  it_id = entry.co",
            "-00010ff0: 6d6d 6974 2e69 642e 6465 636f 6465 2822  mmit.id.decode(\"",
            "-00011000: 6173 6369 6922 290a 2020 2020 2020 2020  ascii\").        ",
            "-00011010: 2020 2020 666f 7220 7461 6720 696e 2073      for tag in s",
            "-00011020: 6f72 7465 645f 7461 6773 3a0a 2020 2020  orted_tags:.    ",
            "-00011030: 2020 2020 2020 2020 2020 2020 7461 675f              tag_",
            "-00011040: 6e61 6d65 203d 2074 6167 5b30 5d0a 2020  name = tag[0].  ",
            "-00011050: 2020 2020 2020 2020 2020 2020 2020 7461                ta",
            "-00011060: 675f 636f 6d6d 6974 203d 2074 6167 5b31  g_commit = tag[1",
            "-00011070: 5d5b 315d 0a20 2020 2020 2020 2020 2020  ][1].           ",
            "-00011080: 2020 2020 2069 6620 636f 6d6d 6974 5f69       if commit_i",
            "-00011090: 6420 3d3d 2074 6167 5f63 6f6d 6d69 743a  d == tag_commit:",
            "-000110a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "-000110b0: 2020 2020 2069 6620 636f 6d6d 6974 5f63       if commit_c",
            "-000110c0: 6f75 6e74 203d 3d20 303a 0a20 2020 2020  ount == 0:.     ",
            "-000110d0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000110e0: 2020 2072 6574 7572 6e20 7461 675f 6e61     return tag_na",
            "-000110f0: 6d65 0a20 2020 2020 2020 2020 2020 2020  me.             ",
            "-00011100: 2020 2020 2020 2065 6c73 653a 0a20 2020         else:.   ",
            "-00011110: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00011120: 2020 2020 2072 6574 7572 6e20 227b 7d2d       return \"{}-",
            "-00011130: 7b7d 2d67 7b7d 222e 666f 726d 6174 280a  {}-g{}\".format(.",
            "-00011140: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00011150: 2020 2020 2020 2020 2020 2020 7461 675f              tag_",
            "-00011160: 6e61 6d65 2c0a 2020 2020 2020 2020 2020  name,.          ",
            "-00011170: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-00011180: 2020 636f 6d6d 6974 5f63 6f75 6e74 2c0a    commit_count,.",
            "-00011190: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "-000111a0: 2020 2020 2020 2020 2020 2020 6c61 7465              late",
            "-000111b0: 7374 5f63 6f6d 6d69 742e 6964 2e64 6563  st_commit.id.dec",
            "-000111c0: 6f64 6528 2261 7363 6969 2229 5b3a 6162  ode(\"ascii\")[:ab",
            "-000111d0: 6272 6576 5d2c 0a20 2020 2020 2020 2020  brev],.         ",
            "-000111e0: 2020 2020 2020 2020 2020 2020 2020 2029                 )",
            "-000111f0: 0a0a 2020 2020 2020 2020 2020 2020 636f  ..            co",
            "-00011200: 6d6d 6974 5f63 6f75 6e74 202b 3d20 310a  mmit_count += 1.",
            "-00011210: 0a20 2020 2020 2020 2023 2052 6574 7572  .        # Retur",
            "-00011220: 6e20 706c 6169 6e20 636f 6d6d 6974 2069  n plain commit i",
            "-00011230: 6620 6e6f 2070 6172 656e 7420 7461 6720  f no parent tag ",
            "-00011240: 6361 6e20 6265 2066 6f75 6e64 0a20 2020  can be found.   ",
            "-00011250: 2020 2020 2072 6574 7572 6e20 2267 7b7d       return \"g{}",
            "-00011260: 222e 666f 726d 6174 286c 6174 6573 745f  \".format(latest_",
            "-00011270: 636f 6d6d 6974 2e69 642e 6465 636f 6465  commit.id.decode",
            "-00011280: 2822 6173 6369 6922 295b 3a61 6262 7265  (\"ascii\")[:abbre",
            "-00011290: 765d 290a 0a0a 6465 6620 6765 745f 6f62  v])...def get_ob",
            "-000112a0: 6a65 6374 5f62 795f 7061 7468 2872 6570  ject_by_path(rep",
            "-000112b0: 6f2c 2070 6174 682c 2063 6f6d 6d69 7474  o, path, committ",
            "-000112c0: 6973 683d 4e6f 6e65 293a 0a20 2020 2022  ish=None):.    \"",
            "-000112d0: 2222 4765 7420 616e 206f 626a 6563 7420  \"\"Get an object ",
            "-000112e0: 6279 2070 6174 682e 0a0a 2020 2020 4172  by path...    Ar",
            "-000112f0: 6773 3a0a 2020 2020 2020 7265 706f 3a20  gs:.      repo: ",
            "-00011300: 4120 7061 7468 2074 6f20 7468 6520 7265  A path to the re",
            "-00011310: 706f 7369 746f 7279 0a20 2020 2020 2070  pository.      p",
            "-00011320: 6174 683a 2050 6174 6820 746f 206c 6f6f  ath: Path to loo",
            "-00011330: 6b20 7570 0a20 2020 2020 2063 6f6d 6d69  k up.      commi",
            "-00011340: 7474 6973 683a 2043 6f6d 6d69 7420 746f  ttish: Commit to",
            "-00011350: 206c 6f6f 6b20 7570 2070 6174 6820 696e   look up path in",
            "-00011360: 0a20 2020 2052 6574 7572 6e73 3a20 4120  .    Returns: A ",
            "-00011370: 6053 6861 4669 6c65 6020 6f62 6a65 6374  `ShaFile` object",
            "-00011380: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    \"\"\".    if ",
            "-00011390: 636f 6d6d 6974 7469 7368 2069 7320 4e6f  committish is No",
            "-000113a0: 6e65 3a0a 2020 2020 2020 2020 636f 6d6d  ne:.        comm",
            "-000113b0: 6974 7469 7368 203d 2022 4845 4144 220a  ittish = \"HEAD\".",
            "-000113c0: 2020 2020 2320 4765 7420 7468 6520 7265      # Get the re",
            "-000113d0: 706f 7369 746f 7279 0a20 2020 2077 6974  pository.    wit",
            "-000113e0: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "-000113f0: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "-00011400: 2020 2020 2020 2020 636f 6d6d 6974 203d          commit =",
            "-00011410: 2070 6172 7365 5f63 6f6d 6d69 7428 722c   parse_commit(r,",
            "-00011420: 2063 6f6d 6d69 7474 6973 6829 0a20 2020   committish).   ",
            "-00011430: 2020 2020 2062 6173 655f 7472 6565 203d       base_tree =",
            "-00011440: 2063 6f6d 6d69 742e 7472 6565 0a20 2020   commit.tree.   ",
            "-00011450: 2020 2020 2069 6620 6e6f 7420 6973 696e       if not isin",
            "-00011460: 7374 616e 6365 2870 6174 682c 2062 7974  stance(path, byt",
            "-00011470: 6573 293a 0a20 2020 2020 2020 2020 2020  es):.           ",
            "-00011480: 2070 6174 6820 3d20 636f 6d6d 6974 5f65   path = commit_e",
            "-00011490: 6e63 6f64 6528 636f 6d6d 6974 2c20 7061  ncode(commit, pa",
            "-000114a0: 7468 290a 2020 2020 2020 2020 286d 6f64  th).        (mod",
            "-000114b0: 652c 2073 6861 2920 3d20 7472 6565 5f6c  e, sha) = tree_l",
            "-000114c0: 6f6f 6b75 705f 7061 7468 2872 2e6f 626a  ookup_path(r.obj",
            "-000114d0: 6563 745f 7374 6f72 652e 5f5f 6765 7469  ect_store.__geti",
            "-000114e0: 7465 6d5f 5f2c 2062 6173 655f 7472 6565  tem__, base_tree",
            "-000114f0: 2c20 7061 7468 290a 2020 2020 2020 2020  , path).        ",
            "-00011500: 7265 7475 726e 2072 5b73 6861 5d0a 0a0a  return r[sha]...",
            "-00011510: 6465 6620 7772 6974 655f 7472 6565 2872  def write_tree(r",
            "-00011520: 6570 6f29 3a0a 2020 2020 2222 2257 7269  epo):.    \"\"\"Wri",
            "-00011530: 7465 2061 2074 7265 6520 6f62 6a65 6374  te a tree object",
            "-00011540: 2066 726f 6d20 7468 6520 696e 6465 782e   from the index.",
            "-00011550: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            "-00011560: 2020 7265 706f 3a20 5265 706f 7369 746f    repo: Reposito",
            "-00011570: 7279 2066 6f72 2077 6869 6368 2074 6f20  ry for which to ",
            "-00011580: 7772 6974 6520 7472 6565 0a20 2020 2052  write tree.    R",
            "-00011590: 6574 7572 6e73 3a20 7472 6565 2069 6420  eturns: tree id ",
            "-000115a0: 666f 7220 7468 6520 7472 6565 2074 6861  for the tree tha",
            "-000115b0: 7420 7761 7320 7772 6974 7465 6e0a 2020  t was written.  ",
            "-000115c0: 2020 2222 220a 2020 2020 7769 7468 206f    \"\"\".    with o",
            "-000115d0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "-000115e0: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "-000115f0: 2020 2020 2072 6574 7572 6e20 722e 6f70       return r.op",
            "-00011600: 656e 5f69 6e64 6578 2829 2e63 6f6d 6d69  en_index().commi",
            "-00011610: 7428 722e 6f62 6a65 6374 5f73 746f 7265  t(r.object_store",
            "-00011620: 290a                                     ).",
            "+0000f390: 2065 7272 7374 7265 616d 2e77 7269 7465   errstream.write",
            "+0000f3a0: 2862 2252 6566 2025 7320 7570 6461 7465  (b\"Ref %s update",
            "+0000f3b0: 645c 6e22 2025 2072 6566 290a 0a20 2020  d\\n\" % ref)..   ",
            "+0000f3c0: 2020 2020 2069 6620 7265 6d6f 7465 5f6e       if remote_n",
            "+0000f3d0: 616d 6520 6973 206e 6f74 204e 6f6e 653a  ame is not None:",
            "+0000f3e0: 0a20 2020 2020 2020 2020 2020 205f 696d  .            _im",
            "+0000f3f0: 706f 7274 5f72 656d 6f74 655f 7265 6673  port_remote_refs",
            "+0000f400: 2872 2e72 6566 732c 2072 656d 6f74 655f  (r.refs, remote_",
            "+0000f410: 6e61 6d65 2c20 7265 6d6f 7465 5f63 6861  name, remote_cha",
            "+0000f420: 6e67 6564 5f72 6566 7329 0a0a 2020 2020  nged_refs)..    ",
            "+0000f430: 2020 2020 7265 7475 726e 2072 6573 756c      return resul",
            "+0000f440: 740a 0a20 2020 2023 2054 7269 6767 6572  t..    # Trigger",
            "+0000f450: 2061 7574 6f20 4743 2069 6620 6e65 6564   auto GC if need",
            "+0000f460: 6564 0a20 2020 2066 726f 6d20 2e67 6320  ed.    from .gc ",
            "+0000f470: 696d 706f 7274 206d 6179 6265 5f61 7574  import maybe_aut",
            "+0000f480: 6f5f 6763 0a0a 2020 2020 7769 7468 206f  o_gc..    with o",
            "+0000f490: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+0000f4a0: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+0000f4b0: 2020 2020 206d 6179 6265 5f61 7574 6f5f       maybe_auto_",
            "+0000f4c0: 6763 2872 290a 0a0a 6465 6620 7075 6c6c  gc(r)...def pull",
            "+0000f4d0: 280a 2020 2020 7265 706f 2c0a 2020 2020  (.    repo,.    ",
            "+0000f4e0: 7265 6d6f 7465 5f6c 6f63 6174 696f 6e3d  remote_location=",
            "+0000f4f0: 4e6f 6e65 2c0a 2020 2020 7265 6673 7065  None,.    refspe",
            "+0000f500: 6373 3d4e 6f6e 652c 0a20 2020 206f 7574  cs=None,.    out",
            "+0000f510: 7374 7265 616d 3d64 6566 6175 6c74 5f62  stream=default_b",
            "+0000f520: 7974 6573 5f6f 7574 5f73 7472 6561 6d2c  ytes_out_stream,",
            "+0000f530: 0a20 2020 2065 7272 7374 7265 616d 3d64  .    errstream=d",
            "+0000f540: 6566 6175 6c74 5f62 7974 6573 5f65 7272  efault_bytes_err",
            "+0000f550: 5f73 7472 6561 6d2c 0a20 2020 2066 6173  _stream,.    fas",
            "+0000f560: 745f 666f 7277 6172 643d 5472 7565 2c0a  t_forward=True,.",
            "+0000f570: 2020 2020 6666 5f6f 6e6c 793d 4661 6c73      ff_only=Fals",
            "+0000f580: 652c 0a20 2020 2066 6f72 6365 3d46 616c  e,.    force=Fal",
            "+0000f590: 7365 2c0a 2020 2020 6669 6c74 6572 5f73  se,.    filter_s",
            "+0000f5a0: 7065 633d 4e6f 6e65 2c0a 2020 2020 7072  pec=None,.    pr",
            "+0000f5b0: 6f74 6f63 6f6c 5f76 6572 7369 6f6e 3d4e  otocol_version=N",
            "+0000f5c0: 6f6e 652c 0a20 2020 202a 2a6b 7761 7267  one,.    **kwarg",
            "+0000f5d0: 732c 0a29 202d 3e20 4e6f 6e65 3a0a 2020  s,.) -> None:.  ",
            "+0000f5e0: 2020 2222 2250 756c 6c20 6672 6f6d 2072    \"\"\"Pull from r",
            "+0000f5f0: 656d 6f74 6520 7669 6120 6475 6c77 6963  emote via dulwic",
            "+0000f600: 682e 636c 6965 6e74 2e0a 0a20 2020 2041  h.client...    A",
            "+0000f610: 7267 733a 0a20 2020 2020 2072 6570 6f3a  rgs:.      repo:",
            "+0000f620: 2050 6174 6820 746f 2072 6570 6f73 6974   Path to reposit",
            "+0000f630: 6f72 790a 2020 2020 2020 7265 6d6f 7465  ory.      remote",
            "+0000f640: 5f6c 6f63 6174 696f 6e3a 204c 6f63 6174  _location: Locat",
            "+0000f650: 696f 6e20 6f66 2074 6865 2072 656d 6f74  ion of the remot",
            "+0000f660: 650a 2020 2020 2020 7265 6673 7065 6373  e.      refspecs",
            "+0000f670: 3a20 7265 6673 7065 6373 2074 6f20 6665  : refspecs to fe",
            "+0000f680: 7463 682e 2043 616e 2062 6520 6120 6279  tch. Can be a by",
            "+0000f690: 7465 7374 7269 6e67 2c20 6120 7374 7269  testring, a stri",
            "+0000f6a0: 6e67 2c20 6f72 2061 206c 6973 7420 6f66  ng, or a list of",
            "+0000f6b0: 0a20 2020 2020 2020 2062 7974 6573 7472  .        bytestr",
            "+0000f6c0: 696e 672f 7374 7269 6e67 2e0a 2020 2020  ing/string..    ",
            "+0000f6d0: 2020 6f75 7473 7472 6561 6d3a 2041 2073    outstream: A s",
            "+0000f6e0: 7472 6561 6d20 6669 6c65 2074 6f20 7772  tream file to wr",
            "+0000f6f0: 6974 6520 746f 206f 7574 7075 740a 2020  ite to output.  ",
            "+0000f700: 2020 2020 6572 7273 7472 6561 6d3a 2041      errstream: A",
            "+0000f710: 2073 7472 6561 6d20 6669 6c65 2074 6f20   stream file to ",
            "+0000f720: 7772 6974 6520 746f 2065 7272 6f72 730a  write to errors.",
            "+0000f730: 2020 2020 2020 6661 7374 5f66 6f72 7761        fast_forwa",
            "+0000f740: 7264 3a20 4966 2054 7275 652c 2072 6169  rd: If True, rai",
            "+0000f750: 7365 2061 6e20 6578 6365 7074 696f 6e20  se an exception ",
            "+0000f760: 7768 656e 2066 6173 742d 666f 7277 6172  when fast-forwar",
            "+0000f770: 6420 6973 206e 6f74 2070 6f73 7369 626c  d is not possibl",
            "+0000f780: 650a 2020 2020 2020 6666 5f6f 6e6c 793a  e.      ff_only:",
            "+0000f790: 2049 6620 5472 7565 2c20 6f6e 6c79 2061   If True, only a",
            "+0000f7a0: 6c6c 6f77 2066 6173 742d 666f 7277 6172  llow fast-forwar",
            "+0000f7b0: 6420 6d65 7267 6573 2e20 5261 6973 6573  d merges. Raises",
            "+0000f7c0: 2044 6976 6572 6765 6442 7261 6e63 6865   DivergedBranche",
            "+0000f7d0: 730a 2020 2020 2020 2020 7768 656e 2062  s.        when b",
            "+0000f7e0: 7261 6e63 6865 7320 6861 7665 2064 6976  ranches have div",
            "+0000f7f0: 6572 6765 6420 7261 7468 6572 2074 6861  erged rather tha",
            "+0000f800: 6e20 7065 7266 6f72 6d69 6e67 2061 206d  n performing a m",
            "+0000f810: 6572 6765 2e0a 2020 2020 2020 6669 6c74  erge..      filt",
            "+0000f820: 6572 5f73 7065 633a 2041 2067 6974 2d72  er_spec: A git-r",
            "+0000f830: 6576 2d6c 6973 742d 7374 796c 6520 6f62  ev-list-style ob",
            "+0000f840: 6a65 6374 2066 696c 7465 7220 7370 6563  ject filter spec",
            "+0000f850: 2c20 6173 2061 6e20 4153 4349 4920 7374  , as an ASCII st",
            "+0000f860: 7269 6e67 2e0a 2020 2020 2020 2020 4f6e  ring..        On",
            "+0000f870: 6c79 2075 7365 6420 6966 2074 6865 2073  ly used if the s",
            "+0000f880: 6572 7665 7220 7375 7070 6f72 7473 2074  erver supports t",
            "+0000f890: 6865 2047 6974 2070 726f 746f 636f 6c2d  he Git protocol-",
            "+0000f8a0: 7632 2027 6669 6c74 6572 270a 2020 2020  v2 'filter'.    ",
            "+0000f8b0: 2020 2020 6665 6174 7572 652c 2061 6e64      feature, and",
            "+0000f8c0: 2069 676e 6f72 6564 206f 7468 6572 7769   ignored otherwi",
            "+0000f8d0: 7365 2e0a 2020 2020 2020 7072 6f74 6f63  se..      protoc",
            "+0000f8e0: 6f6c 5f76 6572 7369 6f6e 3a20 6465 7369  ol_version: desi",
            "+0000f8f0: 7265 6420 4769 7420 7072 6f74 6f63 6f6c  red Git protocol",
            "+0000f900: 2076 6572 7369 6f6e 2e20 4279 2064 6566   version. By def",
            "+0000f910: 6175 6c74 2074 6865 2068 6967 6865 7374  ault the highest",
            "+0000f920: 0a20 2020 2020 2020 206d 7574 7561 6c6c  .        mutuall",
            "+0000f930: 7920 7375 7070 6f72 7465 6420 7072 6f74  y supported prot",
            "+0000f940: 6f63 6f6c 2076 6572 7369 6f6e 2077 696c  ocol version wil",
            "+0000f950: 6c20 6265 2075 7365 640a 2020 2020 2222  l be used.    \"\"",
            "+0000f960: 220a 2020 2020 2320 4f70 656e 2074 6865  \".    # Open the",
            "+0000f970: 2072 6570 6f0a 2020 2020 7769 7468 206f   repo.    with o",
            "+0000f980: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+0000f990: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+0000f9a0: 2020 2020 2028 7265 6d6f 7465 5f6e 616d       (remote_nam",
            "+0000f9b0: 652c 2072 656d 6f74 655f 6c6f 6361 7469  e, remote_locati",
            "+0000f9c0: 6f6e 2920 3d20 6765 745f 7265 6d6f 7465  on) = get_remote",
            "+0000f9d0: 5f72 6570 6f28 722c 2072 656d 6f74 655f  _repo(r, remote_",
            "+0000f9e0: 6c6f 6361 7469 6f6e 290a 0a20 2020 2020  location)..     ",
            "+0000f9f0: 2020 2073 656c 6563 7465 645f 7265 6673     selected_refs",
            "+0000fa00: 203d 205b 5d0a 0a20 2020 2020 2020 2069   = []..        i",
            "+0000fa10: 6620 7265 6673 7065 6373 2069 7320 4e6f  f refspecs is No",
            "+0000fa20: 6e65 3a0a 2020 2020 2020 2020 2020 2020  ne:.            ",
            "+0000fa30: 7265 6673 7065 6373 203d 205b 6222 4845  refspecs = [b\"HE",
            "+0000fa40: 4144 225d 0a0a 2020 2020 2020 2020 6465  AD\"]..        de",
            "+0000fa50: 6620 6465 7465 726d 696e 655f 7761 6e74  f determine_want",
            "+0000fa60: 7328 7265 6d6f 7465 5f72 6566 732c 202a  s(remote_refs, *",
            "+0000fa70: 6172 6773 2c20 2a2a 6b77 6172 6773 293a  args, **kwargs):",
            "+0000fa80: 0a20 2020 2020 2020 2020 2020 2073 656c  .            sel",
            "+0000fa90: 6563 7465 645f 7265 6673 2e65 7874 656e  ected_refs.exten",
            "+0000faa0: 6428 0a20 2020 2020 2020 2020 2020 2020  d(.             ",
            "+0000fab0: 2020 2070 6172 7365 5f72 6566 7475 706c     parse_reftupl",
            "+0000fac0: 6573 2872 656d 6f74 655f 7265 6673 2c20  es(remote_refs, ",
            "+0000fad0: 722e 7265 6673 2c20 7265 6673 7065 6373  r.refs, refspecs",
            "+0000fae0: 2c20 666f 7263 653d 666f 7263 6529 0a20  , force=force). ",
            "+0000faf0: 2020 2020 2020 2020 2020 2029 0a20 2020             ).   ",
            "+0000fb00: 2020 2020 2020 2020 2072 6574 7572 6e20           return ",
            "+0000fb10: 5b0a 2020 2020 2020 2020 2020 2020 2020  [.              ",
            "+0000fb20: 2020 7265 6d6f 7465 5f72 6566 735b 6c68    remote_refs[lh",
            "+0000fb30: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              ",
            "+0000fb40: 2020 666f 7220 286c 682c 2072 682c 2066    for (lh, rh, f",
            "+0000fb50: 6f72 6365 5f72 6566 2920 696e 2073 656c  orce_ref) in sel",
            "+0000fb60: 6563 7465 645f 7265 6673 0a20 2020 2020  ected_refs.     ",
            "+0000fb70: 2020 2020 2020 2020 2020 2069 6620 7265             if re",
            "+0000fb80: 6d6f 7465 5f72 6566 735b 6c68 5d20 6e6f  mote_refs[lh] no",
            "+0000fb90: 7420 696e 2072 2e6f 626a 6563 745f 7374  t in r.object_st",
            "+0000fba0: 6f72 650a 2020 2020 2020 2020 2020 2020  ore.            ",
            "+0000fbb0: 5d0a 0a20 2020 2020 2020 2063 6c69 656e  ]..        clien",
            "+0000fbc0: 742c 2070 6174 6820 3d20 6765 745f 7472  t, path = get_tr",
            "+0000fbd0: 616e 7370 6f72 745f 616e 645f 7061 7468  ansport_and_path",
            "+0000fbe0: 280a 2020 2020 2020 2020 2020 2020 7265  (.            re",
            "+0000fbf0: 6d6f 7465 5f6c 6f63 6174 696f 6e2c 2063  mote_location, c",
            "+0000fc00: 6f6e 6669 673d 722e 6765 745f 636f 6e66  onfig=r.get_conf",
            "+0000fc10: 6967 5f73 7461 636b 2829 2c20 2a2a 6b77  ig_stack(), **kw",
            "+0000fc20: 6172 6773 0a20 2020 2020 2020 2029 0a20  args.        ). ",
            "+0000fc30: 2020 2020 2020 2069 6620 6669 6c74 6572         if filter",
            "+0000fc40: 5f73 7065 633a 0a20 2020 2020 2020 2020  _spec:.         ",
            "+0000fc50: 2020 2066 696c 7465 725f 7370 6563 203d     filter_spec =",
            "+0000fc60: 2066 696c 7465 725f 7370 6563 2e65 6e63   filter_spec.enc",
            "+0000fc70: 6f64 6528 2261 7363 6969 2229 0a20 2020  ode(\"ascii\").   ",
            "+0000fc80: 2020 2020 2066 6574 6368 5f72 6573 756c       fetch_resul",
            "+0000fc90: 7420 3d20 636c 6965 6e74 2e66 6574 6368  t = client.fetch",
            "+0000fca0: 280a 2020 2020 2020 2020 2020 2020 7061  (.            pa",
            "+0000fcb0: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            ",
            "+0000fcc0: 722c 0a20 2020 2020 2020 2020 2020 2070  r,.            p",
            "+0000fcd0: 726f 6772 6573 733d 6572 7273 7472 6561  rogress=errstrea",
            "+0000fce0: 6d2e 7772 6974 652c 0a20 2020 2020 2020  m.write,.       ",
            "+0000fcf0: 2020 2020 2064 6574 6572 6d69 6e65 5f77       determine_w",
            "+0000fd00: 616e 7473 3d64 6574 6572 6d69 6e65 5f77  ants=determine_w",
            "+0000fd10: 616e 7473 2c0a 2020 2020 2020 2020 2020  ants,.          ",
            "+0000fd20: 2020 6669 6c74 6572 5f73 7065 633d 6669    filter_spec=fi",
            "+0000fd30: 6c74 6572 5f73 7065 632c 0a20 2020 2020  lter_spec,.     ",
            "+0000fd40: 2020 2020 2020 2070 726f 746f 636f 6c5f         protocol_",
            "+0000fd50: 7665 7273 696f 6e3d 7072 6f74 6f63 6f6c  version=protocol",
            "+0000fd60: 5f76 6572 7369 6f6e 2c0a 2020 2020 2020  _version,.      ",
            "+0000fd70: 2020 290a 0a20 2020 2020 2020 2023 2053    )..        # S",
            "+0000fd80: 746f 7265 2074 6865 206f 6c64 2048 4541  tore the old HEA",
            "+0000fd90: 4420 7472 6565 2062 6566 6f72 6520 6d61  D tree before ma",
            "+0000fda0: 6b69 6e67 2063 6861 6e67 6573 0a20 2020  king changes.   ",
            "+0000fdb0: 2020 2020 2074 7279 3a0a 2020 2020 2020       try:.      ",
            "+0000fdc0: 2020 2020 2020 6f6c 645f 6865 6164 203d        old_head =",
            "+0000fdd0: 2072 2e72 6566 735b 6222 4845 4144 225d   r.refs[b\"HEAD\"]",
            "+0000fde0: 0a20 2020 2020 2020 2020 2020 206f 6c64  .            old",
            "+0000fdf0: 5f74 7265 655f 6964 203d 2072 5b6f 6c64  _tree_id = r[old",
            "+0000fe00: 5f68 6561 645d 2e74 7265 650a 2020 2020  _head].tree.    ",
            "+0000fe10: 2020 2020 6578 6365 7074 204b 6579 4572      except KeyEr",
            "+0000fe20: 726f 723a 0a20 2020 2020 2020 2020 2020  ror:.           ",
            "+0000fe30: 206f 6c64 5f74 7265 655f 6964 203d 204e   old_tree_id = N",
            "+0000fe40: 6f6e 650a 0a20 2020 2020 2020 206d 6572  one..        mer",
            "+0000fe50: 6765 6420 3d20 4661 6c73 650a 2020 2020  ged = False.    ",
            "+0000fe60: 2020 2020 666f 7220 6c68 2c20 7268 2c20      for lh, rh, ",
            "+0000fe70: 666f 7263 655f 7265 6620 696e 2073 656c  force_ref in sel",
            "+0000fe80: 6563 7465 645f 7265 6673 3a0a 2020 2020  ected_refs:.    ",
            "+0000fe90: 2020 2020 2020 2020 6966 206e 6f74 2066          if not f",
            "+0000fea0: 6f72 6365 5f72 6566 2061 6e64 2072 6820  orce_ref and rh ",
            "+0000feb0: 696e 2072 2e72 6566 733a 0a20 2020 2020  in r.refs:.     ",
            "+0000fec0: 2020 2020 2020 2020 2020 2074 7279 3a0a             try:.",
            "+0000fed0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000fee0: 2020 2020 6368 6563 6b5f 6469 7665 7267      check_diverg",
            "+0000fef0: 6564 2872 2c20 722e 7265 6673 2e66 6f6c  ed(r, r.refs.fol",
            "+0000ff00: 6c6f 7728 7268 295b 315d 2c20 6665 7463  low(rh)[1], fetc",
            "+0000ff10: 685f 7265 7375 6c74 2e72 6566 735b 6c68  h_result.refs[lh",
            "+0000ff20: 5d29 0a20 2020 2020 2020 2020 2020 2020  ]).             ",
            "+0000ff30: 2020 2065 7863 6570 7420 4469 7665 7267     except Diverg",
            "+0000ff40: 6564 4272 616e 6368 6573 2061 7320 6578  edBranches as ex",
            "+0000ff50: 633a 0a20 2020 2020 2020 2020 2020 2020  c:.             ",
            "+0000ff60: 2020 2020 2020 2069 6620 6666 5f6f 6e6c         if ff_onl",
            "+0000ff70: 7920 6f72 2066 6173 745f 666f 7277 6172  y or fast_forwar",
            "+0000ff80: 643a 0a20 2020 2020 2020 2020 2020 2020  d:.             ",
            "+0000ff90: 2020 2020 2020 2020 2020 2072 6169 7365             raise",
            "+0000ffa0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0000ffb0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     ",
            "+0000ffc0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0000ffd0: 2020 2023 2050 6572 666f 726d 206d 6572     # Perform mer",
            "+0000ffe0: 6765 0a20 2020 2020 2020 2020 2020 2020  ge.             ",
            "+0000fff0: 2020 2020 2020 2020 2020 206d 6572 6765             merge",
            "+00010000: 5f72 6573 756c 742c 2063 6f6e 666c 6963  _result, conflic",
            "+00010010: 7473 203d 205f 646f 5f6d 6572 6765 2872  ts = _do_merge(r",
            "+00010020: 2c20 6665 7463 685f 7265 7375 6c74 2e72  , fetch_result.r",
            "+00010030: 6566 735b 6c68 5d29 0a20 2020 2020 2020  efs[lh]).       ",
            "+00010040: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00010050: 2069 6620 636f 6e66 6c69 6374 733a 0a20   if conflicts:. ",
            "+00010060: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00010070: 2020 2020 2020 2020 2020 2072 6169 7365             raise",
            "+00010080: 2045 7272 6f72 280a 2020 2020 2020 2020   Error(.        ",
            "+00010090: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000100a0: 2020 2020 2020 2020 6622 4d65 7267 6520          f\"Merge ",
            "+000100b0: 636f 6e66 6c69 6374 7320 6f63 6375 7272  conflicts occurr",
            "+000100c0: 6564 3a20 7b63 6f6e 666c 6963 7473 7d22  ed: {conflicts}\"",
            "+000100d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000100e0: 2020 2020 2020 2020 2020 2020 2029 2066               ) f",
            "+000100f0: 726f 6d20 6578 630a 2020 2020 2020 2020  rom exc.        ",
            "+00010100: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00010110: 6d65 7267 6564 203d 2054 7275 650a 2020  merged = True.  ",
            "+00010120: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00010130: 2020 2020 2020 2320 536b 6970 2075 7064        # Skip upd",
            "+00010140: 6174 696e 6720 7265 6620 7369 6e63 6520  ating ref since ",
            "+00010150: 6d65 7267 6520 616c 7265 6164 7920 7570  merge already up",
            "+00010160: 6461 7465 6420 4845 4144 0a20 2020 2020  dated HEAD.     ",
            "+00010170: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00010180: 2020 2063 6f6e 7469 6e75 650a 2020 2020     continue.    ",
            "+00010190: 2020 2020 2020 2020 722e 7265 6673 5b72          r.refs[r",
            "+000101a0: 685d 203d 2066 6574 6368 5f72 6573 756c  h] = fetch_resul",
            "+000101b0: 742e 7265 6673 5b6c 685d 0a0a 2020 2020  t.refs[lh]..    ",
            "+000101c0: 2020 2020 2320 4f6e 6c79 2075 7064 6174      # Only updat",
            "+000101d0: 6520 4845 4144 2069 6620 7765 2064 6964  e HEAD if we did",
            "+000101e0: 6e27 7420 7065 7266 6f72 6d20 6120 6d65  n't perform a me",
            "+000101f0: 7267 650a 2020 2020 2020 2020 6966 2073  rge.        if s",
            "+00010200: 656c 6563 7465 645f 7265 6673 2061 6e64  elected_refs and",
            "+00010210: 206e 6f74 206d 6572 6765 643a 0a20 2020   not merged:.   ",
            "+00010220: 2020 2020 2020 2020 2072 5b62 2248 4541           r[b\"HEA",
            "+00010230: 4422 5d20 3d20 6665 7463 685f 7265 7375  D\"] = fetch_resu",
            "+00010240: 6c74 2e72 6566 735b 7365 6c65 6374 6564  lt.refs[selected",
            "+00010250: 5f72 6566 735b 305d 5b31 5d5d 0a0a 2020  _refs[0][1]]..  ",
            "+00010260: 2020 2020 2020 2320 5570 6461 7465 2077        # Update w",
            "+00010270: 6f72 6b69 6e67 2074 7265 6520 746f 206d  orking tree to m",
            "+00010280: 6174 6368 2074 6865 206e 6577 2048 4541  atch the new HEA",
            "+00010290: 440a 2020 2020 2020 2020 2320 536b 6970  D.        # Skip",
            "+000102a0: 2069 6620 6d65 7267 6520 7761 7320 7065   if merge was pe",
            "+000102b0: 7266 6f72 6d65 6420 6173 206d 6572 6765  rformed as merge",
            "+000102c0: 2061 6c72 6561 6479 2075 7064 6174 6573   already updates",
            "+000102d0: 2074 6865 2077 6f72 6b69 6e67 2074 7265   the working tre",
            "+000102e0: 650a 2020 2020 2020 2020 6966 206e 6f74  e.        if not",
            "+000102f0: 206d 6572 6765 6420 616e 6420 6f6c 645f   merged and old_",
            "+00010300: 7472 6565 5f69 6420 6973 206e 6f74 204e  tree_id is not N",
            "+00010310: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           ",
            "+00010320: 206e 6577 5f74 7265 655f 6964 203d 2072   new_tree_id = r",
            "+00010330: 5b62 2248 4541 4422 5d2e 7472 6565 0a20  [b\"HEAD\"].tree. ",
            "+00010340: 2020 2020 2020 2020 2020 2062 6c6f 625f             blob_",
            "+00010350: 6e6f 726d 616c 697a 6572 203d 2072 2e67  normalizer = r.g",
            "+00010360: 6574 5f62 6c6f 625f 6e6f 726d 616c 697a  et_blob_normaliz",
            "+00010370: 6572 2829 0a20 2020 2020 2020 2020 2020  er().           ",
            "+00010380: 2075 7064 6174 655f 776f 726b 696e 675f   update_working_",
            "+00010390: 7472 6565 280a 2020 2020 2020 2020 2020  tree(.          ",
            "+000103a0: 2020 2020 2020 722c 206f 6c64 5f74 7265        r, old_tre",
            "+000103b0: 655f 6964 2c20 6e65 775f 7472 6565 5f69  e_id, new_tree_i",
            "+000103c0: 642c 2062 6c6f 625f 6e6f 726d 616c 697a  d, blob_normaliz",
            "+000103d0: 6572 3d62 6c6f 625f 6e6f 726d 616c 697a  er=blob_normaliz",
            "+000103e0: 6572 0a20 2020 2020 2020 2020 2020 2029  er.            )",
            "+000103f0: 0a20 2020 2020 2020 2069 6620 7265 6d6f  .        if remo",
            "+00010400: 7465 5f6e 616d 6520 6973 206e 6f74 204e  te_name is not N",
            "+00010410: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           ",
            "+00010420: 205f 696d 706f 7274 5f72 656d 6f74 655f   _import_remote_",
            "+00010430: 7265 6673 2872 2e72 6566 732c 2072 656d  refs(r.refs, rem",
            "+00010440: 6f74 655f 6e61 6d65 2c20 6665 7463 685f  ote_name, fetch_",
            "+00010450: 7265 7375 6c74 2e72 6566 7329 0a0a 2020  result.refs)..  ",
            "+00010460: 2020 2320 5472 6967 6765 7220 6175 746f    # Trigger auto",
            "+00010470: 2047 4320 6966 206e 6565 6465 640a 2020   GC if needed.  ",
            "+00010480: 2020 6672 6f6d 202e 6763 2069 6d70 6f72    from .gc impor",
            "+00010490: 7420 6d61 7962 655f 6175 746f 5f67 630a  t maybe_auto_gc.",
            "+000104a0: 0a20 2020 2077 6974 6820 6f70 656e 5f72  .    with open_r",
            "+000104b0: 6570 6f5f 636c 6f73 696e 6728 7265 706f  epo_closing(repo",
            "+000104c0: 2920 6173 2072 3a0a 2020 2020 2020 2020  ) as r:.        ",
            "+000104d0: 6d61 7962 655f 6175 746f 5f67 6328 7229  maybe_auto_gc(r)",
            "+000104e0: 0a0a 0a64 6566 2073 7461 7475 7328 7265  ...def status(re",
            "+000104f0: 706f 3d22 2e22 2c20 6967 6e6f 7265 643d  po=\".\", ignored=",
            "+00010500: 4661 6c73 652c 2075 6e74 7261 636b 6564  False, untracked",
            "+00010510: 5f66 696c 6573 3d22 6e6f 726d 616c 2229  _files=\"normal\")",
            "+00010520: 3a0a 2020 2020 2222 2252 6574 7572 6e73  :.    \"\"\"Returns",
            "+00010530: 2073 7461 6765 642c 2075 6e73 7461 6765   staged, unstage",
            "+00010540: 642c 2061 6e64 2075 6e74 7261 636b 6564  d, and untracked",
            "+00010550: 2063 6861 6e67 6573 2072 656c 6174 6976   changes relativ",
            "+00010560: 6520 746f 2074 6865 2048 4541 442e 0a0a  e to the HEAD...",
            "+00010570: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+00010580: 7265 706f 3a20 5061 7468 2074 6f20 7265  repo: Path to re",
            "+00010590: 706f 7369 746f 7279 206f 7220 7265 706f  pository or repo",
            "+000105a0: 7369 746f 7279 206f 626a 6563 740a 2020  sitory object.  ",
            "+000105b0: 2020 2020 6967 6e6f 7265 643a 2057 6865      ignored: Whe",
            "+000105c0: 7468 6572 2074 6f20 696e 636c 7564 6520  ther to include ",
            "+000105d0: 6967 6e6f 7265 6420 6669 6c65 7320 696e  ignored files in",
            "+000105e0: 2075 6e74 7261 636b 6564 0a20 2020 2020   untracked.     ",
            "+000105f0: 2075 6e74 7261 636b 6564 5f66 696c 6573   untracked_files",
            "+00010600: 3a20 486f 7720 746f 2068 616e 646c 6520  : How to handle ",
            "+00010610: 756e 7472 6163 6b65 6420 6669 6c65 732c  untracked files,",
            "+00010620: 2064 6566 6175 6c74 7320 746f 2022 616c   defaults to \"al",
            "+00010630: 6c22 3a0a 2020 2020 2020 2020 2020 226e  l\":.          \"n",
            "+00010640: 6f22 3a20 646f 206e 6f74 2072 6574 7572  o\": do not retur",
            "+00010650: 6e20 756e 7472 6163 6b65 6420 6669 6c65  n untracked file",
            "+00010660: 730a 2020 2020 2020 2020 2020 226e 6f72  s.          \"nor",
            "+00010670: 6d61 6c22 3a20 7265 7475 726e 2075 6e74  mal\": return unt",
            "+00010680: 7261 636b 6564 2064 6972 6563 746f 7269  racked directori",
            "+00010690: 6573 2c20 6e6f 7420 7468 6569 7220 636f  es, not their co",
            "+000106a0: 6e74 656e 7473 0a20 2020 2020 2020 2020  ntents.         ",
            "+000106b0: 2022 616c 6c22 3a20 696e 636c 7564 6520   \"all\": include ",
            "+000106c0: 616c 6c20 6669 6c65 7320 696e 2075 6e74  all files in unt",
            "+000106d0: 7261 636b 6564 2064 6972 6563 746f 7269  racked directori",
            "+000106e0: 6573 0a20 2020 2020 2020 2055 7369 6e67  es.        Using",
            "+000106f0: 2075 6e74 7261 636b 6564 5f66 696c 6573   untracked_files",
            "+00010700: 3d22 6e6f 2220 6361 6e20 6265 2066 6173  =\"no\" can be fas",
            "+00010710: 7465 7220 7468 616e 2022 616c 6c22 2077  ter than \"all\" w",
            "+00010720: 6865 6e20 7468 6520 776f 726b 7472 6565  hen the worktree",
            "+00010730: 0a20 2020 2020 2020 2020 2063 6f6e 7461  .          conta",
            "+00010740: 696e 7320 6d61 6e79 2075 6e74 7261 636b  ins many untrack",
            "+00010750: 6564 2066 696c 6573 2f64 6972 6563 746f  ed files/directo",
            "+00010760: 7269 6573 2e0a 2020 2020 2020 2020 5573  ries..        Us",
            "+00010770: 696e 6720 756e 7472 6163 6b65 645f 6669  ing untracked_fi",
            "+00010780: 6c65 733d 226e 6f72 6d61 6c22 2070 726f  les=\"normal\" pro",
            "+00010790: 7669 6465 7320 6120 676f 6f64 2062 616c  vides a good bal",
            "+000107a0: 616e 6365 2c20 6f6e 6c79 2073 686f 7769  ance, only showi",
            "+000107b0: 6e67 0a20 2020 2020 2020 2020 2064 6972  ng.          dir",
            "+000107c0: 6563 746f 7269 6573 2074 6861 7420 6172  ectories that ar",
            "+000107d0: 6520 656e 7469 7265 6c79 2075 6e74 7261  e entirely untra",
            "+000107e0: 636b 6564 2077 6974 686f 7574 206c 6973  cked without lis",
            "+000107f0: 7469 6e67 2061 6c6c 2074 6865 6972 2063  ting all their c",
            "+00010800: 6f6e 7465 6e74 732e 0a0a 2020 2020 5265  ontents...    Re",
            "+00010810: 7475 726e 733a 2047 6974 5374 6174 7573  turns: GitStatus",
            "+00010820: 2074 7570 6c65 2c0a 2020 2020 2020 2020   tuple,.        ",
            "+00010830: 7374 6167 6564 202d 2020 6469 6374 2077  staged -  dict w",
            "+00010840: 6974 6820 6c69 7374 7320 6f66 2073 7461  ith lists of sta",
            "+00010850: 6765 6420 7061 7468 7320 2864 6966 6620  ged paths (diff ",
            "+00010860: 696e 6465 782f 4845 4144 290a 2020 2020  index/HEAD).    ",
            "+00010870: 2020 2020 756e 7374 6167 6564 202d 2020      unstaged -  ",
            "+00010880: 6c69 7374 206f 6620 756e 7374 6167 6564  list of unstaged",
            "+00010890: 2070 6174 6873 2028 6469 6666 2069 6e64   paths (diff ind",
            "+000108a0: 6578 2f77 6f72 6b69 6e67 2d74 7265 6529  ex/working-tree)",
            "+000108b0: 0a20 2020 2020 2020 2075 6e74 7261 636b  .        untrack",
            "+000108c0: 6564 202d 206c 6973 7420 6f66 2075 6e74  ed - list of unt",
            "+000108d0: 7261 636b 6564 2c20 756e 2d69 676e 6f72  racked, un-ignor",
            "+000108e0: 6564 2026 206e 6f6e 2d2e 6769 7420 7061  ed & non-.git pa",
            "+000108f0: 7468 730a 2020 2020 2222 220a 2020 2020  ths.    \"\"\".    ",
            "+00010900: 7769 7468 206f 7065 6e5f 7265 706f 5f63  with open_repo_c",
            "+00010910: 6c6f 7369 6e67 2872 6570 6f29 2061 7320  losing(repo) as ",
            "+00010920: 723a 0a20 2020 2020 2020 2023 2031 2e20  r:.        # 1. ",
            "+00010930: 4765 7420 7374 6174 7573 206f 6620 7374  Get status of st",
            "+00010940: 6167 6564 0a20 2020 2020 2020 2074 7261  aged.        tra",
            "+00010950: 636b 6564 5f63 6861 6e67 6573 203d 2067  cked_changes = g",
            "+00010960: 6574 5f74 7265 655f 6368 616e 6765 7328  et_tree_changes(",
            "+00010970: 7229 0a20 2020 2020 2020 2023 2032 2e20  r).        # 2. ",
            "+00010980: 4765 7420 7374 6174 7573 206f 6620 756e  Get status of un",
            "+00010990: 7374 6167 6564 0a20 2020 2020 2020 2069  staged.        i",
            "+000109a0: 6e64 6578 203d 2072 2e6f 7065 6e5f 696e  ndex = r.open_in",
            "+000109b0: 6465 7828 290a 2020 2020 2020 2020 6e6f  dex().        no",
            "+000109c0: 726d 616c 697a 6572 203d 2072 2e67 6574  rmalizer = r.get",
            "+000109d0: 5f62 6c6f 625f 6e6f 726d 616c 697a 6572  _blob_normalizer",
            "+000109e0: 2829 0a20 2020 2020 2020 2066 696c 7465  ().        filte",
            "+000109f0: 725f 6361 6c6c 6261 636b 203d 206e 6f72  r_callback = nor",
            "+00010a00: 6d61 6c69 7a65 722e 6368 6563 6b69 6e5f  malizer.checkin_",
            "+00010a10: 6e6f 726d 616c 697a 650a 2020 2020 2020  normalize.      ",
            "+00010a20: 2020 756e 7374 6167 6564 5f63 6861 6e67    unstaged_chang",
            "+00010a30: 6573 203d 206c 6973 7428 6765 745f 756e  es = list(get_un",
            "+00010a40: 7374 6167 6564 5f63 6861 6e67 6573 2869  staged_changes(i",
            "+00010a50: 6e64 6578 2c20 722e 7061 7468 2c20 6669  ndex, r.path, fi",
            "+00010a60: 6c74 6572 5f63 616c 6c62 6163 6b29 290a  lter_callback)).",
            "+00010a70: 0a20 2020 2020 2020 2075 6e74 7261 636b  .        untrack",
            "+00010a80: 6564 5f70 6174 6873 203d 2067 6574 5f75  ed_paths = get_u",
            "+00010a90: 6e74 7261 636b 6564 5f70 6174 6873 280a  ntracked_paths(.",
            "+00010aa0: 2020 2020 2020 2020 2020 2020 722e 7061              r.pa",
            "+00010ab0: 7468 2c0a 2020 2020 2020 2020 2020 2020  th,.            ",
            "+00010ac0: 722e 7061 7468 2c0a 2020 2020 2020 2020  r.path,.        ",
            "+00010ad0: 2020 2020 696e 6465 782c 0a20 2020 2020      index,.     ",
            "+00010ae0: 2020 2020 2020 2065 7863 6c75 6465 5f69         exclude_i",
            "+00010af0: 676e 6f72 6564 3d6e 6f74 2069 676e 6f72  gnored=not ignor",
            "+00010b00: 6564 2c0a 2020 2020 2020 2020 2020 2020  ed,.            ",
            "+00010b10: 756e 7472 6163 6b65 645f 6669 6c65 733d  untracked_files=",
            "+00010b20: 756e 7472 6163 6b65 645f 6669 6c65 732c  untracked_files,",
            "+00010b30: 0a20 2020 2020 2020 2029 0a20 2020 2020  .        ).     ",
            "+00010b40: 2020 2069 6620 7379 732e 706c 6174 666f     if sys.platfo",
            "+00010b50: 726d 203d 3d20 2277 696e 3332 223a 0a20  rm == \"win32\":. ",
            "+00010b60: 2020 2020 2020 2020 2020 2075 6e74 7261             untra",
            "+00010b70: 636b 6564 5f63 6861 6e67 6573 203d 205b  cked_changes = [",
            "+00010b80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00010b90: 2070 6174 682e 7265 706c 6163 6528 6f73   path.replace(os",
            "+00010ba0: 2e70 6174 682e 7365 702c 2022 2f22 2920  .path.sep, \"/\") ",
            "+00010bb0: 666f 7220 7061 7468 2069 6e20 756e 7472  for path in untr",
            "+00010bc0: 6163 6b65 645f 7061 7468 730a 2020 2020  acked_paths.    ",
            "+00010bd0: 2020 2020 2020 2020 5d0a 2020 2020 2020          ].      ",
            "+00010be0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        ",
            "+00010bf0: 2020 2020 756e 7472 6163 6b65 645f 6368      untracked_ch",
            "+00010c00: 616e 6765 7320 3d20 6c69 7374 2875 6e74  anges = list(unt",
            "+00010c10: 7261 636b 6564 5f70 6174 6873 290a 0a20  racked_paths).. ",
            "+00010c20: 2020 2020 2020 2072 6574 7572 6e20 4769         return Gi",
            "+00010c30: 7453 7461 7475 7328 7472 6163 6b65 645f  tStatus(tracked_",
            "+00010c40: 6368 616e 6765 732c 2075 6e73 7461 6765  changes, unstage",
            "+00010c50: 645f 6368 616e 6765 732c 2075 6e74 7261  d_changes, untra",
            "+00010c60: 636b 6564 5f63 6861 6e67 6573 290a 0a0a  cked_changes)...",
            "+00010c70: 6465 6620 5f77 616c 6b5f 776f 726b 696e  def _walk_workin",
            "+00010c80: 675f 6469 725f 7061 7468 7328 6672 6f6d  g_dir_paths(from",
            "+00010c90: 7061 7468 2c20 6261 7365 7061 7468 2c20  path, basepath, ",
            "+00010ca0: 7072 756e 655f 6469 726e 616d 6573 3d4e  prune_dirnames=N",
            "+00010cb0: 6f6e 6529 3a0a 2020 2020 2222 2247 6574  one):.    \"\"\"Get",
            "+00010cc0: 2070 6174 682c 2069 735f 6469 7220 666f   path, is_dir fo",
            "+00010cd0: 7220 6669 6c65 7320 696e 2077 6f72 6b69  r files in worki",
            "+00010ce0: 6e67 2064 6972 2066 726f 6d20 6672 6f6d  ng dir from from",
            "+00010cf0: 7061 7468 2e0a 0a20 2020 2041 7267 733a  path...    Args:",
            "+00010d00: 0a20 2020 2020 2066 726f 6d70 6174 683a  .      frompath:",
            "+00010d10: 2050 6174 6820 746f 2062 6567 696e 2077   Path to begin w",
            "+00010d20: 616c 6b0a 2020 2020 2020 6261 7365 7061  alk.      basepa",
            "+00010d30: 7468 3a20 5061 7468 2074 6f20 636f 6d70  th: Path to comp",
            "+00010d40: 6172 6520 746f 0a20 2020 2020 2070 7275  are to.      pru",
            "+00010d50: 6e65 5f64 6972 6e61 6d65 733a 204f 7074  ne_dirnames: Opt",
            "+00010d60: 696f 6e61 6c20 6361 6c6c 6261 636b 2074  ional callback t",
            "+00010d70: 6f20 7072 756e 6520 6469 726e 616d 6573  o prune dirnames",
            "+00010d80: 2064 7572 696e 6720 6f73 2e77 616c 6b0a   during os.walk.",
            "+00010d90: 2020 2020 2020 2020 6469 726e 616d 6573          dirnames",
            "+00010da0: 2077 696c 6c20 6265 2073 6574 2074 6f20   will be set to ",
            "+00010db0: 7265 7375 6c74 206f 6620 7072 756e 655f  result of prune_",
            "+00010dc0: 6469 726e 616d 6573 2864 6972 7061 7468  dirnames(dirpath",
            "+00010dd0: 2c20 6469 726e 616d 6573 290a 2020 2020  , dirnames).    ",
            "+00010de0: 2222 220a 2020 2020 666f 7220 6469 7270  \"\"\".    for dirp",
            "+00010df0: 6174 682c 2064 6972 6e61 6d65 732c 2066  ath, dirnames, f",
            "+00010e00: 696c 656e 616d 6573 2069 6e20 6f73 2e77  ilenames in os.w",
            "+00010e10: 616c 6b28 6672 6f6d 7061 7468 293a 0a20  alk(frompath):. ",
            "+00010e20: 2020 2020 2020 2023 2053 6b69 7020 2e67         # Skip .g",
            "+00010e30: 6974 2061 6e64 2062 656c 6f77 2e0a 2020  it and below..  ",
            "+00010e40: 2020 2020 2020 6966 2022 2e67 6974 2220        if \".git\" ",
            "+00010e50: 696e 2064 6972 6e61 6d65 733a 0a20 2020  in dirnames:.   ",
            "+00010e60: 2020 2020 2020 2020 2064 6972 6e61 6d65           dirname",
            "+00010e70: 732e 7265 6d6f 7665 2822 2e67 6974 2229  s.remove(\".git\")",
            "+00010e80: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if ",
            "+00010e90: 6469 7270 6174 6820 213d 2062 6173 6570  dirpath != basep",
            "+00010ea0: 6174 683a 0a20 2020 2020 2020 2020 2020  ath:.           ",
            "+00010eb0: 2020 2020 2063 6f6e 7469 6e75 650a 0a20       continue.. ",
            "+00010ec0: 2020 2020 2020 2069 6620 222e 6769 7422         if \".git\"",
            "+00010ed0: 2069 6e20 6669 6c65 6e61 6d65 733a 0a20   in filenames:. ",
            "+00010ee0: 2020 2020 2020 2020 2020 2066 696c 656e             filen",
            "+00010ef0: 616d 6573 2e72 656d 6f76 6528 222e 6769  ames.remove(\".gi",
            "+00010f00: 7422 290a 2020 2020 2020 2020 2020 2020  t\").            ",
            "+00010f10: 6966 2064 6972 7061 7468 2021 3d20 6261  if dirpath != ba",
            "+00010f20: 7365 7061 7468 3a0a 2020 2020 2020 2020  sepath:.        ",
            "+00010f30: 2020 2020 2020 2020 636f 6e74 696e 7565          continue",
            "+00010f40: 0a0a 2020 2020 2020 2020 6966 2064 6972  ..        if dir",
            "+00010f50: 7061 7468 2021 3d20 6672 6f6d 7061 7468  path != frompath",
            "+00010f60: 3a0a 2020 2020 2020 2020 2020 2020 7969  :.            yi",
            "+00010f70: 656c 6420 6469 7270 6174 682c 2054 7275  eld dirpath, Tru",
            "+00010f80: 650a 0a20 2020 2020 2020 2066 6f72 2066  e..        for f",
            "+00010f90: 696c 656e 616d 6520 696e 2066 696c 656e  ilename in filen",
            "+00010fa0: 616d 6573 3a0a 2020 2020 2020 2020 2020  ames:.          ",
            "+00010fb0: 2020 6669 6c65 7061 7468 203d 206f 732e    filepath = os.",
            "+00010fc0: 7061 7468 2e6a 6f69 6e28 6469 7270 6174  path.join(dirpat",
            "+00010fd0: 682c 2066 696c 656e 616d 6529 0a20 2020  h, filename).   ",
            "+00010fe0: 2020 2020 2020 2020 2079 6965 6c64 2066           yield f",
            "+00010ff0: 696c 6570 6174 682c 2046 616c 7365 0a0a  ilepath, False..",
            "+00011000: 2020 2020 2020 2020 6966 2070 7275 6e65          if prune",
            "+00011010: 5f64 6972 6e61 6d65 733a 0a20 2020 2020  _dirnames:.     ",
            "+00011020: 2020 2020 2020 2064 6972 6e61 6d65 735b         dirnames[",
            "+00011030: 3a5d 203d 2070 7275 6e65 5f64 6972 6e61  :] = prune_dirna",
            "+00011040: 6d65 7328 6469 7270 6174 682c 2064 6972  mes(dirpath, dir",
            "+00011050: 6e61 6d65 7329 0a0a 0a64 6566 2067 6574  names)...def get",
            "+00011060: 5f75 6e74 7261 636b 6564 5f70 6174 6873  _untracked_paths",
            "+00011070: 280a 2020 2020 6672 6f6d 7061 7468 2c20  (.    frompath, ",
            "+00011080: 6261 7365 7061 7468 2c20 696e 6465 782c  basepath, index,",
            "+00011090: 2065 7863 6c75 6465 5f69 676e 6f72 6564   exclude_ignored",
            "+000110a0: 3d46 616c 7365 2c20 756e 7472 6163 6b65  =False, untracke",
            "+000110b0: 645f 6669 6c65 733d 2261 6c6c 220a 293a  d_files=\"all\".):",
            "+000110c0: 0a20 2020 2022 2222 4765 7420 756e 7472  .    \"\"\"Get untr",
            "+000110d0: 6163 6b65 6420 7061 7468 732e 0a0a 2020  acked paths...  ",
            "+000110e0: 2020 4172 6773 3a0a 2020 2020 2020 6672    Args:.      fr",
            "+000110f0: 6f6d 7061 7468 3a20 5061 7468 2074 6f20  ompath: Path to ",
            "+00011100: 7761 6c6b 0a20 2020 2020 2062 6173 6570  walk.      basep",
            "+00011110: 6174 683a 2050 6174 6820 746f 2063 6f6d  ath: Path to com",
            "+00011120: 7061 7265 2074 6f0a 2020 2020 2020 696e  pare to.      in",
            "+00011130: 6465 783a 2049 6e64 6578 2074 6f20 6368  dex: Index to ch",
            "+00011140: 6563 6b20 6167 6169 6e73 740a 2020 2020  eck against.    ",
            "+00011150: 2020 6578 636c 7564 655f 6967 6e6f 7265    exclude_ignore",
            "+00011160: 643a 2057 6865 7468 6572 2074 6f20 6578  d: Whether to ex",
            "+00011170: 636c 7564 6520 6967 6e6f 7265 6420 7061  clude ignored pa",
            "+00011180: 7468 730a 2020 2020 2020 756e 7472 6163  ths.      untrac",
            "+00011190: 6b65 645f 6669 6c65 733a 2048 6f77 2074  ked_files: How t",
            "+000111a0: 6f20 6861 6e64 6c65 2075 6e74 7261 636b  o handle untrack",
            "+000111b0: 6564 2066 696c 6573 3a0a 2020 2020 2020  ed files:.      ",
            "+000111c0: 2020 2d20 226e 6f22 3a20 7265 7475 726e    - \"no\": return",
            "+000111d0: 2061 6e20 656d 7074 7920 6c69 7374 0a20   an empty list. ",
            "+000111e0: 2020 2020 2020 202d 2022 616c 6c22 3a20         - \"all\": ",
            "+000111f0: 7265 7475 726e 2061 6c6c 2066 696c 6573  return all files",
            "+00011200: 2069 6e20 756e 7472 6163 6b65 6420 6469   in untracked di",
            "+00011210: 7265 6374 6f72 6965 730a 2020 2020 2020  rectories.      ",
            "+00011220: 2020 2d20 226e 6f72 6d61 6c22 3a20 7265    - \"normal\": re",
            "+00011230: 7475 726e 2075 6e74 7261 636b 6564 2064  turn untracked d",
            "+00011240: 6972 6563 746f 7269 6573 2077 6974 686f  irectories witho",
            "+00011250: 7574 206c 6973 7469 6e67 2074 6865 6972  ut listing their",
            "+00011260: 2063 6f6e 7465 6e74 730a 0a20 2020 204e   contents..    N",
            "+00011270: 6f74 653a 2069 676e 6f72 6564 2064 6972  ote: ignored dir",
            "+00011280: 6563 746f 7269 6573 2077 696c 6c20 6e65  ectories will ne",
            "+00011290: 7665 7220 6265 2077 616c 6b65 6420 666f  ver be walked fo",
            "+000112a0: 7220 7065 7266 6f72 6d61 6e63 6520 7265  r performance re",
            "+000112b0: 6173 6f6e 732e 0a20 2020 2020 2049 6620  asons..      If ",
            "+000112c0: 6578 636c 7564 655f 6967 6e6f 7265 6420  exclude_ignored ",
            "+000112d0: 6973 2046 616c 7365 2c20 6f6e 6c79 2074  is False, only t",
            "+000112e0: 6865 2070 6174 6820 746f 2061 6e20 6967  he path to an ig",
            "+000112f0: 6e6f 7265 6420 6469 7265 6374 6f72 7920  nored directory ",
            "+00011300: 7769 6c6c 0a20 2020 2020 2062 6520 7969  will.      be yi",
            "+00011310: 656c 6465 642c 206e 6f20 6669 6c65 7320  elded, no files ",
            "+00011320: 696e 7369 6465 2074 6865 2064 6972 6563  inside the direc",
            "+00011330: 746f 7279 2077 696c 6c20 6265 2072 6574  tory will be ret",
            "+00011340: 7572 6e65 640a 2020 2020 2222 220a 2020  urned.    \"\"\".  ",
            "+00011350: 2020 6966 2075 6e74 7261 636b 6564 5f66    if untracked_f",
            "+00011360: 696c 6573 206e 6f74 2069 6e20 2822 6e6f  iles not in (\"no",
            "+00011370: 222c 2022 616c 6c22 2c20 226e 6f72 6d61  \", \"all\", \"norma",
            "+00011380: 6c22 293a 0a20 2020 2020 2020 2072 6169  l\"):.        rai",
            "+00011390: 7365 2056 616c 7565 4572 726f 7228 2275  se ValueError(\"u",
            "+000113a0: 6e74 7261 636b 6564 5f66 696c 6573 206d  ntracked_files m",
            "+000113b0: 7573 7420 6265 206f 6e65 206f 6620 286e  ust be one of (n",
            "+000113c0: 6f2c 2061 6c6c 2c20 6e6f 726d 616c 2922  o, all, normal)\"",
            "+000113d0: 290a 0a20 2020 2069 6620 756e 7472 6163  )..    if untrac",
            "+000113e0: 6b65 645f 6669 6c65 7320 3d3d 2022 6e6f  ked_files == \"no",
            "+000113f0: 223a 0a20 2020 2020 2020 2072 6574 7572  \":.        retur",
            "+00011400: 6e0a 0a20 2020 2077 6974 6820 6f70 656e  n..    with open",
            "+00011410: 5f72 6570 6f5f 636c 6f73 696e 6728 6261  _repo_closing(ba",
            "+00011420: 7365 7061 7468 2920 6173 2072 3a0a 2020  sepath) as r:.  ",
            "+00011430: 2020 2020 2020 6967 6e6f 7265 5f6d 616e        ignore_man",
            "+00011440: 6167 6572 203d 2049 676e 6f72 6546 696c  ager = IgnoreFil",
            "+00011450: 7465 724d 616e 6167 6572 2e66 726f 6d5f  terManager.from_",
            "+00011460: 7265 706f 2872 290a 0a20 2020 2069 676e  repo(r)..    ign",
            "+00011470: 6f72 6564 5f64 6972 7320 3d20 5b5d 0a20  ored_dirs = []. ",
            "+00011480: 2020 2023 204c 6973 7420 746f 2073 746f     # List to sto",
            "+00011490: 7265 2075 6e74 7261 636b 6564 2064 6972  re untracked dir",
            "+000114a0: 6563 746f 7269 6573 2066 6f75 6e64 2064  ectories found d",
            "+000114b0: 7572 696e 6720 7472 6176 6572 7361 6c0a  uring traversal.",
            "+000114c0: 2020 2020 756e 7472 6163 6b65 645f 6469      untracked_di",
            "+000114d0: 725f 6c69 7374 203d 205b 5d0a 0a20 2020  r_list = []..   ",
            "+000114e0: 2064 6566 2070 7275 6e65 5f64 6972 6e61   def prune_dirna",
            "+000114f0: 6d65 7328 6469 7270 6174 682c 2064 6972  mes(dirpath, dir",
            "+00011500: 6e61 6d65 7329 3a0a 2020 2020 2020 2020  names):.        ",
            "+00011510: 666f 7220 6920 696e 2072 616e 6765 286c  for i in range(l",
            "+00011520: 656e 2864 6972 6e61 6d65 7329 202d 2031  en(dirnames) - 1",
            "+00011530: 2c20 2d31 2c20 2d31 293a 0a20 2020 2020  , -1, -1):.     ",
            "+00011540: 2020 2020 2020 2070 6174 6820 3d20 6f73         path = os",
            "+00011550: 2e70 6174 682e 6a6f 696e 2864 6972 7061  .path.join(dirpa",
            "+00011560: 7468 2c20 6469 726e 616d 6573 5b69 5d29  th, dirnames[i])",
            "+00011570: 0a20 2020 2020 2020 2020 2020 2069 7020  .            ip ",
            "+00011580: 3d20 6f73 2e70 6174 682e 6a6f 696e 286f  = os.path.join(o",
            "+00011590: 732e 7061 7468 2e72 656c 7061 7468 2870  s.path.relpath(p",
            "+000115a0: 6174 682c 2062 6173 6570 6174 6829 2c20  ath, basepath), ",
            "+000115b0: 2222 290a 0a20 2020 2020 2020 2020 2020  \"\")..           ",
            "+000115c0: 2023 2043 6865 636b 2069 6620 6469 7265   # Check if dire",
            "+000115d0: 6374 6f72 7920 6973 2069 676e 6f72 6564  ctory is ignored",
            "+000115e0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if ",
            "+000115f0: 6967 6e6f 7265 5f6d 616e 6167 6572 2e69  ignore_manager.i",
            "+00011600: 735f 6967 6e6f 7265 6428 6970 293a 0a20  s_ignored(ip):. ",
            "+00011610: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "+00011620: 6620 6e6f 7420 6578 636c 7564 655f 6967  f not exclude_ig",
            "+00011630: 6e6f 7265 643a 0a20 2020 2020 2020 2020  nored:.         ",
            "+00011640: 2020 2020 2020 2020 2020 2069 676e 6f72             ignor",
            "+00011650: 6564 5f64 6972 732e 6170 7065 6e64 280a  ed_dirs.append(.",
            "+00011660: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00011670: 2020 2020 2020 2020 6f73 2e70 6174 682e          os.path.",
            "+00011680: 6a6f 696e 286f 732e 7061 7468 2e72 656c  join(os.path.rel",
            "+00011690: 7061 7468 2870 6174 682c 2066 726f 6d70  path(path, fromp",
            "+000116a0: 6174 6829 2c20 2222 290a 2020 2020 2020  ath), \"\").      ",
            "+000116b0: 2020 2020 2020 2020 2020 2020 2020 290a                ).",
            "+000116c0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000116d0: 6465 6c20 6469 726e 616d 6573 5b69 5d0a  del dirnames[i].",
            "+000116e0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000116f0: 636f 6e74 696e 7565 0a0a 2020 2020 2020  continue..      ",
            "+00011700: 2020 2020 2020 2320 466f 7220 226e 6f72        # For \"nor",
            "+00011710: 6d61 6c22 206d 6f64 652c 2063 6865 636b  mal\" mode, check",
            "+00011720: 2069 6620 7468 6520 6469 7265 6374 6f72   if the director",
            "+00011730: 7920 6973 2065 6e74 6972 656c 7920 756e  y is entirely un",
            "+00011740: 7472 6163 6b65 640a 2020 2020 2020 2020  tracked.        ",
            "+00011750: 2020 2020 6966 2075 6e74 7261 636b 6564      if untracked",
            "+00011760: 5f66 696c 6573 203d 3d20 226e 6f72 6d61  _files == \"norma",
            "+00011770: 6c22 3a0a 2020 2020 2020 2020 2020 2020  l\":.            ",
            "+00011780: 2020 2020 2320 436f 6e76 6572 7420 6469      # Convert di",
            "+00011790: 7265 6374 6f72 7920 7061 7468 2074 6f20  rectory path to ",
            "+000117a0: 7472 6565 2070 6174 6820 666f 7220 696e  tree path for in",
            "+000117b0: 6465 7820 6c6f 6f6b 7570 0a20 2020 2020  dex lookup.     ",
            "+000117c0: 2020 2020 2020 2020 2020 2064 6972 5f74             dir_t",
            "+000117d0: 7265 655f 7061 7468 203d 2070 6174 685f  ree_path = path_",
            "+000117e0: 746f 5f74 7265 655f 7061 7468 2862 6173  to_tree_path(bas",
            "+000117f0: 6570 6174 682c 2070 6174 6829 0a0a 2020  epath, path)..  ",
            "+00011800: 2020 2020 2020 2020 2020 2020 2020 2320                # ",
            "+00011810: 4368 6563 6b20 6966 2061 6e79 2066 696c  Check if any fil",
            "+00011820: 6520 696e 2074 6869 7320 6469 7265 6374  e in this direct",
            "+00011830: 6f72 7920 6973 2074 7261 636b 6564 0a20  ory is tracked. ",
            "+00011840: 2020 2020 2020 2020 2020 2020 2020 2064                 d",
            "+00011850: 6972 5f70 7265 6669 7820 3d20 6469 725f  ir_prefix = dir_",
            "+00011860: 7472 6565 5f70 6174 6820 2b20 6222 2f22  tree_path + b\"/\"",
            "+00011870: 2069 6620 6469 725f 7472 6565 5f70 6174   if dir_tree_pat",
            "+00011880: 6820 656c 7365 2062 2222 0a20 2020 2020  h else b\"\".     ",
            "+00011890: 2020 2020 2020 2020 2020 2068 6173 5f74             has_t",
            "+000118a0: 7261 636b 6564 5f66 696c 6573 203d 2061  racked_files = a",
            "+000118b0: 6e79 286e 616d 652e 7374 6172 7473 7769  ny(name.startswi",
            "+000118c0: 7468 2864 6972 5f70 7265 6669 7829 2066  th(dir_prefix) f",
            "+000118d0: 6f72 206e 616d 6520 696e 2069 6e64 6578  or name in index",
            "+000118e0: 290a 0a20 2020 2020 2020 2020 2020 2020  )..             ",
            "+000118f0: 2020 2069 6620 6e6f 7420 6861 735f 7472     if not has_tr",
            "+00011900: 6163 6b65 645f 6669 6c65 733a 0a20 2020  acked_files:.   ",
            "+00011910: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00011920: 2023 2054 6869 7320 6469 7265 6374 6f72   # This director",
            "+00011930: 7920 6973 2065 6e74 6972 656c 7920 756e  y is entirely un",
            "+00011940: 7472 6163 6b65 640a 2020 2020 2020 2020  tracked.        ",
            "+00011950: 2020 2020 2020 2020 2020 2020 2320 4368              # Ch",
            "+00011960: 6563 6b20 6966 2069 7420 7368 6f75 6c64  eck if it should",
            "+00011970: 2062 6520 6578 636c 7564 6564 2064 7565   be excluded due",
            "+00011980: 2074 6f20 6967 6e6f 7265 2072 756c 6573   to ignore rules",
            "+00011990: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000119a0: 2020 2020 2069 735f 6967 6e6f 7265 6420       is_ignored ",
            "+000119b0: 3d20 6967 6e6f 7265 5f6d 616e 6167 6572  = ignore_manager",
            "+000119c0: 2e69 735f 6967 6e6f 7265 6428 0a20 2020  .is_ignored(.   ",
            "+000119d0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000119e0: 2020 2020 206f 732e 7061 7468 2e72 656c       os.path.rel",
            "+000119f0: 7061 7468 2870 6174 682c 2062 6173 6570  path(path, basep",
            "+00011a00: 6174 6829 0a20 2020 2020 2020 2020 2020  ath).           ",
            "+00011a10: 2020 2020 2020 2020 2029 0a20 2020 2020           ).     ",
            "+00011a20: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "+00011a30: 6620 6e6f 7420 6578 636c 7564 655f 6967  f not exclude_ig",
            "+00011a40: 6e6f 7265 6420 6f72 206e 6f74 2069 735f  nored or not is_",
            "+00011a50: 6967 6e6f 7265 643a 0a20 2020 2020 2020  ignored:.       ",
            "+00011a60: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00011a70: 2072 656c 5f70 6174 6820 3d20 6f73 2e70   rel_path = os.p",
            "+00011a80: 6174 682e 6a6f 696e 286f 732e 7061 7468  ath.join(os.path",
            "+00011a90: 2e72 656c 7061 7468 2870 6174 682c 2066  .relpath(path, f",
            "+00011aa0: 726f 6d70 6174 6829 2c20 2222 290a 2020  rompath), \"\").  ",
            "+00011ab0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00011ac0: 2020 2020 2020 756e 7472 6163 6b65 645f        untracked_",
            "+00011ad0: 6469 725f 6c69 7374 2e61 7070 656e 6428  dir_list.append(",
            "+00011ae0: 7265 6c5f 7061 7468 290a 2020 2020 2020  rel_path).      ",
            "+00011af0: 2020 2020 2020 2020 2020 2020 2020 6465                de",
            "+00011b00: 6c20 6469 726e 616d 6573 5b69 5d0a 0a20  l dirnames[i].. ",
            "+00011b10: 2020 2020 2020 2072 6574 7572 6e20 6469         return di",
            "+00011b20: 726e 616d 6573 0a0a 2020 2020 2320 466f  rnames..    # Fo",
            "+00011b30: 7220 2261 6c6c 2220 6d6f 6465 2c20 7573  r \"all\" mode, us",
            "+00011b40: 6520 7468 6520 6f72 6967 696e 616c 2062  e the original b",
            "+00011b50: 6568 6176 696f 720a 2020 2020 6966 2075  ehavior.    if u",
            "+00011b60: 6e74 7261 636b 6564 5f66 696c 6573 203d  ntracked_files =",
            "+00011b70: 3d20 2261 6c6c 223a 0a20 2020 2020 2020  = \"all\":.       ",
            "+00011b80: 2066 6f72 2061 702c 2069 735f 6469 7220   for ap, is_dir ",
            "+00011b90: 696e 205f 7761 6c6b 5f77 6f72 6b69 6e67  in _walk_working",
            "+00011ba0: 5f64 6972 5f70 6174 6873 280a 2020 2020  _dir_paths(.    ",
            "+00011bb0: 2020 2020 2020 2020 6672 6f6d 7061 7468          frompath",
            "+00011bc0: 2c20 6261 7365 7061 7468 2c20 7072 756e  , basepath, prun",
            "+00011bd0: 655f 6469 726e 616d 6573 3d70 7275 6e65  e_dirnames=prune",
            "+00011be0: 5f64 6972 6e61 6d65 730a 2020 2020 2020  _dirnames.      ",
            "+00011bf0: 2020 293a 0a20 2020 2020 2020 2020 2020    ):.           ",
            "+00011c00: 2069 6620 6e6f 7420 6973 5f64 6972 3a0a   if not is_dir:.",
            "+00011c10: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00011c20: 6970 203d 2070 6174 685f 746f 5f74 7265  ip = path_to_tre",
            "+00011c30: 655f 7061 7468 2862 6173 6570 6174 682c  e_path(basepath,",
            "+00011c40: 2061 7029 0a20 2020 2020 2020 2020 2020   ap).           ",
            "+00011c50: 2020 2020 2069 6620 6970 206e 6f74 2069       if ip not i",
            "+00011c60: 6e20 696e 6465 783a 0a20 2020 2020 2020  n index:.       ",
            "+00011c70: 2020 2020 2020 2020 2020 2020 2069 6620               if ",
            "+00011c80: 6e6f 7420 6578 636c 7564 655f 6967 6e6f  not exclude_igno",
            "+00011c90: 7265 6420 6f72 206e 6f74 2069 676e 6f72  red or not ignor",
            "+00011ca0: 655f 6d61 6e61 6765 722e 6973 5f69 676e  e_manager.is_ign",
            "+00011cb0: 6f72 6564 280a 2020 2020 2020 2020 2020  ored(.          ",
            "+00011cc0: 2020 2020 2020 2020 2020 2020 2020 6f73                os",
            "+00011cd0: 2e70 6174 682e 7265 6c70 6174 6828 6170  .path.relpath(ap",
            "+00011ce0: 2c20 6261 7365 7061 7468 290a 2020 2020  , basepath).    ",
            "+00011cf0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00011d00: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             ",
            "+00011d10: 2020 2020 2020 2020 2020 2079 6965 6c64             yield",
            "+00011d20: 206f 732e 7061 7468 2e72 656c 7061 7468   os.path.relpath",
            "+00011d30: 2861 702c 2066 726f 6d70 6174 6829 0a20  (ap, frompath). ",
            "+00011d40: 2020 2065 6c73 653a 2020 2320 226e 6f72     else:  # \"nor",
            "+00011d50: 6d61 6c22 206d 6f64 650a 2020 2020 2020  mal\" mode.      ",
            "+00011d60: 2020 2320 5761 6c6b 2064 6972 6563 746f    # Walk directo",
            "+00011d70: 7269 6573 2c20 6861 6e64 6c69 6e67 2062  ries, handling b",
            "+00011d80: 6f74 6820 6669 6c65 7320 616e 6420 6469  oth files and di",
            "+00011d90: 7265 6374 6f72 6965 730a 2020 2020 2020  rectories.      ",
            "+00011da0: 2020 666f 7220 6170 2c20 6973 5f64 6972    for ap, is_dir",
            "+00011db0: 2069 6e20 5f77 616c 6b5f 776f 726b 696e   in _walk_workin",
            "+00011dc0: 675f 6469 725f 7061 7468 7328 0a20 2020  g_dir_paths(.   ",
            "+00011dd0: 2020 2020 2020 2020 2066 726f 6d70 6174           frompat",
            "+00011de0: 682c 2062 6173 6570 6174 682c 2070 7275  h, basepath, pru",
            "+00011df0: 6e65 5f64 6972 6e61 6d65 733d 7072 756e  ne_dirnames=prun",
            "+00011e00: 655f 6469 726e 616d 6573 0a20 2020 2020  e_dirnames.     ",
            "+00011e10: 2020 2029 3a0a 2020 2020 2020 2020 2020     ):.          ",
            "+00011e20: 2020 2320 5468 6973 2070 6172 7420 776f    # This part wo",
            "+00011e30: 6e27 7420 6265 2072 6561 6368 6564 2066  n't be reached f",
            "+00011e40: 6f72 2070 7275 6e65 6420 6469 7265 6374  or pruned direct",
            "+00011e50: 6f72 6965 730a 2020 2020 2020 2020 2020  ories.          ",
            "+00011e60: 2020 6966 2069 735f 6469 723a 0a20 2020    if is_dir:.   ",
            "+00011e70: 2020 2020 2020 2020 2020 2020 2023 2043               # C",
            "+00011e80: 6865 636b 2069 6620 7468 6973 2064 6972  heck if this dir",
            "+00011e90: 6563 746f 7279 2069 7320 656e 7469 7265  ectory is entire",
            "+00011ea0: 6c79 2075 6e74 7261 636b 6564 0a20 2020  ly untracked.   ",
            "+00011eb0: 2020 2020 2020 2020 2020 2020 2064 6972               dir",
            "+00011ec0: 5f74 7265 655f 7061 7468 203d 2070 6174  _tree_path = pat",
            "+00011ed0: 685f 746f 5f74 7265 655f 7061 7468 2862  h_to_tree_path(b",
            "+00011ee0: 6173 6570 6174 682c 2061 7029 0a20 2020  asepath, ap).   ",
            "+00011ef0: 2020 2020 2020 2020 2020 2020 2064 6972               dir",
            "+00011f00: 5f70 7265 6669 7820 3d20 6469 725f 7472  _prefix = dir_tr",
            "+00011f10: 6565 5f70 6174 6820 2b20 6222 2f22 2069  ee_path + b\"/\" i",
            "+00011f20: 6620 6469 725f 7472 6565 5f70 6174 6820  f dir_tree_path ",
            "+00011f30: 656c 7365 2062 2222 0a20 2020 2020 2020  else b\"\".       ",
            "+00011f40: 2020 2020 2020 2020 2068 6173 5f74 7261           has_tra",
            "+00011f50: 636b 6564 5f66 696c 6573 203d 2061 6e79  cked_files = any",
            "+00011f60: 286e 616d 652e 7374 6172 7473 7769 7468  (name.startswith",
            "+00011f70: 2864 6972 5f70 7265 6669 7829 2066 6f72  (dir_prefix) for",
            "+00011f80: 206e 616d 6520 696e 2069 6e64 6578 290a   name in index).",
            "+00011f90: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00011fa0: 6966 206e 6f74 2068 6173 5f74 7261 636b  if not has_track",
            "+00011fb0: 6564 5f66 696c 6573 3a0a 2020 2020 2020  ed_files:.      ",
            "+00011fc0: 2020 2020 2020 2020 2020 2020 2020 6966                if",
            "+00011fd0: 206e 6f74 2065 7863 6c75 6465 5f69 676e   not exclude_ign",
            "+00011fe0: 6f72 6564 206f 7220 6e6f 7420 6967 6e6f  ored or not igno",
            "+00011ff0: 7265 5f6d 616e 6167 6572 2e69 735f 6967  re_manager.is_ig",
            "+00012000: 6e6f 7265 6428 0a20 2020 2020 2020 2020  nored(.         ",
            "+00012010: 2020 2020 2020 2020 2020 2020 2020 206f                 o",
            "+00012020: 732e 7061 7468 2e72 656c 7061 7468 2861  s.path.relpath(a",
            "+00012030: 702c 2062 6173 6570 6174 6829 0a20 2020  p, basepath).   ",
            "+00012040: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00012050: 2029 3a0a 2020 2020 2020 2020 2020 2020   ):.            ",
            "+00012060: 2020 2020 2020 2020 2020 2020 7969 656c              yiel",
            "+00012070: 6420 6f73 2e70 6174 682e 6a6f 696e 286f  d os.path.join(o",
            "+00012080: 732e 7061 7468 2e72 656c 7061 7468 2861  s.path.relpath(a",
            "+00012090: 702c 2066 726f 6d70 6174 6829 2c20 2222  p, frompath), \"\"",
            "+000120a0: 290a 2020 2020 2020 2020 2020 2020 656c  ).            el",
            "+000120b0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            ",
            "+000120c0: 2020 2020 2320 4368 6563 6b20 696e 6469      # Check indi",
            "+000120d0: 7669 6475 616c 2066 696c 6573 2069 6e20  vidual files in ",
            "+000120e0: 6469 7265 6374 6f72 6965 7320 7468 6174  directories that",
            "+000120f0: 2063 6f6e 7461 696e 2074 7261 636b 6564   contain tracked",
            "+00012100: 2066 696c 6573 0a20 2020 2020 2020 2020   files.         ",
            "+00012110: 2020 2020 2020 2069 7020 3d20 7061 7468         ip = path",
            "+00012120: 5f74 6f5f 7472 6565 5f70 6174 6828 6261  _to_tree_path(ba",
            "+00012130: 7365 7061 7468 2c20 6170 290a 2020 2020  sepath, ap).    ",
            "+00012140: 2020 2020 2020 2020 2020 2020 6966 2069              if i",
            "+00012150: 7020 6e6f 7420 696e 2069 6e64 6578 3a0a  p not in index:.",
            "+00012160: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00012170: 2020 2020 6966 206e 6f74 2065 7863 6c75      if not exclu",
            "+00012180: 6465 5f69 676e 6f72 6564 206f 7220 6e6f  de_ignored or no",
            "+00012190: 7420 6967 6e6f 7265 5f6d 616e 6167 6572  t ignore_manager",
            "+000121a0: 2e69 735f 6967 6e6f 7265 6428 0a20 2020  .is_ignored(.   ",
            "+000121b0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000121c0: 2020 2020 206f 732e 7061 7468 2e72 656c       os.path.rel",
            "+000121d0: 7061 7468 2861 702c 2062 6173 6570 6174  path(ap, basepat",
            "+000121e0: 6829 0a20 2020 2020 2020 2020 2020 2020  h).             ",
            "+000121f0: 2020 2020 2020 2029 3a0a 2020 2020 2020         ):.      ",
            "+00012200: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00012210: 2020 7969 656c 6420 6f73 2e70 6174 682e    yield os.path.",
            "+00012220: 7265 6c70 6174 6828 6170 2c20 6672 6f6d  relpath(ap, from",
            "+00012230: 7061 7468 290a 0a20 2020 2020 2020 2023  path)..        #",
            "+00012240: 2059 6965 6c64 2061 6e79 2075 6e74 7261   Yield any untra",
            "+00012250: 636b 6564 2064 6972 6563 746f 7269 6573  cked directories",
            "+00012260: 2066 6f75 6e64 2064 7572 696e 6720 7072   found during pr",
            "+00012270: 756e 696e 670a 2020 2020 2020 2020 7969  uning.        yi",
            "+00012280: 656c 6420 6672 6f6d 2075 6e74 7261 636b  eld from untrack",
            "+00012290: 6564 5f64 6972 5f6c 6973 740a 0a20 2020  ed_dir_list..   ",
            "+000122a0: 2079 6965 6c64 2066 726f 6d20 6967 6e6f   yield from igno",
            "+000122b0: 7265 645f 6469 7273 0a0a 0a64 6566 2067  red_dirs...def g",
            "+000122c0: 6574 5f74 7265 655f 6368 616e 6765 7328  et_tree_changes(",
            "+000122d0: 7265 706f 293a 0a20 2020 2022 2222 5265  repo):.    \"\"\"Re",
            "+000122e0: 7475 726e 2061 6464 2f64 656c 6574 652f  turn add/delete/",
            "+000122f0: 6d6f 6469 6679 2063 6861 6e67 6573 2074  modify changes t",
            "+00012300: 6f20 7472 6565 2062 7920 636f 6d70 6172  o tree by compar",
            "+00012310: 696e 6720 696e 6465 7820 746f 2048 4541  ing index to HEA",
            "+00012320: 442e 0a0a 2020 2020 4172 6773 3a0a 2020  D...    Args:.  ",
            "+00012330: 2020 2020 7265 706f 3a20 7265 706f 2070      repo: repo p",
            "+00012340: 6174 6820 6f72 206f 626a 6563 740a 2020  ath or object.  ",
            "+00012350: 2020 5265 7475 726e 733a 2064 6963 7420    Returns: dict ",
            "+00012360: 7769 7468 206c 6973 7473 2066 6f72 2065  with lists for e",
            "+00012370: 6163 6820 7479 7065 206f 6620 6368 616e  ach type of chan",
            "+00012380: 6765 0a20 2020 2022 2222 0a20 2020 2077  ge.    \"\"\".    w",
            "+00012390: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "+000123a0: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "+000123b0: 3a0a 2020 2020 2020 2020 696e 6465 7820  :.        index ",
            "+000123c0: 3d20 722e 6f70 656e 5f69 6e64 6578 2829  = r.open_index()",
            "+000123d0: 0a0a 2020 2020 2020 2020 2320 436f 6d70  ..        # Comp",
            "+000123e0: 6172 6573 2074 6865 2049 6e64 6578 2074  ares the Index t",
            "+000123f0: 6f20 7468 6520 4845 4144 2026 2064 6574  o the HEAD & det",
            "+00012400: 6572 6d69 6e65 7320 6368 616e 6765 730a  ermines changes.",
            "+00012410: 2020 2020 2020 2020 2320 4974 6572 6174          # Iterat",
            "+00012420: 6520 7468 726f 7567 6820 7468 6520 6368  e through the ch",
            "+00012430: 616e 6765 7320 616e 6420 7265 706f 7274  anges and report",
            "+00012440: 2061 6464 2f64 656c 6574 652f 6d6f 6469   add/delete/modi",
            "+00012450: 6679 0a20 2020 2020 2020 2023 2054 4f44  fy.        # TOD",
            "+00012460: 4f3a 2063 616c 6c20 6f75 7420 746f 2064  O: call out to d",
            "+00012470: 756c 7769 6368 2e64 6966 665f 7472 6565  ulwich.diff_tree",
            "+00012480: 2073 6f6d 6568 6f77 2e0a 2020 2020 2020   somehow..      ",
            "+00012490: 2020 7472 6163 6b65 645f 6368 616e 6765    tracked_change",
            "+000124a0: 7320 3d20 7b0a 2020 2020 2020 2020 2020  s = {.          ",
            "+000124b0: 2020 2261 6464 223a 205b 5d2c 0a20 2020    \"add\": [],.   ",
            "+000124c0: 2020 2020 2020 2020 2022 6465 6c65 7465           \"delete",
            "+000124d0: 223a 205b 5d2c 0a20 2020 2020 2020 2020  \": [],.         ",
            "+000124e0: 2020 2022 6d6f 6469 6679 223a 205b 5d2c     \"modify\": [],",
            "+000124f0: 0a20 2020 2020 2020 207d 0a20 2020 2020  .        }.     ",
            "+00012500: 2020 2074 7279 3a0a 2020 2020 2020 2020     try:.        ",
            "+00012510: 2020 2020 7472 6565 5f69 6420 3d20 725b      tree_id = r[",
            "+00012520: 6222 4845 4144 225d 2e74 7265 650a 2020  b\"HEAD\"].tree.  ",
            "+00012530: 2020 2020 2020 6578 6365 7074 204b 6579        except Key",
            "+00012540: 4572 726f 723a 0a20 2020 2020 2020 2020  Error:.         ",
            "+00012550: 2020 2074 7265 655f 6964 203d 204e 6f6e     tree_id = Non",
            "+00012560: 650a 0a20 2020 2020 2020 2066 6f72 2063  e..        for c",
            "+00012570: 6861 6e67 6520 696e 2069 6e64 6578 2e63  hange in index.c",
            "+00012580: 6861 6e67 6573 5f66 726f 6d5f 7472 6565  hanges_from_tree",
            "+00012590: 2872 2e6f 626a 6563 745f 7374 6f72 652c  (r.object_store,",
            "+000125a0: 2074 7265 655f 6964 293a 0a20 2020 2020   tree_id):.     ",
            "+000125b0: 2020 2020 2020 2069 6620 6e6f 7420 6368         if not ch",
            "+000125c0: 616e 6765 5b30 5d5b 305d 3a0a 2020 2020  ange[0][0]:.    ",
            "+000125d0: 2020 2020 2020 2020 2020 2020 7472 6163              trac",
            "+000125e0: 6b65 645f 6368 616e 6765 735b 2261 6464  ked_changes[\"add",
            "+000125f0: 225d 2e61 7070 656e 6428 6368 616e 6765  \"].append(change",
            "+00012600: 5b30 5d5b 315d 290a 2020 2020 2020 2020  [0][1]).        ",
            "+00012610: 2020 2020 656c 6966 206e 6f74 2063 6861      elif not cha",
            "+00012620: 6e67 655b 305d 5b31 5d3a 0a20 2020 2020  nge[0][1]:.     ",
            "+00012630: 2020 2020 2020 2020 2020 2074 7261 636b             track",
            "+00012640: 6564 5f63 6861 6e67 6573 5b22 6465 6c65  ed_changes[\"dele",
            "+00012650: 7465 225d 2e61 7070 656e 6428 6368 616e  te\"].append(chan",
            "+00012660: 6765 5b30 5d5b 305d 290a 2020 2020 2020  ge[0][0]).      ",
            "+00012670: 2020 2020 2020 656c 6966 2063 6861 6e67        elif chang",
            "+00012680: 655b 305d 5b30 5d20 3d3d 2063 6861 6e67  e[0][0] == chang",
            "+00012690: 655b 305d 5b31 5d3a 0a20 2020 2020 2020  e[0][1]:.       ",
            "+000126a0: 2020 2020 2020 2020 2074 7261 636b 6564           tracked",
            "+000126b0: 5f63 6861 6e67 6573 5b22 6d6f 6469 6679  _changes[\"modify",
            "+000126c0: 225d 2e61 7070 656e 6428 6368 616e 6765  \"].append(change",
            "+000126d0: 5b30 5d5b 305d 290a 2020 2020 2020 2020  [0][0]).        ",
            "+000126e0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      ",
            "+000126f0: 2020 2020 2020 2020 2020 7261 6973 6520            raise ",
            "+00012700: 4e6f 7449 6d70 6c65 6d65 6e74 6564 4572  NotImplementedEr",
            "+00012710: 726f 7228 2267 6974 206d 7620 6f70 7320  ror(\"git mv ops ",
            "+00012720: 6e6f 7420 7965 7420 7375 7070 6f72 7465  not yet supporte",
            "+00012730: 6422 290a 2020 2020 2020 2020 7265 7475  d\").        retu",
            "+00012740: 726e 2074 7261 636b 6564 5f63 6861 6e67  rn tracked_chang",
            "+00012750: 6573 0a0a 0a64 6566 2064 6165 6d6f 6e28  es...def daemon(",
            "+00012760: 7061 7468 3d22 2e22 2c20 6164 6472 6573  path=\".\", addres",
            "+00012770: 733d 4e6f 6e65 2c20 706f 7274 3d4e 6f6e  s=None, port=Non",
            "+00012780: 6529 202d 3e20 4e6f 6e65 3a0a 2020 2020  e) -> None:.    ",
            "+00012790: 2222 2252 756e 2061 2064 6165 6d6f 6e20  \"\"\"Run a daemon ",
            "+000127a0: 7365 7276 696e 6720 4769 7420 7265 7175  serving Git requ",
            "+000127b0: 6573 7473 206f 7665 7220 5443 502f 4950  ests over TCP/IP",
            "+000127c0: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "+000127d0: 2020 2070 6174 683a 2050 6174 6820 746f     path: Path to",
            "+000127e0: 2074 6865 2064 6972 6563 746f 7279 2074   the directory t",
            "+000127f0: 6f20 7365 7276 652e 0a20 2020 2020 2061  o serve..      a",
            "+00012800: 6464 7265 7373 3a20 4f70 7469 6f6e 616c  ddress: Optional",
            "+00012810: 2061 6464 7265 7373 2074 6f20 6c69 7374   address to list",
            "+00012820: 656e 206f 6e20 2864 6566 6175 6c74 7320  en on (defaults ",
            "+00012830: 746f 203a 3a29 0a20 2020 2020 2070 6f72  to ::).      por",
            "+00012840: 743a 204f 7074 696f 6e61 6c20 706f 7274  t: Optional port",
            "+00012850: 2074 6f20 6c69 7374 656e 206f 6e20 2864   to listen on (d",
            "+00012860: 6566 6175 6c74 7320 746f 2054 4350 5f47  efaults to TCP_G",
            "+00012870: 4954 5f50 4f52 5429 0a20 2020 2022 2222  IT_PORT).    \"\"\"",
            "+00012880: 0a20 2020 2023 2054 4f44 4f28 6a65 6c6d  .    # TODO(jelm",
            "+00012890: 6572 293a 2053 7570 706f 7274 2067 6974  er): Support git",
            "+000128a0: 2d64 6165 6d6f 6e2d 6578 706f 7274 2d6f  -daemon-export-o",
            "+000128b0: 6b20 616e 6420 2d2d 6578 706f 7274 2d61  k and --export-a",
            "+000128c0: 6c6c 2e0a 2020 2020 6261 636b 656e 6420  ll..    backend ",
            "+000128d0: 3d20 4669 6c65 5379 7374 656d 4261 636b  = FileSystemBack",
            "+000128e0: 656e 6428 7061 7468 290a 2020 2020 7365  end(path).    se",
            "+000128f0: 7276 6572 203d 2054 4350 4769 7453 6572  rver = TCPGitSer",
            "+00012900: 7665 7228 6261 636b 656e 642c 2061 6464  ver(backend, add",
            "+00012910: 7265 7373 2c20 706f 7274 290a 2020 2020  ress, port).    ",
            "+00012920: 7365 7276 6572 2e73 6572 7665 5f66 6f72  server.serve_for",
            "+00012930: 6576 6572 2829 0a0a 0a64 6566 2077 6562  ever()...def web",
            "+00012940: 5f64 6165 6d6f 6e28 7061 7468 3d22 2e22  _daemon(path=\".\"",
            "+00012950: 2c20 6164 6472 6573 733d 4e6f 6e65 2c20  , address=None, ",
            "+00012960: 706f 7274 3d4e 6f6e 6529 202d 3e20 4e6f  port=None) -> No",
            "+00012970: 6e65 3a0a 2020 2020 2222 2252 756e 2061  ne:.    \"\"\"Run a",
            "+00012980: 2064 6165 6d6f 6e20 7365 7276 696e 6720   daemon serving ",
            "+00012990: 4769 7420 7265 7175 6573 7473 206f 7665  Git requests ove",
            "+000129a0: 7220 4854 5450 2e0a 0a20 2020 2041 7267  r HTTP...    Arg",
            "+000129b0: 733a 0a20 2020 2020 2070 6174 683a 2050  s:.      path: P",
            "+000129c0: 6174 6820 746f 2074 6865 2064 6972 6563  ath to the direc",
            "+000129d0: 746f 7279 2074 6f20 7365 7276 650a 2020  tory to serve.  ",
            "+000129e0: 2020 2020 6164 6472 6573 733a 204f 7074      address: Opt",
            "+000129f0: 696f 6e61 6c20 6164 6472 6573 7320 746f  ional address to",
            "+00012a00: 206c 6973 7465 6e20 6f6e 2028 6465 6661   listen on (defa",
            "+00012a10: 756c 7473 2074 6f20 3a3a 290a 2020 2020  ults to ::).    ",
            "+00012a20: 2020 706f 7274 3a20 4f70 7469 6f6e 616c    port: Optional",
            "+00012a30: 2070 6f72 7420 746f 206c 6973 7465 6e20   port to listen ",
            "+00012a40: 6f6e 2028 6465 6661 756c 7473 2074 6f20  on (defaults to ",
            "+00012a50: 3830 290a 2020 2020 2222 220a 2020 2020  80).    \"\"\".    ",
            "+00012a60: 6672 6f6d 202e 7765 6220 696d 706f 7274  from .web import",
            "+00012a70: 2028 0a20 2020 2020 2020 2057 5347 4952   (.        WSGIR",
            "+00012a80: 6571 7565 7374 4861 6e64 6c65 724c 6f67  equestHandlerLog",
            "+00012a90: 6765 722c 0a20 2020 2020 2020 2057 5347  ger,.        WSG",
            "+00012aa0: 4953 6572 7665 724c 6f67 6765 722c 0a20  IServerLogger,. ",
            "+00012ab0: 2020 2020 2020 206d 616b 655f 7365 7276         make_serv",
            "+00012ac0: 6572 2c0a 2020 2020 2020 2020 6d61 6b65  er,.        make",
            "+00012ad0: 5f77 7367 695f 6368 6169 6e2c 0a20 2020  _wsgi_chain,.   ",
            "+00012ae0: 2029 0a0a 2020 2020 6261 636b 656e 6420   )..    backend ",
            "+00012af0: 3d20 4669 6c65 5379 7374 656d 4261 636b  = FileSystemBack",
            "+00012b00: 656e 6428 7061 7468 290a 2020 2020 6170  end(path).    ap",
            "+00012b10: 7020 3d20 6d61 6b65 5f77 7367 695f 6368  p = make_wsgi_ch",
            "+00012b20: 6169 6e28 6261 636b 656e 6429 0a20 2020  ain(backend).   ",
            "+00012b30: 2073 6572 7665 7220 3d20 6d61 6b65 5f73   server = make_s",
            "+00012b40: 6572 7665 7228 0a20 2020 2020 2020 2061  erver(.        a",
            "+00012b50: 6464 7265 7373 2c0a 2020 2020 2020 2020  ddress,.        ",
            "+00012b60: 706f 7274 2c0a 2020 2020 2020 2020 6170  port,.        ap",
            "+00012b70: 702c 0a20 2020 2020 2020 2068 616e 646c  p,.        handl",
            "+00012b80: 6572 5f63 6c61 7373 3d57 5347 4952 6571  er_class=WSGIReq",
            "+00012b90: 7565 7374 4861 6e64 6c65 724c 6f67 6765  uestHandlerLogge",
            "+00012ba0: 722c 0a20 2020 2020 2020 2073 6572 7665  r,.        serve",
            "+00012bb0: 725f 636c 6173 733d 5753 4749 5365 7276  r_class=WSGIServ",
            "+00012bc0: 6572 4c6f 6767 6572 2c0a 2020 2020 290a  erLogger,.    ).",
            "+00012bd0: 2020 2020 7365 7276 6572 2e73 6572 7665      server.serve",
            "+00012be0: 5f66 6f72 6576 6572 2829 0a0a 0a64 6566  _forever()...def",
            "+00012bf0: 2075 706c 6f61 645f 7061 636b 2870 6174   upload_pack(pat",
            "+00012c00: 683d 222e 222c 2069 6e66 3d4e 6f6e 652c  h=\".\", inf=None,",
            "+00012c10: 206f 7574 663d 4e6f 6e65 2920 2d3e 2069   outf=None) -> i",
            "+00012c20: 6e74 3a0a 2020 2020 2222 2255 706c 6f61  nt:.    \"\"\"Uploa",
            "+00012c30: 6420 6120 7061 636b 2066 696c 6520 6166  d a pack file af",
            "+00012c40: 7465 7220 6e65 676f 7469 6174 696e 6720  ter negotiating ",
            "+00012c50: 6974 7320 636f 6e74 656e 7473 2075 7369  its contents usi",
            "+00012c60: 6e67 2073 6d61 7274 2070 726f 746f 636f  ng smart protoco",
            "+00012c70: 6c2e 0a0a 2020 2020 4172 6773 3a0a 2020  l...    Args:.  ",
            "+00012c80: 2020 2020 7061 7468 3a20 5061 7468 2074      path: Path t",
            "+00012c90: 6f20 7468 6520 7265 706f 7369 746f 7279  o the repository",
            "+00012ca0: 0a20 2020 2020 2069 6e66 3a20 496e 7075  .      inf: Inpu",
            "+00012cb0: 7420 7374 7265 616d 2074 6f20 636f 6d6d  t stream to comm",
            "+00012cc0: 756e 6963 6174 6520 7769 7468 2063 6c69  unicate with cli",
            "+00012cd0: 656e 740a 2020 2020 2020 6f75 7466 3a20  ent.      outf: ",
            "+00012ce0: 4f75 7470 7574 2073 7472 6561 6d20 746f  Output stream to",
            "+00012cf0: 2063 6f6d 6d75 6e69 6361 7465 2077 6974   communicate wit",
            "+00012d00: 6820 636c 6965 6e74 0a20 2020 2022 2222  h client.    \"\"\"",
            "+00012d10: 0a20 2020 2069 6620 6f75 7466 2069 7320  .    if outf is ",
            "+00012d20: 4e6f 6e65 3a0a 2020 2020 2020 2020 6f75  None:.        ou",
            "+00012d30: 7466 203d 2067 6574 6174 7472 2873 7973  tf = getattr(sys",
            "+00012d40: 2e73 7464 6f75 742c 2022 6275 6666 6572  .stdout, \"buffer",
            "+00012d50: 222c 2073 7973 2e73 7464 6f75 7429 0a20  \", sys.stdout). ",
            "+00012d60: 2020 2069 6620 696e 6620 6973 204e 6f6e     if inf is Non",
            "+00012d70: 653a 0a20 2020 2020 2020 2069 6e66 203d  e:.        inf =",
            "+00012d80: 2067 6574 6174 7472 2873 7973 2e73 7464   getattr(sys.std",
            "+00012d90: 696e 2c20 2262 7566 6665 7222 2c20 7379  in, \"buffer\", sy",
            "+00012da0: 732e 7374 6469 6e29 0a20 2020 2070 6174  s.stdin).    pat",
            "+00012db0: 6820 3d20 6f73 2e70 6174 682e 6578 7061  h = os.path.expa",
            "+00012dc0: 6e64 7573 6572 2870 6174 6829 0a20 2020  nduser(path).   ",
            "+00012dd0: 2062 6163 6b65 6e64 203d 2046 696c 6553   backend = FileS",
            "+00012de0: 7973 7465 6d42 6163 6b65 6e64 2870 6174  ystemBackend(pat",
            "+00012df0: 6829 0a0a 2020 2020 6465 6620 7365 6e64  h)..    def send",
            "+00012e00: 5f66 6e28 6461 7461 2920 2d3e 204e 6f6e  _fn(data) -> Non",
            "+00012e10: 653a 0a20 2020 2020 2020 206f 7574 662e  e:.        outf.",
            "+00012e20: 7772 6974 6528 6461 7461 290a 2020 2020  write(data).    ",
            "+00012e30: 2020 2020 6f75 7466 2e66 6c75 7368 2829      outf.flush()",
            "+00012e40: 0a0a 2020 2020 7072 6f74 6f20 3d20 5072  ..    proto = Pr",
            "+00012e50: 6f74 6f63 6f6c 2869 6e66 2e72 6561 642c  otocol(inf.read,",
            "+00012e60: 2073 656e 645f 666e 290a 2020 2020 6861   send_fn).    ha",
            "+00012e70: 6e64 6c65 7220 3d20 5570 6c6f 6164 5061  ndler = UploadPa",
            "+00012e80: 636b 4861 6e64 6c65 7228 6261 636b 656e  ckHandler(backen",
            "+00012e90: 642c 205b 7061 7468 5d2c 2070 726f 746f  d, [path], proto",
            "+00012ea0: 290a 2020 2020 2320 4649 584d 453a 2043  ).    # FIXME: C",
            "+00012eb0: 6174 6368 2065 7863 6570 7469 6f6e 7320  atch exceptions ",
            "+00012ec0: 616e 6420 7772 6974 6520 6120 7369 6e67  and write a sing",
            "+00012ed0: 6c65 2d6c 696e 6520 7375 6d6d 6172 7920  le-line summary ",
            "+00012ee0: 746f 206f 7574 662e 0a20 2020 2068 616e  to outf..    han",
            "+00012ef0: 646c 6572 2e68 616e 646c 6528 290a 2020  dler.handle().  ",
            "+00012f00: 2020 7265 7475 726e 2030 0a0a 0a64 6566    return 0...def",
            "+00012f10: 2072 6563 6569 7665 5f70 6163 6b28 7061   receive_pack(pa",
            "+00012f20: 7468 3d22 2e22 2c20 696e 663d 4e6f 6e65  th=\".\", inf=None",
            "+00012f30: 2c20 6f75 7466 3d4e 6f6e 6529 202d 3e20  , outf=None) -> ",
            "+00012f40: 696e 743a 0a20 2020 2022 2222 5265 6365  int:.    \"\"\"Rece",
            "+00012f50: 6976 6520 6120 7061 636b 2066 696c 6520  ive a pack file ",
            "+00012f60: 6166 7465 7220 6e65 676f 7469 6174 696e  after negotiatin",
            "+00012f70: 6720 6974 7320 636f 6e74 656e 7473 2075  g its contents u",
            "+00012f80: 7369 6e67 2073 6d61 7274 2070 726f 746f  sing smart proto",
            "+00012f90: 636f 6c2e 0a0a 2020 2020 4172 6773 3a0a  col...    Args:.",
            "+00012fa0: 2020 2020 2020 7061 7468 3a20 5061 7468        path: Path",
            "+00012fb0: 2074 6f20 7468 6520 7265 706f 7369 746f   to the reposito",
            "+00012fc0: 7279 0a20 2020 2020 2069 6e66 3a20 496e  ry.      inf: In",
            "+00012fd0: 7075 7420 7374 7265 616d 2074 6f20 636f  put stream to co",
            "+00012fe0: 6d6d 756e 6963 6174 6520 7769 7468 2063  mmunicate with c",
            "+00012ff0: 6c69 656e 740a 2020 2020 2020 6f75 7466  lient.      outf",
            "+00013000: 3a20 4f75 7470 7574 2073 7472 6561 6d20  : Output stream ",
            "+00013010: 746f 2063 6f6d 6d75 6e69 6361 7465 2077  to communicate w",
            "+00013020: 6974 6820 636c 6965 6e74 0a20 2020 2022  ith client.    \"",
            "+00013030: 2222 0a20 2020 2069 6620 6f75 7466 2069  \"\".    if outf i",
            "+00013040: 7320 4e6f 6e65 3a0a 2020 2020 2020 2020  s None:.        ",
            "+00013050: 6f75 7466 203d 2067 6574 6174 7472 2873  outf = getattr(s",
            "+00013060: 7973 2e73 7464 6f75 742c 2022 6275 6666  ys.stdout, \"buff",
            "+00013070: 6572 222c 2073 7973 2e73 7464 6f75 7429  er\", sys.stdout)",
            "+00013080: 0a20 2020 2069 6620 696e 6620 6973 204e  .    if inf is N",
            "+00013090: 6f6e 653a 0a20 2020 2020 2020 2069 6e66  one:.        inf",
            "+000130a0: 203d 2067 6574 6174 7472 2873 7973 2e73   = getattr(sys.s",
            "+000130b0: 7464 696e 2c20 2262 7566 6665 7222 2c20  tdin, \"buffer\", ",
            "+000130c0: 7379 732e 7374 6469 6e29 0a20 2020 2070  sys.stdin).    p",
            "+000130d0: 6174 6820 3d20 6f73 2e70 6174 682e 6578  ath = os.path.ex",
            "+000130e0: 7061 6e64 7573 6572 2870 6174 6829 0a20  panduser(path). ",
            "+000130f0: 2020 2062 6163 6b65 6e64 203d 2046 696c     backend = Fil",
            "+00013100: 6553 7973 7465 6d42 6163 6b65 6e64 2870  eSystemBackend(p",
            "+00013110: 6174 6829 0a0a 2020 2020 6465 6620 7365  ath)..    def se",
            "+00013120: 6e64 5f66 6e28 6461 7461 2920 2d3e 204e  nd_fn(data) -> N",
            "+00013130: 6f6e 653a 0a20 2020 2020 2020 206f 7574  one:.        out",
            "+00013140: 662e 7772 6974 6528 6461 7461 290a 2020  f.write(data).  ",
            "+00013150: 2020 2020 2020 6f75 7466 2e66 6c75 7368        outf.flush",
            "+00013160: 2829 0a0a 2020 2020 7072 6f74 6f20 3d20  ()..    proto = ",
            "+00013170: 5072 6f74 6f63 6f6c 2869 6e66 2e72 6561  Protocol(inf.rea",
            "+00013180: 642c 2073 656e 645f 666e 290a 2020 2020  d, send_fn).    ",
            "+00013190: 6861 6e64 6c65 7220 3d20 5265 6365 6976  handler = Receiv",
            "+000131a0: 6550 6163 6b48 616e 646c 6572 2862 6163  ePackHandler(bac",
            "+000131b0: 6b65 6e64 2c20 5b70 6174 685d 2c20 7072  kend, [path], pr",
            "+000131c0: 6f74 6f29 0a20 2020 2023 2046 4958 4d45  oto).    # FIXME",
            "+000131d0: 3a20 4361 7463 6820 6578 6365 7074 696f  : Catch exceptio",
            "+000131e0: 6e73 2061 6e64 2077 7269 7465 2061 2073  ns and write a s",
            "+000131f0: 696e 676c 652d 6c69 6e65 2073 756d 6d61  ingle-line summa",
            "+00013200: 7279 2074 6f20 6f75 7466 2e0a 2020 2020  ry to outf..    ",
            "+00013210: 6861 6e64 6c65 722e 6861 6e64 6c65 2829  handler.handle()",
            "+00013220: 0a20 2020 2072 6574 7572 6e20 300a 0a0a  .    return 0...",
            "+00013230: 6465 6620 5f6d 616b 655f 6272 616e 6368  def _make_branch",
            "+00013240: 5f72 6566 286e 616d 653a 2055 6e69 6f6e  _ref(name: Union",
            "+00013250: 5b73 7472 2c20 6279 7465 735d 2920 2d3e  [str, bytes]) ->",
            "+00013260: 2052 6566 3a0a 2020 2020 6966 2069 7369   Ref:.    if isi",
            "+00013270: 6e73 7461 6e63 6528 6e61 6d65 2c20 7374  nstance(name, st",
            "+00013280: 7229 3a0a 2020 2020 2020 2020 6e61 6d65  r):.        name",
            "+00013290: 203d 206e 616d 652e 656e 636f 6465 2844   = name.encode(D",
            "+000132a0: 4546 4155 4c54 5f45 4e43 4f44 494e 4729  EFAULT_ENCODING)",
            "+000132b0: 0a20 2020 2072 6574 7572 6e20 4c4f 4341  .    return LOCA",
            "+000132c0: 4c5f 4252 414e 4348 5f50 5245 4649 5820  L_BRANCH_PREFIX ",
            "+000132d0: 2b20 6e61 6d65 0a0a 0a64 6566 205f 6d61  + name...def _ma",
            "+000132e0: 6b65 5f74 6167 5f72 6566 286e 616d 653a  ke_tag_ref(name:",
            "+000132f0: 2055 6e69 6f6e 5b73 7472 2c20 6279 7465   Union[str, byte",
            "+00013300: 735d 2920 2d3e 2052 6566 3a0a 2020 2020  s]) -> Ref:.    ",
            "+00013310: 6966 2069 7369 6e73 7461 6e63 6528 6e61  if isinstance(na",
            "+00013320: 6d65 2c20 7374 7229 3a0a 2020 2020 2020  me, str):.      ",
            "+00013330: 2020 6e61 6d65 203d 206e 616d 652e 656e    name = name.en",
            "+00013340: 636f 6465 2844 4546 4155 4c54 5f45 4e43  code(DEFAULT_ENC",
            "+00013350: 4f44 494e 4729 0a20 2020 2072 6574 7572  ODING).    retur",
            "+00013360: 6e20 4c4f 4341 4c5f 5441 475f 5052 4546  n LOCAL_TAG_PREF",
            "+00013370: 4958 202b 206e 616d 650a 0a0a 6465 6620  IX + name...def ",
            "+00013380: 6272 616e 6368 5f64 656c 6574 6528 7265  branch_delete(re",
            "+00013390: 706f 2c20 6e61 6d65 2920 2d3e 204e 6f6e  po, name) -> Non",
            "+000133a0: 653a 0a20 2020 2022 2222 4465 6c65 7465  e:.    \"\"\"Delete",
            "+000133b0: 2061 2062 7261 6e63 682e 0a0a 2020 2020   a branch...    ",
            "+000133c0: 4172 6773 3a0a 2020 2020 2020 7265 706f  Args:.      repo",
            "+000133d0: 3a20 5061 7468 2074 6f20 7468 6520 7265  : Path to the re",
            "+000133e0: 706f 7369 746f 7279 0a20 2020 2020 206e  pository.      n",
            "+000133f0: 616d 653a 204e 616d 6520 6f66 2074 6865  ame: Name of the",
            "+00013400: 2062 7261 6e63 680a 2020 2020 2222 220a   branch.    \"\"\".",
            "+00013410: 2020 2020 7769 7468 206f 7065 6e5f 7265      with open_re",
            "+00013420: 706f 5f63 6c6f 7369 6e67 2872 6570 6f29  po_closing(repo)",
            "+00013430: 2061 7320 723a 0a20 2020 2020 2020 2069   as r:.        i",
            "+00013440: 6620 6973 696e 7374 616e 6365 286e 616d  f isinstance(nam",
            "+00013450: 652c 206c 6973 7429 3a0a 2020 2020 2020  e, list):.      ",
            "+00013460: 2020 2020 2020 6e61 6d65 7320 3d20 6e61        names = na",
            "+00013470: 6d65 0a20 2020 2020 2020 2065 6c73 653a  me.        else:",
            "+00013480: 0a20 2020 2020 2020 2020 2020 206e 616d  .            nam",
            "+00013490: 6573 203d 205b 6e61 6d65 5d0a 2020 2020  es = [name].    ",
            "+000134a0: 2020 2020 666f 7220 6e61 6d65 2069 6e20      for name in ",
            "+000134b0: 6e61 6d65 733a 0a20 2020 2020 2020 2020  names:.         ",
            "+000134c0: 2020 2064 656c 2072 2e72 6566 735b 5f6d     del r.refs[_m",
            "+000134d0: 616b 655f 6272 616e 6368 5f72 6566 286e  ake_branch_ref(n",
            "+000134e0: 616d 6529 5d0a 0a0a 6465 6620 6272 616e  ame)]...def bran",
            "+000134f0: 6368 5f63 7265 6174 6528 7265 706f 2c20  ch_create(repo, ",
            "+00013500: 6e61 6d65 2c20 6f62 6a65 6374 6973 683d  name, objectish=",
            "+00013510: 4e6f 6e65 2c20 666f 7263 653d 4661 6c73  None, force=Fals",
            "+00013520: 6529 202d 3e20 4e6f 6e65 3a0a 2020 2020  e) -> None:.    ",
            "+00013530: 2222 2243 7265 6174 6520 6120 6272 616e  \"\"\"Create a bran",
            "+00013540: 6368 2e0a 0a20 2020 2041 7267 733a 0a20  ch...    Args:. ",
            "+00013550: 2020 2020 2072 6570 6f3a 2050 6174 6820       repo: Path ",
            "+00013560: 746f 2074 6865 2072 6570 6f73 6974 6f72  to the repositor",
            "+00013570: 790a 2020 2020 2020 6e61 6d65 3a20 4e61  y.      name: Na",
            "+00013580: 6d65 206f 6620 7468 6520 6e65 7720 6272  me of the new br",
            "+00013590: 616e 6368 0a20 2020 2020 206f 626a 6563  anch.      objec",
            "+000135a0: 7469 7368 3a20 5461 7267 6574 206f 626a  tish: Target obj",
            "+000135b0: 6563 7420 746f 2070 6f69 6e74 206e 6577  ect to point new",
            "+000135c0: 2062 7261 6e63 6820 6174 2028 6465 6661   branch at (defa",
            "+000135d0: 756c 7473 2074 6f20 4845 4144 290a 2020  ults to HEAD).  ",
            "+000135e0: 2020 2020 666f 7263 653a 2046 6f72 6365      force: Force",
            "+000135f0: 2063 7265 6174 696f 6e20 6f66 2062 7261   creation of bra",
            "+00013600: 6e63 682c 2065 7665 6e20 6966 2069 7420  nch, even if it ",
            "+00013610: 616c 7265 6164 7920 6578 6973 7473 0a20  already exists. ",
            "+00013620: 2020 2022 2222 0a20 2020 2077 6974 6820     \"\"\".    with ",
            "+00013630: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "+00013640: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "+00013650: 2020 2020 2020 6966 206f 626a 6563 7469        if objecti",
            "+00013660: 7368 2069 7320 4e6f 6e65 3a0a 2020 2020  sh is None:.    ",
            "+00013670: 2020 2020 2020 2020 6f62 6a65 6374 6973          objectis",
            "+00013680: 6820 3d20 2248 4541 4422 0a0a 2020 2020  h = \"HEAD\"..    ",
            "+00013690: 2020 2020 2320 5472 7920 746f 2065 7870      # Try to exp",
            "+000136a0: 616e 6420 6272 616e 6368 2073 686f 7274  and branch short",
            "+000136b0: 6861 6e64 2062 6566 6f72 6520 7061 7273  hand before pars",
            "+000136c0: 696e 670a 2020 2020 2020 2020 6f72 6967  ing.        orig",
            "+000136d0: 696e 616c 5f6f 626a 6563 7469 7368 203d  inal_objectish =",
            "+000136e0: 206f 626a 6563 7469 7368 0a20 2020 2020   objectish.     ",
            "+000136f0: 2020 206f 626a 6563 7469 7368 5f62 7974     objectish_byt",
            "+00013700: 6573 203d 2028 0a20 2020 2020 2020 2020  es = (.         ",
            "+00013710: 2020 206f 626a 6563 7469 7368 2e65 6e63     objectish.enc",
            "+00013720: 6f64 6528 4445 4641 554c 545f 454e 434f  ode(DEFAULT_ENCO",
            "+00013730: 4449 4e47 290a 2020 2020 2020 2020 2020  DING).          ",
            "+00013740: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(",
            "+00013750: 6f62 6a65 6374 6973 682c 2073 7472 290a  objectish, str).",
            "+00013760: 2020 2020 2020 2020 2020 2020 656c 7365              else",
            "+00013770: 206f 626a 6563 7469 7368 0a20 2020 2020   objectish.     ",
            "+00013780: 2020 2029 0a20 2020 2020 2020 2069 6620     ).        if ",
            "+00013790: 6222 7265 6673 2f72 656d 6f74 6573 2f22  b\"refs/remotes/\"",
            "+000137a0: 202b 206f 626a 6563 7469 7368 5f62 7974   + objectish_byt",
            "+000137b0: 6573 2069 6e20 722e 7265 6673 3a0a 2020  es in r.refs:.  ",
            "+000137c0: 2020 2020 2020 2020 2020 6f62 6a65 6374            object",
            "+000137d0: 6973 6820 3d20 6222 7265 6673 2f72 656d  ish = b\"refs/rem",
            "+000137e0: 6f74 6573 2f22 202b 206f 626a 6563 7469  otes/\" + objecti",
            "+000137f0: 7368 5f62 7974 6573 0a20 2020 2020 2020  sh_bytes.       ",
            "+00013800: 2065 6c69 6620 6222 7265 6673 2f68 6561   elif b\"refs/hea",
            "+00013810: 6473 2f22 202b 206f 626a 6563 7469 7368  ds/\" + objectish",
            "+00013820: 5f62 7974 6573 2069 6e20 722e 7265 6673  _bytes in r.refs",
            "+00013830: 3a0a 2020 2020 2020 2020 2020 2020 6f62  :.            ob",
            "+00013840: 6a65 6374 6973 6820 3d20 6222 7265 6673  jectish = b\"refs",
            "+00013850: 2f68 6561 6473 2f22 202b 206f 626a 6563  /heads/\" + objec",
            "+00013860: 7469 7368 5f62 7974 6573 0a0a 2020 2020  tish_bytes..    ",
            "+00013870: 2020 2020 6f62 6a65 6374 203d 2070 6172      object = par",
            "+00013880: 7365 5f6f 626a 6563 7428 722c 206f 626a  se_object(r, obj",
            "+00013890: 6563 7469 7368 290a 2020 2020 2020 2020  ectish).        ",
            "+000138a0: 7265 666e 616d 6520 3d20 5f6d 616b 655f  refname = _make_",
            "+000138b0: 6272 616e 6368 5f72 6566 286e 616d 6529  branch_ref(name)",
            "+000138c0: 0a20 2020 2020 2020 2072 6566 5f6d 6573  .        ref_mes",
            "+000138d0: 7361 6765 203d 2028 0a20 2020 2020 2020  sage = (.       ",
            "+000138e0: 2020 2020 2062 2262 7261 6e63 683a 2043       b\"branch: C",
            "+000138f0: 7265 6174 6564 2066 726f 6d20 2220 2b20  reated from \" + ",
            "+00013900: 6f72 6967 696e 616c 5f6f 626a 6563 7469  original_objecti",
            "+00013910: 7368 2e65 6e63 6f64 6528 4445 4641 554c  sh.encode(DEFAUL",
            "+00013920: 545f 454e 434f 4449 4e47 290a 2020 2020  T_ENCODING).    ",
            "+00013930: 2020 2020 2020 2020 6966 2069 7369 6e73          if isins",
            "+00013940: 7461 6e63 6528 6f72 6967 696e 616c 5f6f  tance(original_o",
            "+00013950: 626a 6563 7469 7368 2c20 7374 7229 0a20  bjectish, str). ",
            "+00013960: 2020 2020 2020 2020 2020 2065 6c73 6520             else ",
            "+00013970: 6222 6272 616e 6368 3a20 4372 6561 7465  b\"branch: Create",
            "+00013980: 6420 6672 6f6d 2022 202b 206f 7269 6769  d from \" + origi",
            "+00013990: 6e61 6c5f 6f62 6a65 6374 6973 680a 2020  nal_objectish.  ",
            "+000139a0: 2020 2020 2020 290a 2020 2020 2020 2020        ).        ",
            "+000139b0: 6966 2066 6f72 6365 3a0a 2020 2020 2020  if force:.      ",
            "+000139c0: 2020 2020 2020 722e 7265 6673 2e73 6574        r.refs.set",
            "+000139d0: 5f69 665f 6571 7561 6c73 2872 6566 6e61  _if_equals(refna",
            "+000139e0: 6d65 2c20 4e6f 6e65 2c20 6f62 6a65 6374  me, None, object",
            "+000139f0: 2e69 642c 206d 6573 7361 6765 3d72 6566  .id, message=ref",
            "+00013a00: 5f6d 6573 7361 6765 290a 2020 2020 2020  _message).      ",
            "+00013a10: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        ",
            "+00013a20: 2020 2020 6966 206e 6f74 2072 2e72 6566      if not r.ref",
            "+00013a30: 732e 6164 645f 6966 5f6e 6577 2872 6566  s.add_if_new(ref",
            "+00013a40: 6e61 6d65 2c20 6f62 6a65 6374 2e69 642c  name, object.id,",
            "+00013a50: 206d 6573 7361 6765 3d72 6566 5f6d 6573   message=ref_mes",
            "+00013a60: 7361 6765 293a 0a20 2020 2020 2020 2020  sage):.         ",
            "+00013a70: 2020 2020 2020 2072 6169 7365 2045 7272         raise Err",
            "+00013a80: 6f72 2866 2242 7261 6e63 6820 7769 7468  or(f\"Branch with",
            "+00013a90: 206e 616d 6520 7b6e 616d 657d 2061 6c72   name {name} alr",
            "+00013aa0: 6561 6479 2065 7869 7374 732e 2229 0a0a  eady exists.\")..",
            "+00013ab0: 2020 2020 2020 2020 2320 4368 6563 6b20          # Check ",
            "+00013ac0: 6966 2077 6520 7368 6f75 6c64 2073 6574  if we should set",
            "+00013ad0: 2075 7020 7472 6163 6b69 6e67 0a20 2020   up tracking.   ",
            "+00013ae0: 2020 2020 2063 6f6e 6669 6720 3d20 722e       config = r.",
            "+00013af0: 6765 745f 636f 6e66 6967 5f73 7461 636b  get_config_stack",
            "+00013b00: 2829 0a20 2020 2020 2020 2074 7279 3a0a  ().        try:.",
            "+00013b10: 2020 2020 2020 2020 2020 2020 6175 746f              auto",
            "+00013b20: 5f73 6574 7570 5f6d 6572 6765 203d 2063  _setup_merge = c",
            "+00013b30: 6f6e 6669 672e 6765 7428 2862 2262 7261  onfig.get((b\"bra",
            "+00013b40: 6e63 6822 2c29 2c20 6222 6175 746f 5365  nch\",), b\"autoSe",
            "+00013b50: 7475 704d 6572 6765 2229 2e64 6563 6f64  tupMerge\").decod",
            "+00013b60: 6528 290a 2020 2020 2020 2020 6578 6365  e().        exce",
            "+00013b70: 7074 204b 6579 4572 726f 723a 0a20 2020  pt KeyError:.   ",
            "+00013b80: 2020 2020 2020 2020 2061 7574 6f5f 7365           auto_se",
            "+00013b90: 7475 705f 6d65 7267 6520 3d20 2274 7275  tup_merge = \"tru",
            "+00013ba0: 6522 2020 2320 4465 6661 756c 7420 7661  e\"  # Default va",
            "+00013bb0: 6c75 650a 0a20 2020 2020 2020 2023 2044  lue..        # D",
            "+00013bc0: 6574 6572 6d69 6e65 2069 6620 7468 6520  etermine if the ",
            "+00013bd0: 6f62 6a65 6374 6973 6820 7265 6665 7273  objectish refers",
            "+00013be0: 2074 6f20 6120 7265 6d6f 7465 2d74 7261   to a remote-tra",
            "+00013bf0: 636b 696e 6720 6272 616e 6368 0a20 2020  cking branch.   ",
            "+00013c00: 2020 2020 206f 626a 6563 7469 7368 5f72       objectish_r",
            "+00013c10: 6566 203d 204e 6f6e 650a 2020 2020 2020  ef = None.      ",
            "+00013c20: 2020 6966 206f 7269 6769 6e61 6c5f 6f62    if original_ob",
            "+00013c30: 6a65 6374 6973 6820 213d 2022 4845 4144  jectish != \"HEAD",
            "+00013c40: 223a 0a20 2020 2020 2020 2020 2020 2023  \":.            #",
            "+00013c50: 2054 7279 2074 6f20 7265 736f 6c76 6520   Try to resolve ",
            "+00013c60: 6f62 6a65 6374 6973 6820 6173 2061 2072  objectish as a r",
            "+00013c70: 6566 0a20 2020 2020 2020 2020 2020 206f  ef.            o",
            "+00013c80: 626a 6563 7469 7368 5f62 7974 6573 203d  bjectish_bytes =",
            "+00013c90: 2028 0a20 2020 2020 2020 2020 2020 2020   (.             ",
            "+00013ca0: 2020 206f 7269 6769 6e61 6c5f 6f62 6a65     original_obje",
            "+00013cb0: 6374 6973 682e 656e 636f 6465 2844 4546  ctish.encode(DEF",
            "+00013cc0: 4155 4c54 5f45 4e43 4f44 494e 4729 0a20  AULT_ENCODING). ",
            "+00013cd0: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "+00013ce0: 6620 6973 696e 7374 616e 6365 286f 7269  f isinstance(ori",
            "+00013cf0: 6769 6e61 6c5f 6f62 6a65 6374 6973 682c  ginal_objectish,",
            "+00013d00: 2073 7472 290a 2020 2020 2020 2020 2020   str).          ",
            "+00013d10: 2020 2020 2020 656c 7365 206f 7269 6769        else origi",
            "+00013d20: 6e61 6c5f 6f62 6a65 6374 6973 680a 2020  nal_objectish.  ",
            "+00013d30: 2020 2020 2020 2020 2020 290a 2020 2020            ).    ",
            "+00013d40: 2020 2020 2020 2020 6966 206f 626a 6563          if objec",
            "+00013d50: 7469 7368 5f62 7974 6573 2069 6e20 722e  tish_bytes in r.",
            "+00013d60: 7265 6673 3a0a 2020 2020 2020 2020 2020  refs:.          ",
            "+00013d70: 2020 2020 2020 6f62 6a65 6374 6973 685f        objectish_",
            "+00013d80: 7265 6620 3d20 6f62 6a65 6374 6973 685f  ref = objectish_",
            "+00013d90: 6279 7465 730a 2020 2020 2020 2020 2020  bytes.          ",
            "+00013da0: 2020 656c 6966 2062 2272 6566 732f 7265    elif b\"refs/re",
            "+00013db0: 6d6f 7465 732f 2220 2b20 6f62 6a65 6374  motes/\" + object",
            "+00013dc0: 6973 685f 6279 7465 7320 696e 2072 2e72  ish_bytes in r.r",
            "+00013dd0: 6566 733a 0a20 2020 2020 2020 2020 2020  efs:.           ",
            "+00013de0: 2020 2020 206f 626a 6563 7469 7368 5f72       objectish_r",
            "+00013df0: 6566 203d 2062 2272 6566 732f 7265 6d6f  ef = b\"refs/remo",
            "+00013e00: 7465 732f 2220 2b20 6f62 6a65 6374 6973  tes/\" + objectis",
            "+00013e10: 685f 6279 7465 730a 2020 2020 2020 2020  h_bytes.        ",
            "+00013e20: 2020 2020 656c 6966 2062 2272 6566 732f      elif b\"refs/",
            "+00013e30: 6865 6164 732f 2220 2b20 6f62 6a65 6374  heads/\" + object",
            "+00013e40: 6973 685f 6279 7465 7320 696e 2072 2e72  ish_bytes in r.r",
            "+00013e50: 6566 733a 0a20 2020 2020 2020 2020 2020  efs:.           ",
            "+00013e60: 2020 2020 206f 626a 6563 7469 7368 5f72       objectish_r",
            "+00013e70: 6566 203d 2062 2272 6566 732f 6865 6164  ef = b\"refs/head",
            "+00013e80: 732f 2220 2b20 6f62 6a65 6374 6973 685f  s/\" + objectish_",
            "+00013e90: 6279 7465 730a 2020 2020 2020 2020 656c  bytes.        el",
            "+00013ea0: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            ",
            "+00013eb0: 2320 4845 4144 206d 6967 6874 2070 6f69  # HEAD might poi",
            "+00013ec0: 6e74 2074 6f20 6120 7265 6d6f 7465 2d74  nt to a remote-t",
            "+00013ed0: 7261 636b 696e 6720 6272 616e 6368 0a20  racking branch. ",
            "+00013ee0: 2020 2020 2020 2020 2020 2068 6561 645f             head_",
            "+00013ef0: 7265 6620 3d20 722e 7265 6673 2e66 6f6c  ref = r.refs.fol",
            "+00013f00: 6c6f 7728 6222 4845 4144 2229 5b30 5d5b  low(b\"HEAD\")[0][",
            "+00013f10: 315d 0a20 2020 2020 2020 2020 2020 2069  1].            i",
            "+00013f20: 6620 6865 6164 5f72 6566 2e73 7461 7274  f head_ref.start",
            "+00013f30: 7377 6974 6828 6222 7265 6673 2f72 656d  swith(b\"refs/rem",
            "+00013f40: 6f74 6573 2f22 293a 0a20 2020 2020 2020  otes/\"):.       ",
            "+00013f50: 2020 2020 2020 2020 206f 626a 6563 7469           objecti",
            "+00013f60: 7368 5f72 6566 203d 2068 6561 645f 7265  sh_ref = head_re",
            "+00013f70: 660a 0a20 2020 2020 2020 2023 2053 6574  f..        # Set",
            "+00013f80: 2075 7020 7472 6163 6b69 6e67 2069 6620   up tracking if ",
            "+00013f90: 6170 7072 6f70 7269 6174 650a 2020 2020  appropriate.    ",
            "+00013fa0: 2020 2020 6966 206f 626a 6563 7469 7368      if objectish",
            "+00013fb0: 5f72 6566 2061 6e64 2028 0a20 2020 2020  _ref and (.     ",
            "+00013fc0: 2020 2020 2020 2028 6175 746f 5f73 6574         (auto_set",
            "+00013fd0: 7570 5f6d 6572 6765 203d 3d20 2261 6c77  up_merge == \"alw",
            "+00013fe0: 6179 7322 290a 2020 2020 2020 2020 2020  ays\").          ",
            "+00013ff0: 2020 6f72 2028 0a20 2020 2020 2020 2020    or (.         ",
            "+00014000: 2020 2020 2020 2061 7574 6f5f 7365 7475         auto_setu",
            "+00014010: 705f 6d65 7267 6520 3d3d 2022 7472 7565  p_merge == \"true",
            "+00014020: 220a 2020 2020 2020 2020 2020 2020 2020  \".              ",
            "+00014030: 2020 616e 6420 6f62 6a65 6374 6973 685f    and objectish_",
            "+00014040: 7265 662e 7374 6172 7473 7769 7468 2862  ref.startswith(b",
            "+00014050: 2272 6566 732f 7265 6d6f 7465 732f 2229  \"refs/remotes/\")",
            "+00014060: 0a20 2020 2020 2020 2020 2020 2029 0a20  .            ). ",
            "+00014070: 2020 2020 2020 2029 3a0a 2020 2020 2020         ):.      ",
            "+00014080: 2020 2020 2020 2320 4578 7472 6163 7420        # Extract ",
            "+00014090: 7265 6d6f 7465 206e 616d 6520 616e 6420  remote name and ",
            "+000140a0: 6272 616e 6368 2066 726f 6d20 7468 6520  branch from the ",
            "+000140b0: 7265 660a 2020 2020 2020 2020 2020 2020  ref.            ",
            "+000140c0: 6966 206f 626a 6563 7469 7368 5f72 6566  if objectish_ref",
            "+000140d0: 2e73 7461 7274 7377 6974 6828 6222 7265  .startswith(b\"re",
            "+000140e0: 6673 2f72 656d 6f74 6573 2f22 293a 0a20  fs/remotes/\"):. ",
            "+000140f0: 2020 2020 2020 2020 2020 2020 2020 2070                 p",
            "+00014100: 6172 7473 203d 206f 626a 6563 7469 7368  arts = objectish",
            "+00014110: 5f72 6566 5b6c 656e 2862 2272 6566 732f  _ref[len(b\"refs/",
            "+00014120: 7265 6d6f 7465 732f 2229 203a 5d2e 7370  remotes/\") :].sp",
            "+00014130: 6c69 7428 6222 2f22 2c20 3129 0a20 2020  lit(b\"/\", 1).   ",
            "+00014140: 2020 2020 2020 2020 2020 2020 2069 6620               if ",
            "+00014150: 6c65 6e28 7061 7274 7329 203d 3d20 323a  len(parts) == 2:",
            "+00014160: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00014170: 2020 2020 2072 656d 6f74 655f 6e61 6d65       remote_name",
            "+00014180: 203d 2070 6172 7473 5b30 5d0a 2020 2020   = parts[0].    ",
            "+00014190: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000141a0: 7265 6d6f 7465 5f62 7261 6e63 6820 3d20  remote_branch = ",
            "+000141b0: 6222 7265 6673 2f68 6561 6473 2f22 202b  b\"refs/heads/\" +",
            "+000141c0: 2070 6172 7473 5b31 5d0a 0a20 2020 2020   parts[1]..     ",
            "+000141d0: 2020 2020 2020 2020 2020 2020 2020 2023                 #",
            "+000141e0: 2053 6574 2075 7020 7472 6163 6b69 6e67   Set up tracking",
            "+000141f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00014200: 2020 2020 2063 6f6e 6669 6720 3d20 722e       config = r.",
            "+00014210: 6765 745f 636f 6e66 6967 2829 0a20 2020  get_config().   ",
            "+00014220: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00014230: 2062 7261 6e63 685f 6e61 6d65 5f62 7974   branch_name_byt",
            "+00014240: 6573 203d 2028 0a20 2020 2020 2020 2020  es = (.         ",
            "+00014250: 2020 2020 2020 2020 2020 2020 2020 206e                 n",
            "+00014260: 616d 652e 656e 636f 6465 2844 4546 4155  ame.encode(DEFAU",
            "+00014270: 4c54 5f45 4e43 4f44 494e 4729 2069 6620  LT_ENCODING) if ",
            "+00014280: 6973 696e 7374 616e 6365 286e 616d 652c  isinstance(name,",
            "+00014290: 2073 7472 2920 656c 7365 206e 616d 650a   str) else name.",
            "+000142a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000142b0: 2020 2020 290a 2020 2020 2020 2020 2020      ).          ",
            "+000142c0: 2020 2020 2020 2020 2020 636f 6e66 6967            config",
            "+000142d0: 2e73 6574 2828 6222 6272 616e 6368 222c  .set((b\"branch\",",
            "+000142e0: 2062 7261 6e63 685f 6e61 6d65 5f62 7974   branch_name_byt",
            "+000142f0: 6573 292c 2062 2272 656d 6f74 6522 2c20  es), b\"remote\", ",
            "+00014300: 7265 6d6f 7465 5f6e 616d 6529 0a20 2020  remote_name).   ",
            "+00014310: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00014320: 2063 6f6e 6669 672e 7365 7428 2862 2262   config.set((b\"b",
            "+00014330: 7261 6e63 6822 2c20 6272 616e 6368 5f6e  ranch\", branch_n",
            "+00014340: 616d 655f 6279 7465 7329 2c20 6222 6d65  ame_bytes), b\"me",
            "+00014350: 7267 6522 2c20 7265 6d6f 7465 5f62 7261  rge\", remote_bra",
            "+00014360: 6e63 6829 0a20 2020 2020 2020 2020 2020  nch).           ",
            "+00014370: 2020 2020 2020 2020 2063 6f6e 6669 672e           config.",
            "+00014380: 7772 6974 655f 746f 5f70 6174 6828 290a  write_to_path().",
            "+00014390: 0a0a 6465 6620 6272 616e 6368 5f6c 6973  ..def branch_lis",
            "+000143a0: 7428 7265 706f 293a 0a20 2020 2022 2222  t(repo):.    \"\"\"",
            "+000143b0: 4c69 7374 2061 6c6c 2062 7261 6e63 6865  List all branche",
            "+000143c0: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  ",
            "+000143d0: 2020 2020 7265 706f 3a20 5061 7468 2074      repo: Path t",
            "+000143e0: 6f20 7468 6520 7265 706f 7369 746f 7279  o the repository",
            "+000143f0: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  ",
            "+00014400: 2020 2020 4c69 7374 206f 6620 6272 616e      List of bran",
            "+00014410: 6368 206e 616d 6573 2028 7769 7468 6f75  ch names (withou",
            "+00014420: 7420 7265 6673 2f68 6561 6473 2f20 7072  t refs/heads/ pr",
            "+00014430: 6566 6978 290a 2020 2020 2222 220a 2020  efix).    \"\"\".  ",
            "+00014440: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+00014450: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+00014460: 7320 723a 0a20 2020 2020 2020 2062 7261  s r:.        bra",
            "+00014470: 6e63 6865 7320 3d20 6c69 7374 2872 2e72  nches = list(r.r",
            "+00014480: 6566 732e 6b65 7973 2862 6173 653d 4c4f  efs.keys(base=LO",
            "+00014490: 4341 4c5f 4252 414e 4348 5f50 5245 4649  CAL_BRANCH_PREFI",
            "+000144a0: 5829 290a 0a20 2020 2020 2020 2023 2043  X))..        # C",
            "+000144b0: 6865 636b 2066 6f72 2062 7261 6e63 682e  heck for branch.",
            "+000144c0: 736f 7274 2063 6f6e 6669 6775 7261 7469  sort configurati",
            "+000144d0: 6f6e 0a20 2020 2020 2020 2063 6f6e 6669  on.        confi",
            "+000144e0: 6720 3d20 722e 6765 745f 636f 6e66 6967  g = r.get_config",
            "+000144f0: 5f73 7461 636b 2829 0a20 2020 2020 2020  _stack().       ",
            "+00014500: 2074 7279 3a0a 2020 2020 2020 2020 2020   try:.          ",
            "+00014510: 2020 736f 7274 5f6b 6579 203d 2063 6f6e    sort_key = con",
            "+00014520: 6669 672e 6765 7428 2862 2262 7261 6e63  fig.get((b\"branc",
            "+00014530: 6822 2c29 2c20 6222 736f 7274 2229 2e64  h\",), b\"sort\").d",
            "+00014540: 6563 6f64 6528 290a 2020 2020 2020 2020  ecode().        ",
            "+00014550: 6578 6365 7074 204b 6579 4572 726f 723a  except KeyError:",
            "+00014560: 0a20 2020 2020 2020 2020 2020 2023 2044  .            # D",
            "+00014570: 6566 6175 6c74 2069 7320 7265 666e 616d  efault is refnam",
            "+00014580: 6520 2861 6c70 6861 6265 7469 6361 6c29  e (alphabetical)",
            "+00014590: 0a20 2020 2020 2020 2020 2020 2073 6f72  .            sor",
            "+000145a0: 745f 6b65 7920 3d20 2272 6566 6e61 6d65  t_key = \"refname",
            "+000145b0: 220a 0a20 2020 2020 2020 2023 2050 6172  \"..        # Par",
            "+000145c0: 7365 2073 6f72 7420 6b65 790a 2020 2020  se sort key.    ",
            "+000145d0: 2020 2020 7265 7665 7273 6520 3d20 4661      reverse = Fa",
            "+000145e0: 6c73 650a 2020 2020 2020 2020 6966 2073  lse.        if s",
            "+000145f0: 6f72 745f 6b65 792e 7374 6172 7473 7769  ort_key.startswi",
            "+00014600: 7468 2822 2d22 293a 0a20 2020 2020 2020  th(\"-\"):.       ",
            "+00014610: 2020 2020 2072 6576 6572 7365 203d 2054       reverse = T",
            "+00014620: 7275 650a 2020 2020 2020 2020 2020 2020  rue.            ",
            "+00014630: 736f 7274 5f6b 6579 203d 2073 6f72 745f  sort_key = sort_",
            "+00014640: 6b65 795b 313a 5d0a 0a20 2020 2020 2020  key[1:]..       ",
            "+00014650: 2023 2041 7070 6c79 2073 6f72 7469 6e67   # Apply sorting",
            "+00014660: 0a20 2020 2020 2020 2069 6620 736f 7274  .        if sort",
            "+00014670: 5f6b 6579 203d 3d20 2272 6566 6e61 6d65  _key == \"refname",
            "+00014680: 223a 0a20 2020 2020 2020 2020 2020 2023  \":.            #",
            "+00014690: 2053 696d 706c 6520 616c 7068 6162 6574   Simple alphabet",
            "+000146a0: 6963 616c 2073 6f72 7420 2864 6566 6175  ical sort (defau",
            "+000146b0: 6c74 290a 2020 2020 2020 2020 2020 2020  lt).            ",
            "+000146c0: 6272 616e 6368 6573 2e73 6f72 7428 7265  branches.sort(re",
            "+000146d0: 7665 7273 653d 7265 7665 7273 6529 0a20  verse=reverse). ",
            "+000146e0: 2020 2020 2020 2065 6c69 6620 736f 7274         elif sort",
            "+000146f0: 5f6b 6579 2069 6e20 2822 636f 6d6d 6974  _key in (\"commit",
            "+00014700: 7465 7264 6174 6522 2c20 2261 7574 686f  terdate\", \"autho",
            "+00014710: 7264 6174 6522 293a 0a20 2020 2020 2020  rdate\"):.       ",
            "+00014720: 2020 2020 2023 2053 6f72 7420 6279 2064       # Sort by d",
            "+00014730: 6174 650a 2020 2020 2020 2020 2020 2020  ate.            ",
            "+00014740: 6465 6620 6765 745f 636f 6d6d 6974 5f64  def get_commit_d",
            "+00014750: 6174 6528 6272 616e 6368 5f6e 616d 6529  ate(branch_name)",
            "+00014760: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "+00014770: 2020 7265 6620 3d20 4c4f 4341 4c5f 4252    ref = LOCAL_BR",
            "+00014780: 414e 4348 5f50 5245 4649 5820 2b20 6272  ANCH_PREFIX + br",
            "+00014790: 616e 6368 5f6e 616d 650a 2020 2020 2020  anch_name.      ",
            "+000147a0: 2020 2020 2020 2020 2020 7368 6120 3d20            sha = ",
            "+000147b0: 722e 7265 6673 5b72 6566 5d0a 2020 2020  r.refs[ref].    ",
            "+000147c0: 2020 2020 2020 2020 2020 2020 636f 6d6d              comm",
            "+000147d0: 6974 203d 2072 2e6f 626a 6563 745f 7374  it = r.object_st",
            "+000147e0: 6f72 655b 7368 615d 0a20 2020 2020 2020  ore[sha].       ",
            "+000147f0: 2020 2020 2020 2020 2069 6620 736f 7274           if sort",
            "+00014800: 5f6b 6579 203d 3d20 2263 6f6d 6d69 7474  _key == \"committ",
            "+00014810: 6572 6461 7465 223a 0a20 2020 2020 2020  erdate\":.       ",
            "+00014820: 2020 2020 2020 2020 2020 2020 2072 6574               ret",
            "+00014830: 7572 6e20 636f 6d6d 6974 2e63 6f6d 6d69  urn commit.commi",
            "+00014840: 745f 7469 6d65 0a20 2020 2020 2020 2020  t_time.         ",
            "+00014850: 2020 2020 2020 2065 6c73 653a 2020 2320         else:  # ",
            "+00014860: 6175 7468 6f72 6461 7465 0a20 2020 2020  authordate.     ",
            "+00014870: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+00014880: 6574 7572 6e20 636f 6d6d 6974 2e61 7574  eturn commit.aut",
            "+00014890: 686f 725f 7469 6d65 0a0a 2020 2020 2020  hor_time..      ",
            "+000148a0: 2020 2020 2020 2320 536f 7274 2062 7261        # Sort bra",
            "+000148b0: 6e63 6865 7320 6279 2064 6174 650a 2020  nches by date.  ",
            "+000148c0: 2020 2020 2020 2020 2020 2320 4e6f 7465            # Note",
            "+000148d0: 3a20 5079 7468 6f6e 2773 2073 6f72 7420  : Python's sort ",
            "+000148e0: 6e61 7475 7261 6c6c 7920 6f72 6465 7273  naturally orders",
            "+000148f0: 2073 6d61 6c6c 6572 2076 616c 7565 7320   smaller values ",
            "+00014900: 6669 7273 7420 2861 7363 656e 6469 6e67  first (ascending",
            "+00014910: 290a 2020 2020 2020 2020 2020 2020 2320  ).            # ",
            "+00014920: 466f 7220 6461 7465 732c 2074 6869 7320  For dates, this ",
            "+00014930: 6d65 616e 7320 6f6c 6465 7374 2066 6972  means oldest fir",
            "+00014940: 7374 2062 7920 6465 6661 756c 740a 2020  st by default.  ",
            "+00014950: 2020 2020 2020 2020 2020 2320 5573 6520            # Use ",
            "+00014960: 6120 7374 6162 6c65 2073 6f72 7420 7769  a stable sort wi",
            "+00014970: 7468 2062 7261 6e63 6820 6e61 6d65 2061  th branch name a",
            "+00014980: 7320 7365 636f 6e64 6172 7920 6b65 7920  s secondary key ",
            "+00014990: 666f 7220 636f 6e73 6973 7465 6e74 206f  for consistent o",
            "+000149a0: 7264 6572 696e 670a 2020 2020 2020 2020  rdering.        ",
            "+000149b0: 2020 2020 6966 2072 6576 6572 7365 3a0a      if reverse:.",
            "+000149c0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000149d0: 2320 466f 7220 7265 7665 7273 6520 736f  # For reverse so",
            "+000149e0: 7274 2c20 7765 2077 616e 7420 6e65 7765  rt, we want newe",
            "+000149f0: 7374 2064 6174 6573 2066 6972 7374 2062  st dates first b",
            "+00014a00: 7574 2061 6c70 6861 6265 7469 6361 6c20  ut alphabetical ",
            "+00014a10: 6e61 6d65 7320 7365 636f 6e64 0a20 2020  names second.   ",
            "+00014a20: 2020 2020 2020 2020 2020 2020 2062 7261               bra",
            "+00014a30: 6e63 6865 732e 736f 7274 286b 6579 3d6c  nches.sort(key=l",
            "+00014a40: 616d 6264 6120 623a 2028 2d67 6574 5f63  ambda b: (-get_c",
            "+00014a50: 6f6d 6d69 745f 6461 7465 2862 292c 2062  ommit_date(b), b",
            "+00014a60: 2929 0a20 2020 2020 2020 2020 2020 2065  )).            e",
            "+00014a70: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           ",
            "+00014a80: 2020 2020 2062 7261 6e63 6865 732e 736f       branches.so",
            "+00014a90: 7274 286b 6579 3d6c 616d 6264 6120 623a  rt(key=lambda b:",
            "+00014aa0: 2028 6765 745f 636f 6d6d 6974 5f64 6174   (get_commit_dat",
            "+00014ab0: 6528 6229 2c20 6229 290a 2020 2020 2020  e(b), b)).      ",
            "+00014ac0: 2020 656c 7365 3a0a 2020 2020 2020 2020    else:.        ",
            "+00014ad0: 2020 2020 2320 556e 6b6e 6f77 6e20 736f      # Unknown so",
            "+00014ae0: 7274 206b 6579 2c20 6661 6c6c 2062 6163  rt key, fall bac",
            "+00014af0: 6b20 746f 2064 6566 6175 6c74 0a20 2020  k to default.   ",
            "+00014b00: 2020 2020 2020 2020 2062 7261 6e63 6865           branche",
            "+00014b10: 732e 736f 7274 2829 0a0a 2020 2020 2020  s.sort()..      ",
            "+00014b20: 2020 7265 7475 726e 2062 7261 6e63 6865    return branche",
            "+00014b30: 730a 0a0a 6465 6620 6163 7469 7665 5f62  s...def active_b",
            "+00014b40: 7261 6e63 6828 7265 706f 293a 0a20 2020  ranch(repo):.   ",
            "+00014b50: 2022 2222 5265 7475 726e 2074 6865 2061   \"\"\"Return the a",
            "+00014b60: 6374 6976 6520 6272 616e 6368 2069 6e20  ctive branch in ",
            "+00014b70: 7468 6520 7265 706f 7369 746f 7279 2c20  the repository, ",
            "+00014b80: 6966 2061 6e79 2e0a 0a20 2020 2041 7267  if any...    Arg",
            "+00014b90: 733a 0a20 2020 2020 2072 6570 6f3a 2052  s:.      repo: R",
            "+00014ba0: 6570 6f73 6974 6f72 7920 746f 206f 7065  epository to ope",
            "+00014bb0: 6e0a 2020 2020 5265 7475 726e 733a 0a20  n.    Returns:. ",
            "+00014bc0: 2020 2020 2062 7261 6e63 6820 6e61 6d65       branch name",
            "+00014bd0: 0a20 2020 2052 6169 7365 733a 0a20 2020  .    Raises:.   ",
            "+00014be0: 2020 204b 6579 4572 726f 723a 2069 6620     KeyError: if ",
            "+00014bf0: 7468 6520 7265 706f 7369 746f 7279 2064  the repository d",
            "+00014c00: 6f65 7320 6e6f 7420 6861 7665 2061 2077  oes not have a w",
            "+00014c10: 6f72 6b69 6e67 2074 7265 650a 2020 2020  orking tree.    ",
            "+00014c20: 2020 496e 6465 7845 7272 6f72 3a20 6966    IndexError: if",
            "+00014c30: 2048 4541 4420 6973 2066 6c6f 6174 696e   HEAD is floatin",
            "+00014c40: 670a 2020 2020 2222 220a 2020 2020 7769  g.    \"\"\".    wi",
            "+00014c50: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "+00014c60: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "+00014c70: 0a20 2020 2020 2020 2061 6374 6976 655f  .        active_",
            "+00014c80: 7265 6620 3d20 722e 7265 6673 2e66 6f6c  ref = r.refs.fol",
            "+00014c90: 6c6f 7728 6222 4845 4144 2229 5b30 5d5b  low(b\"HEAD\")[0][",
            "+00014ca0: 315d 0a20 2020 2020 2020 2069 6620 6e6f  1].        if no",
            "+00014cb0: 7420 6163 7469 7665 5f72 6566 2e73 7461  t active_ref.sta",
            "+00014cc0: 7274 7377 6974 6828 4c4f 4341 4c5f 4252  rtswith(LOCAL_BR",
            "+00014cd0: 414e 4348 5f50 5245 4649 5829 3a0a 2020  ANCH_PREFIX):.  ",
            "+00014ce0: 2020 2020 2020 2020 2020 7261 6973 6520            raise ",
            "+00014cf0: 5661 6c75 6545 7272 6f72 2861 6374 6976  ValueError(activ",
            "+00014d00: 655f 7265 6629 0a20 2020 2020 2020 2072  e_ref).        r",
            "+00014d10: 6574 7572 6e20 6163 7469 7665 5f72 6566  eturn active_ref",
            "+00014d20: 5b6c 656e 284c 4f43 414c 5f42 5241 4e43  [len(LOCAL_BRANC",
            "+00014d30: 485f 5052 4546 4958 2920 3a5d 0a0a 0a64  H_PREFIX) :]...d",
            "+00014d40: 6566 2067 6574 5f62 7261 6e63 685f 7265  ef get_branch_re",
            "+00014d50: 6d6f 7465 2872 6570 6f29 3a0a 2020 2020  mote(repo):.    ",
            "+00014d60: 2222 2252 6574 7572 6e20 7468 6520 6163  \"\"\"Return the ac",
            "+00014d70: 7469 7665 2062 7261 6e63 6827 7320 7265  tive branch's re",
            "+00014d80: 6d6f 7465 206e 616d 652c 2069 6620 616e  mote name, if an",
            "+00014d90: 792e 0a0a 2020 2020 4172 6773 3a0a 2020  y...    Args:.  ",
            "+00014da0: 2020 2020 7265 706f 3a20 5265 706f 7369      repo: Reposi",
            "+00014db0: 746f 7279 2074 6f20 6f70 656e 0a20 2020  tory to open.   ",
            "+00014dc0: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      ",
            "+00014dd0: 7265 6d6f 7465 206e 616d 650a 2020 2020  remote name.    ",
            "+00014de0: 5261 6973 6573 3a0a 2020 2020 2020 4b65  Raises:.      Ke",
            "+00014df0: 7945 7272 6f72 3a20 6966 2074 6865 2072  yError: if the r",
            "+00014e00: 6570 6f73 6974 6f72 7920 646f 6573 206e  epository does n",
            "+00014e10: 6f74 2068 6176 6520 6120 776f 726b 696e  ot have a workin",
            "+00014e20: 6720 7472 6565 0a20 2020 2022 2222 0a20  g tree.    \"\"\". ",
            "+00014e30: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "+00014e40: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "+00014e50: 6173 2072 3a0a 2020 2020 2020 2020 6272  as r:.        br",
            "+00014e60: 616e 6368 5f6e 616d 6520 3d20 6163 7469  anch_name = acti",
            "+00014e70: 7665 5f62 7261 6e63 6828 722e 7061 7468  ve_branch(r.path",
            "+00014e80: 290a 2020 2020 2020 2020 636f 6e66 6967  ).        config",
            "+00014e90: 203d 2072 2e67 6574 5f63 6f6e 6669 6728   = r.get_config(",
            "+00014ea0: 290a 2020 2020 2020 2020 7472 793a 0a20  ).        try:. ",
            "+00014eb0: 2020 2020 2020 2020 2020 2072 656d 6f74             remot",
            "+00014ec0: 655f 6e61 6d65 203d 2063 6f6e 6669 672e  e_name = config.",
            "+00014ed0: 6765 7428 2862 2262 7261 6e63 6822 2c20  get((b\"branch\", ",
            "+00014ee0: 6272 616e 6368 5f6e 616d 6529 2c20 6222  branch_name), b\"",
            "+00014ef0: 7265 6d6f 7465 2229 0a20 2020 2020 2020  remote\").       ",
            "+00014f00: 2065 7863 6570 7420 4b65 7945 7272 6f72   except KeyError",
            "+00014f10: 3a0a 2020 2020 2020 2020 2020 2020 7265  :.            re",
            "+00014f20: 6d6f 7465 5f6e 616d 6520 3d20 6222 6f72  mote_name = b\"or",
            "+00014f30: 6967 696e 220a 2020 2020 7265 7475 726e  igin\".    return",
            "+00014f40: 2072 656d 6f74 655f 6e61 6d65 0a0a 0a64   remote_name...d",
            "+00014f50: 6566 2067 6574 5f62 7261 6e63 685f 6d65  ef get_branch_me",
            "+00014f60: 7267 6528 7265 706f 2c20 6272 616e 6368  rge(repo, branch",
            "+00014f70: 5f6e 616d 653d 4e6f 6e65 293a 0a20 2020  _name=None):.   ",
            "+00014f80: 2022 2222 5265 7475 726e 2074 6865 2062   \"\"\"Return the b",
            "+00014f90: 7261 6e63 6827 7320 6d65 7267 6520 7265  ranch's merge re",
            "+00014fa0: 6665 7265 6e63 6520 2875 7073 7472 6561  ference (upstrea",
            "+00014fb0: 6d20 6272 616e 6368 292c 2069 6620 616e  m branch), if an",
            "+00014fc0: 792e 0a0a 2020 2020 4172 6773 3a0a 2020  y...    Args:.  ",
            "+00014fd0: 2020 2020 7265 706f 3a20 5265 706f 7369      repo: Reposi",
            "+00014fe0: 746f 7279 2074 6f20 6f70 656e 0a20 2020  tory to open.   ",
            "+00014ff0: 2020 2062 7261 6e63 685f 6e61 6d65 3a20     branch_name: ",
            "+00015000: 4e61 6d65 206f 6620 7468 6520 6272 616e  Name of the bran",
            "+00015010: 6368 2028 6465 6661 756c 7473 2074 6f20  ch (defaults to ",
            "+00015020: 6163 7469 7665 2062 7261 6e63 6829 0a0a  active branch)..",
            "+00015030: 2020 2020 5265 7475 726e 733a 0a20 2020      Returns:.   ",
            "+00015040: 2020 206d 6572 6765 2072 6566 6572 656e     merge referen",
            "+00015050: 6365 206e 616d 6520 2865 2e67 2e20 6222  ce name (e.g. b\"",
            "+00015060: 7265 6673 2f68 6561 6473 2f6d 6169 6e22  refs/heads/main\"",
            "+00015070: 290a 0a20 2020 2052 6169 7365 733a 0a20  )..    Raises:. ",
            "+00015080: 2020 2020 204b 6579 4572 726f 723a 2069       KeyError: i",
            "+00015090: 6620 7468 6520 6272 616e 6368 2064 6f65  f the branch doe",
            "+000150a0: 7320 6e6f 7420 6861 7665 2061 206d 6572  s not have a mer",
            "+000150b0: 6765 2063 6f6e 6669 6775 7261 7469 6f6e  ge configuration",
            "+000150c0: 0a20 2020 2022 2222 0a20 2020 2077 6974  .    \"\"\".    wit",
            "+000150d0: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "+000150e0: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "+000150f0: 2020 2020 2020 2020 6966 2062 7261 6e63          if branc",
            "+00015100: 685f 6e61 6d65 2069 7320 4e6f 6e65 3a0a  h_name is None:.",
            "+00015110: 2020 2020 2020 2020 2020 2020 6272 616e              bran",
            "+00015120: 6368 5f6e 616d 6520 3d20 6163 7469 7665  ch_name = active",
            "+00015130: 5f62 7261 6e63 6828 722e 7061 7468 290a  _branch(r.path).",
            "+00015140: 2020 2020 2020 2020 636f 6e66 6967 203d          config =",
            "+00015150: 2072 2e67 6574 5f63 6f6e 6669 6728 290a   r.get_config().",
            "+00015160: 2020 2020 2020 2020 7265 7475 726e 2063          return c",
            "+00015170: 6f6e 6669 672e 6765 7428 2862 2262 7261  onfig.get((b\"bra",
            "+00015180: 6e63 6822 2c20 6272 616e 6368 5f6e 616d  nch\", branch_nam",
            "+00015190: 6529 2c20 6222 6d65 7267 6522 290a 0a0a  e), b\"merge\")...",
            "+000151a0: 6465 6620 7365 745f 6272 616e 6368 5f74  def set_branch_t",
            "+000151b0: 7261 636b 696e 6728 7265 706f 2c20 6272  racking(repo, br",
            "+000151c0: 616e 6368 5f6e 616d 652c 2072 656d 6f74  anch_name, remot",
            "+000151d0: 655f 6e61 6d65 2c20 7265 6d6f 7465 5f72  e_name, remote_r",
            "+000151e0: 6566 293a 0a20 2020 2022 2222 5365 7420  ef):.    \"\"\"Set ",
            "+000151f0: 7570 2062 7261 6e63 6820 7472 6163 6b69  up branch tracki",
            "+00015200: 6e67 2063 6f6e 6669 6775 7261 7469 6f6e  ng configuration",
            "+00015210: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "+00015220: 2020 2072 6570 6f3a 2052 6570 6f73 6974     repo: Reposit",
            "+00015230: 6f72 7920 746f 206f 7065 6e0a 2020 2020  ory to open.    ",
            "+00015240: 2020 6272 616e 6368 5f6e 616d 653a 204e    branch_name: N",
            "+00015250: 616d 6520 6f66 2074 6865 206c 6f63 616c  ame of the local",
            "+00015260: 2062 7261 6e63 680a 2020 2020 2020 7265   branch.      re",
            "+00015270: 6d6f 7465 5f6e 616d 653a 204e 616d 6520  mote_name: Name ",
            "+00015280: 6f66 2074 6865 2072 656d 6f74 6520 2865  of the remote (e",
            "+00015290: 2e67 2e20 6222 6f72 6967 696e 2229 0a20  .g. b\"origin\"). ",
            "+000152a0: 2020 2020 2072 656d 6f74 655f 7265 663a       remote_ref:",
            "+000152b0: 2052 656d 6f74 6520 7265 6665 7265 6e63   Remote referenc",
            "+000152c0: 6520 746f 2074 7261 636b 2028 652e 672e  e to track (e.g.",
            "+000152d0: 2062 2272 6566 732f 6865 6164 732f 6d61   b\"refs/heads/ma",
            "+000152e0: 696e 2229 0a20 2020 2022 2222 0a20 2020  in\").    \"\"\".   ",
            "+000152f0: 2077 6974 6820 6f70 656e 5f72 6570 6f5f   with open_repo_",
            "+00015300: 636c 6f73 696e 6728 7265 706f 2920 6173  closing(repo) as",
            "+00015310: 2072 3a0a 2020 2020 2020 2020 636f 6e66   r:.        conf",
            "+00015320: 6967 203d 2072 2e67 6574 5f63 6f6e 6669  ig = r.get_confi",
            "+00015330: 6728 290a 2020 2020 2020 2020 636f 6e66  g().        conf",
            "+00015340: 6967 2e73 6574 2828 6222 6272 616e 6368  ig.set((b\"branch",
            "+00015350: 222c 2062 7261 6e63 685f 6e61 6d65 292c  \", branch_name),",
            "+00015360: 2062 2272 656d 6f74 6522 2c20 7265 6d6f   b\"remote\", remo",
            "+00015370: 7465 5f6e 616d 6529 0a20 2020 2020 2020  te_name).       ",
            "+00015380: 2063 6f6e 6669 672e 7365 7428 2862 2262   config.set((b\"b",
            "+00015390: 7261 6e63 6822 2c20 6272 616e 6368 5f6e  ranch\", branch_n",
            "+000153a0: 616d 6529 2c20 6222 6d65 7267 6522 2c20  ame), b\"merge\", ",
            "+000153b0: 7265 6d6f 7465 5f72 6566 290a 2020 2020  remote_ref).    ",
            "+000153c0: 2020 2020 636f 6e66 6967 2e77 7269 7465      config.write",
            "+000153d0: 5f74 6f5f 7061 7468 2829 0a0a 0a64 6566  _to_path()...def",
            "+000153e0: 2066 6574 6368 280a 2020 2020 7265 706f   fetch(.    repo",
            "+000153f0: 2c0a 2020 2020 7265 6d6f 7465 5f6c 6f63  ,.    remote_loc",
            "+00015400: 6174 696f 6e3d 4e6f 6e65 2c0a 2020 2020  ation=None,.    ",
            "+00015410: 6f75 7473 7472 6561 6d3d 7379 732e 7374  outstream=sys.st",
            "+00015420: 646f 7574 2c0a 2020 2020 6572 7273 7472  dout,.    errstr",
            "+00015430: 6561 6d3d 6465 6661 756c 745f 6279 7465  eam=default_byte",
            "+00015440: 735f 6572 725f 7374 7265 616d 2c0a 2020  s_err_stream,.  ",
            "+00015450: 2020 6d65 7373 6167 653d 4e6f 6e65 2c0a    message=None,.",
            "+00015460: 2020 2020 6465 7074 683d 4e6f 6e65 2c0a      depth=None,.",
            "+00015470: 2020 2020 7072 756e 653d 4661 6c73 652c      prune=False,",
            "+00015480: 0a20 2020 2070 7275 6e65 5f74 6167 733d  .    prune_tags=",
            "+00015490: 4661 6c73 652c 0a20 2020 2066 6f72 6365  False,.    force",
            "+000154a0: 3d46 616c 7365 2c0a 2020 2020 2a2a 6b77  =False,.    **kw",
            "+000154b0: 6172 6773 2c0a 293a 0a20 2020 2022 2222  args,.):.    \"\"\"",
            "+000154c0: 4665 7463 6820 6f62 6a65 6374 7320 6672  Fetch objects fr",
            "+000154d0: 6f6d 2061 2072 656d 6f74 6520 7365 7276  om a remote serv",
            "+000154e0: 6572 2e0a 0a20 2020 2041 7267 733a 0a20  er...    Args:. ",
            "+000154f0: 2020 2020 2072 6570 6f3a 2050 6174 6820       repo: Path ",
            "+00015500: 746f 2074 6865 2072 6570 6f73 6974 6f72  to the repositor",
            "+00015510: 790a 2020 2020 2020 7265 6d6f 7465 5f6c  y.      remote_l",
            "+00015520: 6f63 6174 696f 6e3a 2053 7472 696e 6720  ocation: String ",
            "+00015530: 6964 656e 7469 6679 696e 6720 6120 7265  identifying a re",
            "+00015540: 6d6f 7465 2073 6572 7665 720a 2020 2020  mote server.    ",
            "+00015550: 2020 6f75 7473 7472 6561 6d3a 204f 7574    outstream: Out",
            "+00015560: 7075 7420 7374 7265 616d 2028 6465 6661  put stream (defa",
            "+00015570: 756c 7473 2074 6f20 7374 646f 7574 290a  ults to stdout).",
            "+00015580: 2020 2020 2020 6572 7273 7472 6561 6d3a        errstream:",
            "+00015590: 2045 7272 6f72 2073 7472 6561 6d20 2864   Error stream (d",
            "+000155a0: 6566 6175 6c74 7320 746f 2073 7464 6572  efaults to stder",
            "+000155b0: 7229 0a20 2020 2020 206d 6573 7361 6765  r).      message",
            "+000155c0: 3a20 5265 666c 6f67 206d 6573 7361 6765  : Reflog message",
            "+000155d0: 2028 6465 6661 756c 7473 2074 6f20 6222   (defaults to b\"",
            "+000155e0: 6665 7463 683a 2066 726f 6d20 3c72 656d  fetch: from <rem",
            "+000155f0: 6f74 655f 6e61 6d65 3e22 290a 2020 2020  ote_name>\").    ",
            "+00015600: 2020 6465 7074 683a 2044 6570 7468 2074    depth: Depth t",
            "+00015610: 6f20 6665 7463 6820 6174 0a20 2020 2020  o fetch at.     ",
            "+00015620: 2070 7275 6e65 3a20 5072 756e 6520 7265   prune: Prune re",
            "+00015630: 6d6f 7465 2072 656d 6f76 6564 2072 6566  mote removed ref",
            "+00015640: 730a 2020 2020 2020 7072 756e 655f 7461  s.      prune_ta",
            "+00015650: 6773 3a20 5072 756e 6520 7265 6f6d 7465  gs: Prune reomte",
            "+00015660: 2072 656d 6f76 6564 2074 6167 730a 2020   removed tags.  ",
            "+00015670: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     ",
            "+00015680: 2044 6963 7469 6f6e 6172 7920 7769 7468   Dictionary with",
            "+00015690: 2072 6566 7320 6f6e 2074 6865 2072 656d   refs on the rem",
            "+000156a0: 6f74 650a 2020 2020 2222 220a 2020 2020  ote.    \"\"\".    ",
            "+000156b0: 7769 7468 206f 7065 6e5f 7265 706f 5f63  with open_repo_c",
            "+000156c0: 6c6f 7369 6e67 2872 6570 6f29 2061 7320  losing(repo) as ",
            "+000156d0: 723a 0a20 2020 2020 2020 2028 7265 6d6f  r:.        (remo",
            "+000156e0: 7465 5f6e 616d 652c 2072 656d 6f74 655f  te_name, remote_",
            "+000156f0: 6c6f 6361 7469 6f6e 2920 3d20 6765 745f  location) = get_",
            "+00015700: 7265 6d6f 7465 5f72 6570 6f28 722c 2072  remote_repo(r, r",
            "+00015710: 656d 6f74 655f 6c6f 6361 7469 6f6e 290a  emote_location).",
            "+00015720: 2020 2020 2020 2020 6966 206d 6573 7361          if messa",
            "+00015730: 6765 2069 7320 4e6f 6e65 3a0a 2020 2020  ge is None:.    ",
            "+00015740: 2020 2020 2020 2020 6d65 7373 6167 6520          message ",
            "+00015750: 3d20 6222 6665 7463 683a 2066 726f 6d20  = b\"fetch: from ",
            "+00015760: 2220 2b20 7265 6d6f 7465 5f6c 6f63 6174  \" + remote_locat",
            "+00015770: 696f 6e2e 656e 636f 6465 2844 4546 4155  ion.encode(DEFAU",
            "+00015780: 4c54 5f45 4e43 4f44 494e 4729 0a20 2020  LT_ENCODING).   ",
            "+00015790: 2020 2020 2063 6c69 656e 742c 2070 6174       client, pat",
            "+000157a0: 6820 3d20 6765 745f 7472 616e 7370 6f72  h = get_transpor",
            "+000157b0: 745f 616e 645f 7061 7468 280a 2020 2020  t_and_path(.    ",
            "+000157c0: 2020 2020 2020 2020 7265 6d6f 7465 5f6c          remote_l",
            "+000157d0: 6f63 6174 696f 6e2c 2063 6f6e 6669 673d  ocation, config=",
            "+000157e0: 722e 6765 745f 636f 6e66 6967 5f73 7461  r.get_config_sta",
            "+000157f0: 636b 2829 2c20 2a2a 6b77 6172 6773 0a20  ck(), **kwargs. ",
            "+00015800: 2020 2020 2020 2029 0a20 2020 2020 2020         ).       ",
            "+00015810: 2066 6574 6368 5f72 6573 756c 7420 3d20   fetch_result = ",
            "+00015820: 636c 6965 6e74 2e66 6574 6368 2870 6174  client.fetch(pat",
            "+00015830: 682c 2072 2c20 7072 6f67 7265 7373 3d65  h, r, progress=e",
            "+00015840: 7272 7374 7265 616d 2e77 7269 7465 2c20  rrstream.write, ",
            "+00015850: 6465 7074 683d 6465 7074 6829 0a20 2020  depth=depth).   ",
            "+00015860: 2020 2020 2069 6620 7265 6d6f 7465 5f6e       if remote_n",
            "+00015870: 616d 6520 6973 206e 6f74 204e 6f6e 653a  ame is not None:",
            "+00015880: 0a20 2020 2020 2020 2020 2020 205f 696d  .            _im",
            "+00015890: 706f 7274 5f72 656d 6f74 655f 7265 6673  port_remote_refs",
            "+000158a0: 280a 2020 2020 2020 2020 2020 2020 2020  (.              ",
            "+000158b0: 2020 722e 7265 6673 2c0a 2020 2020 2020    r.refs,.      ",
            "+000158c0: 2020 2020 2020 2020 2020 7265 6d6f 7465            remote",
            "+000158d0: 5f6e 616d 652c 0a20 2020 2020 2020 2020  _name,.         ",
            "+000158e0: 2020 2020 2020 2066 6574 6368 5f72 6573         fetch_res",
            "+000158f0: 756c 742e 7265 6673 2c0a 2020 2020 2020  ult.refs,.      ",
            "+00015900: 2020 2020 2020 2020 2020 6d65 7373 6167            messag",
            "+00015910: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             ",
            "+00015920: 2020 2070 7275 6e65 3d70 7275 6e65 2c0a     prune=prune,.",
            "+00015930: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00015940: 7072 756e 655f 7461 6773 3d70 7275 6e65  prune_tags=prune",
            "+00015950: 5f74 6167 732c 0a20 2020 2020 2020 2020  _tags,.         ",
            "+00015960: 2020 2029 0a0a 2020 2020 2320 5472 6967     )..    # Trig",
            "+00015970: 6765 7220 6175 746f 2047 4320 6966 206e  ger auto GC if n",
            "+00015980: 6565 6465 640a 2020 2020 6672 6f6d 202e  eeded.    from .",
            "+00015990: 6763 2069 6d70 6f72 7420 6d61 7962 655f  gc import maybe_",
            "+000159a0: 6175 746f 5f67 630a 0a20 2020 2077 6974  auto_gc..    wit",
            "+000159b0: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "+000159c0: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "+000159d0: 2020 2020 2020 2020 6d61 7962 655f 6175          maybe_au",
            "+000159e0: 746f 5f67 6328 7229 0a0a 2020 2020 7265  to_gc(r)..    re",
            "+000159f0: 7475 726e 2066 6574 6368 5f72 6573 756c  turn fetch_resul",
            "+00015a00: 740a 0a0a 6465 6620 666f 725f 6561 6368  t...def for_each",
            "+00015a10: 5f72 6566 280a 2020 2020 7265 706f 3a20  _ref(.    repo: ",
            "+00015a20: 556e 696f 6e5b 5265 706f 2c20 7374 725d  Union[Repo, str]",
            "+00015a30: 203d 2022 2e22 2c0a 2020 2020 7061 7474   = \".\",.    patt",
            "+00015a40: 6572 6e3a 204f 7074 696f 6e61 6c5b 556e  ern: Optional[Un",
            "+00015a50: 696f 6e5b 7374 722c 2062 7974 6573 5d5d  ion[str, bytes]]",
            "+00015a60: 203d 204e 6f6e 652c 0a29 202d 3e20 6c69   = None,.) -> li",
            "+00015a70: 7374 5b74 7570 6c65 5b62 7974 6573 2c20  st[tuple[bytes, ",
            "+00015a80: 6279 7465 732c 2062 7974 6573 5d5d 3a0a  bytes, bytes]]:.",
            "+00015a90: 2020 2020 2222 2249 7465 7261 7465 206f      \"\"\"Iterate o",
            "+00015aa0: 7665 7220 616c 6c20 7265 6673 2074 6861  ver all refs tha",
            "+00015ab0: 7420 6d61 7463 6820 7468 6520 286f 7074  t match the (opt",
            "+00015ac0: 696f 6e61 6c29 2070 6174 7465 726e 2e0a  ional) pattern..",
            "+00015ad0: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "+00015ae0: 2072 6570 6f3a 2050 6174 6820 746f 2074   repo: Path to t",
            "+00015af0: 6865 2072 6570 6f73 6974 6f72 790a 2020  he repository.  ",
            "+00015b00: 2020 2020 7061 7474 6572 6e3a 204f 7074      pattern: Opt",
            "+00015b10: 696f 6e61 6c20 676c 6f62 2028 3729 2070  ional glob (7) p",
            "+00015b20: 6174 7465 726e 7320 746f 2066 696c 7465  atterns to filte",
            "+00015b30: 7220 7468 6520 7265 6673 2077 6974 680a  r the refs with.",
            "+00015b40: 2020 2020 5265 7475 726e 733a 204c 6973      Returns: Lis",
            "+00015b50: 7420 6f66 2062 7974 6573 2074 7570 6c65  t of bytes tuple",
            "+00015b60: 7320 7769 7468 3a20 2873 6861 2c20 6f62  s with: (sha, ob",
            "+00015b70: 6a65 6374 5f74 7970 652c 2072 6566 5f6e  ject_type, ref_n",
            "+00015b80: 616d 6529 0a20 2020 2022 2222 0a20 2020  ame).    \"\"\".   ",
            "+00015b90: 2069 6620 6973 696e 7374 616e 6365 2870   if isinstance(p",
            "+00015ba0: 6174 7465 726e 2c20 7374 7229 3a0a 2020  attern, str):.  ",
            "+00015bb0: 2020 2020 2020 7061 7474 6572 6e20 3d20        pattern = ",
            "+00015bc0: 6f73 2e66 7365 6e63 6f64 6528 7061 7474  os.fsencode(patt",
            "+00015bd0: 6572 6e29 0a0a 2020 2020 7769 7468 206f  ern)..    with o",
            "+00015be0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+00015bf0: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+00015c00: 2020 2020 2072 6566 7320 3d20 722e 6765       refs = r.ge",
            "+00015c10: 745f 7265 6673 2829 0a0a 2020 2020 6966  t_refs()..    if",
            "+00015c20: 2070 6174 7465 726e 3a0a 2020 2020 2020   pattern:.      ",
            "+00015c30: 2020 6d61 7463 6869 6e67 5f72 6566 733a    matching_refs:",
            "+00015c40: 2064 6963 745b 6279 7465 732c 2062 7974   dict[bytes, byt",
            "+00015c50: 6573 5d20 3d20 7b7d 0a20 2020 2020 2020  es] = {}.       ",
            "+00015c60: 2070 6174 7465 726e 5f70 6172 7473 203d   pattern_parts =",
            "+00015c70: 2070 6174 7465 726e 2e73 706c 6974 2862   pattern.split(b",
            "+00015c80: 222f 2229 0a20 2020 2020 2020 2066 6f72  \"/\").        for",
            "+00015c90: 2072 6566 2c20 7368 6120 696e 2072 6566   ref, sha in ref",
            "+00015ca0: 732e 6974 656d 7328 293a 0a20 2020 2020  s.items():.     ",
            "+00015cb0: 2020 2020 2020 206d 6174 6368 6573 203d         matches =",
            "+00015cc0: 2046 616c 7365 0a0a 2020 2020 2020 2020   False..        ",
            "+00015cd0: 2020 2020 2320 6769 7420 666f 722d 6561      # git for-ea",
            "+00015ce0: 6368 2d72 6566 2075 7365 7320 676c 6f62  ch-ref uses glob",
            "+00015cf0: 2028 3729 2073 7479 6c65 2070 6174 7465   (7) style patte",
            "+00015d00: 726e 732c 2062 7574 2066 6e6d 6174 6368  rns, but fnmatch",
            "+00015d10: 0a20 2020 2020 2020 2020 2020 2023 2069  .            # i",
            "+00015d20: 7320 6772 6565 6479 2061 6e64 2061 6c73  s greedy and als",
            "+00015d30: 6f20 6d61 7463 6865 7320 736c 6173 6865  o matches slashe",
            "+00015d40: 732c 2075 6e6c 696b 6520 676c 6f62 2e67  s, unlike glob.g",
            "+00015d50: 6c6f 622e 0a20 2020 2020 2020 2020 2020  lob..           ",
            "+00015d60: 2023 2057 6520 6861 7665 2074 6f20 6368   # We have to ch",
            "+00015d70: 6563 6b20 7061 7274 7320 6f66 2074 6865  eck parts of the",
            "+00015d80: 2070 6174 7465 726e 2069 6e64 6976 6964   pattern individ",
            "+00015d90: 7561 6c6c 792e 0a20 2020 2020 2020 2020  ually..         ",
            "+00015da0: 2020 2023 2053 6565 2068 7474 7073 3a2f     # See https:/",
            "+00015db0: 2f67 6974 6875 622e 636f 6d2f 7079 7468  /github.com/pyth",
            "+00015dc0: 6f6e 2f63 7079 7468 6f6e 2f69 7373 7565  on/cpython/issue",
            "+00015dd0: 732f 3732 3930 340a 2020 2020 2020 2020  s/72904.        ",
            "+00015de0: 2020 2020 7265 665f 7061 7274 7320 3d20      ref_parts = ",
            "+00015df0: 7265 662e 7370 6c69 7428 6222 2f22 290a  ref.split(b\"/\").",
            "+00015e00: 2020 2020 2020 2020 2020 2020 6966 206c              if l",
            "+00015e10: 656e 2872 6566 5f70 6172 7473 2920 3e20  en(ref_parts) > ",
            "+00015e20: 6c65 6e28 7061 7474 6572 6e5f 7061 7274  len(pattern_part",
            "+00015e30: 7329 3a0a 2020 2020 2020 2020 2020 2020  s):.            ",
            "+00015e40: 2020 2020 636f 6e74 696e 7565 0a0a 2020      continue..  ",
            "+00015e50: 2020 2020 2020 2020 2020 666f 7220 7061            for pa",
            "+00015e60: 742c 2072 6566 5f70 6172 7420 696e 207a  t, ref_part in z",
            "+00015e70: 6970 2870 6174 7465 726e 5f70 6172 7473  ip(pattern_parts",
            "+00015e80: 2c20 7265 665f 7061 7274 7329 3a0a 2020  , ref_parts):.  ",
            "+00015e90: 2020 2020 2020 2020 2020 2020 2020 6d61                ma",
            "+00015ea0: 7463 6865 7320 3d20 666e 6d61 7463 682e  tches = fnmatch.",
            "+00015eb0: 666e 6d61 7463 6863 6173 6528 7265 665f  fnmatchcase(ref_",
            "+00015ec0: 7061 7274 2c20 7061 7429 0a20 2020 2020  part, pat).     ",
            "+00015ed0: 2020 2020 2020 2020 2020 2069 6620 6e6f             if no",
            "+00015ee0: 7420 6d61 7463 6865 733a 0a20 2020 2020  t matches:.     ",
            "+00015ef0: 2020 2020 2020 2020 2020 2020 2020 2062                 b",
            "+00015f00: 7265 616b 0a0a 2020 2020 2020 2020 2020  reak..          ",
            "+00015f10: 2020 6966 206d 6174 6368 6573 3a0a 2020    if matches:.  ",
            "+00015f20: 2020 2020 2020 2020 2020 2020 2020 6d61                ma",
            "+00015f30: 7463 6869 6e67 5f72 6566 735b 7265 665d  tching_refs[ref]",
            "+00015f40: 203d 2073 6861 0a0a 2020 2020 2020 2020   = sha..        ",
            "+00015f50: 7265 6673 203d 206d 6174 6368 696e 675f  refs = matching_",
            "+00015f60: 7265 6673 0a0a 2020 2020 7265 743a 206c  refs..    ret: l",
            "+00015f70: 6973 745b 7475 706c 655b 6279 7465 732c  ist[tuple[bytes,",
            "+00015f80: 2062 7974 6573 2c20 6279 7465 735d 5d20   bytes, bytes]] ",
            "+00015f90: 3d20 5b0a 2020 2020 2020 2020 2873 6861  = [.        (sha",
            "+00015fa0: 2c20 722e 6765 745f 6f62 6a65 6374 2873  , r.get_object(s",
            "+00015fb0: 6861 292e 7479 7065 5f6e 616d 652c 2072  ha).type_name, r",
            "+00015fc0: 6566 290a 2020 2020 2020 2020 666f 7220  ef).        for ",
            "+00015fd0: 7265 662c 2073 6861 2069 6e20 736f 7274  ref, sha in sort",
            "+00015fe0: 6564 280a 2020 2020 2020 2020 2020 2020  ed(.            ",
            "+00015ff0: 7265 6673 2e69 7465 6d73 2829 2c0a 2020  refs.items(),.  ",
            "+00016000: 2020 2020 2020 2020 2020 6b65 793d 6c61            key=la",
            "+00016010: 6d62 6461 2072 6566 5f73 6861 3a20 7265  mbda ref_sha: re",
            "+00016020: 665f 7368 615b 305d 2c0a 2020 2020 2020  f_sha[0],.      ",
            "+00016030: 2020 290a 2020 2020 2020 2020 6966 2072    ).        if r",
            "+00016040: 6566 2021 3d20 6222 4845 4144 220a 2020  ef != b\"HEAD\".  ",
            "+00016050: 2020 5d0a 0a20 2020 2072 6574 7572 6e20    ]..    return ",
            "+00016060: 7265 740a 0a0a 6465 6620 6c73 5f72 656d  ret...def ls_rem",
            "+00016070: 6f74 6528 7265 6d6f 7465 2c20 636f 6e66  ote(remote, conf",
            "+00016080: 6967 3a20 4f70 7469 6f6e 616c 5b43 6f6e  ig: Optional[Con",
            "+00016090: 6669 675d 203d 204e 6f6e 652c 202a 2a6b  fig] = None, **k",
            "+000160a0: 7761 7267 7329 3a0a 2020 2020 2222 224c  wargs):.    \"\"\"L",
            "+000160b0: 6973 7420 7468 6520 7265 6673 2069 6e20  ist the refs in ",
            "+000160c0: 6120 7265 6d6f 7465 2e0a 0a20 2020 2041  a remote...    A",
            "+000160d0: 7267 733a 0a20 2020 2020 2072 656d 6f74  rgs:.      remot",
            "+000160e0: 653a 2052 656d 6f74 6520 7265 706f 7369  e: Remote reposi",
            "+000160f0: 746f 7279 206c 6f63 6174 696f 6e0a 2020  tory location.  ",
            "+00016100: 2020 2020 636f 6e66 6967 3a20 436f 6e66      config: Conf",
            "+00016110: 6967 7572 6174 696f 6e20 746f 2075 7365  iguration to use",
            "+00016120: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  ",
            "+00016130: 2020 2020 4c73 5265 6d6f 7465 5265 7375      LsRemoteResu",
            "+00016140: 6c74 206f 626a 6563 7420 7769 7468 2072  lt object with r",
            "+00016150: 6566 7320 616e 6420 7379 6d72 6566 730a  efs and symrefs.",
            "+00016160: 2020 2020 2222 220a 2020 2020 6966 2063      \"\"\".    if c",
            "+00016170: 6f6e 6669 6720 6973 204e 6f6e 653a 0a20  onfig is None:. ",
            "+00016180: 2020 2020 2020 2063 6f6e 6669 6720 3d20         config = ",
            "+00016190: 5374 6163 6b65 6443 6f6e 6669 672e 6465  StackedConfig.de",
            "+000161a0: 6661 756c 7428 290a 2020 2020 636c 6965  fault().    clie",
            "+000161b0: 6e74 2c20 686f 7374 5f70 6174 6820 3d20  nt, host_path = ",
            "+000161c0: 6765 745f 7472 616e 7370 6f72 745f 616e  get_transport_an",
            "+000161d0: 645f 7061 7468 2872 656d 6f74 652c 2063  d_path(remote, c",
            "+000161e0: 6f6e 6669 673d 636f 6e66 6967 2c20 2a2a  onfig=config, **",
            "+000161f0: 6b77 6172 6773 290a 2020 2020 7265 7475  kwargs).    retu",
            "+00016200: 726e 2063 6c69 656e 742e 6765 745f 7265  rn client.get_re",
            "+00016210: 6673 2868 6f73 745f 7061 7468 290a 0a0a  fs(host_path)...",
            "+00016220: 6465 6620 7265 7061 636b 2872 6570 6f29  def repack(repo)",
            "+00016230: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2222   -> None:.    \"\"",
            "+00016240: 2252 6570 6163 6b20 6c6f 6f73 6520 6669  \"Repack loose fi",
            "+00016250: 6c65 7320 696e 2061 2072 6570 6f73 6974  les in a reposit",
            "+00016260: 6f72 792e 0a0a 2020 2020 4375 7272 656e  ory...    Curren",
            "+00016270: 746c 7920 7468 6973 206f 6e6c 7920 7061  tly this only pa",
            "+00016280: 636b 7320 6c6f 6f73 6520 6f62 6a65 6374  cks loose object",
            "+00016290: 732e 0a0a 2020 2020 4172 6773 3a0a 2020  s...    Args:.  ",
            "+000162a0: 2020 2020 7265 706f 3a20 5061 7468 2074      repo: Path t",
            "+000162b0: 6f20 7468 6520 7265 706f 7369 746f 7279  o the repository",
            "+000162c0: 0a20 2020 2022 2222 0a20 2020 2077 6974  .    \"\"\".    wit",
            "+000162d0: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "+000162e0: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "+000162f0: 2020 2020 2020 2020 722e 6f62 6a65 6374          r.object",
            "+00016300: 5f73 746f 7265 2e70 6163 6b5f 6c6f 6f73  _store.pack_loos",
            "+00016310: 655f 6f62 6a65 6374 7328 290a 0a0a 6465  e_objects()...de",
            "+00016320: 6620 7061 636b 5f6f 626a 6563 7473 280a  f pack_objects(.",
            "+00016330: 2020 2020 7265 706f 2c0a 2020 2020 6f62      repo,.    ob",
            "+00016340: 6a65 6374 5f69 6473 2c0a 2020 2020 7061  ject_ids,.    pa",
            "+00016350: 636b 662c 0a20 2020 2069 6478 662c 0a20  ckf,.    idxf,. ",
            "+00016360: 2020 2064 656c 7461 5f77 696e 646f 775f     delta_window_",
            "+00016370: 7369 7a65 3d4e 6f6e 652c 0a20 2020 2064  size=None,.    d",
            "+00016380: 656c 7469 6679 3d4e 6f6e 652c 0a20 2020  eltify=None,.   ",
            "+00016390: 2072 6575 7365 5f64 656c 7461 733d 5472   reuse_deltas=Tr",
            "+000163a0: 7565 2c0a 2020 2020 7061 636b 5f69 6e64  ue,.    pack_ind",
            "+000163b0: 6578 5f76 6572 7369 6f6e 3d4e 6f6e 652c  ex_version=None,",
            "+000163c0: 0a29 202d 3e20 4e6f 6e65 3a0a 2020 2020  .) -> None:.    ",
            "+000163d0: 2222 2250 6163 6b20 6f62 6a65 6374 7320  \"\"\"Pack objects ",
            "+000163e0: 696e 746f 2061 2066 696c 652e 0a0a 2020  into a file...  ",
            "+000163f0: 2020 4172 6773 3a0a 2020 2020 2020 7265    Args:.      re",
            "+00016400: 706f 3a20 5061 7468 2074 6f20 7468 6520  po: Path to the ",
            "+00016410: 7265 706f 7369 746f 7279 0a20 2020 2020  repository.     ",
            "+00016420: 206f 626a 6563 745f 6964 733a 204c 6973   object_ids: Lis",
            "+00016430: 7420 6f66 206f 626a 6563 7420 6964 7320  t of object ids ",
            "+00016440: 746f 2077 7269 7465 0a20 2020 2020 2070  to write.      p",
            "+00016450: 6163 6b66 3a20 4669 6c65 2d6c 696b 6520  ackf: File-like ",
            "+00016460: 6f62 6a65 6374 2074 6f20 7772 6974 6520  object to write ",
            "+00016470: 746f 0a20 2020 2020 2069 6478 663a 2046  to.      idxf: F",
            "+00016480: 696c 652d 6c69 6b65 206f 626a 6563 7420  ile-like object ",
            "+00016490: 746f 2077 7269 7465 2074 6f20 2863 616e  to write to (can",
            "+000164a0: 2062 6520 4e6f 6e65 290a 2020 2020 2020   be None).      ",
            "+000164b0: 6465 6c74 615f 7769 6e64 6f77 5f73 697a  delta_window_siz",
            "+000164c0: 653a 2053 6c69 6469 6e67 2077 696e 646f  e: Sliding windo",
            "+000164d0: 7720 7369 7a65 2066 6f72 2073 6561 7263  w size for searc",
            "+000164e0: 6869 6e67 2066 6f72 2064 656c 7461 733b  hing for deltas;",
            "+000164f0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00016500: 2020 2020 2020 2020 2020 5365 7420 746f            Set to",
            "+00016510: 204e 6f6e 6520 666f 7220 6465 6661 756c   None for defaul",
            "+00016520: 7420 7769 6e64 6f77 2073 697a 652e 0a20  t window size.. ",
            "+00016530: 2020 2020 2064 656c 7469 6679 3a20 5768       deltify: Wh",
            "+00016540: 6574 6865 7220 746f 2064 656c 7469 6679  ether to deltify",
            "+00016550: 206f 626a 6563 7473 0a20 2020 2020 2072   objects.      r",
            "+00016560: 6575 7365 5f64 656c 7461 733a 2041 6c6c  euse_deltas: All",
            "+00016570: 6f77 2072 6575 7365 206f 6620 6578 6973  ow reuse of exis",
            "+00016580: 7469 6e67 2064 656c 7461 7320 7768 696c  ting deltas whil",
            "+00016590: 6520 6465 6c74 6966 7969 6e67 0a20 2020  e deltifying.   ",
            "+000165a0: 2020 2070 6163 6b5f 696e 6465 785f 7665     pack_index_ve",
            "+000165b0: 7273 696f 6e3a 2050 6163 6b20 696e 6465  rsion: Pack inde",
            "+000165c0: 7820 7665 7273 696f 6e20 746f 2075 7365  x version to use",
            "+000165d0: 2028 312c 2032 2c20 6f72 2033 292e 2049   (1, 2, or 3). I",
            "+000165e0: 6620 4e6f 6e65 2c20 7573 6573 2064 6566  f None, uses def",
            "+000165f0: 6175 6c74 2076 6572 7369 6f6e 2e0a 2020  ault version..  ",
            "+00016600: 2020 2222 220a 2020 2020 7769 7468 206f    \"\"\".    with o",
            "+00016610: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+00016620: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+00016630: 2020 2020 2065 6e74 7269 6573 2c20 6461       entries, da",
            "+00016640: 7461 5f73 756d 203d 2077 7269 7465 5f70  ta_sum = write_p",
            "+00016650: 6163 6b5f 6672 6f6d 5f63 6f6e 7461 696e  ack_from_contain",
            "+00016660: 6572 280a 2020 2020 2020 2020 2020 2020  er(.            ",
            "+00016670: 7061 636b 662e 7772 6974 652c 0a20 2020  packf.write,.   ",
            "+00016680: 2020 2020 2020 2020 2072 2e6f 626a 6563           r.objec",
            "+00016690: 745f 7374 6f72 652c 0a20 2020 2020 2020  t_store,.       ",
            "+000166a0: 2020 2020 205b 286f 6964 2c20 4e6f 6e65       [(oid, None",
            "+000166b0: 2920 666f 7220 6f69 6420 696e 206f 626a  ) for oid in obj",
            "+000166c0: 6563 745f 6964 735d 2c0a 2020 2020 2020  ect_ids],.      ",
            "+000166d0: 2020 2020 2020 6465 6c74 6966 793d 6465        deltify=de",
            "+000166e0: 6c74 6966 792c 0a20 2020 2020 2020 2020  ltify,.         ",
            "+000166f0: 2020 2064 656c 7461 5f77 696e 646f 775f     delta_window_",
            "+00016700: 7369 7a65 3d64 656c 7461 5f77 696e 646f  size=delta_windo",
            "+00016710: 775f 7369 7a65 2c0a 2020 2020 2020 2020  w_size,.        ",
            "+00016720: 2020 2020 7265 7573 655f 6465 6c74 6173      reuse_deltas",
            "+00016730: 3d72 6575 7365 5f64 656c 7461 732c 0a20  =reuse_deltas,. ",
            "+00016740: 2020 2020 2020 2029 0a20 2020 2069 6620         ).    if ",
            "+00016750: 6964 7866 2069 7320 6e6f 7420 4e6f 6e65  idxf is not None",
            "+00016760: 3a0a 2020 2020 2020 2020 656e 7472 6965  :.        entrie",
            "+00016770: 7320 3d20 736f 7274 6564 285b 286b 2c20  s = sorted([(k, ",
            "+00016780: 765b 305d 2c20 765b 315d 2920 666f 7220  v[0], v[1]) for ",
            "+00016790: 286b 2c20 7629 2069 6e20 656e 7472 6965  (k, v) in entrie",
            "+000167a0: 732e 6974 656d 7328 295d 290a 2020 2020  s.items()]).    ",
            "+000167b0: 2020 2020 7772 6974 655f 7061 636b 5f69      write_pack_i",
            "+000167c0: 6e64 6578 2869 6478 662c 2065 6e74 7269  ndex(idxf, entri",
            "+000167d0: 6573 2c20 6461 7461 5f73 756d 2c20 7665  es, data_sum, ve",
            "+000167e0: 7273 696f 6e3d 7061 636b 5f69 6e64 6578  rsion=pack_index",
            "+000167f0: 5f76 6572 7369 6f6e 290a 0a0a 6465 6620  _version)...def ",
            "+00016800: 6c73 5f74 7265 6528 0a20 2020 2072 6570  ls_tree(.    rep",
            "+00016810: 6f2c 0a20 2020 2074 7265 6569 7368 3d62  o,.    treeish=b",
            "+00016820: 2248 4541 4422 2c0a 2020 2020 6f75 7473  \"HEAD\",.    outs",
            "+00016830: 7472 6561 6d3d 7379 732e 7374 646f 7574  tream=sys.stdout",
            "+00016840: 2c0a 2020 2020 7265 6375 7273 6976 653d  ,.    recursive=",
            "+00016850: 4661 6c73 652c 0a20 2020 206e 616d 655f  False,.    name_",
            "+00016860: 6f6e 6c79 3d46 616c 7365 2c0a 2920 2d3e  only=False,.) ->",
            "+00016870: 204e 6f6e 653a 0a20 2020 2022 2222 4c69   None:.    \"\"\"Li",
            "+00016880: 7374 2063 6f6e 7465 6e74 7320 6f66 2061  st contents of a",
            "+00016890: 2074 7265 652e 0a0a 2020 2020 4172 6773   tree...    Args",
            "+000168a0: 3a0a 2020 2020 2020 7265 706f 3a20 5061  :.      repo: Pa",
            "+000168b0: 7468 2074 6f20 7468 6520 7265 706f 7369  th to the reposi",
            "+000168c0: 746f 7279 0a20 2020 2020 2074 7265 6569  tory.      treei",
            "+000168d0: 7368 3a20 5472 6565 2069 6420 746f 206c  sh: Tree id to l",
            "+000168e0: 6973 740a 2020 2020 2020 6f75 7473 7472  ist.      outstr",
            "+000168f0: 6561 6d3a 204f 7574 7075 7420 7374 7265  eam: Output stre",
            "+00016900: 616d 2028 6465 6661 756c 7473 2074 6f20  am (defaults to ",
            "+00016910: 7374 646f 7574 290a 2020 2020 2020 7265  stdout).      re",
            "+00016920: 6375 7273 6976 653a 2057 6865 7468 6572  cursive: Whether",
            "+00016930: 2074 6f20 7265 6375 7273 6976 656c 7920   to recursively ",
            "+00016940: 6c69 7374 2066 696c 6573 0a20 2020 2020  list files.     ",
            "+00016950: 206e 616d 655f 6f6e 6c79 3a20 4f6e 6c79   name_only: Only",
            "+00016960: 2070 7269 6e74 2069 7465 6d20 6e61 6d65   print item name",
            "+00016970: 0a20 2020 2022 2222 0a0a 2020 2020 6465  .    \"\"\"..    de",
            "+00016980: 6620 6c69 7374 5f74 7265 6528 7374 6f72  f list_tree(stor",
            "+00016990: 652c 2074 7265 6569 642c 2062 6173 6529  e, treeid, base)",
            "+000169a0: 202d 3e20 4e6f 6e65 3a0a 2020 2020 2020   -> None:.      ",
            "+000169b0: 2020 666f 7220 6e61 6d65 2c20 6d6f 6465    for name, mode",
            "+000169c0: 2c20 7368 6120 696e 2073 746f 7265 5b74  , sha in store[t",
            "+000169d0: 7265 6569 645d 2e69 7465 7269 7465 6d73  reeid].iteritems",
            "+000169e0: 2829 3a0a 2020 2020 2020 2020 2020 2020  ():.            ",
            "+000169f0: 6966 2062 6173 653a 0a20 2020 2020 2020  if base:.       ",
            "+00016a00: 2020 2020 2020 2020 206e 616d 6520 3d20           name = ",
            "+00016a10: 706f 7369 7870 6174 682e 6a6f 696e 2862  posixpath.join(b",
            "+00016a20: 6173 652c 206e 616d 6529 0a20 2020 2020  ase, name).     ",
            "+00016a30: 2020 2020 2020 2069 6620 6e61 6d65 5f6f         if name_o",
            "+00016a40: 6e6c 793a 0a20 2020 2020 2020 2020 2020  nly:.           ",
            "+00016a50: 2020 2020 206f 7574 7374 7265 616d 2e77       outstream.w",
            "+00016a60: 7269 7465 286e 616d 6520 2b20 6222 5c6e  rite(name + b\"\\n",
            "+00016a70: 2229 0a20 2020 2020 2020 2020 2020 2065  \").            e",
            "+00016a80: 6c73 653a 0a20 2020 2020 2020 2020 2020  lse:.           ",
            "+00016a90: 2020 2020 206f 7574 7374 7265 616d 2e77       outstream.w",
            "+00016aa0: 7269 7465 2870 7265 7474 795f 666f 726d  rite(pretty_form",
            "+00016ab0: 6174 5f74 7265 655f 656e 7472 7928 6e61  at_tree_entry(na",
            "+00016ac0: 6d65 2c20 6d6f 6465 2c20 7368 6129 290a  me, mode, sha)).",
            "+00016ad0: 2020 2020 2020 2020 2020 2020 6966 2073              if s",
            "+00016ae0: 7461 742e 535f 4953 4449 5228 6d6f 6465  tat.S_ISDIR(mode",
            "+00016af0: 2920 616e 6420 7265 6375 7273 6976 653a  ) and recursive:",
            "+00016b00: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00016b10: 206c 6973 745f 7472 6565 2873 746f 7265   list_tree(store",
            "+00016b20: 2c20 7368 612c 206e 616d 6529 0a0a 2020  , sha, name)..  ",
            "+00016b30: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+00016b40: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+00016b50: 7320 723a 0a20 2020 2020 2020 2074 7265  s r:.        tre",
            "+00016b60: 6520 3d20 7061 7273 655f 7472 6565 2872  e = parse_tree(r",
            "+00016b70: 2c20 7472 6565 6973 6829 0a20 2020 2020  , treeish).     ",
            "+00016b80: 2020 206c 6973 745f 7472 6565 2872 2e6f     list_tree(r.o",
            "+00016b90: 626a 6563 745f 7374 6f72 652c 2074 7265  bject_store, tre",
            "+00016ba0: 652e 6964 2c20 2222 290a 0a0a 6465 6620  e.id, \"\")...def ",
            "+00016bb0: 7265 6d6f 7465 5f61 6464 2872 6570 6f2c  remote_add(repo,",
            "+00016bc0: 206e 616d 653a 2055 6e69 6f6e 5b62 7974   name: Union[byt",
            "+00016bd0: 6573 2c20 7374 725d 2c20 7572 6c3a 2055  es, str], url: U",
            "+00016be0: 6e69 6f6e 5b62 7974 6573 2c20 7374 725d  nion[bytes, str]",
            "+00016bf0: 2920 2d3e 204e 6f6e 653a 0a20 2020 2022  ) -> None:.    \"",
            "+00016c00: 2222 4164 6420 6120 7265 6d6f 7465 2e0a  \"\"Add a remote..",
            "+00016c10: 0a20 2020 2041 7267 733a 0a20 2020 2020  .    Args:.     ",
            "+00016c20: 2072 6570 6f3a 2050 6174 6820 746f 2074   repo: Path to t",
            "+00016c30: 6865 2072 6570 6f73 6974 6f72 790a 2020  he repository.  ",
            "+00016c40: 2020 2020 6e61 6d65 3a20 5265 6d6f 7465      name: Remote",
            "+00016c50: 206e 616d 650a 2020 2020 2020 7572 6c3a   name.      url:",
            "+00016c60: 2052 656d 6f74 6520 5552 4c0a 2020 2020   Remote URL.    ",
            "+00016c70: 2222 220a 2020 2020 6966 206e 6f74 2069  \"\"\".    if not i",
            "+00016c80: 7369 6e73 7461 6e63 6528 6e61 6d65 2c20  sinstance(name, ",
            "+00016c90: 6279 7465 7329 3a0a 2020 2020 2020 2020  bytes):.        ",
            "+00016ca0: 6e61 6d65 203d 206e 616d 652e 656e 636f  name = name.enco",
            "+00016cb0: 6465 2844 4546 4155 4c54 5f45 4e43 4f44  de(DEFAULT_ENCOD",
            "+00016cc0: 494e 4729 0a20 2020 2069 6620 6e6f 7420  ING).    if not ",
            "+00016cd0: 6973 696e 7374 616e 6365 2875 726c 2c20  isinstance(url, ",
            "+00016ce0: 6279 7465 7329 3a0a 2020 2020 2020 2020  bytes):.        ",
            "+00016cf0: 7572 6c20 3d20 7572 6c2e 656e 636f 6465  url = url.encode",
            "+00016d00: 2844 4546 4155 4c54 5f45 4e43 4f44 494e  (DEFAULT_ENCODIN",
            "+00016d10: 4729 0a20 2020 2077 6974 6820 6f70 656e  G).    with open",
            "+00016d20: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "+00016d30: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "+00016d40: 2020 6320 3d20 722e 6765 745f 636f 6e66    c = r.get_conf",
            "+00016d50: 6967 2829 0a20 2020 2020 2020 2073 6563  ig().        sec",
            "+00016d60: 7469 6f6e 203d 2028 6222 7265 6d6f 7465  tion = (b\"remote",
            "+00016d70: 222c 206e 616d 6529 0a20 2020 2020 2020  \", name).       ",
            "+00016d80: 2069 6620 632e 6861 735f 7365 6374 696f   if c.has_sectio",
            "+00016d90: 6e28 7365 6374 696f 6e29 3a0a 2020 2020  n(section):.    ",
            "+00016da0: 2020 2020 2020 2020 7261 6973 6520 5265          raise Re",
            "+00016db0: 6d6f 7465 4578 6973 7473 2873 6563 7469  moteExists(secti",
            "+00016dc0: 6f6e 290a 2020 2020 2020 2020 632e 7365  on).        c.se",
            "+00016dd0: 7428 7365 6374 696f 6e2c 2062 2275 726c  t(section, b\"url",
            "+00016de0: 222c 2075 726c 290a 2020 2020 2020 2020  \", url).        ",
            "+00016df0: 632e 7772 6974 655f 746f 5f70 6174 6828  c.write_to_path(",
            "+00016e00: 290a 0a0a 6465 6620 7265 6d6f 7465 5f72  )...def remote_r",
            "+00016e10: 656d 6f76 6528 7265 706f 3a20 5265 706f  emove(repo: Repo",
            "+00016e20: 2c20 6e61 6d65 3a20 556e 696f 6e5b 6279  , name: Union[by",
            "+00016e30: 7465 732c 2073 7472 5d29 202d 3e20 4e6f  tes, str]) -> No",
            "+00016e40: 6e65 3a0a 2020 2020 2222 2252 656d 6f76  ne:.    \"\"\"Remov",
            "+00016e50: 6520 6120 7265 6d6f 7465 2e0a 0a20 2020  e a remote...   ",
            "+00016e60: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "+00016e70: 6f3a 2050 6174 6820 746f 2074 6865 2072  o: Path to the r",
            "+00016e80: 6570 6f73 6974 6f72 790a 2020 2020 2020  epository.      ",
            "+00016e90: 6e61 6d65 3a20 5265 6d6f 7465 206e 616d  name: Remote nam",
            "+00016ea0: 650a 2020 2020 2222 220a 2020 2020 6966  e.    \"\"\".    if",
            "+00016eb0: 206e 6f74 2069 7369 6e73 7461 6e63 6528   not isinstance(",
            "+00016ec0: 6e61 6d65 2c20 6279 7465 7329 3a0a 2020  name, bytes):.  ",
            "+00016ed0: 2020 2020 2020 6e61 6d65 203d 206e 616d        name = nam",
            "+00016ee0: 652e 656e 636f 6465 2844 4546 4155 4c54  e.encode(DEFAULT",
            "+00016ef0: 5f45 4e43 4f44 494e 4729 0a20 2020 2077  _ENCODING).    w",
            "+00016f00: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "+00016f10: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "+00016f20: 3a0a 2020 2020 2020 2020 6320 3d20 722e  :.        c = r.",
            "+00016f30: 6765 745f 636f 6e66 6967 2829 0a20 2020  get_config().   ",
            "+00016f40: 2020 2020 2073 6563 7469 6f6e 203d 2028       section = (",
            "+00016f50: 6222 7265 6d6f 7465 222c 206e 616d 6529  b\"remote\", name)",
            "+00016f60: 0a20 2020 2020 2020 2064 656c 2063 5b73  .        del c[s",
            "+00016f70: 6563 7469 6f6e 5d0a 2020 2020 2020 2020  ection].        ",
            "+00016f80: 632e 7772 6974 655f 746f 5f70 6174 6828  c.write_to_path(",
            "+00016f90: 290a 0a0a 6465 6620 5f71 756f 7465 5f70  )...def _quote_p",
            "+00016fa0: 6174 6828 7061 7468 3a20 7374 7229 202d  ath(path: str) -",
            "+00016fb0: 3e20 7374 723a 0a20 2020 2022 2222 5175  > str:.    \"\"\"Qu",
            "+00016fc0: 6f74 6520 6120 7061 7468 2075 7369 6e67  ote a path using",
            "+00016fd0: 2043 2d73 7479 6c65 2071 756f 7469 6e67   C-style quoting",
            "+00016fe0: 2073 696d 696c 6172 2074 6f20 6769 7427   similar to git'",
            "+00016ff0: 7320 636f 7265 2e71 756f 7465 5061 7468  s core.quotePath",
            "+00017000: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "+00017010: 2020 2020 2070 6174 683a 2050 6174 6820       path: Path ",
            "+00017020: 746f 2071 756f 7465 0a0a 2020 2020 5265  to quote..    Re",
            "+00017030: 7475 726e 733a 0a20 2020 2020 2020 2051  turns:.        Q",
            "+00017040: 756f 7465 6420 7061 7468 2073 7472 696e  uoted path strin",
            "+00017050: 670a 2020 2020 2222 220a 2020 2020 2320  g.    \"\"\".    # ",
            "+00017060: 4368 6563 6b20 6966 2070 6174 6820 6e65  Check if path ne",
            "+00017070: 6564 7320 7175 6f74 696e 6720 286e 6f6e  eds quoting (non",
            "+00017080: 2d41 5343 4949 206f 7220 7370 6563 6961  -ASCII or specia",
            "+00017090: 6c20 6368 6172 6163 7465 7273 290a 2020  l characters).  ",
            "+000170a0: 2020 6e65 6564 735f 7175 6f74 696e 6720    needs_quoting ",
            "+000170b0: 3d20 4661 6c73 650a 2020 2020 666f 7220  = False.    for ",
            "+000170c0: 6368 6172 2069 6e20 7061 7468 3a0a 2020  char in path:.  ",
            "+000170d0: 2020 2020 2020 6966 206f 7264 2863 6861        if ord(cha",
            "+000170e0: 7229 203e 2031 3237 206f 7220 6368 6172  r) > 127 or char",
            "+000170f0: 2069 6e20 2722 5c5c 273a 0a20 2020 2020   in '\"\\\\':.     ",
            "+00017100: 2020 2020 2020 206e 6565 6473 5f71 756f         needs_quo",
            "+00017110: 7469 6e67 203d 2054 7275 650a 2020 2020  ting = True.    ",
            "+00017120: 2020 2020 2020 2020 6272 6561 6b0a 0a20          break.. ",
            "+00017130: 2020 2069 6620 6e6f 7420 6e65 6564 735f     if not needs_",
            "+00017140: 7175 6f74 696e 673a 0a20 2020 2020 2020  quoting:.       ",
            "+00017150: 2072 6574 7572 6e20 7061 7468 0a0a 2020   return path..  ",
            "+00017160: 2020 2320 4170 706c 7920 432d 7374 796c    # Apply C-styl",
            "+00017170: 6520 7175 6f74 696e 670a 2020 2020 7175  e quoting.    qu",
            "+00017180: 6f74 6564 203d 2027 2227 0a20 2020 2066  oted = '\"'.    f",
            "+00017190: 6f72 2063 6861 7220 696e 2070 6174 683a  or char in path:",
            "+000171a0: 0a20 2020 2020 2020 2069 6620 6f72 6428  .        if ord(",
            "+000171b0: 6368 6172 2920 3e20 3132 373a 0a20 2020  char) > 127:.   ",
            "+000171c0: 2020 2020 2020 2020 2023 204e 6f6e 2d41           # Non-A",
            "+000171d0: 5343 4949 2063 6861 7261 6374 6572 2c20  SCII character, ",
            "+000171e0: 656e 636f 6465 2061 7320 6f63 7461 6c20  encode as octal ",
            "+000171f0: 6573 6361 7065 0a20 2020 2020 2020 2020  escape.         ",
            "+00017200: 2020 2075 7466 385f 6279 7465 7320 3d20     utf8_bytes = ",
            "+00017210: 6368 6172 2e65 6e63 6f64 6528 2275 7466  char.encode(\"utf",
            "+00017220: 2d38 2229 0a20 2020 2020 2020 2020 2020  -8\").           ",
            "+00017230: 2066 6f72 2062 7974 6520 696e 2075 7466   for byte in utf",
            "+00017240: 385f 6279 7465 733a 0a20 2020 2020 2020  8_bytes:.       ",
            "+00017250: 2020 2020 2020 2020 2071 756f 7465 6420           quoted ",
            "+00017260: 2b3d 2066 225c 5c7b 6279 7465 3a30 336f  += f\"\\\\{byte:03o",
            "+00017270: 7d22 0a20 2020 2020 2020 2065 6c69 6620  }\".        elif ",
            "+00017280: 6368 6172 203d 3d20 2722 273a 0a20 2020  char == '\"':.   ",
            "+00017290: 2020 2020 2020 2020 2071 756f 7465 6420           quoted ",
            "+000172a0: 2b3d 2027 5c5c 2227 0a20 2020 2020 2020  += '\\\\\"'.       ",
            "+000172b0: 2065 6c69 6620 6368 6172 203d 3d20 225c   elif char == \"\\",
            "+000172c0: 5c22 3a0a 2020 2020 2020 2020 2020 2020  \\\":.            ",
            "+000172d0: 7175 6f74 6564 202b 3d20 225c 5c5c 5c22  quoted += \"\\\\\\\\\"",
            "+000172e0: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. ",
            "+000172f0: 2020 2020 2020 2020 2020 2071 756f 7465             quote",
            "+00017300: 6420 2b3d 2063 6861 720a 2020 2020 7175  d += char.    qu",
            "+00017310: 6f74 6564 202b 3d20 2722 270a 2020 2020  oted += '\"'.    ",
            "+00017320: 7265 7475 726e 2071 756f 7465 640a 0a0a  return quoted...",
            "+00017330: 6465 6620 6368 6563 6b5f 6967 6e6f 7265  def check_ignore",
            "+00017340: 2872 6570 6f2c 2070 6174 6873 2c20 6e6f  (repo, paths, no",
            "+00017350: 5f69 6e64 6578 3d46 616c 7365 2c20 7175  _index=False, qu",
            "+00017360: 6f74 655f 7061 7468 3d54 7275 6529 3a0a  ote_path=True):.",
            "+00017370: 2020 2020 7222 2222 4465 6275 6720 6769      r\"\"\"Debug gi",
            "+00017380: 7469 676e 6f72 6520 6669 6c65 732e 0a0a  tignore files...",
            "+00017390: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+000173a0: 7265 706f 3a20 5061 7468 2074 6f20 7468  repo: Path to th",
            "+000173b0: 6520 7265 706f 7369 746f 7279 0a20 2020  e repository.   ",
            "+000173c0: 2020 2070 6174 6873 3a20 4c69 7374 206f     paths: List o",
            "+000173d0: 6620 7061 7468 7320 746f 2063 6865 636b  f paths to check",
            "+000173e0: 2066 6f72 0a20 2020 2020 206e 6f5f 696e   for.      no_in",
            "+000173f0: 6465 783a 2044 6f6e 2774 2063 6865 636b  dex: Don't check",
            "+00017400: 2069 6e64 6578 0a20 2020 2020 2071 756f   index.      quo",
            "+00017410: 7465 5f70 6174 683a 2049 6620 5472 7565  te_path: If True",
            "+00017420: 2c20 7175 6f74 6520 6e6f 6e2d 4153 4349  , quote non-ASCI",
            "+00017430: 4920 6368 6172 6163 7465 7273 2069 6e20  I characters in ",
            "+00017440: 7265 7475 726e 6564 2070 6174 6873 2075  returned paths u",
            "+00017450: 7369 6e67 0a20 2020 2020 2020 2020 2020  sing.           ",
            "+00017460: 2020 2020 2020 2043 2d73 7479 6c65 206f         C-style o",
            "+00017470: 6374 616c 2065 7363 6170 6573 2028 652e  ctal escapes (e.",
            "+00017480: 672e 2022 d182 d0b5 d181 d182 2e74 7874  g. \".........txt",
            "+00017490: 2220 6265 636f 6d65 7320 225c 5c33 3231  \" becomes \"\\\\321",
            "+000174a0: 5c5c 3230 325c 5c33 3230 5c5c 3236 355c  \\\\202\\\\320\\\\265\\",
            "+000174b0: 5c33 3231 5c5c 3230 315c 5c33 3231 5c5c  \\321\\\\201\\\\321\\\\",
            "+000174c0: 3230 322e 7478 7422 292e 0a20 2020 2020  202.txt\")..     ",
            "+000174d0: 2020 2020 2020 2020 2020 2020 2049 6620               If ",
            "+000174e0: 4661 6c73 652c 2072 6574 7572 6e20 7261  False, return ra",
            "+000174f0: 7720 756e 6963 6f64 6520 7061 7468 732e  w unicode paths.",
            "+00017500: 0a20 2020 2052 6574 7572 6e73 3a20 4c69  .    Returns: Li",
            "+00017510: 7374 206f 6620 6967 6e6f 7265 6420 6669  st of ignored fi",
            "+00017520: 6c65 730a 2020 2020 2222 220a 2020 2020  les.    \"\"\".    ",
            "+00017530: 7769 7468 206f 7065 6e5f 7265 706f 5f63  with open_repo_c",
            "+00017540: 6c6f 7369 6e67 2872 6570 6f29 2061 7320  losing(repo) as ",
            "+00017550: 723a 0a20 2020 2020 2020 2069 6e64 6578  r:.        index",
            "+00017560: 203d 2072 2e6f 7065 6e5f 696e 6465 7828   = r.open_index(",
            "+00017570: 290a 2020 2020 2020 2020 6967 6e6f 7265  ).        ignore",
            "+00017580: 5f6d 616e 6167 6572 203d 2049 676e 6f72  _manager = Ignor",
            "+00017590: 6546 696c 7465 724d 616e 6167 6572 2e66  eFilterManager.f",
            "+000175a0: 726f 6d5f 7265 706f 2872 290a 2020 2020  rom_repo(r).    ",
            "+000175b0: 2020 2020 666f 7220 6f72 6967 696e 616c      for original",
            "+000175c0: 5f70 6174 6820 696e 2070 6174 6873 3a0a  _path in paths:.",
            "+000175d0: 2020 2020 2020 2020 2020 2020 6966 206e              if n",
            "+000175e0: 6f74 206e 6f5f 696e 6465 7820 616e 6420  ot no_index and ",
            "+000175f0: 7061 7468 5f74 6f5f 7472 6565 5f70 6174  path_to_tree_pat",
            "+00017600: 6828 722e 7061 7468 2c20 6f72 6967 696e  h(r.path, origin",
            "+00017610: 616c 5f70 6174 6829 2069 6e20 696e 6465  al_path) in inde",
            "+00017620: 783a 0a20 2020 2020 2020 2020 2020 2020  x:.             ",
            "+00017630: 2020 2063 6f6e 7469 6e75 650a 0a20 2020     continue..   ",
            "+00017640: 2020 2020 2020 2020 2023 2050 7265 7365           # Prese",
            "+00017650: 7276 6520 7768 6574 6865 7220 7468 6520  rve whether the ",
            "+00017660: 6f72 6967 696e 616c 2070 6174 6820 6861  original path ha",
            "+00017670: 6420 6120 7472 6169 6c69 6e67 2073 6c61  d a trailing sla",
            "+00017680: 7368 0a20 2020 2020 2020 2020 2020 2068  sh.            h",
            "+00017690: 6164 5f74 7261 696c 696e 675f 736c 6173  ad_trailing_slas",
            "+000176a0: 6820 3d20 6f72 6967 696e 616c 5f70 6174  h = original_pat",
            "+000176b0: 682e 656e 6473 7769 7468 2828 222f 222c  h.endswith((\"/\",",
            "+000176c0: 206f 732e 7061 7468 2e73 6570 2929 0a0a   os.path.sep))..",
            "+000176d0: 2020 2020 2020 2020 2020 2020 6966 206f              if o",
            "+000176e0: 732e 7061 7468 2e69 7361 6273 286f 7269  s.path.isabs(ori",
            "+000176f0: 6769 6e61 6c5f 7061 7468 293a 0a20 2020  ginal_path):.   ",
            "+00017700: 2020 2020 2020 2020 2020 2020 2070 6174               pat",
            "+00017710: 6820 3d20 6f73 2e70 6174 682e 7265 6c70  h = os.path.relp",
            "+00017720: 6174 6828 6f72 6967 696e 616c 5f70 6174  ath(original_pat",
            "+00017730: 682c 2072 2e70 6174 6829 0a20 2020 2020  h, r.path).     ",
            "+00017740: 2020 2020 2020 2020 2020 2023 204e 6f72             # Nor",
            "+00017750: 6d61 6c69 7a65 2057 696e 646f 7773 2070  malize Windows p",
            "+00017760: 6174 6873 2074 6f20 7573 6520 666f 7277  aths to use forw",
            "+00017770: 6172 6420 736c 6173 6865 730a 2020 2020  ard slashes.    ",
            "+00017780: 2020 2020 2020 2020 2020 2020 6966 206f              if o",
            "+00017790: 732e 7061 7468 2e73 6570 2021 3d20 222f  s.path.sep != \"/",
            "+000177a0: 223a 0a20 2020 2020 2020 2020 2020 2020  \":.             ",
            "+000177b0: 2020 2020 2020 2070 6174 6820 3d20 7061         path = pa",
            "+000177c0: 7468 2e72 6570 6c61 6365 286f 732e 7061  th.replace(os.pa",
            "+000177d0: 7468 2e73 6570 2c20 222f 2229 0a20 2020  th.sep, \"/\").   ",
            "+000177e0: 2020 2020 2020 2020 2065 6c73 653a 0a20           else:. ",
            "+000177f0: 2020 2020 2020 2020 2020 2020 2020 2070                 p",
            "+00017800: 6174 6820 3d20 6f72 6967 696e 616c 5f70  ath = original_p",
            "+00017810: 6174 680a 0a20 2020 2020 2020 2020 2020  ath..           ",
            "+00017820: 2023 2052 6573 746f 7265 2074 7261 696c   # Restore trail",
            "+00017830: 696e 6720 736c 6173 6820 6966 2069 7420  ing slash if it ",
            "+00017840: 7761 7320 696e 2074 6865 206f 7269 6769  was in the origi",
            "+00017850: 6e61 6c0a 2020 2020 2020 2020 2020 2020  nal.            ",
            "+00017860: 6966 2068 6164 5f74 7261 696c 696e 675f  if had_trailing_",
            "+00017870: 736c 6173 6820 616e 6420 6e6f 7420 7061  slash and not pa",
            "+00017880: 7468 2e65 6e64 7377 6974 6828 222f 2229  th.endswith(\"/\")",
            "+00017890: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "+000178a0: 2020 7061 7468 203d 2070 6174 6820 2b20    path = path + ",
            "+000178b0: 222f 220a 0a20 2020 2020 2020 2020 2020  \"/\"..           ",
            "+000178c0: 2023 2046 6f72 2064 6972 6563 746f 7269   # For directori",
            "+000178d0: 6573 2c20 6368 6563 6b20 7769 7468 2074  es, check with t",
            "+000178e0: 7261 696c 696e 6720 736c 6173 6820 746f  railing slash to",
            "+000178f0: 2067 6574 2063 6f72 7265 6374 2069 676e   get correct ign",
            "+00017900: 6f72 6520 6265 6861 7669 6f72 0a20 2020  ore behavior.   ",
            "+00017910: 2020 2020 2020 2020 2074 6573 745f 7061           test_pa",
            "+00017920: 7468 203d 2070 6174 680a 2020 2020 2020  th = path.      ",
            "+00017930: 2020 2020 2020 7061 7468 5f77 6974 686f        path_witho",
            "+00017940: 7574 5f73 6c61 7368 203d 2070 6174 682e  ut_slash = path.",
            "+00017950: 7273 7472 6970 2822 2f22 290a 2020 2020  rstrip(\"/\").    ",
            "+00017960: 2020 2020 2020 2020 6973 5f64 6972 6563          is_direc",
            "+00017970: 746f 7279 203d 206f 732e 7061 7468 2e69  tory = os.path.i",
            "+00017980: 7364 6972 286f 732e 7061 7468 2e6a 6f69  sdir(os.path.joi",
            "+00017990: 6e28 722e 7061 7468 2c20 7061 7468 5f77  n(r.path, path_w",
            "+000179a0: 6974 686f 7574 5f73 6c61 7368 2929 0a0a  ithout_slash))..",
            "+000179b0: 2020 2020 2020 2020 2020 2020 2320 4966              # If",
            "+000179c0: 2074 6869 7320 6973 2061 2064 6972 6563   this is a direc",
            "+000179d0: 746f 7279 2070 6174 682c 2065 6e73 7572  tory path, ensur",
            "+000179e0: 6520 7765 2074 6573 7420 6974 2063 6f72  e we test it cor",
            "+000179f0: 7265 6374 6c79 0a20 2020 2020 2020 2020  rectly.         ",
            "+00017a00: 2020 2069 6620 6973 5f64 6972 6563 746f     if is_directo",
            "+00017a10: 7279 2061 6e64 206e 6f74 2070 6174 682e  ry and not path.",
            "+00017a20: 656e 6473 7769 7468 2822 2f22 293a 0a20  endswith(\"/\"):. ",
            "+00017a30: 2020 2020 2020 2020 2020 2020 2020 2074                 t",
            "+00017a40: 6573 745f 7061 7468 203d 2070 6174 6820  est_path = path ",
            "+00017a50: 2b20 222f 220a 0a20 2020 2020 2020 2020  + \"/\"..         ",
            "+00017a60: 2020 2069 6620 6967 6e6f 7265 5f6d 616e     if ignore_man",
            "+00017a70: 6167 6572 2e69 735f 6967 6e6f 7265 6428  ager.is_ignored(",
            "+00017a80: 7465 7374 5f70 6174 6829 3a0a 2020 2020  test_path):.    ",
            "+00017a90: 2020 2020 2020 2020 2020 2020 2320 5265              # Re",
            "+00017aa0: 7475 726e 2072 656c 6174 6976 6520 7061  turn relative pa",
            "+00017ab0: 7468 2028 6c69 6b65 2067 6974 2064 6f65  th (like git doe",
            "+00017ac0: 7329 2077 6865 6e20 6162 736f 6c75 7465  s) when absolute",
            "+00017ad0: 2070 6174 6820 7761 7320 7072 6f76 6964   path was provid",
            "+00017ae0: 6564 0a20 2020 2020 2020 2020 2020 2020  ed.             ",
            "+00017af0: 2020 2069 6620 6f73 2e70 6174 682e 6973     if os.path.is",
            "+00017b00: 6162 7328 6f72 6967 696e 616c 5f70 6174  abs(original_pat",
            "+00017b10: 6829 3a0a 2020 2020 2020 2020 2020 2020  h):.            ",
            "+00017b20: 2020 2020 2020 2020 6f75 7470 7574 5f70          output_p",
            "+00017b30: 6174 6820 3d20 7061 7468 0a20 2020 2020  ath = path.     ",
            "+00017b40: 2020 2020 2020 2020 2020 2065 6c73 653a             else:",
            "+00017b50: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00017b60: 2020 2020 206f 7574 7075 745f 7061 7468       output_path",
            "+00017b70: 203d 206f 7269 6769 6e61 6c5f 7061 7468   = original_path",
            "+00017b80: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00017b90: 2079 6965 6c64 205f 7175 6f74 655f 7061   yield _quote_pa",
            "+00017ba0: 7468 286f 7574 7075 745f 7061 7468 2920  th(output_path) ",
            "+00017bb0: 6966 2071 756f 7465 5f70 6174 6820 656c  if quote_path el",
            "+00017bc0: 7365 206f 7574 7075 745f 7061 7468 0a0a  se output_path..",
            "+00017bd0: 0a64 6566 2075 7064 6174 655f 6865 6164  .def update_head",
            "+00017be0: 2872 6570 6f2c 2074 6172 6765 742c 2064  (repo, target, d",
            "+00017bf0: 6574 6163 6865 643d 4661 6c73 652c 206e  etached=False, n",
            "+00017c00: 6577 5f62 7261 6e63 683d 4e6f 6e65 2920  ew_branch=None) ",
            "+00017c10: 2d3e 204e 6f6e 653a 0a20 2020 2022 2222  -> None:.    \"\"\"",
            "+00017c20: 5570 6461 7465 2048 4541 4420 746f 2070  Update HEAD to p",
            "+00017c30: 6f69 6e74 2061 7420 6120 6e65 7720 6272  oint at a new br",
            "+00017c40: 616e 6368 2f63 6f6d 6d69 742e 0a0a 2020  anch/commit...  ",
            "+00017c50: 2020 4e6f 7465 2074 6861 7420 7468 6973    Note that this",
            "+00017c60: 2064 6f65 7320 6e6f 7420 6163 7475 616c   does not actual",
            "+00017c70: 6c79 2075 7064 6174 6520 7468 6520 776f  ly update the wo",
            "+00017c80: 726b 696e 6720 7472 6565 2e0a 0a20 2020  rking tree...   ",
            "+00017c90: 2041 7267 733a 0a20 2020 2020 2072 6570   Args:.      rep",
            "+00017ca0: 6f3a 2050 6174 6820 746f 2074 6865 2072  o: Path to the r",
            "+00017cb0: 6570 6f73 6974 6f72 790a 2020 2020 2020  epository.      ",
            "+00017cc0: 6465 7461 6368 6564 3a20 4372 6561 7465  detached: Create",
            "+00017cd0: 2061 2064 6574 6163 6865 6420 6865 6164   a detached head",
            "+00017ce0: 0a20 2020 2020 2074 6172 6765 743a 2042  .      target: B",
            "+00017cf0: 7261 6e63 6820 6f72 2063 6f6d 6d69 7474  ranch or committ",
            "+00017d00: 6973 6820 746f 2073 7769 7463 6820 746f  ish to switch to",
            "+00017d10: 0a20 2020 2020 206e 6577 5f62 7261 6e63  .      new_branc",
            "+00017d20: 683a 204e 6577 2062 7261 6e63 6820 746f  h: New branch to",
            "+00017d30: 2063 7265 6174 650a 2020 2020 2222 220a   create.    \"\"\".",
            "+00017d40: 2020 2020 7769 7468 206f 7065 6e5f 7265      with open_re",
            "+00017d50: 706f 5f63 6c6f 7369 6e67 2872 6570 6f29  po_closing(repo)",
            "+00017d60: 2061 7320 723a 0a20 2020 2020 2020 2069   as r:.        i",
            "+00017d70: 6620 6e65 775f 6272 616e 6368 2069 7320  f new_branch is ",
            "+00017d80: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      ",
            "+00017d90: 2020 2020 2020 746f 5f73 6574 203d 205f        to_set = _",
            "+00017da0: 6d61 6b65 5f62 7261 6e63 685f 7265 6628  make_branch_ref(",
            "+00017db0: 6e65 775f 6272 616e 6368 290a 2020 2020  new_branch).    ",
            "+00017dc0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      ",
            "+00017dd0: 2020 2020 2020 746f 5f73 6574 203d 2062        to_set = b",
            "+00017de0: 2248 4541 4422 0a20 2020 2020 2020 2069  \"HEAD\".        i",
            "+00017df0: 6620 6465 7461 6368 6564 3a0a 2020 2020  f detached:.    ",
            "+00017e00: 2020 2020 2020 2020 2320 544f 444f 286a          # TODO(j",
            "+00017e10: 656c 6d65 7229 3a20 5072 6f76 6964 6520  elmer): Provide ",
            "+00017e20: 736f 6d65 2077 6179 2073 6f20 7468 6174  some way so that",
            "+00017e30: 2074 6865 2061 6374 7561 6c20 7265 6620   the actual ref ",
            "+00017e40: 6765 7473 0a20 2020 2020 2020 2020 2020  gets.           ",
            "+00017e50: 2023 2075 7064 6174 6564 2072 6174 6865   # updated rathe",
            "+00017e60: 7220 7468 616e 2077 6861 7420 6974 2070  r than what it p",
            "+00017e70: 6f69 6e74 7320 746f 2c20 736f 2074 6865  oints to, so the",
            "+00017e80: 2064 656c 6574 6520 6973 6e27 740a 2020   delete isn't.  ",
            "+00017e90: 2020 2020 2020 2020 2020 2320 6e65 6365            # nece",
            "+00017ea0: 7373 6172 792e 0a20 2020 2020 2020 2020  ssary..         ",
            "+00017eb0: 2020 2064 656c 2072 2e72 6566 735b 746f     del r.refs[to",
            "+00017ec0: 5f73 6574 5d0a 2020 2020 2020 2020 2020  _set].          ",
            "+00017ed0: 2020 722e 7265 6673 5b74 6f5f 7365 745d    r.refs[to_set]",
            "+00017ee0: 203d 2070 6172 7365 5f63 6f6d 6d69 7428   = parse_commit(",
            "+00017ef0: 722c 2074 6172 6765 7429 2e69 640a 2020  r, target).id.  ",
            "+00017f00: 2020 2020 2020 656c 7365 3a0a 2020 2020        else:.    ",
            "+00017f10: 2020 2020 2020 2020 722e 7265 6673 2e73          r.refs.s",
            "+00017f20: 6574 5f73 796d 626f 6c69 635f 7265 6628  et_symbolic_ref(",
            "+00017f30: 746f 5f73 6574 2c20 7061 7273 655f 7265  to_set, parse_re",
            "+00017f40: 6628 722c 2074 6172 6765 7429 290a 2020  f(r, target)).  ",
            "+00017f50: 2020 2020 2020 6966 206e 6577 5f62 7261        if new_bra",
            "+00017f60: 6e63 6820 6973 206e 6f74 204e 6f6e 653a  nch is not None:",
            "+00017f70: 0a20 2020 2020 2020 2020 2020 2072 2e72  .            r.r",
            "+00017f80: 6566 732e 7365 745f 7379 6d62 6f6c 6963  efs.set_symbolic",
            "+00017f90: 5f72 6566 2862 2248 4541 4422 2c20 746f  _ref(b\"HEAD\", to",
            "+00017fa0: 5f73 6574 290a 0a0a 6465 6620 6368 6563  _set)...def chec",
            "+00017fb0: 6b6f 7574 280a 2020 2020 7265 706f 2c0a  kout(.    repo,.",
            "+00017fc0: 2020 2020 7461 7267 6574 3a20 556e 696f      target: Unio",
            "+00017fd0: 6e5b 6279 7465 732c 2073 7472 5d2c 0a20  n[bytes, str],. ",
            "+00017fe0: 2020 2066 6f72 6365 3a20 626f 6f6c 203d     force: bool =",
            "+00017ff0: 2046 616c 7365 2c0a 2020 2020 6e65 775f   False,.    new_",
            "+00018000: 6272 616e 6368 3a20 4f70 7469 6f6e 616c  branch: Optional",
            "+00018010: 5b55 6e69 6f6e 5b62 7974 6573 2c20 7374  [Union[bytes, st",
            "+00018020: 725d 5d20 3d20 4e6f 6e65 2c0a 2920 2d3e  r]] = None,.) ->",
            "+00018030: 204e 6f6e 653a 0a20 2020 2022 2222 5377   None:.    \"\"\"Sw",
            "+00018040: 6974 6368 2074 6f20 6120 6272 616e 6368  itch to a branch",
            "+00018050: 206f 7220 636f 6d6d 6974 2c20 7570 6461   or commit, upda",
            "+00018060: 7469 6e67 2062 6f74 6820 4845 4144 2061  ting both HEAD a",
            "+00018070: 6e64 2074 6865 2077 6f72 6b69 6e67 2074  nd the working t",
            "+00018080: 7265 652e 0a0a 2020 2020 5468 6973 2069  ree...    This i",
            "+00018090: 7320 7369 6d69 6c61 7220 746f 2027 6769  s similar to 'gi",
            "+000180a0: 7420 6368 6563 6b6f 7574 272c 2061 6c6c  t checkout', all",
            "+000180b0: 6f77 696e 6720 796f 7520 746f 2073 7769  owing you to swi",
            "+000180c0: 7463 6820 746f 2061 2062 7261 6e63 682c  tch to a branch,",
            "+000180d0: 0a20 2020 2074 6167 2c20 6f72 2073 7065  .    tag, or spe",
            "+000180e0: 6369 6669 6320 636f 6d6d 6974 2e20 556e  cific commit. Un",
            "+000180f0: 6c69 6b65 2075 7064 6174 655f 6865 6164  like update_head",
            "+00018100: 2c20 7468 6973 2066 756e 6374 696f 6e20  , this function ",
            "+00018110: 616c 736f 2075 7064 6174 6573 0a20 2020  also updates.   ",
            "+00018120: 2074 6865 2077 6f72 6b69 6e67 2074 7265   the working tre",
            "+00018130: 6520 746f 206d 6174 6368 2074 6865 2074  e to match the t",
            "+00018140: 6172 6765 742e 0a0a 2020 2020 4172 6773  arget...    Args",
            "+00018150: 3a0a 2020 2020 2020 7265 706f 3a20 5061  :.      repo: Pa",
            "+00018160: 7468 2074 6f20 7265 706f 7369 746f 7279  th to repository",
            "+00018170: 206f 7220 7265 706f 7369 746f 7279 206f   or repository o",
            "+00018180: 626a 6563 740a 2020 2020 2020 7461 7267  bject.      targ",
            "+00018190: 6574 3a20 4272 616e 6368 206e 616d 652c  et: Branch name,",
            "+000181a0: 2074 6167 2c20 6f72 2063 6f6d 6d69 7420   tag, or commit ",
            "+000181b0: 5348 4120 746f 2063 6865 636b 6f75 740a  SHA to checkout.",
            "+000181c0: 2020 2020 2020 666f 7263 653a 2046 6f72        force: For",
            "+000181d0: 6365 2063 6865 636b 6f75 7420 6576 656e  ce checkout even",
            "+000181e0: 2069 6620 7468 6572 6520 6172 6520 6c6f   if there are lo",
            "+000181f0: 6361 6c20 6368 616e 6765 730a 2020 2020  cal changes.    ",
            "+00018200: 2020 6e65 775f 6272 616e 6368 3a20 4372    new_branch: Cr",
            "+00018210: 6561 7465 2061 206e 6577 2062 7261 6e63  eate a new branc",
            "+00018220: 6820 6174 2074 6172 6765 7420 286c 696b  h at target (lik",
            "+00018230: 6520 6769 7420 6368 6563 6b6f 7574 202d  e git checkout -",
            "+00018240: 6229 0a0a 2020 2020 5261 6973 6573 3a0a  b)..    Raises:.",
            "+00018250: 2020 2020 2020 4368 6563 6b6f 7574 4572        CheckoutEr",
            "+00018260: 726f 723a 2049 6620 6368 6563 6b6f 7574  ror: If checkout",
            "+00018270: 2063 616e 6e6f 7420 6265 2070 6572 666f   cannot be perfo",
            "+00018280: 726d 6564 2064 7565 2074 6f20 636f 6e66  rmed due to conf",
            "+00018290: 6c69 6374 730a 2020 2020 2020 4b65 7945  licts.      KeyE",
            "+000182a0: 7272 6f72 3a20 4966 2074 6865 2074 6172  rror: If the tar",
            "+000182b0: 6765 7420 7265 6665 7265 6e63 6520 6361  get reference ca",
            "+000182c0: 6e6e 6f74 2062 6520 666f 756e 640a 2020  nnot be found.  ",
            "+000182d0: 2020 2222 220a 2020 2020 7769 7468 206f    \"\"\".    with o",
            "+000182e0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+000182f0: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+00018300: 2020 2020 2069 6620 6973 696e 7374 616e       if isinstan",
            "+00018310: 6365 2874 6172 6765 742c 2073 7472 293a  ce(target, str):",
            "+00018320: 0a20 2020 2020 2020 2020 2020 2074 6172  .            tar",
            "+00018330: 6765 7420 3d20 7461 7267 6574 2e65 6e63  get = target.enc",
            "+00018340: 6f64 6528 4445 4641 554c 545f 454e 434f  ode(DEFAULT_ENCO",
            "+00018350: 4449 4e47 290a 2020 2020 2020 2020 6966  DING).        if",
            "+00018360: 2069 7369 6e73 7461 6e63 6528 6e65 775f   isinstance(new_",
            "+00018370: 6272 616e 6368 2c20 7374 7229 3a0a 2020  branch, str):.  ",
            "+00018380: 2020 2020 2020 2020 2020 6e65 775f 6272            new_br",
            "+00018390: 616e 6368 203d 206e 6577 5f62 7261 6e63  anch = new_branc",
            "+000183a0: 682e 656e 636f 6465 2844 4546 4155 4c54  h.encode(DEFAULT",
            "+000183b0: 5f45 4e43 4f44 494e 4729 0a0a 2020 2020  _ENCODING)..    ",
            "+000183c0: 2020 2020 2320 5061 7273 6520 7468 6520      # Parse the ",
            "+000183d0: 7461 7267 6574 2074 6f20 6765 7420 7468  target to get th",
            "+000183e0: 6520 636f 6d6d 6974 0a20 2020 2020 2020  e commit.       ",
            "+000183f0: 2074 6172 6765 745f 636f 6d6d 6974 203d   target_commit =",
            "+00018400: 2070 6172 7365 5f63 6f6d 6d69 7428 722c   parse_commit(r,",
            "+00018410: 2074 6172 6765 7429 0a20 2020 2020 2020   target).       ",
            "+00018420: 2074 6172 6765 745f 7472 6565 5f69 6420   target_tree_id ",
            "+00018430: 3d20 7461 7267 6574 5f63 6f6d 6d69 742e  = target_commit.",
            "+00018440: 7472 6565 0a0a 2020 2020 2020 2020 2320  tree..        # ",
            "+00018450: 4765 7420 6375 7272 656e 7420 4845 4144  Get current HEAD",
            "+00018460: 2074 7265 6520 666f 7220 636f 6d70 6172   tree for compar",
            "+00018470: 6973 6f6e 0a20 2020 2020 2020 2074 7279  ison.        try",
            "+00018480: 3a0a 2020 2020 2020 2020 2020 2020 6375  :.            cu",
            "+00018490: 7272 656e 745f 6865 6164 203d 2072 2e72  rrent_head = r.r",
            "+000184a0: 6566 735b 6222 4845 4144 225d 0a20 2020  efs[b\"HEAD\"].   ",
            "+000184b0: 2020 2020 2020 2020 2063 7572 7265 6e74           current",
            "+000184c0: 5f74 7265 655f 6964 203d 2072 5b63 7572  _tree_id = r[cur",
            "+000184d0: 7265 6e74 5f68 6561 645d 2e74 7265 650a  rent_head].tree.",
            "+000184e0: 2020 2020 2020 2020 6578 6365 7074 204b          except K",
            "+000184f0: 6579 4572 726f 723a 0a20 2020 2020 2020  eyError:.       ",
            "+00018500: 2020 2020 2023 204e 6f20 4845 4144 2079       # No HEAD y",
            "+00018510: 6574 2028 656d 7074 7920 7265 706f 290a  et (empty repo).",
            "+00018520: 2020 2020 2020 2020 2020 2020 6375 7272              curr",
            "+00018530: 656e 745f 7472 6565 5f69 6420 3d20 4e6f  ent_tree_id = No",
            "+00018540: 6e65 0a0a 2020 2020 2020 2020 2320 4368  ne..        # Ch",
            "+00018550: 6563 6b20 666f 7220 756e 636f 6d6d 6974  eck for uncommit",
            "+00018560: 7465 6420 6368 616e 6765 7320 6966 206e  ted changes if n",
            "+00018570: 6f74 2066 6f72 6369 6e67 0a20 2020 2020  ot forcing.     ",
            "+00018580: 2020 2069 6620 6e6f 7420 666f 7263 6520     if not force ",
            "+00018590: 616e 6420 6375 7272 656e 745f 7472 6565  and current_tree",
            "+000185a0: 5f69 6420 6973 206e 6f74 204e 6f6e 653a  _id is not None:",
            "+000185b0: 0a20 2020 2020 2020 2020 2020 2073 7461  .            sta",
            "+000185c0: 7475 735f 7265 706f 7274 203d 2073 7461  tus_report = sta",
            "+000185d0: 7475 7328 7229 0a20 2020 2020 2020 2020  tus(r).         ",
            "+000185e0: 2020 2063 6861 6e67 6573 203d 205b 5d0a     changes = [].",
            "+000185f0: 2020 2020 2020 2020 2020 2020 2320 7374              # st",
            "+00018600: 6167 6564 2069 7320 6120 6469 6374 2077  aged is a dict w",
            "+00018610: 6974 6820 2761 6464 272c 2027 6465 6c65  ith 'add', 'dele",
            "+00018620: 7465 272c 2027 6d6f 6469 6679 2720 6b65  te', 'modify' ke",
            "+00018630: 7973 0a20 2020 2020 2020 2020 2020 2069  ys.            i",
            "+00018640: 6620 6973 696e 7374 616e 6365 2873 7461  f isinstance(sta",
            "+00018650: 7475 735f 7265 706f 7274 2e73 7461 6765  tus_report.stage",
            "+00018660: 642c 2064 6963 7429 3a0a 2020 2020 2020  d, dict):.      ",
            "+00018670: 2020 2020 2020 2020 2020 6368 616e 6765            change",
            "+00018680: 732e 6578 7465 6e64 2873 7461 7475 735f  s.extend(status_",
            "+00018690: 7265 706f 7274 2e73 7461 6765 642e 6765  report.staged.ge",
            "+000186a0: 7428 2261 6464 222c 205b 5d29 290a 2020  t(\"add\", [])).  ",
            "+000186b0: 2020 2020 2020 2020 2020 2020 2020 6368                ch",
            "+000186c0: 616e 6765 732e 6578 7465 6e64 2873 7461  anges.extend(sta",
            "+000186d0: 7475 735f 7265 706f 7274 2e73 7461 6765  tus_report.stage",
            "+000186e0: 642e 6765 7428 2264 656c 6574 6522 2c20  d.get(\"delete\", ",
            "+000186f0: 5b5d 2929 0a20 2020 2020 2020 2020 2020  [])).           ",
            "+00018700: 2020 2020 2063 6861 6e67 6573 2e65 7874       changes.ext",
            "+00018710: 656e 6428 7374 6174 7573 5f72 6570 6f72  end(status_repor",
            "+00018720: 742e 7374 6167 6564 2e67 6574 2822 6d6f  t.staged.get(\"mo",
            "+00018730: 6469 6679 222c 205b 5d29 290a 2020 2020  dify\", [])).    ",
            "+00018740: 2020 2020 2020 2020 2320 756e 7374 6167          # unstag",
            "+00018750: 6564 2069 7320 6120 6c69 7374 0a20 2020  ed is a list.   ",
            "+00018760: 2020 2020 2020 2020 2063 6861 6e67 6573           changes",
            "+00018770: 2e65 7874 656e 6428 7374 6174 7573 5f72  .extend(status_r",
            "+00018780: 6570 6f72 742e 756e 7374 6167 6564 290a  eport.unstaged).",
            "+00018790: 2020 2020 2020 2020 2020 2020 6966 2063              if c",
            "+000187a0: 6861 6e67 6573 3a0a 2020 2020 2020 2020  hanges:.        ",
            "+000187b0: 2020 2020 2020 2020 2320 4368 6563 6b20          # Check ",
            "+000187c0: 6966 2061 6e79 2063 6861 6e67 6573 2077  if any changes w",
            "+000187d0: 6f75 6c64 2063 6f6e 666c 6963 7420 7769  ould conflict wi",
            "+000187e0: 7468 2063 6865 636b 6f75 740a 2020 2020  th checkout.    ",
            "+000187f0: 2020 2020 2020 2020 2020 2020 7461 7267              targ",
            "+00018800: 6574 5f74 7265 6520 3d20 725b 7461 7267  et_tree = r[targ",
            "+00018810: 6574 5f74 7265 655f 6964 5d0a 2020 2020  et_tree_id].    ",
            "+00018820: 2020 2020 2020 2020 2020 2020 666f 7220              for ",
            "+00018830: 6368 616e 6765 2069 6e20 6368 616e 6765  change in change",
            "+00018840: 733a 0a20 2020 2020 2020 2020 2020 2020  s:.             ",
            "+00018850: 2020 2020 2020 2069 6620 6973 696e 7374         if isinst",
            "+00018860: 616e 6365 2863 6861 6e67 652c 2073 7472  ance(change, str",
            "+00018870: 293a 0a20 2020 2020 2020 2020 2020 2020  ):.             ",
            "+00018880: 2020 2020 2020 2020 2020 2063 6861 6e67             chang",
            "+00018890: 6520 3d20 6368 616e 6765 2e65 6e63 6f64  e = change.encod",
            "+000188a0: 6528 4445 4641 554c 545f 454e 434f 4449  e(DEFAULT_ENCODI",
            "+000188b0: 4e47 290a 0a20 2020 2020 2020 2020 2020  NG)..           ",
            "+000188c0: 2020 2020 2020 2020 2074 7279 3a0a 2020           try:.  ",
            "+000188d0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000188e0: 2020 2020 2020 7461 7267 6574 5f74 7265        target_tre",
            "+000188f0: 652e 6c6f 6f6b 7570 5f70 6174 6828 722e  e.lookup_path(r.",
            "+00018900: 6f62 6a65 6374 5f73 746f 7265 2e5f 5f67  object_store.__g",
            "+00018910: 6574 6974 656d 5f5f 2c20 6368 616e 6765  etitem__, change",
            "+00018920: 290a 2020 2020 2020 2020 2020 2020 2020  ).              ",
            "+00018930: 2020 2020 2020 2020 2020 2320 4669 6c65            # File",
            "+00018940: 2065 7869 7374 7320 696e 2074 6172 6765   exists in targe",
            "+00018950: 7420 7472 6565 202d 2077 6f75 6c64 206f  t tree - would o",
            "+00018960: 7665 7277 7269 7465 206c 6f63 616c 2063  verwrite local c",
            "+00018970: 6861 6e67 6573 0a20 2020 2020 2020 2020  hanges.         ",
            "+00018980: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+00018990: 6169 7365 2043 6865 636b 6f75 7445 7272  aise CheckoutErr",
            "+000189a0: 6f72 280a 2020 2020 2020 2020 2020 2020  or(.            ",
            "+000189b0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000189c0: 6622 596f 7572 206c 6f63 616c 2063 6861  f\"Your local cha",
            "+000189d0: 6e67 6573 2074 6f20 277b 6368 616e 6765  nges to '{change",
            "+000189e0: 2e64 6563 6f64 6528 297d 2720 776f 756c  .decode()}' woul",
            "+000189f0: 6420 6265 2022 0a20 2020 2020 2020 2020  d be \".         ",
            "+00018a00: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00018a10: 2020 2022 6f76 6572 7772 6974 7465 6e20     \"overwritten ",
            "+00018a20: 6279 2063 6865 636b 6f75 742e 2050 6c65  by checkout. Ple",
            "+00018a30: 6173 6520 636f 6d6d 6974 206f 7220 7374  ase commit or st",
            "+00018a40: 6173 6820 6265 666f 7265 2073 7769 7463  ash before switc",
            "+00018a50: 6869 6e67 2e22 0a20 2020 2020 2020 2020  hing.\".         ",
            "+00018a60: 2020 2020 2020 2020 2020 2020 2020 2029                 )",
            "+00018a70: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00018a80: 2020 2020 2065 7863 6570 7420 4b65 7945       except KeyE",
            "+00018a90: 7272 6f72 3a0a 2020 2020 2020 2020 2020  rror:.          ",
            "+00018aa0: 2020 2020 2020 2020 2020 2020 2020 2320                # ",
            "+00018ab0: 4669 6c65 2064 6f65 736e 2774 2065 7869  File doesn't exi",
            "+00018ac0: 7374 2069 6e20 7461 7267 6574 2074 7265  st in target tre",
            "+00018ad0: 6520 2d20 6368 616e 6765 2063 616e 2062  e - change can b",
            "+00018ae0: 6520 7072 6573 6572 7665 640a 2020 2020  e preserved.    ",
            "+00018af0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00018b00: 2020 2020 7061 7373 0a0a 2020 2020 2020      pass..      ",
            "+00018b10: 2020 2320 4765 7420 636f 6e66 6967 7572    # Get configur",
            "+00018b20: 6174 696f 6e20 666f 7220 776f 726b 696e  ation for workin",
            "+00018b30: 6720 6469 7265 6374 6f72 7920 7570 6461  g directory upda",
            "+00018b40: 7465 0a20 2020 2020 2020 2063 6f6e 6669  te.        confi",
            "+00018b50: 6720 3d20 722e 6765 745f 636f 6e66 6967  g = r.get_config",
            "+00018b60: 2829 0a20 2020 2020 2020 2068 6f6e 6f72  ().        honor",
            "+00018b70: 5f66 696c 656d 6f64 6520 3d20 636f 6e66  _filemode = conf",
            "+00018b80: 6967 2e67 6574 5f62 6f6f 6c65 616e 2862  ig.get_boolean(b",
            "+00018b90: 2263 6f72 6522 2c20 6222 6669 6c65 6d6f  \"core\", b\"filemo",
            "+00018ba0: 6465 222c 206f 732e 6e61 6d65 2021 3d20  de\", os.name != ",
            "+00018bb0: 226e 7422 290a 0a20 2020 2020 2020 2023  \"nt\")..        #",
            "+00018bc0: 2049 6d70 6f72 7420 7661 6c69 6461 7469   Import validati",
            "+00018bd0: 6f6e 2066 756e 6374 696f 6e73 0a20 2020  on functions.   ",
            "+00018be0: 2020 2020 2066 726f 6d20 2e69 6e64 6578       from .index",
            "+00018bf0: 2069 6d70 6f72 7420 7661 6c69 6461 7465   import validate",
            "+00018c00: 5f70 6174 685f 656c 656d 656e 745f 6465  _path_element_de",
            "+00018c10: 6661 756c 742c 2076 616c 6964 6174 655f  fault, validate_",
            "+00018c20: 7061 7468 5f65 6c65 6d65 6e74 5f6e 7466  path_element_ntf",
            "+00018c30: 730a 0a20 2020 2020 2020 2069 6620 636f  s..        if co",
            "+00018c40: 6e66 6967 2e67 6574 5f62 6f6f 6c65 616e  nfig.get_boolean",
            "+00018c50: 2862 2263 6f72 6522 2c20 6222 636f 7265  (b\"core\", b\"core",
            "+00018c60: 2e70 726f 7465 6374 4e54 4653 222c 206f  .protectNTFS\", o",
            "+00018c70: 732e 6e61 6d65 203d 3d20 226e 7422 293a  s.name == \"nt\"):",
            "+00018c80: 0a20 2020 2020 2020 2020 2020 2076 616c  .            val",
            "+00018c90: 6964 6174 655f 7061 7468 5f65 6c65 6d65  idate_path_eleme",
            "+00018ca0: 6e74 203d 2076 616c 6964 6174 655f 7061  nt = validate_pa",
            "+00018cb0: 7468 5f65 6c65 6d65 6e74 5f6e 7466 730a  th_element_ntfs.",
            "+00018cc0: 2020 2020 2020 2020 656c 7365 3a0a 2020          else:.  ",
            "+00018cd0: 2020 2020 2020 2020 2020 7661 6c69 6461            valida",
            "+00018ce0: 7465 5f70 6174 685f 656c 656d 656e 7420  te_path_element ",
            "+00018cf0: 3d20 7661 6c69 6461 7465 5f70 6174 685f  = validate_path_",
            "+00018d00: 656c 656d 656e 745f 6465 6661 756c 740a  element_default.",
            "+00018d10: 0a20 2020 2020 2020 2069 6620 636f 6e66  .        if conf",
            "+00018d20: 6967 2e67 6574 5f62 6f6f 6c65 616e 2862  ig.get_boolean(b",
            "+00018d30: 2263 6f72 6522 2c20 6222 7379 6d6c 696e  \"core\", b\"symlin",
            "+00018d40: 6b73 222c 2054 7275 6529 3a0a 2020 2020  ks\", True):.    ",
            "+00018d50: 2020 2020 2020 2020 2320 496d 706f 7274          # Import",
            "+00018d60: 2073 796d 6c69 6e6b 2066 756e 6374 696f   symlink functio",
            "+00018d70: 6e0a 2020 2020 2020 2020 2020 2020 6672  n.            fr",
            "+00018d80: 6f6d 202e 696e 6465 7820 696d 706f 7274  om .index import",
            "+00018d90: 2073 796d 6c69 6e6b 0a0a 2020 2020 2020   symlink..      ",
            "+00018da0: 2020 2020 2020 7379 6d6c 696e 6b5f 666e        symlink_fn",
            "+00018db0: 203d 2073 796d 6c69 6e6b 0a20 2020 2020   = symlink.     ",
            "+00018dc0: 2020 2065 6c73 653a 0a0a 2020 2020 2020     else:..      ",
            "+00018dd0: 2020 2020 2020 6465 6620 7379 6d6c 696e        def symlin",
            "+00018de0: 6b5f 666e 2873 6f75 7263 652c 2074 6172  k_fn(source, tar",
            "+00018df0: 6765 7429 202d 3e20 4e6f 6e65 3a20 2023  get) -> None:  #",
            "+00018e00: 2074 7970 653a 2069 676e 6f72 650a 2020   type: ignore.  ",
            "+00018e10: 2020 2020 2020 2020 2020 2020 2020 6d6f                mo",
            "+00018e20: 6465 203d 2022 7722 202b 2028 2262 2220  de = \"w\" + (\"b\" ",
            "+00018e30: 6966 2069 7369 6e73 7461 6e63 6528 736f  if isinstance(so",
            "+00018e40: 7572 6365 2c20 6279 7465 7329 2065 6c73  urce, bytes) els",
            "+00018e50: 6520 2222 290a 2020 2020 2020 2020 2020  e \"\").          ",
            "+00018e60: 2020 2020 2020 7769 7468 206f 7065 6e28        with open(",
            "+00018e70: 7461 7267 6574 2c20 6d6f 6465 2920 6173  target, mode) as",
            "+00018e80: 2066 3a0a 2020 2020 2020 2020 2020 2020   f:.            ",
            "+00018e90: 2020 2020 2020 2020 662e 7772 6974 6528          f.write(",
            "+00018ea0: 736f 7572 6365 290a 0a20 2020 2020 2020  source)..       ",
            "+00018eb0: 2023 2047 6574 2062 6c6f 6220 6e6f 726d   # Get blob norm",
            "+00018ec0: 616c 697a 6572 2066 6f72 206c 696e 6520  alizer for line ",
            "+00018ed0: 656e 6469 6e67 2063 6f6e 7665 7273 696f  ending conversio",
            "+00018ee0: 6e0a 2020 2020 2020 2020 626c 6f62 5f6e  n.        blob_n",
            "+00018ef0: 6f72 6d61 6c69 7a65 7220 3d20 722e 6765  ormalizer = r.ge",
            "+00018f00: 745f 626c 6f62 5f6e 6f72 6d61 6c69 7a65  t_blob_normalize",
            "+00018f10: 7228 290a 0a20 2020 2020 2020 2023 2055  r()..        # U",
            "+00018f20: 7064 6174 6520 776f 726b 696e 6720 7472  pdate working tr",
            "+00018f30: 6565 0a20 2020 2020 2020 2075 7064 6174  ee.        updat",
            "+00018f40: 655f 776f 726b 696e 675f 7472 6565 280a  e_working_tree(.",
            "+00018f50: 2020 2020 2020 2020 2020 2020 722c 0a20              r,. ",
            "+00018f60: 2020 2020 2020 2020 2020 2063 7572 7265             curre",
            "+00018f70: 6e74 5f74 7265 655f 6964 2c0a 2020 2020  nt_tree_id,.    ",
            "+00018f80: 2020 2020 2020 2020 7461 7267 6574 5f74          target_t",
            "+00018f90: 7265 655f 6964 2c0a 2020 2020 2020 2020  ree_id,.        ",
            "+00018fa0: 2020 2020 686f 6e6f 725f 6669 6c65 6d6f      honor_filemo",
            "+00018fb0: 6465 3d68 6f6e 6f72 5f66 696c 656d 6f64  de=honor_filemod",
            "+00018fc0: 652c 0a20 2020 2020 2020 2020 2020 2076  e,.            v",
            "+00018fd0: 616c 6964 6174 655f 7061 7468 5f65 6c65  alidate_path_ele",
            "+00018fe0: 6d65 6e74 3d76 616c 6964 6174 655f 7061  ment=validate_pa",
            "+00018ff0: 7468 5f65 6c65 6d65 6e74 2c0a 2020 2020  th_element,.    ",
            "+00019000: 2020 2020 2020 2020 7379 6d6c 696e 6b5f          symlink_",
            "+00019010: 666e 3d73 796d 6c69 6e6b 5f66 6e2c 0a20  fn=symlink_fn,. ",
            "+00019020: 2020 2020 2020 2020 2020 2066 6f72 6365             force",
            "+00019030: 5f72 656d 6f76 655f 756e 7472 6163 6b65  _remove_untracke",
            "+00019040: 643d 666f 7263 652c 0a20 2020 2020 2020  d=force,.       ",
            "+00019050: 2020 2020 2062 6c6f 625f 6e6f 726d 616c       blob_normal",
            "+00019060: 697a 6572 3d62 6c6f 625f 6e6f 726d 616c  izer=blob_normal",
            "+00019070: 697a 6572 2c0a 2020 2020 2020 2020 290a  izer,.        ).",
            "+00019080: 0a20 2020 2020 2020 2023 2055 7064 6174  .        # Updat",
            "+00019090: 6520 4845 4144 0a20 2020 2020 2020 2069  e HEAD.        i",
            "+000190a0: 6620 6e65 775f 6272 616e 6368 3a0a 2020  f new_branch:.  ",
            "+000190b0: 2020 2020 2020 2020 2020 2320 4372 6561            # Crea",
            "+000190c0: 7465 206e 6577 2062 7261 6e63 6820 616e  te new branch an",
            "+000190d0: 6420 7377 6974 6368 2074 6f20 6974 0a20  d switch to it. ",
            "+000190e0: 2020 2020 2020 2020 2020 2062 7261 6e63             branc",
            "+000190f0: 685f 6372 6561 7465 2872 2c20 6e65 775f  h_create(r, new_",
            "+00019100: 6272 616e 6368 2c20 6f62 6a65 6374 6973  branch, objectis",
            "+00019110: 683d 7461 7267 6574 5f63 6f6d 6d69 742e  h=target_commit.",
            "+00019120: 6964 2e64 6563 6f64 6528 2261 7363 6969  id.decode(\"ascii",
            "+00019130: 2229 290a 2020 2020 2020 2020 2020 2020  \")).            ",
            "+00019140: 7570 6461 7465 5f68 6561 6428 722c 206e  update_head(r, n",
            "+00019150: 6577 5f62 7261 6e63 6829 0a0a 2020 2020  ew_branch)..    ",
            "+00019160: 2020 2020 2020 2020 2320 5365 7420 7570          # Set up",
            "+00019170: 2074 7261 636b 696e 6720 6966 2063 7265   tracking if cre",
            "+00019180: 6174 696e 6720 6672 6f6d 2061 2072 656d  ating from a rem",
            "+00019190: 6f74 6520 6272 616e 6368 0a20 2020 2020  ote branch.     ",
            "+000191a0: 2020 2020 2020 2066 726f 6d20 2e72 6566         from .ref",
            "+000191b0: 7320 696d 706f 7274 204c 4f43 414c 5f52  s import LOCAL_R",
            "+000191c0: 454d 4f54 455f 5052 4546 4958 2c20 7061  EMOTE_PREFIX, pa",
            "+000191d0: 7273 655f 7265 6d6f 7465 5f72 6566 0a0a  rse_remote_ref..",
            "+000191e0: 2020 2020 2020 2020 2020 2020 6966 2074              if t",
            "+000191f0: 6172 6765 742e 7374 6172 7473 7769 7468  arget.startswith",
            "+00019200: 284c 4f43 414c 5f52 454d 4f54 455f 5052  (LOCAL_REMOTE_PR",
            "+00019210: 4546 4958 293a 0a20 2020 2020 2020 2020  EFIX):.         ",
            "+00019220: 2020 2020 2020 2074 7279 3a0a 2020 2020         try:.    ",
            "+00019230: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00019240: 7265 6d6f 7465 5f6e 616d 652c 2062 7261  remote_name, bra",
            "+00019250: 6e63 685f 6e61 6d65 203d 2070 6172 7365  nch_name = parse",
            "+00019260: 5f72 656d 6f74 655f 7265 6628 7461 7267  _remote_ref(targ",
            "+00019270: 6574 290a 2020 2020 2020 2020 2020 2020  et).            ",
            "+00019280: 2020 2020 2020 2020 2320 5365 7420 7472          # Set tr",
            "+00019290: 6163 6b69 6e67 2074 6f20 7265 6673 2f68  acking to refs/h",
            "+000192a0: 6561 6473 2f3c 6272 616e 6368 3e20 6f6e  eads/<branch> on",
            "+000192b0: 2074 6865 2072 656d 6f74 650a 2020 2020   the remote.    ",
            "+000192c0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000192d0: 7365 745f 6272 616e 6368 5f74 7261 636b  set_branch_track",
            "+000192e0: 696e 6728 0a20 2020 2020 2020 2020 2020  ing(.           ",
            "+000192f0: 2020 2020 2020 2020 2020 2020 2072 2c20               r, ",
            "+00019300: 6e65 775f 6272 616e 6368 2c20 7265 6d6f  new_branch, remo",
            "+00019310: 7465 5f6e 616d 652c 2062 2272 6566 732f  te_name, b\"refs/",
            "+00019320: 6865 6164 732f 2220 2b20 6272 616e 6368  heads/\" + branch",
            "+00019330: 5f6e 616d 650a 2020 2020 2020 2020 2020  _name.          ",
            "+00019340: 2020 2020 2020 2020 2020 290a 2020 2020            ).    ",
            "+00019350: 2020 2020 2020 2020 2020 2020 6578 6365              exce",
            "+00019360: 7074 2056 616c 7565 4572 726f 723a 0a20  pt ValueError:. ",
            "+00019370: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00019380: 2020 2023 2049 6e76 616c 6964 2072 656d     # Invalid rem",
            "+00019390: 6f74 6520 7265 6620 666f 726d 6174 2c20  ote ref format, ",
            "+000193a0: 736b 6970 2074 7261 636b 696e 6720 7365  skip tracking se",
            "+000193b0: 7475 700a 2020 2020 2020 2020 2020 2020  tup.            ",
            "+000193c0: 2020 2020 2020 2020 7061 7373 0a20 2020          pass.   ",
            "+000193d0: 2020 2020 2065 6c73 653a 0a20 2020 2020       else:.     ",
            "+000193e0: 2020 2020 2020 2023 2043 6865 636b 2069         # Check i",
            "+000193f0: 6620 7461 7267 6574 2069 7320 6120 6272  f target is a br",
            "+00019400: 616e 6368 206e 616d 6520 2877 6974 6820  anch name (with ",
            "+00019410: 6f72 2077 6974 686f 7574 2072 6566 732f  or without refs/",
            "+00019420: 6865 6164 732f 2070 7265 6669 7829 0a20  heads/ prefix). ",
            "+00019430: 2020 2020 2020 2020 2020 2062 7261 6e63             branc",
            "+00019440: 685f 7265 6620 3d20 4e6f 6e65 0a20 2020  h_ref = None.   ",
            "+00019450: 2020 2020 2020 2020 2069 6620 7461 7267           if targ",
            "+00019460: 6574 2069 6e20 722e 7265 6673 2e6b 6579  et in r.refs.key",
            "+00019470: 7328 293a 0a20 2020 2020 2020 2020 2020  s():.           ",
            "+00019480: 2020 2020 2069 6620 7461 7267 6574 2e73       if target.s",
            "+00019490: 7461 7274 7377 6974 6828 4c4f 4341 4c5f  tartswith(LOCAL_",
            "+000194a0: 4252 414e 4348 5f50 5245 4649 5829 3a0a  BRANCH_PREFIX):.",
            "+000194b0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000194c0: 2020 2020 6272 616e 6368 5f72 6566 203d      branch_ref =",
            "+000194d0: 2074 6172 6765 740a 2020 2020 2020 2020   target.        ",
            "+000194e0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      ",
            "+000194f0: 2020 2020 2020 2020 2020 2320 5472 7920            # Try ",
            "+00019500: 6164 6469 6e67 2072 6566 732f 6865 6164  adding refs/head",
            "+00019510: 732f 2070 7265 6669 780a 2020 2020 2020  s/ prefix.      ",
            "+00019520: 2020 2020 2020 2020 2020 706f 7465 6e74            potent",
            "+00019530: 6961 6c5f 6272 616e 6368 203d 205f 6d61  ial_branch = _ma",
            "+00019540: 6b65 5f62 7261 6e63 685f 7265 6628 7461  ke_branch_ref(ta",
            "+00019550: 7267 6574 290a 2020 2020 2020 2020 2020  rget).          ",
            "+00019560: 2020 2020 2020 6966 2070 6f74 656e 7469        if potenti",
            "+00019570: 616c 5f62 7261 6e63 6820 696e 2072 2e72  al_branch in r.r",
            "+00019580: 6566 732e 6b65 7973 2829 3a0a 2020 2020  efs.keys():.    ",
            "+00019590: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000195a0: 6272 616e 6368 5f72 6566 203d 2070 6f74  branch_ref = pot",
            "+000195b0: 656e 7469 616c 5f62 7261 6e63 680a 0a20  ential_branch.. ",
            "+000195c0: 2020 2020 2020 2020 2020 2069 6620 6272             if br",
            "+000195d0: 616e 6368 5f72 6566 3a0a 2020 2020 2020  anch_ref:.      ",
            "+000195e0: 2020 2020 2020 2020 2020 2320 4974 2773            # It's",
            "+000195f0: 2061 2062 7261 6e63 6820 2d20 7570 6461   a branch - upda",
            "+00019600: 7465 2048 4541 4420 7379 6d62 6f6c 6963  te HEAD symbolic",
            "+00019610: 616c 6c79 0a20 2020 2020 2020 2020 2020  ally.           ",
            "+00019620: 2020 2020 2075 7064 6174 655f 6865 6164       update_head",
            "+00019630: 2872 2c20 6272 616e 6368 5f72 6566 290a  (r, branch_ref).",
            "+00019640: 2020 2020 2020 2020 2020 2020 656c 7365              else",
            "+00019650: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "+00019660: 2020 2320 4974 2773 2061 2074 6167 2c20    # It's a tag, ",
            "+00019670: 6f74 6865 7220 7265 662c 206f 7220 636f  other ref, or co",
            "+00019680: 6d6d 6974 2053 4841 202d 2064 6574 6163  mmit SHA - detac",
            "+00019690: 6865 6420 4845 4144 0a20 2020 2020 2020  hed HEAD.       ",
            "+000196a0: 2020 2020 2020 2020 2075 7064 6174 655f           update_",
            "+000196b0: 6865 6164 2872 2c20 7461 7267 6574 5f63  head(r, target_c",
            "+000196c0: 6f6d 6d69 742e 6964 2e64 6563 6f64 6528  ommit.id.decode(",
            "+000196d0: 2261 7363 6969 2229 2c20 6465 7461 6368  \"ascii\"), detach",
            "+000196e0: 6564 3d54 7275 6529 0a0a 0a64 6566 2072  ed=True)...def r",
            "+000196f0: 6573 6574 5f66 696c 6528 7265 706f 2c20  eset_file(repo, ",
            "+00019700: 6669 6c65 5f70 6174 683a 2073 7472 2c20  file_path: str, ",
            "+00019710: 7461 7267 6574 3a20 6279 7465 7320 3d20  target: bytes = ",
            "+00019720: 6222 4845 4144 222c 2073 796d 6c69 6e6b  b\"HEAD\", symlink",
            "+00019730: 5f66 6e3d 4e6f 6e65 2920 2d3e 204e 6f6e  _fn=None) -> Non",
            "+00019740: 653a 0a20 2020 2022 2222 5265 7365 7420  e:.    \"\"\"Reset ",
            "+00019750: 7468 6520 6669 6c65 2074 6f20 7370 6563  the file to spec",
            "+00019760: 6966 6963 2063 6f6d 6d69 7420 6f72 2062  ific commit or b",
            "+00019770: 7261 6e63 682e 0a0a 2020 2020 4172 6773  ranch...    Args",
            "+00019780: 3a0a 2020 2020 2020 7265 706f 3a20 6475  :.      repo: du",
            "+00019790: 6c77 6963 6820 5265 706f 206f 626a 6563  lwich Repo objec",
            "+000197a0: 740a 2020 2020 2020 6669 6c65 5f70 6174  t.      file_pat",
            "+000197b0: 683a 2066 696c 6520 746f 2072 6573 6574  h: file to reset",
            "+000197c0: 2c20 7265 6c61 7469 7665 2074 6f20 7468  , relative to th",
            "+000197d0: 6520 7265 706f 7369 746f 7279 2070 6174  e repository pat",
            "+000197e0: 680a 2020 2020 2020 7461 7267 6574 3a20  h.      target: ",
            "+000197f0: 6272 616e 6368 206f 7220 636f 6d6d 6974  branch or commit",
            "+00019800: 206f 7220 6227 4845 4144 2720 746f 2072   or b'HEAD' to r",
            "+00019810: 6573 6574 0a20 2020 2022 2222 0a20 2020  eset.    \"\"\".   ",
            "+00019820: 2074 7265 6520 3d20 7061 7273 655f 7472   tree = parse_tr",
            "+00019830: 6565 2872 6570 6f2c 2074 7265 6569 7368  ee(repo, treeish",
            "+00019840: 3d74 6172 6765 7429 0a20 2020 2074 7265  =target).    tre",
            "+00019850: 655f 7061 7468 203d 205f 6673 5f74 6f5f  e_path = _fs_to_",
            "+00019860: 7472 6565 5f70 6174 6828 6669 6c65 5f70  tree_path(file_p",
            "+00019870: 6174 6829 0a0a 2020 2020 6669 6c65 5f65  ath)..    file_e",
            "+00019880: 6e74 7279 203d 2074 7265 652e 6c6f 6f6b  ntry = tree.look",
            "+00019890: 7570 5f70 6174 6828 7265 706f 2e6f 626a  up_path(repo.obj",
            "+000198a0: 6563 745f 7374 6f72 652e 5f5f 6765 7469  ect_store.__geti",
            "+000198b0: 7465 6d5f 5f2c 2074 7265 655f 7061 7468  tem__, tree_path",
            "+000198c0: 290a 2020 2020 6675 6c6c 5f70 6174 6820  ).    full_path ",
            "+000198d0: 3d20 6f73 2e70 6174 682e 6a6f 696e 286f  = os.path.join(o",
            "+000198e0: 732e 6673 656e 636f 6465 2872 6570 6f2e  s.fsencode(repo.",
            "+000198f0: 7061 7468 292c 2074 7265 655f 7061 7468  path), tree_path",
            "+00019900: 290a 2020 2020 626c 6f62 203d 2072 6570  ).    blob = rep",
            "+00019910: 6f2e 6f62 6a65 6374 5f73 746f 7265 5b66  o.object_store[f",
            "+00019920: 696c 655f 656e 7472 795b 315d 5d0a 2020  ile_entry[1]].  ",
            "+00019930: 2020 6d6f 6465 203d 2066 696c 655f 656e    mode = file_en",
            "+00019940: 7472 795b 305d 0a20 2020 2062 7569 6c64  try[0].    build",
            "+00019950: 5f66 696c 655f 6672 6f6d 5f62 6c6f 6228  _file_from_blob(",
            "+00019960: 626c 6f62 2c20 6d6f 6465 2c20 6675 6c6c  blob, mode, full",
            "+00019970: 5f70 6174 682c 2073 796d 6c69 6e6b 5f66  _path, symlink_f",
            "+00019980: 6e3d 7379 6d6c 696e 6b5f 666e 290a 0a0a  n=symlink_fn)...",
            "+00019990: 4072 6570 6c61 6365 5f6d 6528 7369 6e63  @replace_me(sinc",
            "+000199a0: 653d 2230 2e32 322e 3922 2c20 7265 6d6f  e=\"0.22.9\", remo",
            "+000199b0: 7665 5f69 6e3d 2230 2e32 342e 3022 290a  ve_in=\"0.24.0\").",
            "+000199c0: 6465 6620 6368 6563 6b6f 7574 5f62 7261  def checkout_bra",
            "+000199d0: 6e63 6828 7265 706f 2c20 7461 7267 6574  nch(repo, target",
            "+000199e0: 3a20 556e 696f 6e5b 6279 7465 732c 2073  : Union[bytes, s",
            "+000199f0: 7472 5d2c 2066 6f72 6365 3a20 626f 6f6c  tr], force: bool",
            "+00019a00: 203d 2046 616c 7365 2920 2d3e 204e 6f6e   = False) -> Non",
            "+00019a10: 653a 0a20 2020 2022 2222 5377 6974 6368  e:.    \"\"\"Switch",
            "+00019a20: 2062 7261 6e63 6865 7320 6f72 2072 6573   branches or res",
            "+00019a30: 746f 7265 2077 6f72 6b69 6e67 2074 7265  tore working tre",
            "+00019a40: 6520 6669 6c65 732e 0a0a 2020 2020 5468  e files...    Th",
            "+00019a50: 6973 2069 7320 6e6f 7720 6120 7772 6170  is is now a wrap",
            "+00019a60: 7065 7220 6172 6f75 6e64 2074 6865 2067  per around the g",
            "+00019a70: 656e 6572 616c 2063 6865 636b 6f75 7428  eneral checkout(",
            "+00019a80: 2920 6675 6e63 7469 6f6e 2e0a 2020 2020  ) function..    ",
            "+00019a90: 5072 6573 6572 7665 6420 666f 7220 6261  Preserved for ba",
            "+00019aa0: 636b 7761 7264 2063 6f6d 7061 7469 6269  ckward compatibi",
            "+00019ab0: 6c69 7479 2e0a 0a20 2020 2041 7267 733a  lity...    Args:",
            "+00019ac0: 0a20 2020 2020 2072 6570 6f3a 2064 756c  .      repo: dul",
            "+00019ad0: 7769 6368 2052 6570 6f20 6f62 6a65 6374  wich Repo object",
            "+00019ae0: 0a20 2020 2020 2074 6172 6765 743a 2062  .      target: b",
            "+00019af0: 7261 6e63 6820 6e61 6d65 206f 7220 636f  ranch name or co",
            "+00019b00: 6d6d 6974 2073 6861 2074 6f20 6368 6563  mmit sha to chec",
            "+00019b10: 6b6f 7574 0a20 2020 2020 2066 6f72 6365  kout.      force",
            "+00019b20: 3a20 7472 7565 206f 7220 6e6f 7420 746f  : true or not to",
            "+00019b30: 2066 6f72 6365 2063 6865 636b 6f75 740a   force checkout.",
            "+00019b40: 2020 2020 2222 220a 2020 2020 2320 5369      \"\"\".    # Si",
            "+00019b50: 6d70 6c79 2064 656c 6567 6174 6520 746f  mply delegate to",
            "+00019b60: 2074 6865 206e 6577 2063 6865 636b 6f75   the new checkou",
            "+00019b70: 7420 6675 6e63 7469 6f6e 0a20 2020 2072  t function.    r",
            "+00019b80: 6574 7572 6e20 6368 6563 6b6f 7574 2872  eturn checkout(r",
            "+00019b90: 6570 6f2c 2074 6172 6765 742c 2066 6f72  epo, target, for",
            "+00019ba0: 6365 3d66 6f72 6365 290a 0a0a 6465 6620  ce=force)...def ",
            "+00019bb0: 7370 6172 7365 5f63 6865 636b 6f75 7428  sparse_checkout(",
            "+00019bc0: 0a20 2020 2072 6570 6f2c 2070 6174 7465  .    repo, patte",
            "+00019bd0: 726e 733d 4e6f 6e65 2c20 666f 7263 653a  rns=None, force:",
            "+00019be0: 2062 6f6f 6c20 3d20 4661 6c73 652c 2063   bool = False, c",
            "+00019bf0: 6f6e 653a 2055 6e69 6f6e 5b62 6f6f 6c2c  one: Union[bool,",
            "+00019c00: 204e 6f6e 655d 203d 204e 6f6e 650a 293a   None] = None.):",
            "+00019c10: 0a20 2020 2022 2222 5065 7266 6f72 6d20  .    \"\"\"Perform ",
            "+00019c20: 6120 7370 6172 7365 2063 6865 636b 6f75  a sparse checkou",
            "+00019c30: 7420 696e 2074 6865 2072 6570 6f73 6974  t in the reposit",
            "+00019c40: 6f72 7920 2865 6974 6865 7220 2766 756c  ory (either 'ful",
            "+00019c50: 6c27 206f 7220 2763 6f6e 6520 6d6f 6465  l' or 'cone mode",
            "+00019c60: 2729 2e0a 0a20 2020 2050 6572 666f 726d  ')...    Perform",
            "+00019c70: 2073 7061 7273 6520 6368 6563 6b6f 7574   sparse checkout",
            "+00019c80: 2069 6e20 6569 7468 6572 2027 636f 6e65   in either 'cone",
            "+00019c90: 2720 2864 6972 6563 746f 7279 2d62 6173  ' (directory-bas",
            "+00019ca0: 6564 2920 6d6f 6465 206f 720a 2020 2020  ed) mode or.    ",
            "+00019cb0: 2766 756c 6c20 7061 7474 6572 6e27 2028  'full pattern' (",
            "+00019cc0: 2e67 6974 6967 6e6f 7265 2920 6d6f 6465  .gitignore) mode",
            "+00019cd0: 2c20 6465 7065 6e64 696e 6720 6f6e 2074  , depending on t",
            "+00019ce0: 6865 2060 6063 6f6e 6560 6020 7061 7261  he ``cone`` para",
            "+00019cf0: 6d65 7465 722e 0a0a 2020 2020 4966 2060  meter...    If `",
            "+00019d00: 6063 6f6e 6560 6020 6973 2060 604e 6f6e  `cone`` is ``Non",
            "+00019d10: 6560 602c 2074 6865 206d 6f64 6520 6973  e``, the mode is",
            "+00019d20: 2069 6e66 6572 7265 6420 6672 6f6d 2074   inferred from t",
            "+00019d30: 6865 2072 6570 6f73 6974 6f72 7927 730a  he repository's.",
            "+00019d40: 2020 2020 6060 636f 7265 2e73 7061 7273      ``core.spars",
            "+00019d50: 6543 6865 636b 6f75 7443 6f6e 6560 6020  eCheckoutCone`` ",
            "+00019d60: 636f 6e66 6967 2073 6574 7469 6e67 2e0a  config setting..",
            "+00019d70: 0a20 2020 2053 7465 7073 3a0a 2020 2020  .    Steps:.    ",
            "+00019d80: 2020 3129 2049 6620 6060 7061 7474 6572    1) If ``patter",
            "+00019d90: 6e73 6060 2069 7320 7072 6f76 6964 6564  ns`` is provided",
            "+00019da0: 2c20 7772 6974 6520 7468 656d 2074 6f20  , write them to ",
            "+00019db0: 6060 2e67 6974 2f69 6e66 6f2f 7370 6172  ``.git/info/spar",
            "+00019dc0: 7365 2d63 6865 636b 6f75 7460 602e 0a20  se-checkout``.. ",
            "+00019dd0: 2020 2020 2032 2920 4465 7465 726d 696e       2) Determin",
            "+00019de0: 6520 7768 6963 6820 7061 7468 7320 696e  e which paths in",
            "+00019df0: 2074 6865 2069 6e64 6578 2061 7265 2069   the index are i",
            "+00019e00: 6e63 6c75 6465 6420 7673 2e20 6578 636c  ncluded vs. excl",
            "+00019e10: 7564 6564 2e0a 2020 2020 2020 2020 202d  uded..         -",
            "+00019e20: 2049 6620 6060 636f 6e65 3d54 7275 6560   If ``cone=True`",
            "+00019e30: 602c 2075 7365 2022 636f 6e65 2d63 6f6d  `, use \"cone-com",
            "+00019e40: 7061 7469 626c 6522 2064 6972 6563 746f  patible\" directo",
            "+00019e50: 7279 2d62 6173 6564 206c 6f67 6963 2e0a  ry-based logic..",
            "+00019e60: 2020 2020 2020 2020 202d 2049 6620 6060           - If ``",
            "+00019e70: 636f 6e65 3d46 616c 7365 6060 2c20 7573  cone=False``, us",
            "+00019e80: 6520 7374 616e 6461 7264 202e 6769 7469  e standard .giti",
            "+00019e90: 676e 6f72 652d 7374 796c 6520 6d61 7463  gnore-style matc",
            "+00019ea0: 6869 6e67 2e0a 2020 2020 2020 3329 2055  hing..      3) U",
            "+00019eb0: 7064 6174 6520 7468 6520 696e 6465 7827  pdate the index'",
            "+00019ec0: 7320 736b 6970 2d77 6f72 6b74 7265 6520  s skip-worktree ",
            "+00019ed0: 6269 7473 2061 6e64 2061 6464 2f72 656d  bits and add/rem",
            "+00019ee0: 6f76 6520 6669 6c65 7320 696e 0a20 2020  ove files in.   ",
            "+00019ef0: 2020 2020 2020 7468 6520 776f 726b 696e        the workin",
            "+00019f00: 6720 7472 6565 2061 6363 6f72 6469 6e67  g tree according",
            "+00019f10: 6c79 2e0a 2020 2020 2020 3429 2049 6620  ly..      4) If ",
            "+00019f20: 6060 666f 7263 653d 4661 6c73 6560 602c  ``force=False``,",
            "+00019f30: 2072 6566 7573 6520 746f 2072 656d 6f76   refuse to remov",
            "+00019f40: 6520 6669 6c65 7320 7468 6174 2068 6176  e files that hav",
            "+00019f50: 6520 6c6f 6361 6c20 6d6f 6469 6669 6361  e local modifica",
            "+00019f60: 7469 6f6e 732e 0a0a 2020 2020 4172 6773  tions...    Args",
            "+00019f70: 3a0a 2020 2020 2020 7265 706f 3a20 5061  :.      repo: Pa",
            "+00019f80: 7468 2074 6f20 7468 6520 7265 706f 7369  th to the reposi",
            "+00019f90: 746f 7279 206f 7220 6120 5265 706f 206f  tory or a Repo o",
            "+00019fa0: 626a 6563 742e 0a20 2020 2020 2070 6174  bject..      pat",
            "+00019fb0: 7465 726e 733a 204f 7074 696f 6e61 6c20  terns: Optional ",
            "+00019fc0: 6c69 7374 206f 6620 7370 6172 7365 2d63  list of sparse-c",
            "+00019fd0: 6865 636b 6f75 7420 7061 7474 6572 6e73  heckout patterns",
            "+00019fe0: 2074 6f20 7772 6974 652e 0a20 2020 2020   to write..     ",
            "+00019ff0: 2066 6f72 6365 3a20 5768 6574 6865 7220   force: Whether ",
            "+0001a000: 746f 2066 6f72 6365 2072 656d 6f76 616c  to force removal",
            "+0001a010: 206f 6620 6c6f 6361 6c6c 7920 6d6f 6469   of locally modi",
            "+0001a020: 6669 6564 2066 696c 6573 2028 6465 6661  fied files (defa",
            "+0001a030: 756c 7420 4661 6c73 6529 2e0a 2020 2020  ult False)..    ",
            "+0001a040: 2020 636f 6e65 3a20 426f 6f6c 6561 6e20    cone: Boolean ",
            "+0001a050: 696e 6469 6361 7469 6e67 2063 6f6e 6520  indicating cone ",
            "+0001a060: 6d6f 6465 2028 5472 7565 2f46 616c 7365  mode (True/False",
            "+0001a070: 292e 2049 6620 4e6f 6e65 2c20 7265 6164  ). If None, read",
            "+0001a080: 2066 726f 6d20 636f 6e66 6967 2e0a 0a20   from config... ",
            "+0001a090: 2020 2052 6574 7572 6e73 3a0a 2020 2020     Returns:.    ",
            "+0001a0a0: 2020 4e6f 6e65 0a20 2020 2022 2222 0a20    None.    \"\"\". ",
            "+0001a0b0: 2020 2077 6974 6820 6f70 656e 5f72 6570     with open_rep",
            "+0001a0c0: 6f5f 636c 6f73 696e 6728 7265 706f 2920  o_closing(repo) ",
            "+0001a0d0: 6173 2072 6570 6f5f 6f62 6a3a 0a20 2020  as repo_obj:.   ",
            "+0001a0e0: 2020 2020 2023 202d 2d2d 2030 2920 506f       # --- 0) Po",
            "+0001a0f0: 7373 6962 6c79 2069 6e66 6572 2027 636f  ssibly infer 'co",
            "+0001a100: 6e65 2720 6672 6f6d 2063 6f6e 6669 6720  ne' from config ",
            "+0001a110: 2d2d 2d0a 2020 2020 2020 2020 6966 2063  ---.        if c",
            "+0001a120: 6f6e 6520 6973 204e 6f6e 653a 0a20 2020  one is None:.   ",
            "+0001a130: 2020 2020 2020 2020 2063 6f6e 6520 3d20           cone = ",
            "+0001a140: 7265 706f 5f6f 626a 2e69 6e66 6572 5f63  repo_obj.infer_c",
            "+0001a150: 6f6e 655f 6d6f 6465 2829 0a0a 2020 2020  one_mode()..    ",
            "+0001a160: 2020 2020 2320 2d2d 2d20 3129 2052 6561      # --- 1) Rea",
            "+0001a170: 6420 6f72 2077 7269 7465 2070 6174 7465  d or write patte",
            "+0001a180: 726e 7320 2d2d 2d0a 2020 2020 2020 2020  rns ---.        ",
            "+0001a190: 6966 2070 6174 7465 726e 7320 6973 204e  if patterns is N",
            "+0001a1a0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           ",
            "+0001a1b0: 206c 696e 6573 203d 2072 6570 6f5f 6f62   lines = repo_ob",
            "+0001a1c0: 6a2e 6765 745f 7370 6172 7365 5f63 6865  j.get_sparse_che",
            "+0001a1d0: 636b 6f75 745f 7061 7474 6572 6e73 2829  ckout_patterns()",
            "+0001a1e0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if ",
            "+0001a1f0: 6c69 6e65 7320 6973 204e 6f6e 653a 0a20  lines is None:. ",
            "+0001a200: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+0001a210: 6169 7365 2045 7272 6f72 2822 4e6f 2073  aise Error(\"No s",
            "+0001a220: 7061 7273 6520 6368 6563 6b6f 7574 2070  parse checkout p",
            "+0001a230: 6174 7465 726e 7320 666f 756e 642e 2229  atterns found.\")",
            "+0001a240: 0a20 2020 2020 2020 2065 6c73 653a 0a20  .        else:. ",
            "+0001a250: 2020 2020 2020 2020 2020 206c 696e 6573             lines",
            "+0001a260: 203d 2070 6174 7465 726e 730a 2020 2020   = patterns.    ",
            "+0001a270: 2020 2020 2020 2020 7265 706f 5f6f 626a          repo_obj",
            "+0001a280: 2e73 6574 5f73 7061 7273 655f 6368 6563  .set_sparse_chec",
            "+0001a290: 6b6f 7574 5f70 6174 7465 726e 7328 7061  kout_patterns(pa",
            "+0001a2a0: 7474 6572 6e73 290a 0a20 2020 2020 2020  tterns)..       ",
            "+0001a2b0: 2023 202d 2d2d 2032 2920 4465 7465 726d   # --- 2) Determ",
            "+0001a2c0: 696e 6520 7468 6520 7365 7420 6f66 2069  ine the set of i",
            "+0001a2d0: 6e63 6c75 6465 6420 7061 7468 7320 2d2d  ncluded paths --",
            "+0001a2e0: 2d0a 2020 2020 2020 2020 696e 636c 7564  -.        includ",
            "+0001a2f0: 6564 5f70 6174 6873 203d 2064 6574 6572  ed_paths = deter",
            "+0001a300: 6d69 6e65 5f69 6e63 6c75 6465 645f 7061  mine_included_pa",
            "+0001a310: 7468 7328 7265 706f 5f6f 626a 2c20 6c69  ths(repo_obj, li",
            "+0001a320: 6e65 732c 2063 6f6e 6529 0a0a 2020 2020  nes, cone)..    ",
            "+0001a330: 2020 2020 2320 2d2d 2d20 3329 2041 7070      # --- 3) App",
            "+0001a340: 6c79 2074 686f 7365 2072 6573 756c 7473  ly those results",
            "+0001a350: 2074 6f20 7468 6520 696e 6465 7820 2620   to the index & ",
            "+0001a360: 776f 726b 696e 6720 7472 6565 202d 2d2d  working tree ---",
            "+0001a370: 0a20 2020 2020 2020 2074 7279 3a0a 2020  .        try:.  ",
            "+0001a380: 2020 2020 2020 2020 2020 6170 706c 795f            apply_",
            "+0001a390: 696e 636c 7564 6564 5f70 6174 6873 2872  included_paths(r",
            "+0001a3a0: 6570 6f5f 6f62 6a2c 2069 6e63 6c75 6465  epo_obj, include",
            "+0001a3b0: 645f 7061 7468 732c 2066 6f72 6365 3d66  d_paths, force=f",
            "+0001a3c0: 6f72 6365 290a 2020 2020 2020 2020 6578  orce).        ex",
            "+0001a3d0: 6365 7074 2053 7061 7273 6543 6865 636b  cept SparseCheck",
            "+0001a3e0: 6f75 7443 6f6e 666c 6963 7445 7272 6f72  outConflictError",
            "+0001a3f0: 2061 7320 6578 633a 0a20 2020 2020 2020   as exc:.       ",
            "+0001a400: 2020 2020 2072 6169 7365 2043 6865 636b       raise Check",
            "+0001a410: 6f75 7445 7272 6f72 282a 6578 632e 6172  outError(*exc.ar",
            "+0001a420: 6773 2920 6672 6f6d 2065 7863 0a0a 0a64  gs) from exc...d",
            "+0001a430: 6566 2063 6f6e 655f 6d6f 6465 5f69 6e69  ef cone_mode_ini",
            "+0001a440: 7428 7265 706f 293a 0a20 2020 2022 2222  t(repo):.    \"\"\"",
            "+0001a450: 496e 6974 6961 6c69 7a65 2061 2072 6570  Initialize a rep",
            "+0001a460: 6f73 6974 6f72 7920 746f 2075 7365 2073  ository to use s",
            "+0001a470: 7061 7273 6520 6368 6563 6b6f 7574 2069  parse checkout i",
            "+0001a480: 6e20 2763 6f6e 6527 206d 6f64 652e 0a0a  n 'cone' mode...",
            "+0001a490: 2020 2020 5365 7473 2060 6063 6f72 652e      Sets ``core.",
            "+0001a4a0: 7370 6172 7365 4368 6563 6b6f 7574 6060  sparseCheckout``",
            "+0001a4b0: 2061 6e64 2060 6063 6f72 652e 7370 6172   and ``core.spar",
            "+0001a4c0: 7365 4368 6563 6b6f 7574 436f 6e65 6060  seCheckoutCone``",
            "+0001a4d0: 2069 6e20 7468 6520 636f 6e66 6967 2e0a   in the config..",
            "+0001a4e0: 2020 2020 5772 6974 6573 2061 6e20 696e      Writes an in",
            "+0001a4f0: 6974 6961 6c20 6060 2e67 6974 2f69 6e66  itial ``.git/inf",
            "+0001a500: 6f2f 7370 6172 7365 2d63 6865 636b 6f75  o/sparse-checkou",
            "+0001a510: 7460 6020 6669 6c65 2074 6861 7420 696e  t`` file that in",
            "+0001a520: 636c 7564 6573 206f 6e6c 790a 2020 2020  cludes only.    ",
            "+0001a530: 746f 702d 6c65 7665 6c20 6669 6c65 7320  top-level files ",
            "+0001a540: 2861 6e64 2065 7863 6c75 6465 7320 616c  (and excludes al",
            "+0001a550: 6c20 7375 6264 6972 6563 746f 7269 6573  l subdirectories",
            "+0001a560: 292c 2065 2e67 2e20 6060 5b22 2f2a 222c  ), e.g. ``[\"/*\",",
            "+0001a570: 2022 212f 2a2f 225d 6060 2e0a 2020 2020   \"!/*/\"]``..    ",
            "+0001a580: 5468 656e 2070 6572 666f 726d 7320 6120  Then performs a ",
            "+0001a590: 7370 6172 7365 2063 6865 636b 6f75 7420  sparse checkout ",
            "+0001a5a0: 746f 2075 7064 6174 6520 7468 6520 776f  to update the wo",
            "+0001a5b0: 726b 696e 6720 7472 6565 2061 6363 6f72  rking tree accor",
            "+0001a5c0: 6469 6e67 6c79 2e0a 0a20 2020 2049 6620  dingly...    If ",
            "+0001a5d0: 6e6f 2064 6972 6563 746f 7269 6573 2061  no directories a",
            "+0001a5e0: 7265 2073 7065 6369 6669 6564 2c20 7468  re specified, th",
            "+0001a5f0: 656e 206f 6e6c 7920 746f 702d 6c65 7665  en only top-leve",
            "+0001a600: 6c20 6669 6c65 7320 6172 6520 696e 636c  l files are incl",
            "+0001a610: 7564 6564 3a0a 2020 2020 6874 7470 733a  uded:.    https:",
            "+0001a620: 2f2f 6769 742d 7363 6d2e 636f 6d2f 646f  //git-scm.com/do",
            "+0001a630: 6373 2f67 6974 2d73 7061 7273 652d 6368  cs/git-sparse-ch",
            "+0001a640: 6563 6b6f 7574 235f 696e 7465 726e 616c  eckout#_internal",
            "+0001a650: 7363 6f6e 655f 6d6f 6465 5f68 616e 646c  scone_mode_handl",
            "+0001a660: 696e 670a 0a20 2020 2041 7267 733a 0a20  ing..    Args:. ",
            "+0001a670: 2020 2020 2072 6570 6f3a 2050 6174 6820       repo: Path ",
            "+0001a680: 746f 2074 6865 2072 6570 6f73 6974 6f72  to the repositor",
            "+0001a690: 7920 6f72 2061 2052 6570 6f20 6f62 6a65  y or a Repo obje",
            "+0001a6a0: 6374 2e0a 0a20 2020 2052 6574 7572 6e73  ct...    Returns",
            "+0001a6b0: 3a0a 2020 2020 2020 4e6f 6e65 0a20 2020  :.      None.   ",
            "+0001a6c0: 2022 2222 0a20 2020 2077 6974 6820 6f70   \"\"\".    with op",
            "+0001a6d0: 656e 5f72 6570 6f5f 636c 6f73 696e 6728  en_repo_closing(",
            "+0001a6e0: 7265 706f 2920 6173 2072 6570 6f5f 6f62  repo) as repo_ob",
            "+0001a6f0: 6a3a 0a20 2020 2020 2020 2072 6570 6f5f  j:.        repo_",
            "+0001a700: 6f62 6a2e 636f 6e66 6967 7572 655f 666f  obj.configure_fo",
            "+0001a710: 725f 636f 6e65 5f6d 6f64 6528 290a 2020  r_cone_mode().  ",
            "+0001a720: 2020 2020 2020 7061 7474 6572 6e73 203d        patterns =",
            "+0001a730: 205b 222f 2a22 2c20 2221 2f2a 2f22 5d20   [\"/*\", \"!/*/\"] ",
            "+0001a740: 2023 2072 6f6f 742d 6c65 7665 6c20 6669   # root-level fi",
            "+0001a750: 6c65 7320 6f6e 6c79 0a20 2020 2020 2020  les only.       ",
            "+0001a760: 2073 7061 7273 655f 6368 6563 6b6f 7574   sparse_checkout",
            "+0001a770: 2872 6570 6f5f 6f62 6a2c 2070 6174 7465  (repo_obj, patte",
            "+0001a780: 726e 732c 2066 6f72 6365 3d54 7275 652c  rns, force=True,",
            "+0001a790: 2063 6f6e 653d 5472 7565 290a 0a0a 6465   cone=True)...de",
            "+0001a7a0: 6620 636f 6e65 5f6d 6f64 655f 7365 7428  f cone_mode_set(",
            "+0001a7b0: 7265 706f 2c20 6469 7273 2c20 666f 7263  repo, dirs, forc",
            "+0001a7c0: 653d 4661 6c73 6529 3a0a 2020 2020 2222  e=False):.    \"\"",
            "+0001a7d0: 224f 7665 7277 7269 7465 2074 6865 2065  \"Overwrite the e",
            "+0001a7e0: 7869 7374 696e 6720 2763 6f6e 652d 6d6f  xisting 'cone-mo",
            "+0001a7f0: 6465 2720 7370 6172 7365 2070 6174 7465  de' sparse patte",
            "+0001a800: 726e 7320 7769 7468 2061 206e 6577 2073  rns with a new s",
            "+0001a810: 6574 206f 6620 6469 7265 6374 6f72 6965  et of directorie",
            "+0001a820: 732e 0a0a 2020 2020 456e 7375 7265 7320  s...    Ensures ",
            "+0001a830: 6060 636f 7265 2e73 7061 7273 6543 6865  ``core.sparseChe",
            "+0001a840: 636b 6f75 7460 6020 616e 6420 6060 636f  ckout`` and ``co",
            "+0001a850: 7265 2e73 7061 7273 6543 6865 636b 6f75  re.sparseCheckou",
            "+0001a860: 7443 6f6e 6560 6020 6172 6520 656e 6162  tCone`` are enab",
            "+0001a870: 6c65 642e 0a20 2020 2057 7269 7465 7320  led..    Writes ",
            "+0001a880: 6e65 7720 7061 7474 6572 6e73 2073 6f20  new patterns so ",
            "+0001a890: 7468 6174 206f 6e6c 7920 7468 6520 7370  that only the sp",
            "+0001a8a0: 6563 6966 6965 6420 6469 7265 6374 6f72  ecified director",
            "+0001a8b0: 6965 7320 2861 6e64 2074 6f70 2d6c 6576  ies (and top-lev",
            "+0001a8c0: 656c 2066 696c 6573 290a 2020 2020 7265  el files).    re",
            "+0001a8d0: 6d61 696e 2069 6e20 7468 6520 776f 726b  main in the work",
            "+0001a8e0: 696e 6720 7472 6565 2c20 616e 6420 6170  ing tree, and ap",
            "+0001a8f0: 706c 6965 7320 7468 6520 7370 6172 7365  plies the sparse",
            "+0001a900: 2063 6865 636b 6f75 7420 7570 6461 7465   checkout update",
            "+0001a910: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "+0001a920: 2020 2072 6570 6f3a 2050 6174 6820 746f     repo: Path to",
            "+0001a930: 2074 6865 2072 6570 6f73 6974 6f72 7920   the repository ",
            "+0001a940: 6f72 2061 2052 6570 6f20 6f62 6a65 6374  or a Repo object",
            "+0001a950: 2e0a 2020 2020 2020 6469 7273 3a20 4c69  ..      dirs: Li",
            "+0001a960: 7374 206f 6620 6469 7265 6374 6f72 7920  st of directory ",
            "+0001a970: 6e61 6d65 7320 746f 2069 6e63 6c75 6465  names to include",
            "+0001a980: 2e0a 2020 2020 2020 666f 7263 653a 2057  ..      force: W",
            "+0001a990: 6865 7468 6572 2074 6f20 666f 7263 6962  hether to forcib",
            "+0001a9a0: 6c79 2064 6973 6361 7264 206c 6f63 616c  ly discard local",
            "+0001a9b0: 206d 6f64 6966 6963 6174 696f 6e73 2028   modifications (",
            "+0001a9c0: 6465 6661 756c 7420 4661 6c73 6529 2e0a  default False)..",
            "+0001a9d0: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  ",
            "+0001a9e0: 2020 2020 4e6f 6e65 0a20 2020 2022 2222      None.    \"\"\"",
            "+0001a9f0: 0a20 2020 2077 6974 6820 6f70 656e 5f72  .    with open_r",
            "+0001aa00: 6570 6f5f 636c 6f73 696e 6728 7265 706f  epo_closing(repo",
            "+0001aa10: 2920 6173 2072 6570 6f5f 6f62 6a3a 0a20  ) as repo_obj:. ",
            "+0001aa20: 2020 2020 2020 2072 6570 6f5f 6f62 6a2e         repo_obj.",
            "+0001aa30: 636f 6e66 6967 7572 655f 666f 725f 636f  configure_for_co",
            "+0001aa40: 6e65 5f6d 6f64 6528 290a 2020 2020 2020  ne_mode().      ",
            "+0001aa50: 2020 7265 706f 5f6f 626a 2e73 6574 5f63    repo_obj.set_c",
            "+0001aa60: 6f6e 655f 6d6f 6465 5f70 6174 7465 726e  one_mode_pattern",
            "+0001aa70: 7328 6469 7273 3d64 6972 7329 0a20 2020  s(dirs=dirs).   ",
            "+0001aa80: 2020 2020 206e 6577 5f70 6174 7465 726e       new_pattern",
            "+0001aa90: 7320 3d20 7265 706f 5f6f 626a 2e67 6574  s = repo_obj.get",
            "+0001aaa0: 5f73 7061 7273 655f 6368 6563 6b6f 7574  _sparse_checkout",
            "+0001aab0: 5f70 6174 7465 726e 7328 290a 2020 2020  _patterns().    ",
            "+0001aac0: 2020 2020 2320 4669 6e61 6c6c 792c 2061      # Finally, a",
            "+0001aad0: 7070 6c79 2074 6865 2070 6174 7465 726e  pply the pattern",
            "+0001aae0: 7320 616e 6420 7570 6461 7465 2074 6865  s and update the",
            "+0001aaf0: 2077 6f72 6b69 6e67 2074 7265 650a 2020   working tree.  ",
            "+0001ab00: 2020 2020 2020 7370 6172 7365 5f63 6865        sparse_che",
            "+0001ab10: 636b 6f75 7428 7265 706f 5f6f 626a 2c20  ckout(repo_obj, ",
            "+0001ab20: 6e65 775f 7061 7474 6572 6e73 2c20 666f  new_patterns, fo",
            "+0001ab30: 7263 653d 666f 7263 652c 2063 6f6e 653d  rce=force, cone=",
            "+0001ab40: 5472 7565 290a 0a0a 6465 6620 636f 6e65  True)...def cone",
            "+0001ab50: 5f6d 6f64 655f 6164 6428 7265 706f 2c20  _mode_add(repo, ",
            "+0001ab60: 6469 7273 2c20 666f 7263 653d 4661 6c73  dirs, force=Fals",
            "+0001ab70: 6529 3a0a 2020 2020 2222 2241 6464 206e  e):.    \"\"\"Add n",
            "+0001ab80: 6577 2064 6972 6563 746f 7269 6573 2074  ew directories t",
            "+0001ab90: 6f20 7468 6520 6578 6973 7469 6e67 2027  o the existing '",
            "+0001aba0: 636f 6e65 2d6d 6f64 6527 2073 7061 7273  cone-mode' spars",
            "+0001abb0: 652d 6368 6563 6b6f 7574 2070 6174 7465  e-checkout patte",
            "+0001abc0: 726e 732e 0a0a 2020 2020 5265 6164 7320  rns...    Reads ",
            "+0001abd0: 7468 6520 6375 7272 656e 7420 7061 7474  the current patt",
            "+0001abe0: 6572 6e73 2066 726f 6d20 6060 2e67 6974  erns from ``.git",
            "+0001abf0: 2f69 6e66 6f2f 7370 6172 7365 2d63 6865  /info/sparse-che",
            "+0001ac00: 636b 6f75 7460 602c 2061 6464 7320 7061  ckout``, adds pa",
            "+0001ac10: 7474 6572 6e0a 2020 2020 6c69 6e65 7320  ttern.    lines ",
            "+0001ac20: 746f 2069 6e63 6c75 6465 2074 6865 2073  to include the s",
            "+0001ac30: 7065 6369 6669 6564 2064 6972 6563 746f  pecified directo",
            "+0001ac40: 7269 6573 2c20 616e 6420 7468 656e 2070  ries, and then p",
            "+0001ac50: 6572 666f 726d 7320 6120 7370 6172 7365  erforms a sparse",
            "+0001ac60: 0a20 2020 2063 6865 636b 6f75 7420 746f  .    checkout to",
            "+0001ac70: 2075 7064 6174 6520 7468 6520 776f 726b   update the work",
            "+0001ac80: 696e 6720 7472 6565 2061 6363 6f72 6469  ing tree accordi",
            "+0001ac90: 6e67 6c79 2e0a 0a20 2020 2041 7267 733a  ngly...    Args:",
            "+0001aca0: 0a20 2020 2020 2072 6570 6f3a 2050 6174  .      repo: Pat",
            "+0001acb0: 6820 746f 2074 6865 2072 6570 6f73 6974  h to the reposit",
            "+0001acc0: 6f72 7920 6f72 2061 2052 6570 6f20 6f62  ory or a Repo ob",
            "+0001acd0: 6a65 6374 2e0a 2020 2020 2020 6469 7273  ject..      dirs",
            "+0001ace0: 3a20 4c69 7374 206f 6620 6469 7265 6374  : List of direct",
            "+0001acf0: 6f72 7920 6e61 6d65 7320 746f 2061 6464  ory names to add",
            "+0001ad00: 2074 6f20 7468 6520 7370 6172 7365 2d63   to the sparse-c",
            "+0001ad10: 6865 636b 6f75 742e 0a20 2020 2020 2066  heckout..      f",
            "+0001ad20: 6f72 6365 3a20 5768 6574 6865 7220 746f  orce: Whether to",
            "+0001ad30: 2066 6f72 6369 626c 7920 6469 7363 6172   forcibly discar",
            "+0001ad40: 6420 6c6f 6361 6c20 6d6f 6469 6669 6361  d local modifica",
            "+0001ad50: 7469 6f6e 7320 2864 6566 6175 6c74 2046  tions (default F",
            "+0001ad60: 616c 7365 292e 0a0a 2020 2020 5265 7475  alse)...    Retu",
            "+0001ad70: 726e 733a 0a20 2020 2020 204e 6f6e 650a  rns:.      None.",
            "+0001ad80: 2020 2020 2222 220a 2020 2020 7769 7468      \"\"\".    with",
            "+0001ad90: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "+0001ada0: 6e67 2872 6570 6f29 2061 7320 7265 706f  ng(repo) as repo",
            "+0001adb0: 5f6f 626a 3a0a 2020 2020 2020 2020 7265  _obj:.        re",
            "+0001adc0: 706f 5f6f 626a 2e63 6f6e 6669 6775 7265  po_obj.configure",
            "+0001add0: 5f66 6f72 5f63 6f6e 655f 6d6f 6465 2829  _for_cone_mode()",
            "+0001ade0: 0a20 2020 2020 2020 2023 2044 6f20 6e6f  .        # Do no",
            "+0001adf0: 7420 7061 7373 2062 6173 6520 7061 7474  t pass base patt",
            "+0001ae00: 6572 6e73 2061 7320 6469 7273 0a20 2020  erns as dirs.   ",
            "+0001ae10: 2020 2020 2062 6173 655f 7061 7474 6572       base_patter",
            "+0001ae20: 6e73 203d 205b 222f 2a22 2c20 2221 2f2a  ns = [\"/*\", \"!/*",
            "+0001ae30: 2f22 5d0a 2020 2020 2020 2020 6578 6973  /\"].        exis",
            "+0001ae40: 7469 6e67 5f64 6972 7320 3d20 5b0a 2020  ting_dirs = [.  ",
            "+0001ae50: 2020 2020 2020 2020 2020 7061 742e 7374            pat.st",
            "+0001ae60: 7269 7028 222f 2229 0a20 2020 2020 2020  rip(\"/\").       ",
            "+0001ae70: 2020 2020 2066 6f72 2070 6174 2069 6e20       for pat in ",
            "+0001ae80: 7265 706f 5f6f 626a 2e67 6574 5f73 7061  repo_obj.get_spa",
            "+0001ae90: 7273 655f 6368 6563 6b6f 7574 5f70 6174  rse_checkout_pat",
            "+0001aea0: 7465 726e 7328 290a 2020 2020 2020 2020  terns().        ",
            "+0001aeb0: 2020 2020 6966 2070 6174 206e 6f74 2069      if pat not i",
            "+0001aec0: 6e20 6261 7365 5f70 6174 7465 726e 730a  n base_patterns.",
            "+0001aed0: 2020 2020 2020 2020 5d0a 2020 2020 2020          ].      ",
            "+0001aee0: 2020 6164 6465 645f 6469 7273 203d 2065    added_dirs = e",
            "+0001aef0: 7869 7374 696e 675f 6469 7273 202b 2028  xisting_dirs + (",
            "+0001af00: 6469 7273 206f 7220 5b5d 290a 2020 2020  dirs or []).    ",
            "+0001af10: 2020 2020 7265 706f 5f6f 626a 2e73 6574      repo_obj.set",
            "+0001af20: 5f63 6f6e 655f 6d6f 6465 5f70 6174 7465  _cone_mode_patte",
            "+0001af30: 726e 7328 6469 7273 3d61 6464 6564 5f64  rns(dirs=added_d",
            "+0001af40: 6972 7329 0a20 2020 2020 2020 206e 6577  irs).        new",
            "+0001af50: 5f70 6174 7465 726e 7320 3d20 7265 706f  _patterns = repo",
            "+0001af60: 5f6f 626a 2e67 6574 5f73 7061 7273 655f  _obj.get_sparse_",
            "+0001af70: 6368 6563 6b6f 7574 5f70 6174 7465 726e  checkout_pattern",
            "+0001af80: 7328 290a 2020 2020 2020 2020 7370 6172  s().        spar",
            "+0001af90: 7365 5f63 6865 636b 6f75 7428 7265 706f  se_checkout(repo",
            "+0001afa0: 5f6f 626a 2c20 7061 7474 6572 6e73 3d6e  _obj, patterns=n",
            "+0001afb0: 6577 5f70 6174 7465 726e 732c 2066 6f72  ew_patterns, for",
            "+0001afc0: 6365 3d66 6f72 6365 2c20 636f 6e65 3d54  ce=force, cone=T",
            "+0001afd0: 7275 6529 0a0a 0a64 6566 2063 6865 636b  rue)...def check",
            "+0001afe0: 5f6d 6169 6c6d 6170 2872 6570 6f2c 2063  _mailmap(repo, c",
            "+0001aff0: 6f6e 7461 6374 293a 0a20 2020 2022 2222  ontact):.    \"\"\"",
            "+0001b000: 4368 6563 6b20 6361 6e6f 6e69 6361 6c20  Check canonical ",
            "+0001b010: 6e61 6d65 2061 6e64 2065 6d61 696c 206f  name and email o",
            "+0001b020: 6620 636f 6e74 6163 742e 0a0a 2020 2020  f contact...    ",
            "+0001b030: 4172 6773 3a0a 2020 2020 2020 7265 706f  Args:.      repo",
            "+0001b040: 3a20 5061 7468 2074 6f20 7468 6520 7265  : Path to the re",
            "+0001b050: 706f 7369 746f 7279 0a20 2020 2020 2063  pository.      c",
            "+0001b060: 6f6e 7461 6374 3a20 436f 6e74 6163 7420  ontact: Contact ",
            "+0001b070: 6e61 6d65 2061 6e64 2f6f 7220 656d 6169  name and/or emai",
            "+0001b080: 6c0a 2020 2020 5265 7475 726e 733a 2043  l.    Returns: C",
            "+0001b090: 616e 6f6e 6963 616c 2063 6f6e 7461 6374  anonical contact",
            "+0001b0a0: 2064 6174 610a 2020 2020 2222 220a 2020   data.    \"\"\".  ",
            "+0001b0b0: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+0001b0c0: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+0001b0d0: 7320 723a 0a20 2020 2020 2020 2066 726f  s r:.        fro",
            "+0001b0e0: 6d20 2e6d 6169 6c6d 6170 2069 6d70 6f72  m .mailmap impor",
            "+0001b0f0: 7420 4d61 696c 6d61 700a 0a20 2020 2020  t Mailmap..     ",
            "+0001b100: 2020 2074 7279 3a0a 2020 2020 2020 2020     try:.        ",
            "+0001b110: 2020 2020 6d61 696c 6d61 7020 3d20 4d61      mailmap = Ma",
            "+0001b120: 696c 6d61 702e 6672 6f6d 5f70 6174 6828  ilmap.from_path(",
            "+0001b130: 6f73 2e70 6174 682e 6a6f 696e 2872 2e70  os.path.join(r.p",
            "+0001b140: 6174 682c 2022 2e6d 6169 6c6d 6170 2229  ath, \".mailmap\")",
            "+0001b150: 290a 2020 2020 2020 2020 6578 6365 7074  ).        except",
            "+0001b160: 2046 696c 654e 6f74 466f 756e 6445 7272   FileNotFoundErr",
            "+0001b170: 6f72 3a0a 2020 2020 2020 2020 2020 2020  or:.            ",
            "+0001b180: 6d61 696c 6d61 7020 3d20 4d61 696c 6d61  mailmap = Mailma",
            "+0001b190: 7028 290a 2020 2020 2020 2020 7265 7475  p().        retu",
            "+0001b1a0: 726e 206d 6169 6c6d 6170 2e6c 6f6f 6b75  rn mailmap.looku",
            "+0001b1b0: 7028 636f 6e74 6163 7429 0a0a 0a64 6566  p(contact)...def",
            "+0001b1c0: 2066 7363 6b28 7265 706f 293a 0a20 2020   fsck(repo):.   ",
            "+0001b1d0: 2022 2222 4368 6563 6b20 6120 7265 706f   \"\"\"Check a repo",
            "+0001b1e0: 7369 746f 7279 2e0a 0a20 2020 2041 7267  sitory...    Arg",
            "+0001b1f0: 733a 0a20 2020 2020 2072 6570 6f3a 2041  s:.      repo: A",
            "+0001b200: 2070 6174 6820 746f 2074 6865 2072 6570   path to the rep",
            "+0001b210: 6f73 6974 6f72 790a 2020 2020 5265 7475  ository.    Retu",
            "+0001b220: 726e 733a 2049 7465 7261 746f 7220 6f76  rns: Iterator ov",
            "+0001b230: 6572 2065 7272 6f72 732f 7761 726e 696e  er errors/warnin",
            "+0001b240: 6773 0a20 2020 2022 2222 0a20 2020 2077  gs.    \"\"\".    w",
            "+0001b250: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "+0001b260: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "+0001b270: 3a0a 2020 2020 2020 2020 2320 544f 444f  :.        # TODO",
            "+0001b280: 286a 656c 6d65 7229 3a20 6368 6563 6b20  (jelmer): check ",
            "+0001b290: 7061 636b 2066 696c 6573 0a20 2020 2020  pack files.     ",
            "+0001b2a0: 2020 2023 2054 4f44 4f28 6a65 6c6d 6572     # TODO(jelmer",
            "+0001b2b0: 293a 2063 6865 636b 2067 7261 7068 0a20  ): check graph. ",
            "+0001b2c0: 2020 2020 2020 2023 2054 4f44 4f28 6a65         # TODO(je",
            "+0001b2d0: 6c6d 6572 293a 2063 6865 636b 2072 6566  lmer): check ref",
            "+0001b2e0: 730a 2020 2020 2020 2020 666f 7220 7368  s.        for sh",
            "+0001b2f0: 6120 696e 2072 2e6f 626a 6563 745f 7374  a in r.object_st",
            "+0001b300: 6f72 653a 0a20 2020 2020 2020 2020 2020  ore:.           ",
            "+0001b310: 206f 203d 2072 2e6f 626a 6563 745f 7374   o = r.object_st",
            "+0001b320: 6f72 655b 7368 615d 0a20 2020 2020 2020  ore[sha].       ",
            "+0001b330: 2020 2020 2074 7279 3a0a 2020 2020 2020       try:.      ",
            "+0001b340: 2020 2020 2020 2020 2020 6f2e 6368 6563            o.chec",
            "+0001b350: 6b28 290a 2020 2020 2020 2020 2020 2020  k().            ",
            "+0001b360: 6578 6365 7074 2045 7863 6570 7469 6f6e  except Exception",
            "+0001b370: 2061 7320 653a 0a20 2020 2020 2020 2020   as e:.         ",
            "+0001b380: 2020 2020 2020 2079 6965 6c64 2028 7368         yield (sh",
            "+0001b390: 612c 2065 290a 0a0a 6465 6620 7374 6173  a, e)...def stas",
            "+0001b3a0: 685f 6c69 7374 2872 6570 6f29 3a0a 2020  h_list(repo):.  ",
            "+0001b3b0: 2020 2222 224c 6973 7420 616c 6c20 7374    \"\"\"List all st",
            "+0001b3c0: 6173 6865 7320 696e 2061 2072 6570 6f73  ashes in a repos",
            "+0001b3d0: 6974 6f72 792e 2222 220a 2020 2020 7769  itory.\"\"\".    wi",
            "+0001b3e0: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "+0001b3f0: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "+0001b400: 0a20 2020 2020 2020 2066 726f 6d20 2e73  .        from .s",
            "+0001b410: 7461 7368 2069 6d70 6f72 7420 5374 6173  tash import Stas",
            "+0001b420: 680a 0a20 2020 2020 2020 2073 7461 7368  h..        stash",
            "+0001b430: 203d 2053 7461 7368 2e66 726f 6d5f 7265   = Stash.from_re",
            "+0001b440: 706f 2872 290a 2020 2020 2020 2020 7265  po(r).        re",
            "+0001b450: 7475 726e 2065 6e75 6d65 7261 7465 286c  turn enumerate(l",
            "+0001b460: 6973 7428 7374 6173 682e 7374 6173 6865  ist(stash.stashe",
            "+0001b470: 7328 2929 290a 0a0a 6465 6620 7374 6173  s()))...def stas",
            "+0001b480: 685f 7075 7368 2872 6570 6f29 202d 3e20  h_push(repo) -> ",
            "+0001b490: 4e6f 6e65 3a0a 2020 2020 2222 2250 7573  None:.    \"\"\"Pus",
            "+0001b4a0: 6820 6120 6e65 7720 7374 6173 6820 6f6e  h a new stash on",
            "+0001b4b0: 746f 2074 6865 2073 7461 636b 2e22 2222  to the stack.\"\"\"",
            "+0001b4c0: 0a20 2020 2077 6974 6820 6f70 656e 5f72  .    with open_r",
            "+0001b4d0: 6570 6f5f 636c 6f73 696e 6728 7265 706f  epo_closing(repo",
            "+0001b4e0: 2920 6173 2072 3a0a 2020 2020 2020 2020  ) as r:.        ",
            "+0001b4f0: 6672 6f6d 202e 7374 6173 6820 696d 706f  from .stash impo",
            "+0001b500: 7274 2053 7461 7368 0a0a 2020 2020 2020  rt Stash..      ",
            "+0001b510: 2020 7374 6173 6820 3d20 5374 6173 682e    stash = Stash.",
            "+0001b520: 6672 6f6d 5f72 6570 6f28 7229 0a20 2020  from_repo(r).   ",
            "+0001b530: 2020 2020 2073 7461 7368 2e70 7573 6828       stash.push(",
            "+0001b540: 290a 0a0a 6465 6620 7374 6173 685f 706f  )...def stash_po",
            "+0001b550: 7028 7265 706f 2920 2d3e 204e 6f6e 653a  p(repo) -> None:",
            "+0001b560: 0a20 2020 2022 2222 506f 7020 6120 7374  .    \"\"\"Pop a st",
            "+0001b570: 6173 6820 6672 6f6d 2074 6865 2073 7461  ash from the sta",
            "+0001b580: 636b 2e22 2222 0a20 2020 2077 6974 6820  ck.\"\"\".    with ",
            "+0001b590: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "+0001b5a0: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "+0001b5b0: 2020 2020 2020 6672 6f6d 202e 7374 6173        from .stas",
            "+0001b5c0: 6820 696d 706f 7274 2053 7461 7368 0a0a  h import Stash..",
            "+0001b5d0: 2020 2020 2020 2020 7374 6173 6820 3d20          stash = ",
            "+0001b5e0: 5374 6173 682e 6672 6f6d 5f72 6570 6f28  Stash.from_repo(",
            "+0001b5f0: 7229 0a20 2020 2020 2020 2073 7461 7368  r).        stash",
            "+0001b600: 2e70 6f70 2830 290a 0a0a 6465 6620 7374  .pop(0)...def st",
            "+0001b610: 6173 685f 6472 6f70 2872 6570 6f2c 2069  ash_drop(repo, i",
            "+0001b620: 6e64 6578 2920 2d3e 204e 6f6e 653a 0a20  ndex) -> None:. ",
            "+0001b630: 2020 2022 2222 4472 6f70 2061 2073 7461     \"\"\"Drop a sta",
            "+0001b640: 7368 2066 726f 6d20 7468 6520 7374 6163  sh from the stac",
            "+0001b650: 6b2e 2222 220a 2020 2020 7769 7468 206f  k.\"\"\".    with o",
            "+0001b660: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+0001b670: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+0001b680: 2020 2020 2066 726f 6d20 2e73 7461 7368       from .stash",
            "+0001b690: 2069 6d70 6f72 7420 5374 6173 680a 0a20   import Stash.. ",
            "+0001b6a0: 2020 2020 2020 2073 7461 7368 203d 2053         stash = S",
            "+0001b6b0: 7461 7368 2e66 726f 6d5f 7265 706f 2872  tash.from_repo(r",
            "+0001b6c0: 290a 2020 2020 2020 2020 7374 6173 682e  ).        stash.",
            "+0001b6d0: 6472 6f70 2869 6e64 6578 290a 0a0a 6465  drop(index)...de",
            "+0001b6e0: 6620 6c73 5f66 696c 6573 2872 6570 6f29  f ls_files(repo)",
            "+0001b6f0: 3a0a 2020 2020 2222 224c 6973 7420 616c  :.    \"\"\"List al",
            "+0001b700: 6c20 6669 6c65 7320 696e 2061 6e20 696e  l files in an in",
            "+0001b710: 6465 782e 2222 220a 2020 2020 7769 7468  dex.\"\"\".    with",
            "+0001b720: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "+0001b730: 6e67 2872 6570 6f29 2061 7320 723a 0a20  ng(repo) as r:. ",
            "+0001b740: 2020 2020 2020 2072 6574 7572 6e20 736f         return so",
            "+0001b750: 7274 6564 2872 2e6f 7065 6e5f 696e 6465  rted(r.open_inde",
            "+0001b760: 7828 2929 0a0a 0a64 6566 2066 696e 645f  x())...def find_",
            "+0001b770: 756e 6971 7565 5f61 6262 7265 7628 6f62  unique_abbrev(ob",
            "+0001b780: 6a65 6374 5f73 746f 7265 2c20 6f62 6a65  ject_store, obje",
            "+0001b790: 6374 5f69 642c 206d 696e 5f6c 656e 6774  ct_id, min_lengt",
            "+0001b7a0: 683d 3729 3a0a 2020 2020 2222 2246 696e  h=7):.    \"\"\"Fin",
            "+0001b7b0: 6420 7468 6520 7368 6f72 7465 7374 2075  d the shortest u",
            "+0001b7c0: 6e69 7175 6520 6162 6272 6576 6961 7469  nique abbreviati",
            "+0001b7d0: 6f6e 2066 6f72 2061 6e20 6f62 6a65 6374  on for an object",
            "+0001b7e0: 2049 442e 0a0a 2020 2020 4172 6773 3a0a   ID...    Args:.",
            "+0001b7f0: 2020 2020 2020 6f62 6a65 6374 5f73 746f        object_sto",
            "+0001b800: 7265 3a20 4f62 6a65 6374 2073 746f 7265  re: Object store",
            "+0001b810: 2074 6f20 7365 6172 6368 2069 6e0a 2020   to search in.  ",
            "+0001b820: 2020 2020 6f62 6a65 6374 5f69 643a 2054      object_id: T",
            "+0001b830: 6865 2066 756c 6c20 6f62 6a65 6374 2049  he full object I",
            "+0001b840: 4420 746f 2061 6262 7265 7669 6174 650a  D to abbreviate.",
            "+0001b850: 2020 2020 2020 6d69 6e5f 6c65 6e67 7468        min_length",
            "+0001b860: 3a20 4d69 6e69 6d75 6d20 6c65 6e67 7468  : Minimum length",
            "+0001b870: 206f 6620 6162 6272 6576 6961 7469 6f6e   of abbreviation",
            "+0001b880: 2028 6465 6661 756c 7420 3729 0a0a 2020   (default 7)..  ",
            "+0001b890: 2020 5265 7475 726e 733a 0a20 2020 2020    Returns:.     ",
            "+0001b8a0: 2054 6865 2073 686f 7274 6573 7420 756e   The shortest un",
            "+0001b8b0: 6971 7565 2070 7265 6669 7820 6f66 2074  ique prefix of t",
            "+0001b8c0: 6865 206f 626a 6563 7420 4944 2028 6174  he object ID (at",
            "+0001b8d0: 206c 6561 7374 206d 696e 5f6c 656e 6774   least min_lengt",
            "+0001b8e0: 6820 6368 6172 7329 0a20 2020 2022 2222  h chars).    \"\"\"",
            "+0001b8f0: 0a20 2020 2069 6620 6973 696e 7374 616e  .    if isinstan",
            "+0001b900: 6365 286f 626a 6563 745f 6964 2c20 6279  ce(object_id, by",
            "+0001b910: 7465 7329 3a0a 2020 2020 2020 2020 6865  tes):.        he",
            "+0001b920: 785f 6964 203d 206f 626a 6563 745f 6964  x_id = object_id",
            "+0001b930: 2e64 6563 6f64 6528 2261 7363 6969 2229  .decode(\"ascii\")",
            "+0001b940: 0a20 2020 2065 6c73 653a 0a20 2020 2020  .    else:.     ",
            "+0001b950: 2020 2068 6578 5f69 6420 3d20 6f62 6a65     hex_id = obje",
            "+0001b960: 6374 5f69 640a 0a20 2020 2023 2053 7461  ct_id..    # Sta",
            "+0001b970: 7274 2077 6974 6820 6d69 6e69 6d75 6d20  rt with minimum ",
            "+0001b980: 6c65 6e67 7468 0a20 2020 2066 6f72 206c  length.    for l",
            "+0001b990: 656e 6774 6820 696e 2072 616e 6765 286d  ength in range(m",
            "+0001b9a0: 696e 5f6c 656e 6774 682c 206c 656e 2868  in_length, len(h",
            "+0001b9b0: 6578 5f69 6429 202b 2031 293a 0a20 2020  ex_id) + 1):.   ",
            "+0001b9c0: 2020 2020 2070 7265 6669 7820 3d20 6865       prefix = he",
            "+0001b9d0: 785f 6964 5b3a 6c65 6e67 7468 5d0a 2020  x_id[:length].  ",
            "+0001b9e0: 2020 2020 2020 6d61 7463 6865 7320 3d20        matches = ",
            "+0001b9f0: 300a 0a20 2020 2020 2020 2023 2043 6865  0..        # Che",
            "+0001ba00: 636b 2069 6620 7468 6973 2070 7265 6669  ck if this prefi",
            "+0001ba10: 7820 6973 2075 6e69 7175 650a 2020 2020  x is unique.    ",
            "+0001ba20: 2020 2020 666f 7220 6f62 6a5f 6964 2069      for obj_id i",
            "+0001ba30: 6e20 6f62 6a65 6374 5f73 746f 7265 3a0a  n object_store:.",
            "+0001ba40: 2020 2020 2020 2020 2020 2020 6966 206f              if o",
            "+0001ba50: 626a 5f69 642e 6465 636f 6465 2822 6173  bj_id.decode(\"as",
            "+0001ba60: 6369 6922 292e 7374 6172 7473 7769 7468  cii\").startswith",
            "+0001ba70: 2870 7265 6669 7829 3a0a 2020 2020 2020  (prefix):.      ",
            "+0001ba80: 2020 2020 2020 2020 2020 6d61 7463 6865            matche",
            "+0001ba90: 7320 2b3d 2031 0a20 2020 2020 2020 2020  s += 1.         ",
            "+0001baa0: 2020 2020 2020 2069 6620 6d61 7463 6865         if matche",
            "+0001bab0: 7320 3e20 313a 0a20 2020 2020 2020 2020  s > 1:.         ",
            "+0001bac0: 2020 2020 2020 2020 2020 2023 204e 6f74             # Not",
            "+0001bad0: 2075 6e69 7175 652c 206e 6565 6420 6d6f   unique, need mo",
            "+0001bae0: 7265 2063 6861 7261 6374 6572 730a 2020  re characters.  ",
            "+0001baf0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001bb00: 2020 6272 6561 6b0a 0a20 2020 2020 2020    break..       ",
            "+0001bb10: 2069 6620 6d61 7463 6865 7320 3d3d 2031   if matches == 1",
            "+0001bb20: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # ",
            "+0001bb30: 466f 756e 6420 756e 6971 7565 2070 7265  Found unique pre",
            "+0001bb40: 6669 780a 2020 2020 2020 2020 2020 2020  fix.            ",
            "+0001bb50: 7265 7475 726e 2070 7265 6669 780a 0a20  return prefix.. ",
            "+0001bb60: 2020 2023 2049 6620 7765 2067 6574 2068     # If we get h",
            "+0001bb70: 6572 652c 2072 6574 7572 6e20 7468 6520  ere, return the ",
            "+0001bb80: 6675 6c6c 2049 440a 2020 2020 7265 7475  full ID.    retu",
            "+0001bb90: 726e 2068 6578 5f69 640a 0a0a 6465 6620  rn hex_id...def ",
            "+0001bba0: 6465 7363 7269 6265 2872 6570 6f2c 2061  describe(repo, a",
            "+0001bbb0: 6262 7265 763d 4e6f 6e65 293a 0a20 2020  bbrev=None):.   ",
            "+0001bbc0: 2022 2222 4465 7363 7269 6265 2074 6865   \"\"\"Describe the",
            "+0001bbd0: 2072 6570 6f73 6974 6f72 7920 7665 7273   repository vers",
            "+0001bbe0: 696f 6e2e 0a0a 2020 2020 4172 6773 3a0a  ion...    Args:.",
            "+0001bbf0: 2020 2020 2020 7265 706f 3a20 6769 7420        repo: git ",
            "+0001bc00: 7265 706f 7369 746f 7279 0a20 2020 2020  repository.     ",
            "+0001bc10: 2061 6262 7265 763a 206e 756d 6265 7220   abbrev: number ",
            "+0001bc20: 6f66 2063 6861 7261 6374 6572 7320 6f66  of characters of",
            "+0001bc30: 2063 6f6d 6d69 7420 746f 2074 616b 652c   commit to take,",
            "+0001bc40: 2064 6566 6175 6c74 2069 7320 370a 2020   default is 7.  ",
            "+0001bc50: 2020 5265 7475 726e 733a 2061 2073 7472    Returns: a str",
            "+0001bc60: 696e 6720 6465 7363 7269 7074 696f 6e20  ing description ",
            "+0001bc70: 6f66 2074 6865 2063 7572 7265 6e74 2067  of the current g",
            "+0001bc80: 6974 2072 6576 6973 696f 6e0a 0a20 2020  it revision..   ",
            "+0001bc90: 2045 7861 6d70 6c65 733a 2022 6761 6263   Examples: \"gabc",
            "+0001bca0: 6465 6668 222c 2022 7630 2e31 2220 6f72  defh\", \"v0.1\" or",
            "+0001bcb0: 2022 7630 2e31 2d35 2d67 6162 6364 6566   \"v0.1-5-gabcdef",
            "+0001bcc0: 6822 2e0a 2020 2020 2222 220a 2020 2020  h\"..    \"\"\".    ",
            "+0001bcd0: 6162 6272 6576 5f73 6c69 6365 203d 2073  abbrev_slice = s",
            "+0001bce0: 6c69 6365 2830 2c20 6162 6272 6576 2069  lice(0, abbrev i",
            "+0001bcf0: 6620 6162 6272 6576 2069 7320 6e6f 7420  f abbrev is not ",
            "+0001bd00: 4e6f 6e65 2065 6c73 6520 3729 0a20 2020  None else 7).   ",
            "+0001bd10: 2023 2047 6574 2074 6865 2072 6570 6f73   # Get the repos",
            "+0001bd20: 6974 6f72 790a 2020 2020 7769 7468 206f  itory.    with o",
            "+0001bd30: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+0001bd40: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+0001bd50: 2020 2020 2023 2047 6574 2061 206c 6973       # Get a lis",
            "+0001bd60: 7420 6f66 2061 6c6c 2074 6167 730a 2020  t of all tags.  ",
            "+0001bd70: 2020 2020 2020 7265 6673 203d 2072 2e67        refs = r.g",
            "+0001bd80: 6574 5f72 6566 7328 290a 2020 2020 2020  et_refs().      ",
            "+0001bd90: 2020 7461 6773 203d 207b 7d0a 2020 2020    tags = {}.    ",
            "+0001bda0: 2020 2020 666f 7220 6b65 792c 2076 616c      for key, val",
            "+0001bdb0: 7565 2069 6e20 7265 6673 2e69 7465 6d73  ue in refs.items",
            "+0001bdc0: 2829 3a0a 2020 2020 2020 2020 2020 2020  ():.            ",
            "+0001bdd0: 6b65 7920 3d20 6b65 792e 6465 636f 6465  key = key.decode",
            "+0001bde0: 2829 0a20 2020 2020 2020 2020 2020 206f  ().            o",
            "+0001bdf0: 626a 203d 2072 2e67 6574 5f6f 626a 6563  bj = r.get_objec",
            "+0001be00: 7428 7661 6c75 6529 0a20 2020 2020 2020  t(value).       ",
            "+0001be10: 2020 2020 2069 6620 2274 6167 7322 206e       if \"tags\" n",
            "+0001be20: 6f74 2069 6e20 6b65 793a 0a20 2020 2020  ot in key:.     ",
            "+0001be30: 2020 2020 2020 2020 2020 2063 6f6e 7469             conti",
            "+0001be40: 6e75 650a 0a20 2020 2020 2020 2020 2020  nue..           ",
            "+0001be50: 205f 2c20 7461 6720 3d20 6b65 792e 7273   _, tag = key.rs",
            "+0001be60: 706c 6974 2822 2f22 2c20 3129 0a0a 2020  plit(\"/\", 1)..  ",
            "+0001be70: 2020 2020 2020 2020 2020 7472 793a 0a20            try:. ",
            "+0001be80: 2020 2020 2020 2020 2020 2020 2020 2023                 #",
            "+0001be90: 2041 6e6e 6f74 6174 6564 2074 6167 2063   Annotated tag c",
            "+0001bea0: 6173 650a 2020 2020 2020 2020 2020 2020  ase.            ",
            "+0001beb0: 2020 2020 636f 6d6d 6974 203d 206f 626a      commit = obj",
            "+0001bec0: 2e6f 626a 6563 740a 2020 2020 2020 2020  .object.        ",
            "+0001bed0: 2020 2020 2020 2020 636f 6d6d 6974 203d          commit =",
            "+0001bee0: 2072 2e67 6574 5f6f 626a 6563 7428 636f   r.get_object(co",
            "+0001bef0: 6d6d 6974 5b31 5d29 0a20 2020 2020 2020  mmit[1]).       ",
            "+0001bf00: 2020 2020 2065 7863 6570 7420 4174 7472       except Attr",
            "+0001bf10: 6962 7574 6545 7272 6f72 3a0a 2020 2020  ibuteError:.    ",
            "+0001bf20: 2020 2020 2020 2020 2020 2020 2320 4c69              # Li",
            "+0001bf30: 6768 7477 6569 6768 7420 7461 6720 6361  ghtweight tag ca",
            "+0001bf40: 7365 202d 206f 626a 2069 7320 616c 7265  se - obj is alre",
            "+0001bf50: 6164 7920 7468 6520 636f 6d6d 6974 0a20  ady the commit. ",
            "+0001bf60: 2020 2020 2020 2020 2020 2020 2020 2063                 c",
            "+0001bf70: 6f6d 6d69 7420 3d20 6f62 6a0a 2020 2020  ommit = obj.    ",
            "+0001bf80: 2020 2020 2020 2020 7461 6773 5b74 6167          tags[tag",
            "+0001bf90: 5d20 3d20 5b0a 2020 2020 2020 2020 2020  ] = [.          ",
            "+0001bfa0: 2020 2020 2020 6461 7465 7469 6d65 2e64        datetime.d",
            "+0001bfb0: 6174 6574 696d 6528 2a74 696d 652e 676d  atetime(*time.gm",
            "+0001bfc0: 7469 6d65 2863 6f6d 6d69 742e 636f 6d6d  time(commit.comm",
            "+0001bfd0: 6974 5f74 696d 6529 5b3a 365d 292c 0a20  it_time)[:6]),. ",
            "+0001bfe0: 2020 2020 2020 2020 2020 2020 2020 2063                 c",
            "+0001bff0: 6f6d 6d69 742e 6964 2e64 6563 6f64 6528  ommit.id.decode(",
            "+0001c000: 2261 7363 6969 2229 2c0a 2020 2020 2020  \"ascii\"),.      ",
            "+0001c010: 2020 2020 2020 5d0a 0a20 2020 2020 2020        ]..       ",
            "+0001c020: 2073 6f72 7465 645f 7461 6773 203d 2073   sorted_tags = s",
            "+0001c030: 6f72 7465 6428 7461 6773 2e69 7465 6d73  orted(tags.items",
            "+0001c040: 2829 2c20 6b65 793d 6c61 6d62 6461 2074  (), key=lambda t",
            "+0001c050: 6167 3a20 7461 675b 315d 5b30 5d2c 2072  ag: tag[1][0], r",
            "+0001c060: 6576 6572 7365 3d54 7275 6529 0a0a 2020  everse=True)..  ",
            "+0001c070: 2020 2020 2020 2320 4765 7420 7468 6520        # Get the ",
            "+0001c080: 6c61 7465 7374 2063 6f6d 6d69 740a 2020  latest commit.  ",
            "+0001c090: 2020 2020 2020 6c61 7465 7374 5f63 6f6d        latest_com",
            "+0001c0a0: 6d69 7420 3d20 725b 722e 6865 6164 2829  mit = r[r.head()",
            "+0001c0b0: 5d0a 0a20 2020 2020 2020 2023 2049 6620  ]..        # If ",
            "+0001c0c0: 7468 6572 6520 6172 6520 6e6f 2074 6167  there are no tag",
            "+0001c0d0: 732c 2072 6574 7572 6e20 7468 6520 6c61  s, return the la",
            "+0001c0e0: 7465 7374 2063 6f6d 6d69 740a 2020 2020  test commit.    ",
            "+0001c0f0: 2020 2020 6966 206c 656e 2873 6f72 7465      if len(sorte",
            "+0001c100: 645f 7461 6773 2920 3d3d 2030 3a0a 2020  d_tags) == 0:.  ",
            "+0001c110: 2020 2020 2020 2020 2020 6966 2061 6262            if abb",
            "+0001c120: 7265 7620 6973 206e 6f74 204e 6f6e 653a  rev is not None:",
            "+0001c130: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0001c140: 2072 6574 7572 6e20 2267 7b7d 222e 666f   return \"g{}\".fo",
            "+0001c150: 726d 6174 286c 6174 6573 745f 636f 6d6d  rmat(latest_comm",
            "+0001c160: 6974 2e69 642e 6465 636f 6465 2822 6173  it.id.decode(\"as",
            "+0001c170: 6369 6922 295b 6162 6272 6576 5f73 6c69  cii\")[abbrev_sli",
            "+0001c180: 6365 5d29 0a20 2020 2020 2020 2020 2020  ce]).           ",
            "+0001c190: 2072 6574 7572 6e20 6622 677b 6669 6e64   return f\"g{find",
            "+0001c1a0: 5f75 6e69 7175 655f 6162 6272 6576 2872  _unique_abbrev(r",
            "+0001c1b0: 2e6f 626a 6563 745f 7374 6f72 652c 206c  .object_store, l",
            "+0001c1c0: 6174 6573 745f 636f 6d6d 6974 2e69 6429  atest_commit.id)",
            "+0001c1d0: 7d22 0a0a 2020 2020 2020 2020 2320 5765  }\"..        # We",
            "+0001c1e0: 2772 6520 6e6f 7720 3020 636f 6d6d 6974  're now 0 commit",
            "+0001c1f0: 7320 6672 6f6d 2074 6865 2074 6f70 0a20  s from the top. ",
            "+0001c200: 2020 2020 2020 2063 6f6d 6d69 745f 636f         commit_co",
            "+0001c210: 756e 7420 3d20 300a 0a20 2020 2020 2020  unt = 0..       ",
            "+0001c220: 2023 2057 616c 6b20 7468 726f 7567 6820   # Walk through ",
            "+0001c230: 616c 6c20 636f 6d6d 6974 730a 2020 2020  all commits.    ",
            "+0001c240: 2020 2020 7761 6c6b 6572 203d 2072 2e67      walker = r.g",
            "+0001c250: 6574 5f77 616c 6b65 7228 290a 2020 2020  et_walker().    ",
            "+0001c260: 2020 2020 666f 7220 656e 7472 7920 696e      for entry in",
            "+0001c270: 2077 616c 6b65 723a 0a20 2020 2020 2020   walker:.       ",
            "+0001c280: 2020 2020 2023 2043 6865 636b 2069 6620       # Check if ",
            "+0001c290: 7461 670a 2020 2020 2020 2020 2020 2020  tag.            ",
            "+0001c2a0: 636f 6d6d 6974 5f69 6420 3d20 656e 7472  commit_id = entr",
            "+0001c2b0: 792e 636f 6d6d 6974 2e69 642e 6465 636f  y.commit.id.deco",
            "+0001c2c0: 6465 2822 6173 6369 6922 290a 2020 2020  de(\"ascii\").    ",
            "+0001c2d0: 2020 2020 2020 2020 666f 7220 7461 6720          for tag ",
            "+0001c2e0: 696e 2073 6f72 7465 645f 7461 6773 3a0a  in sorted_tags:.",
            "+0001c2f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001c300: 7461 675f 6e61 6d65 203d 2074 6167 5b30  tag_name = tag[0",
            "+0001c310: 5d0a 2020 2020 2020 2020 2020 2020 2020  ].              ",
            "+0001c320: 2020 7461 675f 636f 6d6d 6974 203d 2074    tag_commit = t",
            "+0001c330: 6167 5b31 5d5b 315d 0a20 2020 2020 2020  ag[1][1].       ",
            "+0001c340: 2020 2020 2020 2020 2069 6620 636f 6d6d           if comm",
            "+0001c350: 6974 5f69 6420 3d3d 2074 6167 5f63 6f6d  it_id == tag_com",
            "+0001c360: 6d69 743a 0a20 2020 2020 2020 2020 2020  mit:.           ",
            "+0001c370: 2020 2020 2020 2020 2069 6620 636f 6d6d           if comm",
            "+0001c380: 6974 5f63 6f75 6e74 203d 3d20 303a 0a20  it_count == 0:. ",
            "+0001c390: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001c3a0: 2020 2020 2020 2072 6574 7572 6e20 7461         return ta",
            "+0001c3b0: 675f 6e61 6d65 0a20 2020 2020 2020 2020  g_name.         ",
            "+0001c3c0: 2020 2020 2020 2020 2020 2065 6c73 653a             else:",
            "+0001c3d0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0001c3e0: 2020 2020 2020 2020 2069 6620 6162 6272           if abbr",
            "+0001c3f0: 6576 2069 7320 6e6f 7420 4e6f 6e65 3a0a  ev is not None:.",
            "+0001c400: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001c410: 2020 2020 2020 2020 2020 2020 6162 6272              abbr",
            "+0001c420: 6576 5f68 6173 6820 3d20 6c61 7465 7374  ev_hash = latest",
            "+0001c430: 5f63 6f6d 6d69 742e 6964 2e64 6563 6f64  _commit.id.decod",
            "+0001c440: 6528 2261 7363 6969 2229 5b61 6262 7265  e(\"ascii\")[abbre",
            "+0001c450: 765f 736c 6963 655d 0a20 2020 2020 2020  v_slice].       ",
            "+0001c460: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001c470: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         ",
            "+0001c480: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001c490: 2020 2061 6262 7265 765f 6861 7368 203d     abbrev_hash =",
            "+0001c4a0: 2066 696e 645f 756e 6971 7565 5f61 6262   find_unique_abb",
            "+0001c4b0: 7265 7628 0a20 2020 2020 2020 2020 2020  rev(.           ",
            "+0001c4c0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001c4d0: 2020 2020 2072 2e6f 626a 6563 745f 7374       r.object_st",
            "+0001c4e0: 6f72 652c 206c 6174 6573 745f 636f 6d6d  ore, latest_comm",
            "+0001c4f0: 6974 2e69 640a 2020 2020 2020 2020 2020  it.id.          ",
            "+0001c500: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001c510: 2020 290a 2020 2020 2020 2020 2020 2020    ).            ",
            "+0001c520: 2020 2020 2020 2020 2020 2020 7265 7475              retu",
            "+0001c530: 726e 2066 227b 7461 675f 6e61 6d65 7d2d  rn f\"{tag_name}-",
            "+0001c540: 7b63 6f6d 6d69 745f 636f 756e 747d 2d67  {commit_count}-g",
            "+0001c550: 7b61 6262 7265 765f 6861 7368 7d22 0a0a  {abbrev_hash}\"..",
            "+0001c560: 2020 2020 2020 2020 2020 2020 636f 6d6d              comm",
            "+0001c570: 6974 5f63 6f75 6e74 202b 3d20 310a 0a20  it_count += 1.. ",
            "+0001c580: 2020 2020 2020 2023 2052 6574 7572 6e20         # Return ",
            "+0001c590: 706c 6169 6e20 636f 6d6d 6974 2069 6620  plain commit if ",
            "+0001c5a0: 6e6f 2070 6172 656e 7420 7461 6720 6361  no parent tag ca",
            "+0001c5b0: 6e20 6265 2066 6f75 6e64 0a20 2020 2020  n be found.     ",
            "+0001c5c0: 2020 2069 6620 6162 6272 6576 2069 7320     if abbrev is ",
            "+0001c5d0: 6e6f 7420 4e6f 6e65 3a0a 2020 2020 2020  not None:.      ",
            "+0001c5e0: 2020 2020 2020 7265 7475 726e 2022 677b        return \"g{",
            "+0001c5f0: 7d22 2e66 6f72 6d61 7428 6c61 7465 7374  }\".format(latest",
            "+0001c600: 5f63 6f6d 6d69 742e 6964 2e64 6563 6f64  _commit.id.decod",
            "+0001c610: 6528 2261 7363 6969 2229 5b61 6262 7265  e(\"ascii\")[abbre",
            "+0001c620: 765f 736c 6963 655d 290a 2020 2020 2020  v_slice]).      ",
            "+0001c630: 2020 7265 7475 726e 2066 2267 7b66 696e    return f\"g{fin",
            "+0001c640: 645f 756e 6971 7565 5f61 6262 7265 7628  d_unique_abbrev(",
            "+0001c650: 722e 6f62 6a65 6374 5f73 746f 7265 2c20  r.object_store, ",
            "+0001c660: 6c61 7465 7374 5f63 6f6d 6d69 742e 6964  latest_commit.id",
            "+0001c670: 297d 220a 0a0a 6465 6620 6765 745f 6f62  )}\"...def get_ob",
            "+0001c680: 6a65 6374 5f62 795f 7061 7468 2872 6570  ject_by_path(rep",
            "+0001c690: 6f2c 2070 6174 682c 2063 6f6d 6d69 7474  o, path, committ",
            "+0001c6a0: 6973 683d 4e6f 6e65 293a 0a20 2020 2022  ish=None):.    \"",
            "+0001c6b0: 2222 4765 7420 616e 206f 626a 6563 7420  \"\"Get an object ",
            "+0001c6c0: 6279 2070 6174 682e 0a0a 2020 2020 4172  by path...    Ar",
            "+0001c6d0: 6773 3a0a 2020 2020 2020 7265 706f 3a20  gs:.      repo: ",
            "+0001c6e0: 4120 7061 7468 2074 6f20 7468 6520 7265  A path to the re",
            "+0001c6f0: 706f 7369 746f 7279 0a20 2020 2020 2070  pository.      p",
            "+0001c700: 6174 683a 2050 6174 6820 746f 206c 6f6f  ath: Path to loo",
            "+0001c710: 6b20 7570 0a20 2020 2020 2063 6f6d 6d69  k up.      commi",
            "+0001c720: 7474 6973 683a 2043 6f6d 6d69 7420 746f  ttish: Commit to",
            "+0001c730: 206c 6f6f 6b20 7570 2070 6174 6820 696e   look up path in",
            "+0001c740: 0a20 2020 2052 6574 7572 6e73 3a20 4120  .    Returns: A ",
            "+0001c750: 6053 6861 4669 6c65 6020 6f62 6a65 6374  `ShaFile` object",
            "+0001c760: 0a20 2020 2022 2222 0a20 2020 2069 6620  .    \"\"\".    if ",
            "+0001c770: 636f 6d6d 6974 7469 7368 2069 7320 4e6f  committish is No",
            "+0001c780: 6e65 3a0a 2020 2020 2020 2020 636f 6d6d  ne:.        comm",
            "+0001c790: 6974 7469 7368 203d 2022 4845 4144 220a  ittish = \"HEAD\".",
            "+0001c7a0: 2020 2020 2320 4765 7420 7468 6520 7265      # Get the re",
            "+0001c7b0: 706f 7369 746f 7279 0a20 2020 2077 6974  pository.    wit",
            "+0001c7c0: 6820 6f70 656e 5f72 6570 6f5f 636c 6f73  h open_repo_clos",
            "+0001c7d0: 696e 6728 7265 706f 2920 6173 2072 3a0a  ing(repo) as r:.",
            "+0001c7e0: 2020 2020 2020 2020 636f 6d6d 6974 203d          commit =",
            "+0001c7f0: 2070 6172 7365 5f63 6f6d 6d69 7428 722c   parse_commit(r,",
            "+0001c800: 2063 6f6d 6d69 7474 6973 6829 0a20 2020   committish).   ",
            "+0001c810: 2020 2020 2062 6173 655f 7472 6565 203d       base_tree =",
            "+0001c820: 2063 6f6d 6d69 742e 7472 6565 0a20 2020   commit.tree.   ",
            "+0001c830: 2020 2020 2069 6620 6e6f 7420 6973 696e       if not isin",
            "+0001c840: 7374 616e 6365 2870 6174 682c 2062 7974  stance(path, byt",
            "+0001c850: 6573 293a 0a20 2020 2020 2020 2020 2020  es):.           ",
            "+0001c860: 2070 6174 6820 3d20 636f 6d6d 6974 5f65   path = commit_e",
            "+0001c870: 6e63 6f64 6528 636f 6d6d 6974 2c20 7061  ncode(commit, pa",
            "+0001c880: 7468 290a 2020 2020 2020 2020 286d 6f64  th).        (mod",
            "+0001c890: 652c 2073 6861 2920 3d20 7472 6565 5f6c  e, sha) = tree_l",
            "+0001c8a0: 6f6f 6b75 705f 7061 7468 2872 2e6f 626a  ookup_path(r.obj",
            "+0001c8b0: 6563 745f 7374 6f72 652e 5f5f 6765 7469  ect_store.__geti",
            "+0001c8c0: 7465 6d5f 5f2c 2062 6173 655f 7472 6565  tem__, base_tree",
            "+0001c8d0: 2c20 7061 7468 290a 2020 2020 2020 2020  , path).        ",
            "+0001c8e0: 7265 7475 726e 2072 5b73 6861 5d0a 0a0a  return r[sha]...",
            "+0001c8f0: 6465 6620 7772 6974 655f 7472 6565 2872  def write_tree(r",
            "+0001c900: 6570 6f29 3a0a 2020 2020 2222 2257 7269  epo):.    \"\"\"Wri",
            "+0001c910: 7465 2061 2074 7265 6520 6f62 6a65 6374  te a tree object",
            "+0001c920: 2066 726f 6d20 7468 6520 696e 6465 782e   from the index.",
            "+0001c930: 0a0a 2020 2020 4172 6773 3a0a 2020 2020  ..    Args:.    ",
            "+0001c940: 2020 7265 706f 3a20 5265 706f 7369 746f    repo: Reposito",
            "+0001c950: 7279 2066 6f72 2077 6869 6368 2074 6f20  ry for which to ",
            "+0001c960: 7772 6974 6520 7472 6565 0a20 2020 2052  write tree.    R",
            "+0001c970: 6574 7572 6e73 3a20 7472 6565 2069 6420  eturns: tree id ",
            "+0001c980: 666f 7220 7468 6520 7472 6565 2074 6861  for the tree tha",
            "+0001c990: 7420 7761 7320 7772 6974 7465 6e0a 2020  t was written.  ",
            "+0001c9a0: 2020 2222 220a 2020 2020 7769 7468 206f    \"\"\".    with o",
            "+0001c9b0: 7065 6e5f 7265 706f 5f63 6c6f 7369 6e67  pen_repo_closing",
            "+0001c9c0: 2872 6570 6f29 2061 7320 723a 0a20 2020  (repo) as r:.   ",
            "+0001c9d0: 2020 2020 2072 6574 7572 6e20 722e 6f70       return r.op",
            "+0001c9e0: 656e 5f69 6e64 6578 2829 2e63 6f6d 6d69  en_index().commi",
            "+0001c9f0: 7428 722e 6f62 6a65 6374 5f73 746f 7265  t(r.object_store",
            "+0001ca00: 290a 0a0a 6465 6620 5f64 6f5f 6d65 7267  )...def _do_merg",
            "+0001ca10: 6528 0a20 2020 2072 2c0a 2020 2020 6d65  e(.    r,.    me",
            "+0001ca20: 7267 655f 636f 6d6d 6974 5f69 642c 0a20  rge_commit_id,. ",
            "+0001ca30: 2020 206e 6f5f 636f 6d6d 6974 3d46 616c     no_commit=Fal",
            "+0001ca40: 7365 2c0a 2020 2020 6e6f 5f66 663d 4661  se,.    no_ff=Fa",
            "+0001ca50: 6c73 652c 0a20 2020 206d 6573 7361 6765  lse,.    message",
            "+0001ca60: 3d4e 6f6e 652c 0a20 2020 2061 7574 686f  =None,.    autho",
            "+0001ca70: 723d 4e6f 6e65 2c0a 2020 2020 636f 6d6d  r=None,.    comm",
            "+0001ca80: 6974 7465 723d 4e6f 6e65 2c0a 293a 0a20  itter=None,.):. ",
            "+0001ca90: 2020 2022 2222 496e 7465 726e 616c 206d     \"\"\"Internal m",
            "+0001caa0: 6572 6765 2069 6d70 6c65 6d65 6e74 6174  erge implementat",
            "+0001cab0: 696f 6e20 7468 6174 206f 7065 7261 7465  ion that operate",
            "+0001cac0: 7320 6f6e 2061 6e20 6f70 656e 2072 6570  s on an open rep",
            "+0001cad0: 6f73 6974 6f72 792e 0a0a 2020 2020 4172  ository...    Ar",
            "+0001cae0: 6773 3a0a 2020 2020 2020 723a 204f 7065  gs:.      r: Ope",
            "+0001caf0: 6e20 7265 706f 7369 746f 7279 206f 626a  n repository obj",
            "+0001cb00: 6563 740a 2020 2020 2020 6d65 7267 655f  ect.      merge_",
            "+0001cb10: 636f 6d6d 6974 5f69 643a 2053 4841 206f  commit_id: SHA o",
            "+0001cb20: 6620 636f 6d6d 6974 2074 6f20 6d65 7267  f commit to merg",
            "+0001cb30: 650a 2020 2020 2020 6e6f 5f63 6f6d 6d69  e.      no_commi",
            "+0001cb40: 743a 2049 6620 5472 7565 2c20 646f 206e  t: If True, do n",
            "+0001cb50: 6f74 2063 7265 6174 6520 6120 6d65 7267  ot create a merg",
            "+0001cb60: 6520 636f 6d6d 6974 0a20 2020 2020 206e  e commit.      n",
            "+0001cb70: 6f5f 6666 3a20 4966 2054 7275 652c 2066  o_ff: If True, f",
            "+0001cb80: 6f72 6365 2063 7265 6174 696f 6e20 6f66  orce creation of",
            "+0001cb90: 2061 206d 6572 6765 2063 6f6d 6d69 740a   a merge commit.",
            "+0001cba0: 2020 2020 2020 6d65 7373 6167 653a 204f        message: O",
            "+0001cbb0: 7074 696f 6e61 6c20 6d65 7267 6520 636f  ptional merge co",
            "+0001cbc0: 6d6d 6974 206d 6573 7361 6765 0a20 2020  mmit message.   ",
            "+0001cbd0: 2020 2061 7574 686f 723a 204f 7074 696f     author: Optio",
            "+0001cbe0: 6e61 6c20 6175 7468 6f72 2066 6f72 206d  nal author for m",
            "+0001cbf0: 6572 6765 2063 6f6d 6d69 740a 2020 2020  erge commit.    ",
            "+0001cc00: 2020 636f 6d6d 6974 7465 723a 204f 7074    committer: Opt",
            "+0001cc10: 696f 6e61 6c20 636f 6d6d 6974 7465 7220  ional committer ",
            "+0001cc20: 666f 7220 6d65 7267 6520 636f 6d6d 6974  for merge commit",
            "+0001cc30: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. ",
            "+0001cc40: 2020 2020 2054 7570 6c65 206f 6620 286d       Tuple of (m",
            "+0001cc50: 6572 6765 5f63 6f6d 6d69 745f 7368 612c  erge_commit_sha,",
            "+0001cc60: 2063 6f6e 666c 6963 7473 2920 7768 6572   conflicts) wher",
            "+0001cc70: 6520 6d65 7267 655f 636f 6d6d 6974 5f73  e merge_commit_s",
            "+0001cc80: 6861 2069 7320 4e6f 6e65 0a20 2020 2020  ha is None.     ",
            "+0001cc90: 2069 6620 6e6f 5f63 6f6d 6d69 743d 5472   if no_commit=Tr",
            "+0001cca0: 7565 206f 7220 7468 6572 6520 7765 7265  ue or there were",
            "+0001ccb0: 2063 6f6e 666c 6963 7473 0a20 2020 2022   conflicts.    \"",
            "+0001ccc0: 2222 0a20 2020 2066 726f 6d20 2e67 7261  \"\".    from .gra",
            "+0001ccd0: 7068 2069 6d70 6f72 7420 6669 6e64 5f6d  ph import find_m",
            "+0001cce0: 6572 6765 5f62 6173 650a 2020 2020 6672  erge_base.    fr",
            "+0001ccf0: 6f6d 202e 6d65 7267 6520 696d 706f 7274  om .merge import",
            "+0001cd00: 2074 6872 6565 5f77 6179 5f6d 6572 6765   three_way_merge",
            "+0001cd10: 0a0a 2020 2020 2320 4765 7420 4845 4144  ..    # Get HEAD",
            "+0001cd20: 2063 6f6d 6d69 740a 2020 2020 7472 793a   commit.    try:",
            "+0001cd30: 0a20 2020 2020 2020 2068 6561 645f 636f  .        head_co",
            "+0001cd40: 6d6d 6974 5f69 6420 3d20 722e 7265 6673  mmit_id = r.refs",
            "+0001cd50: 5b62 2248 4541 4422 5d0a 2020 2020 6578  [b\"HEAD\"].    ex",
            "+0001cd60: 6365 7074 204b 6579 4572 726f 723a 0a20  cept KeyError:. ",
            "+0001cd70: 2020 2020 2020 2072 6169 7365 2045 7272         raise Err",
            "+0001cd80: 6f72 2822 4e6f 2048 4541 4420 7265 6665  or(\"No HEAD refe",
            "+0001cd90: 7265 6e63 6520 666f 756e 6422 290a 0a20  rence found\").. ",
            "+0001cda0: 2020 2068 6561 645f 636f 6d6d 6974 203d     head_commit =",
            "+0001cdb0: 2072 5b68 6561 645f 636f 6d6d 6974 5f69   r[head_commit_i",
            "+0001cdc0: 645d 0a20 2020 206d 6572 6765 5f63 6f6d  d].    merge_com",
            "+0001cdd0: 6d69 7420 3d20 725b 6d65 7267 655f 636f  mit = r[merge_co",
            "+0001cde0: 6d6d 6974 5f69 645d 0a0a 2020 2020 2320  mmit_id]..    # ",
            "+0001cdf0: 4368 6563 6b20 6966 2066 6173 742d 666f  Check if fast-fo",
            "+0001ce00: 7277 6172 6420 6973 2070 6f73 7369 626c  rward is possibl",
            "+0001ce10: 650a 2020 2020 6d65 7267 655f 6261 7365  e.    merge_base",
            "+0001ce20: 7320 3d20 6669 6e64 5f6d 6572 6765 5f62  s = find_merge_b",
            "+0001ce30: 6173 6528 722c 205b 6865 6164 5f63 6f6d  ase(r, [head_com",
            "+0001ce40: 6d69 745f 6964 2c20 6d65 7267 655f 636f  mit_id, merge_co",
            "+0001ce50: 6d6d 6974 5f69 645d 290a 0a20 2020 2069  mmit_id])..    i",
            "+0001ce60: 6620 6e6f 7420 6d65 7267 655f 6261 7365  f not merge_base",
            "+0001ce70: 733a 0a20 2020 2020 2020 2072 6169 7365  s:.        raise",
            "+0001ce80: 2045 7272 6f72 2822 4e6f 2063 6f6d 6d6f   Error(\"No commo",
            "+0001ce90: 6e20 616e 6365 7374 6f72 2066 6f75 6e64  n ancestor found",
            "+0001cea0: 2229 0a0a 2020 2020 2320 5573 6520 7468  \")..    # Use th",
            "+0001ceb0: 6520 6669 7273 7420 6d65 7267 6520 6261  e first merge ba",
            "+0001cec0: 7365 0a20 2020 2062 6173 655f 636f 6d6d  se.    base_comm",
            "+0001ced0: 6974 5f69 6420 3d20 6d65 7267 655f 6261  it_id = merge_ba",
            "+0001cee0: 7365 735b 305d 0a0a 2020 2020 2320 4368  ses[0]..    # Ch",
            "+0001cef0: 6563 6b20 6966 2077 6527 7265 2074 7279  eck if we're try",
            "+0001cf00: 696e 6720 746f 206d 6572 6765 2074 6865  ing to merge the",
            "+0001cf10: 2073 616d 6520 636f 6d6d 6974 0a20 2020   same commit.   ",
            "+0001cf20: 2069 6620 6865 6164 5f63 6f6d 6d69 745f   if head_commit_",
            "+0001cf30: 6964 203d 3d20 6d65 7267 655f 636f 6d6d  id == merge_comm",
            "+0001cf40: 6974 5f69 643a 0a20 2020 2020 2020 2023  it_id:.        #",
            "+0001cf50: 2041 6c72 6561 6479 2075 7020 746f 2064   Already up to d",
            "+0001cf60: 6174 650a 2020 2020 2020 2020 7265 7475  ate.        retu",
            "+0001cf70: 726e 2028 4e6f 6e65 2c20 5b5d 290a 0a20  rn (None, []).. ",
            "+0001cf80: 2020 2023 2043 6865 636b 2066 6f72 2066     # Check for f",
            "+0001cf90: 6173 742d 666f 7277 6172 640a 2020 2020  ast-forward.    ",
            "+0001cfa0: 6966 2062 6173 655f 636f 6d6d 6974 5f69  if base_commit_i",
            "+0001cfb0: 6420 3d3d 2068 6561 645f 636f 6d6d 6974  d == head_commit",
            "+0001cfc0: 5f69 6420 616e 6420 6e6f 7420 6e6f 5f66  _id and not no_f",
            "+0001cfd0: 663a 0a20 2020 2020 2020 2023 2046 6173  f:.        # Fas",
            "+0001cfe0: 742d 666f 7277 6172 6420 6d65 7267 650a  t-forward merge.",
            "+0001cff0: 2020 2020 2020 2020 722e 7265 6673 5b62          r.refs[b",
            "+0001d000: 2248 4541 4422 5d20 3d20 6d65 7267 655f  \"HEAD\"] = merge_",
            "+0001d010: 636f 6d6d 6974 5f69 640a 2020 2020 2020  commit_id.      ",
            "+0001d020: 2020 2320 5570 6461 7465 2074 6865 2077    # Update the w",
            "+0001d030: 6f72 6b69 6e67 2064 6972 6563 746f 7279  orking directory",
            "+0001d040: 0a20 2020 2020 2020 2075 7064 6174 655f  .        update_",
            "+0001d050: 776f 726b 696e 675f 7472 6565 2872 2c20  working_tree(r, ",
            "+0001d060: 6865 6164 5f63 6f6d 6d69 742e 7472 6565  head_commit.tree",
            "+0001d070: 2c20 6d65 7267 655f 636f 6d6d 6974 2e74  , merge_commit.t",
            "+0001d080: 7265 6529 0a20 2020 2020 2020 2072 6574  ree).        ret",
            "+0001d090: 7572 6e20 286d 6572 6765 5f63 6f6d 6d69  urn (merge_commi",
            "+0001d0a0: 745f 6964 2c20 5b5d 290a 0a20 2020 2069  t_id, [])..    i",
            "+0001d0b0: 6620 6261 7365 5f63 6f6d 6d69 745f 6964  f base_commit_id",
            "+0001d0c0: 203d 3d20 6d65 7267 655f 636f 6d6d 6974   == merge_commit",
            "+0001d0d0: 5f69 643a 0a20 2020 2020 2020 2023 2041  _id:.        # A",
            "+0001d0e0: 6c72 6561 6479 2075 7020 746f 2064 6174  lready up to dat",
            "+0001d0f0: 650a 2020 2020 2020 2020 7265 7475 726e  e.        return",
            "+0001d100: 2028 4e6f 6e65 2c20 5b5d 290a 0a20 2020   (None, [])..   ",
            "+0001d110: 2023 2050 6572 666f 726d 2074 6872 6565   # Perform three",
            "+0001d120: 2d77 6179 206d 6572 6765 0a20 2020 2062  -way merge.    b",
            "+0001d130: 6173 655f 636f 6d6d 6974 203d 2072 5b62  ase_commit = r[b",
            "+0001d140: 6173 655f 636f 6d6d 6974 5f69 645d 0a20  ase_commit_id]. ",
            "+0001d150: 2020 206d 6572 6765 645f 7472 6565 2c20     merged_tree, ",
            "+0001d160: 636f 6e66 6c69 6374 7320 3d20 7468 7265  conflicts = thre",
            "+0001d170: 655f 7761 795f 6d65 7267 6528 0a20 2020  e_way_merge(.   ",
            "+0001d180: 2020 2020 2072 2e6f 626a 6563 745f 7374       r.object_st",
            "+0001d190: 6f72 652c 2062 6173 655f 636f 6d6d 6974  ore, base_commit",
            "+0001d1a0: 2c20 6865 6164 5f63 6f6d 6d69 742c 206d  , head_commit, m",
            "+0001d1b0: 6572 6765 5f63 6f6d 6d69 740a 2020 2020  erge_commit.    ",
            "+0001d1c0: 290a 0a20 2020 2023 2041 6464 206d 6572  )..    # Add mer",
            "+0001d1d0: 6765 6420 7472 6565 2074 6f20 6f62 6a65  ged tree to obje",
            "+0001d1e0: 6374 2073 746f 7265 0a20 2020 2072 2e6f  ct store.    r.o",
            "+0001d1f0: 626a 6563 745f 7374 6f72 652e 6164 645f  bject_store.add_",
            "+0001d200: 6f62 6a65 6374 286d 6572 6765 645f 7472  object(merged_tr",
            "+0001d210: 6565 290a 0a20 2020 2023 2055 7064 6174  ee)..    # Updat",
            "+0001d220: 6520 696e 6465 7820 616e 6420 776f 726b  e index and work",
            "+0001d230: 696e 6720 6469 7265 6374 6f72 790a 2020  ing directory.  ",
            "+0001d240: 2020 7570 6461 7465 5f77 6f72 6b69 6e67    update_working",
            "+0001d250: 5f74 7265 6528 722c 2068 6561 645f 636f  _tree(r, head_co",
            "+0001d260: 6d6d 6974 2e74 7265 652c 206d 6572 6765  mmit.tree, merge",
            "+0001d270: 645f 7472 6565 2e69 6429 0a0a 2020 2020  d_tree.id)..    ",
            "+0001d280: 6966 2063 6f6e 666c 6963 7473 206f 7220  if conflicts or ",
            "+0001d290: 6e6f 5f63 6f6d 6d69 743a 0a20 2020 2020  no_commit:.     ",
            "+0001d2a0: 2020 2023 2044 6f6e 2774 2063 7265 6174     # Don't creat",
            "+0001d2b0: 6520 6120 636f 6d6d 6974 2069 6620 7468  e a commit if th",
            "+0001d2c0: 6572 6520 6172 6520 636f 6e66 6c69 6374  ere are conflict",
            "+0001d2d0: 7320 6f72 206e 6f5f 636f 6d6d 6974 2069  s or no_commit i",
            "+0001d2e0: 7320 5472 7565 0a20 2020 2020 2020 2072  s True.        r",
            "+0001d2f0: 6574 7572 6e20 284e 6f6e 652c 2063 6f6e  eturn (None, con",
            "+0001d300: 666c 6963 7473 290a 0a20 2020 2023 2043  flicts)..    # C",
            "+0001d310: 7265 6174 6520 6d65 7267 6520 636f 6d6d  reate merge comm",
            "+0001d320: 6974 0a20 2020 206d 6572 6765 5f63 6f6d  it.    merge_com",
            "+0001d330: 6d69 745f 6f62 6a20 3d20 436f 6d6d 6974  mit_obj = Commit",
            "+0001d340: 2829 0a20 2020 206d 6572 6765 5f63 6f6d  ().    merge_com",
            "+0001d350: 6d69 745f 6f62 6a2e 7472 6565 203d 206d  mit_obj.tree = m",
            "+0001d360: 6572 6765 645f 7472 6565 2e69 640a 2020  erged_tree.id.  ",
            "+0001d370: 2020 6d65 7267 655f 636f 6d6d 6974 5f6f    merge_commit_o",
            "+0001d380: 626a 2e70 6172 656e 7473 203d 205b 6865  bj.parents = [he",
            "+0001d390: 6164 5f63 6f6d 6d69 745f 6964 2c20 6d65  ad_commit_id, me",
            "+0001d3a0: 7267 655f 636f 6d6d 6974 5f69 645d 0a0a  rge_commit_id]..",
            "+0001d3b0: 2020 2020 2320 5365 7420 6175 7468 6f72      # Set author",
            "+0001d3c0: 2f63 6f6d 6d69 7474 6572 0a20 2020 2069  /committer.    i",
            "+0001d3d0: 6620 6175 7468 6f72 2069 7320 4e6f 6e65  f author is None",
            "+0001d3e0: 3a0a 2020 2020 2020 2020 6175 7468 6f72  :.        author",
            "+0001d3f0: 203d 2067 6574 5f75 7365 725f 6964 656e   = get_user_iden",
            "+0001d400: 7469 7479 2872 2e67 6574 5f63 6f6e 6669  tity(r.get_confi",
            "+0001d410: 675f 7374 6163 6b28 2929 0a20 2020 2069  g_stack()).    i",
            "+0001d420: 6620 636f 6d6d 6974 7465 7220 6973 204e  f committer is N",
            "+0001d430: 6f6e 653a 0a20 2020 2020 2020 2063 6f6d  one:.        com",
            "+0001d440: 6d69 7474 6572 203d 2061 7574 686f 720a  mitter = author.",
            "+0001d450: 0a20 2020 206d 6572 6765 5f63 6f6d 6d69  .    merge_commi",
            "+0001d460: 745f 6f62 6a2e 6175 7468 6f72 203d 2061  t_obj.author = a",
            "+0001d470: 7574 686f 720a 2020 2020 6d65 7267 655f  uthor.    merge_",
            "+0001d480: 636f 6d6d 6974 5f6f 626a 2e63 6f6d 6d69  commit_obj.commi",
            "+0001d490: 7474 6572 203d 2063 6f6d 6d69 7474 6572  tter = committer",
            "+0001d4a0: 0a0a 2020 2020 2320 5365 7420 7469 6d65  ..    # Set time",
            "+0001d4b0: 7374 616d 7073 0a20 2020 2074 696d 6573  stamps.    times",
            "+0001d4c0: 7461 6d70 203d 2069 6e74 2874 696d 652e  tamp = int(time.",
            "+0001d4d0: 7469 6d65 2829 290a 2020 2020 7469 6d65  time()).    time",
            "+0001d4e0: 7a6f 6e65 203d 2030 2020 2320 5554 430a  zone = 0  # UTC.",
            "+0001d4f0: 2020 2020 6d65 7267 655f 636f 6d6d 6974      merge_commit",
            "+0001d500: 5f6f 626a 2e61 7574 686f 725f 7469 6d65  _obj.author_time",
            "+0001d510: 203d 2074 696d 6573 7461 6d70 0a20 2020   = timestamp.   ",
            "+0001d520: 206d 6572 6765 5f63 6f6d 6d69 745f 6f62   merge_commit_ob",
            "+0001d530: 6a2e 6175 7468 6f72 5f74 696d 657a 6f6e  j.author_timezon",
            "+0001d540: 6520 3d20 7469 6d65 7a6f 6e65 0a20 2020  e = timezone.   ",
            "+0001d550: 206d 6572 6765 5f63 6f6d 6d69 745f 6f62   merge_commit_ob",
            "+0001d560: 6a2e 636f 6d6d 6974 5f74 696d 6520 3d20  j.commit_time = ",
            "+0001d570: 7469 6d65 7374 616d 700a 2020 2020 6d65  timestamp.    me",
            "+0001d580: 7267 655f 636f 6d6d 6974 5f6f 626a 2e63  rge_commit_obj.c",
            "+0001d590: 6f6d 6d69 745f 7469 6d65 7a6f 6e65 203d  ommit_timezone =",
            "+0001d5a0: 2074 696d 657a 6f6e 650a 0a20 2020 2023   timezone..    #",
            "+0001d5b0: 2053 6574 2063 6f6d 6d69 7420 6d65 7373   Set commit mess",
            "+0001d5c0: 6167 650a 2020 2020 6966 206d 6573 7361  age.    if messa",
            "+0001d5d0: 6765 2069 7320 4e6f 6e65 3a0a 2020 2020  ge is None:.    ",
            "+0001d5e0: 2020 2020 6d65 7373 6167 6520 3d20 6622      message = f\"",
            "+0001d5f0: 4d65 7267 6520 636f 6d6d 6974 2027 7b6d  Merge commit '{m",
            "+0001d600: 6572 6765 5f63 6f6d 6d69 745f 6964 2e64  erge_commit_id.d",
            "+0001d610: 6563 6f64 6528 295b 3a37 5d7d 275c 6e22  ecode()[:7]}'\\n\"",
            "+0001d620: 0a20 2020 206d 6572 6765 5f63 6f6d 6d69  .    merge_commi",
            "+0001d630: 745f 6f62 6a2e 6d65 7373 6167 6520 3d20  t_obj.message = ",
            "+0001d640: 6d65 7373 6167 652e 656e 636f 6465 2829  message.encode()",
            "+0001d650: 2069 6620 6973 696e 7374 616e 6365 286d   if isinstance(m",
            "+0001d660: 6573 7361 6765 2c20 7374 7229 2065 6c73  essage, str) els",
            "+0001d670: 6520 6d65 7373 6167 650a 0a20 2020 2023  e message..    #",
            "+0001d680: 2041 6464 2063 6f6d 6d69 7420 746f 206f   Add commit to o",
            "+0001d690: 626a 6563 7420 7374 6f72 650a 2020 2020  bject store.    ",
            "+0001d6a0: 722e 6f62 6a65 6374 5f73 746f 7265 2e61  r.object_store.a",
            "+0001d6b0: 6464 5f6f 626a 6563 7428 6d65 7267 655f  dd_object(merge_",
            "+0001d6c0: 636f 6d6d 6974 5f6f 626a 290a 0a20 2020  commit_obj)..   ",
            "+0001d6d0: 2023 2055 7064 6174 6520 4845 4144 0a20   # Update HEAD. ",
            "+0001d6e0: 2020 2072 2e72 6566 735b 6222 4845 4144     r.refs[b\"HEAD",
            "+0001d6f0: 225d 203d 206d 6572 6765 5f63 6f6d 6d69  \"] = merge_commi",
            "+0001d700: 745f 6f62 6a2e 6964 0a0a 2020 2020 7265  t_obj.id..    re",
            "+0001d710: 7475 726e 2028 6d65 7267 655f 636f 6d6d  turn (merge_comm",
            "+0001d720: 6974 5f6f 626a 2e69 642c 205b 5d29 0a0a  it_obj.id, [])..",
            "+0001d730: 0a64 6566 206d 6572 6765 280a 2020 2020  .def merge(.    ",
            "+0001d740: 7265 706f 2c0a 2020 2020 636f 6d6d 6974  repo,.    commit",
            "+0001d750: 7469 7368 2c0a 2020 2020 6e6f 5f63 6f6d  tish,.    no_com",
            "+0001d760: 6d69 743d 4661 6c73 652c 0a20 2020 206e  mit=False,.    n",
            "+0001d770: 6f5f 6666 3d46 616c 7365 2c0a 2020 2020  o_ff=False,.    ",
            "+0001d780: 6d65 7373 6167 653d 4e6f 6e65 2c0a 2020  message=None,.  ",
            "+0001d790: 2020 6175 7468 6f72 3d4e 6f6e 652c 0a20    author=None,. ",
            "+0001d7a0: 2020 2063 6f6d 6d69 7474 6572 3d4e 6f6e     committer=Non",
            "+0001d7b0: 652c 0a29 3a0a 2020 2020 2222 224d 6572  e,.):.    \"\"\"Mer",
            "+0001d7c0: 6765 2061 2063 6f6d 6d69 7420 696e 746f  ge a commit into",
            "+0001d7d0: 2074 6865 2063 7572 7265 6e74 2062 7261   the current bra",
            "+0001d7e0: 6e63 682e 0a0a 2020 2020 4172 6773 3a0a  nch...    Args:.",
            "+0001d7f0: 2020 2020 2020 7265 706f 3a20 5265 706f        repo: Repo",
            "+0001d800: 7369 746f 7279 2074 6f20 6d65 7267 6520  sitory to merge ",
            "+0001d810: 696e 746f 0a20 2020 2020 2063 6f6d 6d69  into.      commi",
            "+0001d820: 7474 6973 683a 2043 6f6d 6d69 7420 746f  ttish: Commit to",
            "+0001d830: 206d 6572 6765 0a20 2020 2020 206e 6f5f   merge.      no_",
            "+0001d840: 636f 6d6d 6974 3a20 4966 2054 7275 652c  commit: If True,",
            "+0001d850: 2064 6f20 6e6f 7420 6372 6561 7465 2061   do not create a",
            "+0001d860: 206d 6572 6765 2063 6f6d 6d69 740a 2020   merge commit.  ",
            "+0001d870: 2020 2020 6e6f 5f66 663a 2049 6620 5472      no_ff: If Tr",
            "+0001d880: 7565 2c20 666f 7263 6520 6372 6561 7469  ue, force creati",
            "+0001d890: 6f6e 206f 6620 6120 6d65 7267 6520 636f  on of a merge co",
            "+0001d8a0: 6d6d 6974 0a20 2020 2020 206d 6573 7361  mmit.      messa",
            "+0001d8b0: 6765 3a20 4f70 7469 6f6e 616c 206d 6572  ge: Optional mer",
            "+0001d8c0: 6765 2063 6f6d 6d69 7420 6d65 7373 6167  ge commit messag",
            "+0001d8d0: 650a 2020 2020 2020 6175 7468 6f72 3a20  e.      author: ",
            "+0001d8e0: 4f70 7469 6f6e 616c 2061 7574 686f 7220  Optional author ",
            "+0001d8f0: 666f 7220 6d65 7267 6520 636f 6d6d 6974  for merge commit",
            "+0001d900: 0a20 2020 2020 2063 6f6d 6d69 7474 6572  .      committer",
            "+0001d910: 3a20 4f70 7469 6f6e 616c 2063 6f6d 6d69  : Optional commi",
            "+0001d920: 7474 6572 2066 6f72 206d 6572 6765 2063  tter for merge c",
            "+0001d930: 6f6d 6d69 740a 0a20 2020 2052 6574 7572  ommit..    Retur",
            "+0001d940: 6e73 3a0a 2020 2020 2020 5475 706c 6520  ns:.      Tuple ",
            "+0001d950: 6f66 2028 6d65 7267 655f 636f 6d6d 6974  of (merge_commit",
            "+0001d960: 5f73 6861 2c20 636f 6e66 6c69 6374 7329  _sha, conflicts)",
            "+0001d970: 2077 6865 7265 206d 6572 6765 5f63 6f6d   where merge_com",
            "+0001d980: 6d69 745f 7368 6120 6973 204e 6f6e 650a  mit_sha is None.",
            "+0001d990: 2020 2020 2020 6966 206e 6f5f 636f 6d6d        if no_comm",
            "+0001d9a0: 6974 3d54 7275 6520 6f72 2074 6865 7265  it=True or there",
            "+0001d9b0: 2077 6572 6520 636f 6e66 6c69 6374 730a   were conflicts.",
            "+0001d9c0: 0a20 2020 2052 6169 7365 733a 0a20 2020  .    Raises:.   ",
            "+0001d9d0: 2020 2045 7272 6f72 3a20 4966 2074 6865     Error: If the",
            "+0001d9e0: 7265 2069 7320 6e6f 2048 4541 4420 7265  re is no HEAD re",
            "+0001d9f0: 6665 7265 6e63 6520 6f72 2063 6f6d 6d69  ference or commi",
            "+0001da00: 7420 6361 6e6e 6f74 2062 6520 666f 756e  t cannot be foun",
            "+0001da10: 640a 2020 2020 2222 220a 2020 2020 7769  d.    \"\"\".    wi",
            "+0001da20: 7468 206f 7065 6e5f 7265 706f 5f63 6c6f  th open_repo_clo",
            "+0001da30: 7369 6e67 2872 6570 6f29 2061 7320 723a  sing(repo) as r:",
            "+0001da40: 0a20 2020 2020 2020 2023 2050 6172 7365  .        # Parse",
            "+0001da50: 2074 6865 2063 6f6d 6d69 7420 746f 206d   the commit to m",
            "+0001da60: 6572 6765 0a20 2020 2020 2020 2074 7279  erge.        try",
            "+0001da70: 3a0a 2020 2020 2020 2020 2020 2020 6d65  :.            me",
            "+0001da80: 7267 655f 636f 6d6d 6974 5f69 6420 3d20  rge_commit_id = ",
            "+0001da90: 7061 7273 655f 636f 6d6d 6974 2872 2c20  parse_commit(r, ",
            "+0001daa0: 636f 6d6d 6974 7469 7368 292e 6964 0a20  committish).id. ",
            "+0001dab0: 2020 2020 2020 2065 7863 6570 7420 4b65         except Ke",
            "+0001dac0: 7945 7272 6f72 3a0a 2020 2020 2020 2020  yError:.        ",
            "+0001dad0: 2020 2020 7261 6973 6520 4572 726f 7228      raise Error(",
            "+0001dae0: 6622 4361 6e6e 6f74 2066 696e 6420 636f  f\"Cannot find co",
            "+0001daf0: 6d6d 6974 2027 7b63 6f6d 6d69 7474 6973  mmit '{committis",
            "+0001db00: 687d 2722 290a 0a20 2020 2020 2020 2072  h}'\")..        r",
            "+0001db10: 6573 756c 7420 3d20 5f64 6f5f 6d65 7267  esult = _do_merg",
            "+0001db20: 6528 0a20 2020 2020 2020 2020 2020 2072  e(.            r",
            "+0001db30: 2c20 6d65 7267 655f 636f 6d6d 6974 5f69  , merge_commit_i",
            "+0001db40: 642c 206e 6f5f 636f 6d6d 6974 2c20 6e6f  d, no_commit, no",
            "+0001db50: 5f66 662c 206d 6573 7361 6765 2c20 6175  _ff, message, au",
            "+0001db60: 7468 6f72 2c20 636f 6d6d 6974 7465 720a  thor, committer.",
            "+0001db70: 2020 2020 2020 2020 290a 0a20 2020 2020          )..     ",
            "+0001db80: 2020 2023 2054 7269 6767 6572 2061 7574     # Trigger aut",
            "+0001db90: 6f20 4743 2069 6620 6e65 6564 6564 0a20  o GC if needed. ",
            "+0001dba0: 2020 2020 2020 2066 726f 6d20 2e67 6320         from .gc ",
            "+0001dbb0: 696d 706f 7274 206d 6179 6265 5f61 7574  import maybe_aut",
            "+0001dbc0: 6f5f 6763 0a0a 2020 2020 2020 2020 6d61  o_gc..        ma",
            "+0001dbd0: 7962 655f 6175 746f 5f67 6328 7229 0a0a  ybe_auto_gc(r)..",
            "+0001dbe0: 2020 2020 2020 2020 7265 7475 726e 2072          return r",
            "+0001dbf0: 6573 756c 740a 0a0a 6465 6620 756e 7061  esult...def unpa",
            "+0001dc00: 636b 5f6f 626a 6563 7473 2870 6163 6b5f  ck_objects(pack_",
            "+0001dc10: 7061 7468 2c20 7461 7267 6574 3d22 2e22  path, target=\".\"",
            "+0001dc20: 293a 0a20 2020 2022 2222 556e 7061 636b  ):.    \"\"\"Unpack",
            "+0001dc30: 206f 626a 6563 7473 2066 726f 6d20 6120   objects from a ",
            "+0001dc40: 7061 636b 2066 696c 6520 696e 746f 2074  pack file into t",
            "+0001dc50: 6865 2072 6570 6f73 6974 6f72 792e 0a0a  he repository...",
            "+0001dc60: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+0001dc70: 7061 636b 5f70 6174 683a 2050 6174 6820  pack_path: Path ",
            "+0001dc80: 746f 2074 6865 2070 6163 6b20 6669 6c65  to the pack file",
            "+0001dc90: 2074 6f20 756e 7061 636b 0a20 2020 2020   to unpack.     ",
            "+0001dca0: 2074 6172 6765 743a 2050 6174 6820 746f   target: Path to",
            "+0001dcb0: 2074 6865 2072 6570 6f73 6974 6f72 7920   the repository ",
            "+0001dcc0: 746f 2075 6e70 6163 6b20 696e 746f 0a0a  to unpack into..",
            "+0001dcd0: 2020 2020 5265 7475 726e 733a 0a20 2020      Returns:.   ",
            "+0001dce0: 2020 204e 756d 6265 7220 6f66 206f 626a     Number of obj",
            "+0001dcf0: 6563 7473 2075 6e70 6163 6b65 640a 2020  ects unpacked.  ",
            "+0001dd00: 2020 2222 220a 2020 2020 6672 6f6d 202e    \"\"\".    from .",
            "+0001dd10: 7061 636b 2069 6d70 6f72 7420 5061 636b  pack import Pack",
            "+0001dd20: 0a0a 2020 2020 7769 7468 206f 7065 6e5f  ..    with open_",
            "+0001dd30: 7265 706f 5f63 6c6f 7369 6e67 2874 6172  repo_closing(tar",
            "+0001dd40: 6765 7429 2061 7320 723a 0a20 2020 2020  get) as r:.     ",
            "+0001dd50: 2020 2070 6163 6b5f 6261 7365 6e61 6d65     pack_basename",
            "+0001dd60: 203d 206f 732e 7061 7468 2e73 706c 6974   = os.path.split",
            "+0001dd70: 6578 7428 7061 636b 5f70 6174 6829 5b30  ext(pack_path)[0",
            "+0001dd80: 5d0a 2020 2020 2020 2020 7769 7468 2050  ].        with P",
            "+0001dd90: 6163 6b28 7061 636b 5f62 6173 656e 616d  ack(pack_basenam",
            "+0001dda0: 6529 2061 7320 7061 636b 3a0a 2020 2020  e) as pack:.    ",
            "+0001ddb0: 2020 2020 2020 2020 636f 756e 7420 3d20          count = ",
            "+0001ddc0: 300a 2020 2020 2020 2020 2020 2020 666f  0.            fo",
            "+0001ddd0: 7220 756e 7061 636b 6564 2069 6e20 7061  r unpacked in pa",
            "+0001dde0: 636b 2e69 7465 725f 756e 7061 636b 6564  ck.iter_unpacked",
            "+0001ddf0: 2829 3a0a 2020 2020 2020 2020 2020 2020  ():.            ",
            "+0001de00: 2020 2020 6f62 6a20 3d20 756e 7061 636b      obj = unpack",
            "+0001de10: 6564 2e73 6861 5f66 696c 6528 290a 2020  ed.sha_file().  ",
            "+0001de20: 2020 2020 2020 2020 2020 2020 2020 722e                r.",
            "+0001de30: 6f62 6a65 6374 5f73 746f 7265 2e61 6464  object_store.add",
            "+0001de40: 5f6f 626a 6563 7428 6f62 6a29 0a20 2020  _object(obj).   ",
            "+0001de50: 2020 2020 2020 2020 2020 2020 2063 6f75               cou",
            "+0001de60: 6e74 202b 3d20 310a 2020 2020 2020 2020  nt += 1.        ",
            "+0001de70: 2020 2020 7265 7475 726e 2063 6f75 6e74      return count",
            "+0001de80: 0a0a 0a64 6566 206d 6572 6765 5f74 7265  ...def merge_tre",
            "+0001de90: 6528 7265 706f 2c20 6261 7365 5f74 7265  e(repo, base_tre",
            "+0001dea0: 652c 206f 7572 5f74 7265 652c 2074 6865  e, our_tree, the",
            "+0001deb0: 6972 5f74 7265 6529 3a0a 2020 2020 2222  ir_tree):.    \"\"",
            "+0001dec0: 2250 6572 666f 726d 2061 2074 6872 6565  \"Perform a three",
            "+0001ded0: 2d77 6179 2074 7265 6520 6d65 7267 6520  -way tree merge ",
            "+0001dee0: 7769 7468 6f75 7420 746f 7563 6869 6e67  without touching",
            "+0001def0: 2074 6865 2077 6f72 6b69 6e67 2064 6972   the working dir",
            "+0001df00: 6563 746f 7279 2e0a 0a20 2020 2054 6869  ectory...    Thi",
            "+0001df10: 7320 6973 2073 696d 696c 6172 2074 6f20  s is similar to ",
            "+0001df20: 6769 7420 6d65 7267 652d 7472 6565 2c20  git merge-tree, ",
            "+0001df30: 7065 7266 6f72 6d69 6e67 2061 206d 6572  performing a mer",
            "+0001df40: 6765 2061 7420 7468 6520 7472 6565 206c  ge at the tree l",
            "+0001df50: 6576 656c 0a20 2020 2077 6974 686f 7574  evel.    without",
            "+0001df60: 2063 7265 6174 696e 6720 636f 6d6d 6974   creating commit",
            "+0001df70: 7320 6f72 2075 7064 6174 696e 6720 616e  s or updating an",
            "+0001df80: 7920 7265 6665 7265 6e63 6573 2e0a 0a20  y references... ",
            "+0001df90: 2020 2041 7267 733a 0a20 2020 2020 2072     Args:.      r",
            "+0001dfa0: 6570 6f3a 2052 6570 6f73 6974 6f72 7920  epo: Repository ",
            "+0001dfb0: 636f 6e74 6169 6e69 6e67 2074 6865 2074  containing the t",
            "+0001dfc0: 7265 6573 0a20 2020 2020 2062 6173 655f  rees.      base_",
            "+0001dfd0: 7472 6565 3a20 5472 6565 2d69 7368 206f  tree: Tree-ish o",
            "+0001dfe0: 6620 7468 6520 636f 6d6d 6f6e 2061 6e63  f the common anc",
            "+0001dff0: 6573 746f 7220 286f 7220 4e6f 6e65 2066  estor (or None f",
            "+0001e000: 6f72 206e 6f20 636f 6d6d 6f6e 2061 6e63  or no common anc",
            "+0001e010: 6573 746f 7229 0a20 2020 2020 206f 7572  estor).      our",
            "+0001e020: 5f74 7265 653a 2054 7265 652d 6973 6820  _tree: Tree-ish ",
            "+0001e030: 6f66 206f 7572 2073 6964 6520 6f66 2074  of our side of t",
            "+0001e040: 6865 206d 6572 6765 0a20 2020 2020 2074  he merge.      t",
            "+0001e050: 6865 6972 5f74 7265 653a 2054 7265 652d  heir_tree: Tree-",
            "+0001e060: 6973 6820 6f66 2074 6865 6972 2073 6964  ish of their sid",
            "+0001e070: 6520 6f66 2074 6865 206d 6572 6765 0a0a  e of the merge..",
            "+0001e080: 2020 2020 5265 7475 726e 733a 0a20 2020      Returns:.   ",
            "+0001e090: 2020 2074 7570 6c65 3a20 4120 7475 706c     tuple: A tupl",
            "+0001e0a0: 6520 6f66 2028 6d65 7267 6564 5f74 7265  e of (merged_tre",
            "+0001e0b0: 655f 6964 2c20 636f 6e66 6c69 6374 7329  e_id, conflicts)",
            "+0001e0c0: 2077 6865 7265 3a0a 2020 2020 2020 2020   where:.        ",
            "+0001e0d0: 2d20 6d65 7267 6564 5f74 7265 655f 6964  - merged_tree_id",
            "+0001e0e0: 2069 7320 7468 6520 5348 412d 3120 6f66   is the SHA-1 of",
            "+0001e0f0: 2074 6865 206d 6572 6765 6420 7472 6565   the merged tree",
            "+0001e100: 0a20 2020 2020 2020 202d 2063 6f6e 666c  .        - confl",
            "+0001e110: 6963 7473 2069 7320 6120 6c69 7374 206f  icts is a list o",
            "+0001e120: 6620 7061 7468 7320 2861 7320 6279 7465  f paths (as byte",
            "+0001e130: 7329 2074 6861 7420 6861 6420 636f 6e66  s) that had conf",
            "+0001e140: 6c69 6374 730a 0a20 2020 2052 6169 7365  licts..    Raise",
            "+0001e150: 733a 0a20 2020 2020 204b 6579 4572 726f  s:.      KeyErro",
            "+0001e160: 723a 2049 6620 616e 7920 6f66 2074 6865  r: If any of the",
            "+0001e170: 2074 7265 652d 6973 6820 6172 6775 6d65   tree-ish argume",
            "+0001e180: 6e74 7320 6361 6e6e 6f74 2062 6520 7265  nts cannot be re",
            "+0001e190: 736f 6c76 6564 0a20 2020 2022 2222 0a20  solved.    \"\"\". ",
            "+0001e1a0: 2020 2066 726f 6d20 2e6d 6572 6765 2069     from .merge i",
            "+0001e1b0: 6d70 6f72 7420 4d65 7267 6572 0a0a 2020  mport Merger..  ",
            "+0001e1c0: 2020 7769 7468 206f 7065 6e5f 7265 706f    with open_repo",
            "+0001e1d0: 5f63 6c6f 7369 6e67 2872 6570 6f29 2061  _closing(repo) a",
            "+0001e1e0: 7320 723a 0a20 2020 2020 2020 2023 2052  s r:.        # R",
            "+0001e1f0: 6573 6f6c 7665 2074 7265 652d 6973 6820  esolve tree-ish ",
            "+0001e200: 6172 6775 6d65 6e74 7320 746f 2061 6374  arguments to act",
            "+0001e210: 7561 6c20 7472 6565 730a 2020 2020 2020  ual trees.      ",
            "+0001e220: 2020 6261 7365 203d 2070 6172 7365 5f74    base = parse_t",
            "+0001e230: 7265 6528 722c 2062 6173 655f 7472 6565  ree(r, base_tree",
            "+0001e240: 2920 6966 2062 6173 655f 7472 6565 2065  ) if base_tree e",
            "+0001e250: 6c73 6520 4e6f 6e65 0a20 2020 2020 2020  lse None.       ",
            "+0001e260: 206f 7572 7320 3d20 7061 7273 655f 7472   ours = parse_tr",
            "+0001e270: 6565 2872 2c20 6f75 725f 7472 6565 290a  ee(r, our_tree).",
            "+0001e280: 2020 2020 2020 2020 7468 6569 7273 203d          theirs =",
            "+0001e290: 2070 6172 7365 5f74 7265 6528 722c 2074   parse_tree(r, t",
            "+0001e2a0: 6865 6972 5f74 7265 6529 0a0a 2020 2020  heir_tree)..    ",
            "+0001e2b0: 2020 2020 2320 5065 7266 6f72 6d20 7468      # Perform th",
            "+0001e2c0: 6520 6d65 7267 650a 2020 2020 2020 2020  e merge.        ",
            "+0001e2d0: 6d65 7267 6572 203d 204d 6572 6765 7228  merger = Merger(",
            "+0001e2e0: 722e 6f62 6a65 6374 5f73 746f 7265 290a  r.object_store).",
            "+0001e2f0: 2020 2020 2020 2020 6d65 7267 6564 5f74          merged_t",
            "+0001e300: 7265 652c 2063 6f6e 666c 6963 7473 203d  ree, conflicts =",
            "+0001e310: 206d 6572 6765 722e 6d65 7267 655f 7472   merger.merge_tr",
            "+0001e320: 6565 7328 6261 7365 2c20 6f75 7273 2c20  ees(base, ours, ",
            "+0001e330: 7468 6569 7273 290a 0a20 2020 2020 2020  theirs)..       ",
            "+0001e340: 2023 2041 6464 2074 6865 206d 6572 6765   # Add the merge",
            "+0001e350: 6420 7472 6565 2074 6f20 7468 6520 6f62  d tree to the ob",
            "+0001e360: 6a65 6374 2073 746f 7265 0a20 2020 2020  ject store.     ",
            "+0001e370: 2020 2072 2e6f 626a 6563 745f 7374 6f72     r.object_stor",
            "+0001e380: 652e 6164 645f 6f62 6a65 6374 286d 6572  e.add_object(mer",
            "+0001e390: 6765 645f 7472 6565 290a 0a20 2020 2020  ged_tree)..     ",
            "+0001e3a0: 2020 2072 6574 7572 6e20 6d65 7267 6564     return merged",
            "+0001e3b0: 5f74 7265 652e 6964 2c20 636f 6e66 6c69  _tree.id, confli",
            "+0001e3c0: 6374 730a 0a0a 6465 6620 6368 6572 7279  cts...def cherry",
            "+0001e3d0: 5f70 6963 6b28 0a20 2020 2072 6570 6f2c  _pick(.    repo,",
            "+0001e3e0: 0a20 2020 2063 6f6d 6d69 7474 6973 682c  .    committish,",
            "+0001e3f0: 0a20 2020 206e 6f5f 636f 6d6d 6974 3d46  .    no_commit=F",
            "+0001e400: 616c 7365 2c0a 2020 2020 636f 6e74 696e  alse,.    contin",
            "+0001e410: 7565 5f3d 4661 6c73 652c 0a20 2020 2061  ue_=False,.    a",
            "+0001e420: 626f 7274 3d46 616c 7365 2c0a 293a 0a20  bort=False,.):. ",
            "+0001e430: 2020 2072 2222 2243 6865 7272 792d 7069     r\"\"\"Cherry-pi",
            "+0001e440: 636b 2061 2063 6f6d 6d69 7420 6f6e 746f  ck a commit onto",
            "+0001e450: 2074 6865 2063 7572 7265 6e74 2062 7261   the current bra",
            "+0001e460: 6e63 682e 0a0a 2020 2020 4172 6773 3a0a  nch...    Args:.",
            "+0001e470: 2020 2020 2020 7265 706f 3a20 5265 706f        repo: Repo",
            "+0001e480: 7369 746f 7279 2074 6f20 6368 6572 7279  sitory to cherry",
            "+0001e490: 2d70 6963 6b20 696e 746f 0a20 2020 2020  -pick into.     ",
            "+0001e4a0: 2063 6f6d 6d69 7474 6973 683a 2043 6f6d   committish: Com",
            "+0001e4b0: 6d69 7420 746f 2063 6865 7272 792d 7069  mit to cherry-pi",
            "+0001e4c0: 636b 0a20 2020 2020 206e 6f5f 636f 6d6d  ck.      no_comm",
            "+0001e4d0: 6974 3a20 4966 2054 7275 652c 2064 6f20  it: If True, do ",
            "+0001e4e0: 6e6f 7420 6372 6561 7465 2061 2063 6f6d  not create a com",
            "+0001e4f0: 6d69 7420 6166 7465 7220 6170 706c 7969  mit after applyi",
            "+0001e500: 6e67 2063 6861 6e67 6573 0a20 2020 2020  ng changes.     ",
            "+0001e510: 2063 6f6e 7469 6e75 655c 5f3a 2043 6f6e   continue\\_: Con",
            "+0001e520: 7469 6e75 6520 616e 2069 6e2d 7072 6f67  tinue an in-prog",
            "+0001e530: 7265 7373 2063 6865 7272 792d 7069 636b  ress cherry-pick",
            "+0001e540: 2061 6674 6572 2072 6573 6f6c 7669 6e67   after resolving",
            "+0001e550: 2063 6f6e 666c 6963 7473 0a20 2020 2020   conflicts.     ",
            "+0001e560: 2061 626f 7274 3a20 4162 6f72 7420 616e   abort: Abort an",
            "+0001e570: 2069 6e2d 7072 6f67 7265 7373 2063 6865   in-progress che",
            "+0001e580: 7272 792d 7069 636b 0a0a 2020 2020 5265  rry-pick..    Re",
            "+0001e590: 7475 726e 733a 0a20 2020 2020 2054 6865  turns:.      The",
            "+0001e5a0: 2053 4841 206f 6620 7468 6520 6e65 776c   SHA of the newl",
            "+0001e5b0: 7920 6372 6561 7465 6420 636f 6d6d 6974  y created commit",
            "+0001e5c0: 2c20 6f72 204e 6f6e 6520 6966 206e 6f5f  , or None if no_",
            "+0001e5d0: 636f 6d6d 6974 3d54 7275 6520 6f72 2074  commit=True or t",
            "+0001e5e0: 6865 7265 2077 6572 6520 636f 6e66 6c69  here were confli",
            "+0001e5f0: 6374 730a 0a20 2020 2052 6169 7365 733a  cts..    Raises:",
            "+0001e600: 0a20 2020 2020 2045 7272 6f72 3a20 4966  .      Error: If",
            "+0001e610: 2074 6865 7265 2069 7320 6e6f 2048 4541   there is no HEA",
            "+0001e620: 4420 7265 6665 7265 6e63 652c 2063 6f6d  D reference, com",
            "+0001e630: 6d69 7420 6361 6e6e 6f74 2062 6520 666f  mit cannot be fo",
            "+0001e640: 756e 642c 206f 7220 6f70 6572 6174 696f  und, or operatio",
            "+0001e650: 6e20 6661 696c 730a 2020 2020 2222 220a  n fails.    \"\"\".",
            "+0001e660: 2020 2020 6672 6f6d 202e 6d65 7267 6520      from .merge ",
            "+0001e670: 696d 706f 7274 2074 6872 6565 5f77 6179  import three_way",
            "+0001e680: 5f6d 6572 6765 0a0a 2020 2020 7769 7468  _merge..    with",
            "+0001e690: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "+0001e6a0: 6e67 2872 6570 6f29 2061 7320 723a 0a20  ng(repo) as r:. ",
            "+0001e6b0: 2020 2020 2020 2023 2048 616e 646c 6520         # Handle ",
            "+0001e6c0: 6162 6f72 740a 2020 2020 2020 2020 6966  abort.        if",
            "+0001e6d0: 2061 626f 7274 3a0a 2020 2020 2020 2020   abort:.        ",
            "+0001e6e0: 2020 2020 2320 436c 6561 6e20 7570 2061      # Clean up a",
            "+0001e6f0: 6e79 2063 6865 7272 792d 7069 636b 2073  ny cherry-pick s",
            "+0001e700: 7461 7465 0a20 2020 2020 2020 2020 2020  tate.           ",
            "+0001e710: 2074 7279 3a0a 2020 2020 2020 2020 2020   try:.          ",
            "+0001e720: 2020 2020 2020 6f73 2e72 656d 6f76 6528        os.remove(",
            "+0001e730: 6f73 2e70 6174 682e 6a6f 696e 2872 2e63  os.path.join(r.c",
            "+0001e740: 6f6e 7472 6f6c 6469 7228 292c 2022 4348  ontroldir(), \"CH",
            "+0001e750: 4552 5259 5f50 4943 4b5f 4845 4144 2229  ERRY_PICK_HEAD\")",
            "+0001e760: 290a 2020 2020 2020 2020 2020 2020 6578  ).            ex",
            "+0001e770: 6365 7074 2046 696c 654e 6f74 466f 756e  cept FileNotFoun",
            "+0001e780: 6445 7272 6f72 3a0a 2020 2020 2020 2020  dError:.        ",
            "+0001e790: 2020 2020 2020 2020 7061 7373 0a20 2020          pass.   ",
            "+0001e7a0: 2020 2020 2020 2020 2074 7279 3a0a 2020           try:.  ",
            "+0001e7b0: 2020 2020 2020 2020 2020 2020 2020 6f73                os",
            "+0001e7c0: 2e72 656d 6f76 6528 6f73 2e70 6174 682e  .remove(os.path.",
            "+0001e7d0: 6a6f 696e 2872 2e63 6f6e 7472 6f6c 6469  join(r.controldi",
            "+0001e7e0: 7228 292c 2022 4d45 5247 455f 4d53 4722  r(), \"MERGE_MSG\"",
            "+0001e7f0: 2929 0a20 2020 2020 2020 2020 2020 2065  )).            e",
            "+0001e800: 7863 6570 7420 4669 6c65 4e6f 7446 6f75  xcept FileNotFou",
            "+0001e810: 6e64 4572 726f 723a 0a20 2020 2020 2020  ndError:.       ",
            "+0001e820: 2020 2020 2020 2020 2070 6173 730a 2020           pass.  ",
            "+0001e830: 2020 2020 2020 2020 2020 2320 5265 7365            # Rese",
            "+0001e840: 7420 696e 6465 7820 746f 2048 4541 440a  t index to HEAD.",
            "+0001e850: 2020 2020 2020 2020 2020 2020 722e 7265              r.re",
            "+0001e860: 7365 745f 696e 6465 7828 725b 6222 4845  set_index(r[b\"HE",
            "+0001e870: 4144 225d 2e74 7265 6529 0a20 2020 2020  AD\"].tree).     ",
            "+0001e880: 2020 2020 2020 2072 6574 7572 6e20 4e6f         return No",
            "+0001e890: 6e65 0a0a 2020 2020 2020 2020 2320 4861  ne..        # Ha",
            "+0001e8a0: 6e64 6c65 2063 6f6e 7469 6e75 650a 2020  ndle continue.  ",
            "+0001e8b0: 2020 2020 2020 6966 2063 6f6e 7469 6e75        if continu",
            "+0001e8c0: 655f 3a0a 2020 2020 2020 2020 2020 2020  e_:.            ",
            "+0001e8d0: 2320 4368 6563 6b20 6966 2074 6865 7265  # Check if there",
            "+0001e8e0: 2773 2061 2063 6865 7272 792d 7069 636b  's a cherry-pick",
            "+0001e8f0: 2069 6e20 7072 6f67 7265 7373 0a20 2020   in progress.   ",
            "+0001e900: 2020 2020 2020 2020 2063 6865 7272 795f           cherry_",
            "+0001e910: 7069 636b 5f68 6561 645f 7061 7468 203d  pick_head_path =",
            "+0001e920: 206f 732e 7061 7468 2e6a 6f69 6e28 722e   os.path.join(r.",
            "+0001e930: 636f 6e74 726f 6c64 6972 2829 2c20 2243  controldir(), \"C",
            "+0001e940: 4845 5252 595f 5049 434b 5f48 4541 4422  HERRY_PICK_HEAD\"",
            "+0001e950: 290a 2020 2020 2020 2020 2020 2020 7472  ).            tr",
            "+0001e960: 793a 0a20 2020 2020 2020 2020 2020 2020  y:.             ",
            "+0001e970: 2020 2077 6974 6820 6f70 656e 2863 6865     with open(che",
            "+0001e980: 7272 795f 7069 636b 5f68 6561 645f 7061  rry_pick_head_pa",
            "+0001e990: 7468 2c20 2272 6222 2920 6173 2066 3a0a  th, \"rb\") as f:.",
            "+0001e9a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+0001e9b0: 2020 2020 6368 6572 7279 5f70 6963 6b5f      cherry_pick_",
            "+0001e9c0: 636f 6d6d 6974 5f69 6420 3d20 662e 7265  commit_id = f.re",
            "+0001e9d0: 6164 2829 2e73 7472 6970 2829 0a20 2020  ad().strip().   ",
            "+0001e9e0: 2020 2020 2020 2020 2020 2020 2063 6865               che",
            "+0001e9f0: 7272 795f 7069 636b 5f63 6f6d 6d69 7420  rry_pick_commit ",
            "+0001ea00: 3d20 725b 6368 6572 7279 5f70 6963 6b5f  = r[cherry_pick_",
            "+0001ea10: 636f 6d6d 6974 5f69 645d 0a20 2020 2020  commit_id].     ",
            "+0001ea20: 2020 2020 2020 2065 7863 6570 7420 4669         except Fi",
            "+0001ea30: 6c65 4e6f 7446 6f75 6e64 4572 726f 723a  leNotFoundError:",
            "+0001ea40: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0001ea50: 2072 6169 7365 2045 7272 6f72 2822 4e6f   raise Error(\"No",
            "+0001ea60: 2063 6865 7272 792d 7069 636b 2069 6e20   cherry-pick in ",
            "+0001ea70: 7072 6f67 7265 7373 2229 0a0a 2020 2020  progress\")..    ",
            "+0001ea80: 2020 2020 2020 2020 2320 4368 6563 6b20          # Check ",
            "+0001ea90: 666f 7220 756e 7265 736f 6c76 6564 2063  for unresolved c",
            "+0001eaa0: 6f6e 666c 6963 7473 0a20 2020 2020 2020  onflicts.       ",
            "+0001eab0: 2020 2020 2063 6f6e 666c 6963 7473 203d       conflicts =",
            "+0001eac0: 206c 6973 7428 722e 6f70 656e 5f69 6e64   list(r.open_ind",
            "+0001ead0: 6578 2829 2e63 6f6e 666c 6963 7473 2829  ex().conflicts()",
            "+0001eae0: 290a 2020 2020 2020 2020 2020 2020 6966  ).            if",
            "+0001eaf0: 2063 6f6e 666c 6963 7473 3a0a 2020 2020   conflicts:.    ",
            "+0001eb00: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+0001eb10: 6520 4572 726f 7228 2255 6e72 6573 6f6c  e Error(\"Unresol",
            "+0001eb20: 7665 6420 636f 6e66 6c69 6374 7320 7265  ved conflicts re",
            "+0001eb30: 6d61 696e 2229 0a0a 2020 2020 2020 2020  main\")..        ",
            "+0001eb40: 2020 2020 2320 4372 6561 7465 2074 6865      # Create the",
            "+0001eb50: 2063 6f6d 6d69 740a 2020 2020 2020 2020   commit.        ",
            "+0001eb60: 2020 2020 7472 6565 5f69 6420 3d20 722e      tree_id = r.",
            "+0001eb70: 6f70 656e 5f69 6e64 6578 2829 2e63 6f6d  open_index().com",
            "+0001eb80: 6d69 7428 722e 6f62 6a65 6374 5f73 746f  mit(r.object_sto",
            "+0001eb90: 7265 290a 0a20 2020 2020 2020 2020 2020  re)..           ",
            "+0001eba0: 2023 2052 6561 6420 7361 7665 6420 6d65   # Read saved me",
            "+0001ebb0: 7373 6167 6520 6966 2061 6e79 0a20 2020  ssage if any.   ",
            "+0001ebc0: 2020 2020 2020 2020 206d 6572 6765 5f6d           merge_m",
            "+0001ebd0: 7367 5f70 6174 6820 3d20 6f73 2e70 6174  sg_path = os.pat",
            "+0001ebe0: 682e 6a6f 696e 2872 2e63 6f6e 7472 6f6c  h.join(r.control",
            "+0001ebf0: 6469 7228 292c 2022 4d45 5247 455f 4d53  dir(), \"MERGE_MS",
            "+0001ec00: 4722 290a 2020 2020 2020 2020 2020 2020  G\").            ",
            "+0001ec10: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           ",
            "+0001ec20: 2020 2020 2077 6974 6820 6f70 656e 286d       with open(m",
            "+0001ec30: 6572 6765 5f6d 7367 5f70 6174 682c 2022  erge_msg_path, \"",
            "+0001ec40: 7262 2229 2061 7320 663a 0a20 2020 2020  rb\") as f:.     ",
            "+0001ec50: 2020 2020 2020 2020 2020 2020 2020 206d                 m",
            "+0001ec60: 6573 7361 6765 203d 2066 2e72 6561 6428  essage = f.read(",
            "+0001ec70: 290a 2020 2020 2020 2020 2020 2020 6578  ).            ex",
            "+0001ec80: 6365 7074 2046 696c 654e 6f74 466f 756e  cept FileNotFoun",
            "+0001ec90: 6445 7272 6f72 3a0a 2020 2020 2020 2020  dError:.        ",
            "+0001eca0: 2020 2020 2020 2020 6d65 7373 6167 6520          message ",
            "+0001ecb0: 3d20 6368 6572 7279 5f70 6963 6b5f 636f  = cherry_pick_co",
            "+0001ecc0: 6d6d 6974 2e6d 6573 7361 6765 0a0a 2020  mmit.message..  ",
            "+0001ecd0: 2020 2020 2020 2020 2020 6e65 775f 636f            new_co",
            "+0001ece0: 6d6d 6974 203d 2072 2e64 6f5f 636f 6d6d  mmit = r.do_comm",
            "+0001ecf0: 6974 280a 2020 2020 2020 2020 2020 2020  it(.            ",
            "+0001ed00: 2020 2020 6d65 7373 6167 653d 6d65 7373      message=mess",
            "+0001ed10: 6167 652c 0a20 2020 2020 2020 2020 2020  age,.           ",
            "+0001ed20: 2020 2020 2074 7265 653d 7472 6565 5f69       tree=tree_i",
            "+0001ed30: 642c 0a20 2020 2020 2020 2020 2020 2020  d,.             ",
            "+0001ed40: 2020 2061 7574 686f 723d 6368 6572 7279     author=cherry",
            "+0001ed50: 5f70 6963 6b5f 636f 6d6d 6974 2e61 7574  _pick_commit.aut",
            "+0001ed60: 686f 722c 0a20 2020 2020 2020 2020 2020  hor,.           ",
            "+0001ed70: 2020 2020 2061 7574 686f 725f 7469 6d65       author_time",
            "+0001ed80: 7374 616d 703d 6368 6572 7279 5f70 6963  stamp=cherry_pic",
            "+0001ed90: 6b5f 636f 6d6d 6974 2e61 7574 686f 725f  k_commit.author_",
            "+0001eda0: 7469 6d65 2c0a 2020 2020 2020 2020 2020  time,.          ",
            "+0001edb0: 2020 2020 2020 6175 7468 6f72 5f74 696d        author_tim",
            "+0001edc0: 657a 6f6e 653d 6368 6572 7279 5f70 6963  ezone=cherry_pic",
            "+0001edd0: 6b5f 636f 6d6d 6974 2e61 7574 686f 725f  k_commit.author_",
            "+0001ede0: 7469 6d65 7a6f 6e65 2c0a 2020 2020 2020  timezone,.      ",
            "+0001edf0: 2020 2020 2020 290a 0a20 2020 2020 2020        )..       ",
            "+0001ee00: 2020 2020 2023 2043 6c65 616e 2075 7020       # Clean up ",
            "+0001ee10: 7374 6174 6520 6669 6c65 730a 2020 2020  state files.    ",
            "+0001ee20: 2020 2020 2020 2020 7472 793a 0a20 2020          try:.   ",
            "+0001ee30: 2020 2020 2020 2020 2020 2020 206f 732e               os.",
            "+0001ee40: 7265 6d6f 7665 2863 6865 7272 795f 7069  remove(cherry_pi",
            "+0001ee50: 636b 5f68 6561 645f 7061 7468 290a 2020  ck_head_path).  ",
            "+0001ee60: 2020 2020 2020 2020 2020 6578 6365 7074            except",
            "+0001ee70: 2046 696c 654e 6f74 466f 756e 6445 7272   FileNotFoundErr",
            "+0001ee80: 6f72 3a0a 2020 2020 2020 2020 2020 2020  or:.            ",
            "+0001ee90: 2020 2020 7061 7373 0a20 2020 2020 2020      pass.       ",
            "+0001eea0: 2020 2020 2074 7279 3a0a 2020 2020 2020       try:.      ",
            "+0001eeb0: 2020 2020 2020 2020 2020 6f73 2e72 656d            os.rem",
            "+0001eec0: 6f76 6528 6d65 7267 655f 6d73 675f 7061  ove(merge_msg_pa",
            "+0001eed0: 7468 290a 2020 2020 2020 2020 2020 2020  th).            ",
            "+0001eee0: 6578 6365 7074 2046 696c 654e 6f74 466f  except FileNotFo",
            "+0001eef0: 756e 6445 7272 6f72 3a0a 2020 2020 2020  undError:.      ",
            "+0001ef00: 2020 2020 2020 2020 2020 7061 7373 0a0a            pass..",
            "+0001ef10: 2020 2020 2020 2020 2020 2020 7265 7475              retu",
            "+0001ef20: 726e 206e 6577 5f63 6f6d 6d69 740a 0a20  rn new_commit.. ",
            "+0001ef30: 2020 2020 2020 2023 204e 6f72 6d61 6c20         # Normal ",
            "+0001ef40: 6368 6572 7279 2d70 6963 6b20 6f70 6572  cherry-pick oper",
            "+0001ef50: 6174 696f 6e0a 2020 2020 2020 2020 2320  ation.        # ",
            "+0001ef60: 4765 7420 6375 7272 656e 7420 4845 4144  Get current HEAD",
            "+0001ef70: 0a20 2020 2020 2020 2074 7279 3a0a 2020  .        try:.  ",
            "+0001ef80: 2020 2020 2020 2020 2020 6865 6164 5f63            head_c",
            "+0001ef90: 6f6d 6d69 7420 3d20 725b 6222 4845 4144  ommit = r[b\"HEAD",
            "+0001efa0: 225d 0a20 2020 2020 2020 2065 7863 6570  \"].        excep",
            "+0001efb0: 7420 4b65 7945 7272 6f72 3a0a 2020 2020  t KeyError:.    ",
            "+0001efc0: 2020 2020 2020 2020 7261 6973 6520 4572          raise Er",
            "+0001efd0: 726f 7228 224e 6f20 4845 4144 2072 6566  ror(\"No HEAD ref",
            "+0001efe0: 6572 656e 6365 2066 6f75 6e64 2229 0a0a  erence found\")..",
            "+0001eff0: 2020 2020 2020 2020 2320 5061 7273 6520          # Parse ",
            "+0001f000: 7468 6520 636f 6d6d 6974 2074 6f20 6368  the commit to ch",
            "+0001f010: 6572 7279 2d70 6963 6b0a 2020 2020 2020  erry-pick.      ",
            "+0001f020: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         ",
            "+0001f030: 2020 2063 6865 7272 795f 7069 636b 5f63     cherry_pick_c",
            "+0001f040: 6f6d 6d69 7420 3d20 7061 7273 655f 636f  ommit = parse_co",
            "+0001f050: 6d6d 6974 2872 2c20 636f 6d6d 6974 7469  mmit(r, committi",
            "+0001f060: 7368 290a 2020 2020 2020 2020 6578 6365  sh).        exce",
            "+0001f070: 7074 204b 6579 4572 726f 723a 0a20 2020  pt KeyError:.   ",
            "+0001f080: 2020 2020 2020 2020 2072 6169 7365 2045           raise E",
            "+0001f090: 7272 6f72 2866 2243 616e 6e6f 7420 6669  rror(f\"Cannot fi",
            "+0001f0a0: 6e64 2063 6f6d 6d69 7420 277b 636f 6d6d  nd commit '{comm",
            "+0001f0b0: 6974 7469 7368 7d27 2229 0a0a 2020 2020  ittish}'\")..    ",
            "+0001f0c0: 2020 2020 2320 4368 6563 6b20 6966 2063      # Check if c",
            "+0001f0d0: 6f6d 6d69 7420 6861 7320 7061 7265 6e74  ommit has parent",
            "+0001f0e0: 730a 2020 2020 2020 2020 6966 206e 6f74  s.        if not",
            "+0001f0f0: 2063 6865 7272 795f 7069 636b 5f63 6f6d   cherry_pick_com",
            "+0001f100: 6d69 742e 7061 7265 6e74 733a 0a20 2020  mit.parents:.   ",
            "+0001f110: 2020 2020 2020 2020 2072 6169 7365 2045           raise E",
            "+0001f120: 7272 6f72 2822 4361 6e6e 6f74 2063 6865  rror(\"Cannot che",
            "+0001f130: 7272 792d 7069 636b 2072 6f6f 7420 636f  rry-pick root co",
            "+0001f140: 6d6d 6974 2229 0a0a 2020 2020 2020 2020  mmit\")..        ",
            "+0001f150: 2320 4765 7420 7061 7265 6e74 206f 6620  # Get parent of ",
            "+0001f160: 6368 6572 7279 2d70 6963 6b20 636f 6d6d  cherry-pick comm",
            "+0001f170: 6974 0a20 2020 2020 2020 2070 6172 656e  it.        paren",
            "+0001f180: 745f 636f 6d6d 6974 203d 2072 5b63 6865  t_commit = r[che",
            "+0001f190: 7272 795f 7069 636b 5f63 6f6d 6d69 742e  rry_pick_commit.",
            "+0001f1a0: 7061 7265 6e74 735b 305d 5d0a 0a20 2020  parents[0]]..   ",
            "+0001f1b0: 2020 2020 2023 2050 6572 666f 726d 2074       # Perform t",
            "+0001f1c0: 6872 6565 2d77 6179 206d 6572 6765 0a20  hree-way merge. ",
            "+0001f1d0: 2020 2020 2020 2074 7279 3a0a 2020 2020         try:.    ",
            "+0001f1e0: 2020 2020 2020 2020 6d65 7267 6564 5f74          merged_t",
            "+0001f1f0: 7265 652c 2063 6f6e 666c 6963 7473 203d  ree, conflicts =",
            "+0001f200: 2074 6872 6565 5f77 6179 5f6d 6572 6765   three_way_merge",
            "+0001f210: 280a 2020 2020 2020 2020 2020 2020 2020  (.              ",
            "+0001f220: 2020 722e 6f62 6a65 6374 5f73 746f 7265    r.object_store",
            "+0001f230: 2c20 7061 7265 6e74 5f63 6f6d 6d69 742c  , parent_commit,",
            "+0001f240: 2068 6561 645f 636f 6d6d 6974 2c20 6368   head_commit, ch",
            "+0001f250: 6572 7279 5f70 6963 6b5f 636f 6d6d 6974  erry_pick_commit",
            "+0001f260: 0a20 2020 2020 2020 2020 2020 2029 0a20  .            ). ",
            "+0001f270: 2020 2020 2020 2065 7863 6570 7420 4578         except Ex",
            "+0001f280: 6365 7074 696f 6e20 6173 2065 3a0a 2020  ception as e:.  ",
            "+0001f290: 2020 2020 2020 2020 2020 7261 6973 6520            raise ",
            "+0001f2a0: 4572 726f 7228 6622 4368 6572 7279 2d70  Error(f\"Cherry-p",
            "+0001f2b0: 6963 6b20 6661 696c 6564 3a20 7b65 7d22  ick failed: {e}\"",
            "+0001f2c0: 290a 0a20 2020 2020 2020 2023 2041 6464  )..        # Add",
            "+0001f2d0: 206d 6572 6765 6420 7472 6565 2074 6f20   merged tree to ",
            "+0001f2e0: 6f62 6a65 6374 2073 746f 7265 0a20 2020  object store.   ",
            "+0001f2f0: 2020 2020 2072 2e6f 626a 6563 745f 7374       r.object_st",
            "+0001f300: 6f72 652e 6164 645f 6f62 6a65 6374 286d  ore.add_object(m",
            "+0001f310: 6572 6765 645f 7472 6565 290a 0a20 2020  erged_tree)..   ",
            "+0001f320: 2020 2020 2023 2055 7064 6174 6520 776f       # Update wo",
            "+0001f330: 726b 696e 6720 7472 6565 2061 6e64 2069  rking tree and i",
            "+0001f340: 6e64 6578 0a20 2020 2020 2020 2023 2052  ndex.        # R",
            "+0001f350: 6573 6574 2069 6e64 6578 2074 6f20 6d61  eset index to ma",
            "+0001f360: 7463 6820 6d65 7267 6564 2074 7265 650a  tch merged tree.",
            "+0001f370: 2020 2020 2020 2020 722e 7265 7365 745f          r.reset_",
            "+0001f380: 696e 6465 7828 6d65 7267 6564 5f74 7265  index(merged_tre",
            "+0001f390: 652e 6964 290a 0a20 2020 2020 2020 2023  e.id)..        #",
            "+0001f3a0: 2055 7064 6174 6520 776f 726b 696e 6720   Update working ",
            "+0001f3b0: 7472 6565 2066 726f 6d20 7468 6520 6e65  tree from the ne",
            "+0001f3c0: 7720 696e 6465 780a 2020 2020 2020 2020  w index.        ",
            "+0001f3d0: 7570 6461 7465 5f77 6f72 6b69 6e67 5f74  update_working_t",
            "+0001f3e0: 7265 6528 722c 2068 6561 645f 636f 6d6d  ree(r, head_comm",
            "+0001f3f0: 6974 2e74 7265 652c 206d 6572 6765 645f  it.tree, merged_",
            "+0001f400: 7472 6565 2e69 6429 0a0a 2020 2020 2020  tree.id)..      ",
            "+0001f410: 2020 6966 2063 6f6e 666c 6963 7473 3a0a    if conflicts:.",
            "+0001f420: 2020 2020 2020 2020 2020 2020 2320 5361              # Sa",
            "+0001f430: 7665 2073 7461 7465 2066 6f72 206c 6174  ve state for lat",
            "+0001f440: 6572 2063 6f6e 7469 6e75 6174 696f 6e0a  er continuation.",
            "+0001f450: 2020 2020 2020 2020 2020 2020 7769 7468              with",
            "+0001f460: 206f 7065 6e28 6f73 2e70 6174 682e 6a6f   open(os.path.jo",
            "+0001f470: 696e 2872 2e63 6f6e 7472 6f6c 6469 7228  in(r.controldir(",
            "+0001f480: 292c 2022 4348 4552 5259 5f50 4943 4b5f  ), \"CHERRY_PICK_",
            "+0001f490: 4845 4144 2229 2c20 2277 6222 2920 6173  HEAD\"), \"wb\") as",
            "+0001f4a0: 2066 3a0a 2020 2020 2020 2020 2020 2020   f:.            ",
            "+0001f4b0: 2020 2020 662e 7772 6974 6528 6368 6572      f.write(cher",
            "+0001f4c0: 7279 5f70 6963 6b5f 636f 6d6d 6974 2e69  ry_pick_commit.i",
            "+0001f4d0: 6420 2b20 6222 5c6e 2229 0a0a 2020 2020  d + b\"\\n\")..    ",
            "+0001f4e0: 2020 2020 2020 2020 2320 5361 7665 2063          # Save c",
            "+0001f4f0: 6f6d 6d69 7420 6d65 7373 6167 650a 2020  ommit message.  ",
            "+0001f500: 2020 2020 2020 2020 2020 7769 7468 206f            with o",
            "+0001f510: 7065 6e28 6f73 2e70 6174 682e 6a6f 696e  pen(os.path.join",
            "+0001f520: 2872 2e63 6f6e 7472 6f6c 6469 7228 292c  (r.controldir(),",
            "+0001f530: 2022 4d45 5247 455f 4d53 4722 292c 2022   \"MERGE_MSG\"), \"",
            "+0001f540: 7762 2229 2061 7320 663a 0a20 2020 2020  wb\") as f:.     ",
            "+0001f550: 2020 2020 2020 2020 2020 2066 2e77 7269             f.wri",
            "+0001f560: 7465 2863 6865 7272 795f 7069 636b 5f63  te(cherry_pick_c",
            "+0001f570: 6f6d 6d69 742e 6d65 7373 6167 6529 0a0a  ommit.message)..",
            "+0001f580: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+0001f590: 6520 4572 726f 7228 0a20 2020 2020 2020  e Error(.       ",
            "+0001f5a0: 2020 2020 2020 2020 2066 2243 6f6e 666c           f\"Confl",
            "+0001f5b0: 6963 7473 2069 6e3a 207b 272c 2027 2e6a  icts in: {', '.j",
            "+0001f5c0: 6f69 6e28 632e 6465 636f 6465 2827 7574  oin(c.decode('ut",
            "+0001f5d0: 662d 3827 2c20 2772 6570 6c61 6365 2729  f-8', 'replace')",
            "+0001f5e0: 2066 6f72 2063 2069 6e20 636f 6e66 6c69   for c in confli",
            "+0001f5f0: 6374 7329 7d5c 6e22 0a20 2020 2020 2020  cts)}\\n\".       ",
            "+0001f600: 2020 2020 2020 2020 2066 2246 6978 2063           f\"Fix c",
            "+0001f610: 6f6e 666c 6963 7473 2061 6e64 2072 756e  onflicts and run",
            "+0001f620: 2027 6475 6c77 6963 6820 6368 6572 7279   'dulwich cherry",
            "+0001f630: 2d70 6963 6b20 2d2d 636f 6e74 696e 7565  -pick --continue",
            "+0001f640: 2722 0a20 2020 2020 2020 2020 2020 2029  '\".            )",
            "+0001f650: 0a0a 2020 2020 2020 2020 6966 206e 6f5f  ..        if no_",
            "+0001f660: 636f 6d6d 6974 3a0a 2020 2020 2020 2020  commit:.        ",
            "+0001f670: 2020 2020 7265 7475 726e 204e 6f6e 650a      return None.",
            "+0001f680: 0a20 2020 2020 2020 2023 2043 7265 6174  .        # Creat",
            "+0001f690: 6520 7468 6520 636f 6d6d 6974 0a20 2020  e the commit.   ",
            "+0001f6a0: 2020 2020 206e 6577 5f63 6f6d 6d69 7420       new_commit ",
            "+0001f6b0: 3d20 722e 646f 5f63 6f6d 6d69 7428 0a20  = r.do_commit(. ",
            "+0001f6c0: 2020 2020 2020 2020 2020 206d 6573 7361             messa",
            "+0001f6d0: 6765 3d63 6865 7272 795f 7069 636b 5f63  ge=cherry_pick_c",
            "+0001f6e0: 6f6d 6d69 742e 6d65 7373 6167 652c 0a20  ommit.message,. ",
            "+0001f6f0: 2020 2020 2020 2020 2020 2074 7265 653d             tree=",
            "+0001f700: 6d65 7267 6564 5f74 7265 652e 6964 2c0a  merged_tree.id,.",
            "+0001f710: 2020 2020 2020 2020 2020 2020 6175 7468              auth",
            "+0001f720: 6f72 3d63 6865 7272 795f 7069 636b 5f63  or=cherry_pick_c",
            "+0001f730: 6f6d 6d69 742e 6175 7468 6f72 2c0a 2020  ommit.author,.  ",
            "+0001f740: 2020 2020 2020 2020 2020 6175 7468 6f72            author",
            "+0001f750: 5f74 696d 6573 7461 6d70 3d63 6865 7272  _timestamp=cherr",
            "+0001f760: 795f 7069 636b 5f63 6f6d 6d69 742e 6175  y_pick_commit.au",
            "+0001f770: 7468 6f72 5f74 696d 652c 0a20 2020 2020  thor_time,.     ",
            "+0001f780: 2020 2020 2020 2061 7574 686f 725f 7469         author_ti",
            "+0001f790: 6d65 7a6f 6e65 3d63 6865 7272 795f 7069  mezone=cherry_pi",
            "+0001f7a0: 636b 5f63 6f6d 6d69 742e 6175 7468 6f72  ck_commit.author",
            "+0001f7b0: 5f74 696d 657a 6f6e 652c 0a20 2020 2020  _timezone,.     ",
            "+0001f7c0: 2020 2029 0a0a 2020 2020 2020 2020 7265     )..        re",
            "+0001f7d0: 7475 726e 206e 6577 5f63 6f6d 6d69 740a  turn new_commit.",
            "+0001f7e0: 0a0a 6465 6620 7265 7665 7274 280a 2020  ..def revert(.  ",
            "+0001f7f0: 2020 7265 706f 2c0a 2020 2020 636f 6d6d    repo,.    comm",
            "+0001f800: 6974 732c 0a20 2020 206e 6f5f 636f 6d6d  its,.    no_comm",
            "+0001f810: 6974 3d46 616c 7365 2c0a 2020 2020 6d65  it=False,.    me",
            "+0001f820: 7373 6167 653d 4e6f 6e65 2c0a 2020 2020  ssage=None,.    ",
            "+0001f830: 6175 7468 6f72 3d4e 6f6e 652c 0a20 2020  author=None,.   ",
            "+0001f840: 2063 6f6d 6d69 7474 6572 3d4e 6f6e 652c   committer=None,",
            "+0001f850: 0a29 3a0a 2020 2020 2222 2252 6576 6572  .):.    \"\"\"Rever",
            "+0001f860: 7420 6f6e 6520 6f72 206d 6f72 6520 636f  t one or more co",
            "+0001f870: 6d6d 6974 732e 0a0a 2020 2020 5468 6973  mmits...    This",
            "+0001f880: 2063 7265 6174 6573 2061 206e 6577 2063   creates a new c",
            "+0001f890: 6f6d 6d69 7420 7468 6174 2075 6e64 6f65  ommit that undoe",
            "+0001f8a0: 7320 7468 6520 6368 616e 6765 7320 696e  s the changes in",
            "+0001f8b0: 7472 6f64 7563 6564 2062 7920 7468 650a  troduced by the.",
            "+0001f8c0: 2020 2020 7370 6563 6966 6965 6420 636f      specified co",
            "+0001f8d0: 6d6d 6974 732e 2055 6e6c 696b 6520 7265  mmits. Unlike re",
            "+0001f8e0: 7365 742c 2072 6576 6572 7420 6372 6561  set, revert crea",
            "+0001f8f0: 7465 7320 6120 6e65 7720 636f 6d6d 6974  tes a new commit",
            "+0001f900: 2074 6861 740a 2020 2020 7072 6573 6572   that.    preser",
            "+0001f910: 7665 7320 6869 7374 6f72 792e 0a0a 2020  ves history...  ",
            "+0001f920: 2020 4172 6773 3a0a 2020 2020 2020 7265    Args:.      re",
            "+0001f930: 706f 3a20 5061 7468 2074 6f20 7265 706f  po: Path to repo",
            "+0001f940: 7369 746f 7279 206f 7220 7265 706f 7369  sitory or reposi",
            "+0001f950: 746f 7279 206f 626a 6563 740a 2020 2020  tory object.    ",
            "+0001f960: 2020 636f 6d6d 6974 733a 204c 6973 7420    commits: List ",
            "+0001f970: 6f66 2063 6f6d 6d69 742d 6973 6820 2853  of commit-ish (S",
            "+0001f980: 4841 2c20 7265 662c 2065 7463 2e29 2074  HA, ref, etc.) t",
            "+0001f990: 6f20 7265 7665 7274 2c20 6f72 2061 2073  o revert, or a s",
            "+0001f9a0: 696e 676c 6520 636f 6d6d 6974 2d69 7368  ingle commit-ish",
            "+0001f9b0: 0a20 2020 2020 206e 6f5f 636f 6d6d 6974  .      no_commit",
            "+0001f9c0: 3a20 4966 2054 7275 652c 2061 7070 6c79  : If True, apply",
            "+0001f9d0: 2063 6861 6e67 6573 2074 6f20 696e 6465   changes to inde",
            "+0001f9e0: 782f 776f 726b 696e 6720 7472 6565 2062  x/working tree b",
            "+0001f9f0: 7574 2064 6f6e 2774 2063 6f6d 6d69 740a  ut don't commit.",
            "+0001fa00: 2020 2020 2020 6d65 7373 6167 653a 204f        message: O",
            "+0001fa10: 7074 696f 6e61 6c20 636f 6d6d 6974 206d  ptional commit m",
            "+0001fa20: 6573 7361 6765 2028 6465 6661 756c 743a  essage (default:",
            "+0001fa30: 2022 5265 7665 7274 203c 6f72 6967 696e   \"Revert <origin",
            "+0001fa40: 616c 2073 7562 6a65 6374 3e22 290a 2020  al subject>\").  ",
            "+0001fa50: 2020 2020 6175 7468 6f72 3a20 4f70 7469      author: Opti",
            "+0001fa60: 6f6e 616c 2061 7574 686f 7220 666f 7220  onal author for ",
            "+0001fa70: 7265 7665 7274 2063 6f6d 6d69 740a 2020  revert commit.  ",
            "+0001fa80: 2020 2020 636f 6d6d 6974 7465 723a 204f      committer: O",
            "+0001fa90: 7074 696f 6e61 6c20 636f 6d6d 6974 7465  ptional committe",
            "+0001faa0: 7220 666f 7220 7265 7665 7274 2063 6f6d  r for revert com",
            "+0001fab0: 6d69 740a 0a20 2020 2052 6574 7572 6e73  mit..    Returns",
            "+0001fac0: 3a0a 2020 2020 2020 5348 4131 206f 6620  :.      SHA1 of ",
            "+0001fad0: 7468 6520 6e65 7720 7265 7665 7274 2063  the new revert c",
            "+0001fae0: 6f6d 6d69 742c 206f 7220 4e6f 6e65 2069  ommit, or None i",
            "+0001faf0: 6620 6e6f 5f63 6f6d 6d69 743d 5472 7565  f no_commit=True",
            "+0001fb00: 0a0a 2020 2020 5261 6973 6573 3a0a 2020  ..    Raises:.  ",
            "+0001fb10: 2020 2020 4572 726f 723a 2049 6620 7265      Error: If re",
            "+0001fb20: 7665 7274 2066 6169 6c73 2064 7565 2074  vert fails due t",
            "+0001fb30: 6f20 636f 6e66 6c69 6374 7320 6f72 206f  o conflicts or o",
            "+0001fb40: 7468 6572 2069 7373 7565 730a 2020 2020  ther issues.    ",
            "+0001fb50: 2222 220a 2020 2020 6672 6f6d 202e 6d65  \"\"\".    from .me",
            "+0001fb60: 7267 6520 696d 706f 7274 2074 6872 6565  rge import three",
            "+0001fb70: 5f77 6179 5f6d 6572 6765 0a0a 2020 2020  _way_merge..    ",
            "+0001fb80: 2320 4e6f 726d 616c 697a 6520 636f 6d6d  # Normalize comm",
            "+0001fb90: 6974 7320 746f 2061 206c 6973 740a 2020  its to a list.  ",
            "+0001fba0: 2020 6966 2069 7369 6e73 7461 6e63 6528    if isinstance(",
            "+0001fbb0: 636f 6d6d 6974 732c 2028 7374 722c 2062  commits, (str, b",
            "+0001fbc0: 7974 6573 2929 3a0a 2020 2020 2020 2020  ytes)):.        ",
            "+0001fbd0: 636f 6d6d 6974 7320 3d20 5b63 6f6d 6d69  commits = [commi",
            "+0001fbe0: 7473 5d0a 0a20 2020 2077 6974 6820 6f70  ts]..    with op",
            "+0001fbf0: 656e 5f72 6570 6f5f 636c 6f73 696e 6728  en_repo_closing(",
            "+0001fc00: 7265 706f 2920 6173 2072 3a0a 2020 2020  repo) as r:.    ",
            "+0001fc10: 2020 2020 2320 436f 6e76 6572 7420 7374      # Convert st",
            "+0001fc20: 7269 6e67 2072 6566 7320 746f 2062 7974  ring refs to byt",
            "+0001fc30: 6573 0a20 2020 2020 2020 2063 6f6d 6d69  es.        commi",
            "+0001fc40: 7473 5f74 6f5f 7265 7665 7274 203d 205b  ts_to_revert = [",
            "+0001fc50: 5d0a 2020 2020 2020 2020 666f 7220 636f  ].        for co",
            "+0001fc60: 6d6d 6974 5f72 6566 2069 6e20 636f 6d6d  mmit_ref in comm",
            "+0001fc70: 6974 733a 0a20 2020 2020 2020 2020 2020  its:.           ",
            "+0001fc80: 2069 6620 6973 696e 7374 616e 6365 2863   if isinstance(c",
            "+0001fc90: 6f6d 6d69 745f 7265 662c 2073 7472 293a  ommit_ref, str):",
            "+0001fca0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+0001fcb0: 2063 6f6d 6d69 745f 7265 6620 3d20 636f   commit_ref = co",
            "+0001fcc0: 6d6d 6974 5f72 6566 2e65 6e63 6f64 6528  mmit_ref.encode(",
            "+0001fcd0: 2275 7466 2d38 2229 0a20 2020 2020 2020  \"utf-8\").       ",
            "+0001fce0: 2020 2020 2063 6f6d 6d69 7420 3d20 7061       commit = pa",
            "+0001fcf0: 7273 655f 636f 6d6d 6974 2872 2c20 636f  rse_commit(r, co",
            "+0001fd00: 6d6d 6974 5f72 6566 290a 2020 2020 2020  mmit_ref).      ",
            "+0001fd10: 2020 2020 2020 636f 6d6d 6974 735f 746f        commits_to",
            "+0001fd20: 5f72 6576 6572 742e 6170 7065 6e64 2863  _revert.append(c",
            "+0001fd30: 6f6d 6d69 7429 0a0a 2020 2020 2020 2020  ommit)..        ",
            "+0001fd40: 2320 4765 7420 6375 7272 656e 7420 4845  # Get current HE",
            "+0001fd50: 4144 0a20 2020 2020 2020 2074 7279 3a0a  AD.        try:.",
            "+0001fd60: 2020 2020 2020 2020 2020 2020 6865 6164              head",
            "+0001fd70: 5f63 6f6d 6d69 745f 6964 203d 2072 2e72  _commit_id = r.r",
            "+0001fd80: 6566 735b 6222 4845 4144 225d 0a20 2020  efs[b\"HEAD\"].   ",
            "+0001fd90: 2020 2020 2065 7863 6570 7420 4b65 7945       except KeyE",
            "+0001fda0: 7272 6f72 3a0a 2020 2020 2020 2020 2020  rror:.          ",
            "+0001fdb0: 2020 7261 6973 6520 4572 726f 7228 224e    raise Error(\"N",
            "+0001fdc0: 6f20 4845 4144 2072 6566 6572 656e 6365  o HEAD reference",
            "+0001fdd0: 2066 6f75 6e64 2229 0a0a 2020 2020 2020   found\")..      ",
            "+0001fde0: 2020 6865 6164 5f63 6f6d 6d69 7420 3d20    head_commit = ",
            "+0001fdf0: 725b 6865 6164 5f63 6f6d 6d69 745f 6964  r[head_commit_id",
            "+0001fe00: 5d0a 2020 2020 2020 2020 6375 7272 656e  ].        curren",
            "+0001fe10: 745f 7472 6565 203d 2068 6561 645f 636f  t_tree = head_co",
            "+0001fe20: 6d6d 6974 2e74 7265 650a 0a20 2020 2020  mmit.tree..     ",
            "+0001fe30: 2020 2023 2050 726f 6365 7373 2063 6f6d     # Process com",
            "+0001fe40: 6d69 7473 2069 6e20 6f72 6465 720a 2020  mits in order.  ",
            "+0001fe50: 2020 2020 2020 666f 7220 636f 6d6d 6974        for commit",
            "+0001fe60: 5f74 6f5f 7265 7665 7274 2069 6e20 636f  _to_revert in co",
            "+0001fe70: 6d6d 6974 735f 746f 5f72 6576 6572 743a  mmits_to_revert:",
            "+0001fe80: 0a20 2020 2020 2020 2020 2020 2023 2046  .            # F",
            "+0001fe90: 6f72 2072 6576 6572 742c 2077 6520 7761  or revert, we wa",
            "+0001fea0: 6e74 2074 6f20 6170 706c 7920 7468 6520  nt to apply the ",
            "+0001feb0: 696e 7665 7273 6520 6f66 2074 6865 2063  inverse of the c",
            "+0001fec0: 6f6d 6d69 740a 2020 2020 2020 2020 2020  ommit.          ",
            "+0001fed0: 2020 2320 5468 6973 206d 6561 6e73 2075    # This means u",
            "+0001fee0: 7369 6e67 2074 6865 2063 6f6d 6d69 7427  sing the commit'",
            "+0001fef0: 7320 7472 6565 2061 7320 2262 6173 6522  s tree as \"base\"",
            "+0001ff00: 2061 6e64 2069 7473 2070 6172 656e 7420   and its parent ",
            "+0001ff10: 6173 2022 7468 6569 7273 220a 0a20 2020  as \"theirs\"..   ",
            "+0001ff20: 2020 2020 2020 2020 2069 6620 6e6f 7420           if not ",
            "+0001ff30: 636f 6d6d 6974 5f74 6f5f 7265 7665 7274  commit_to_revert",
            "+0001ff40: 2e70 6172 656e 7473 3a0a 2020 2020 2020  .parents:.      ",
            "+0001ff50: 2020 2020 2020 2020 2020 7261 6973 6520            raise ",
            "+0001ff60: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         ",
            "+0001ff70: 2020 2020 2020 2020 2020 2066 2243 616e             f\"Can",
            "+0001ff80: 6e6f 7420 7265 7665 7274 2063 6f6d 6d69  not revert commi",
            "+0001ff90: 7420 7b63 6f6d 6d69 745f 746f 5f72 6576  t {commit_to_rev",
            "+0001ffa0: 6572 742e 6964 7d20 2d20 6974 2068 6173  ert.id} - it has",
            "+0001ffb0: 206e 6f20 7061 7265 6e74 7322 0a20 2020   no parents\".   ",
            "+0001ffc0: 2020 2020 2020 2020 2020 2020 2029 0a0a               )..",
            "+0001ffd0: 2020 2020 2020 2020 2020 2020 2320 466f              # Fo",
            "+0001ffe0: 7220 7369 6d70 6c69 6369 7479 2c20 7765  r simplicity, we",
            "+0001fff0: 206f 6e6c 7920 6861 6e64 6c65 2063 6f6d   only handle com",
            "+00020000: 6d69 7473 2077 6974 6820 6f6e 6520 7061  mits with one pa",
            "+00020010: 7265 6e74 2028 6e6f 206d 6572 6765 2063  rent (no merge c",
            "+00020020: 6f6d 6d69 7473 290a 2020 2020 2020 2020  ommits).        ",
            "+00020030: 2020 2020 6966 206c 656e 2863 6f6d 6d69      if len(commi",
            "+00020040: 745f 746f 5f72 6576 6572 742e 7061 7265  t_to_revert.pare",
            "+00020050: 6e74 7329 203e 2031 3a0a 2020 2020 2020  nts) > 1:.      ",
            "+00020060: 2020 2020 2020 2020 2020 7261 6973 6520            raise ",
            "+00020070: 4572 726f 7228 0a20 2020 2020 2020 2020  Error(.         ",
            "+00020080: 2020 2020 2020 2020 2020 2066 2243 616e             f\"Can",
            "+00020090: 6e6f 7420 7265 7665 7274 206d 6572 6765  not revert merge",
            "+000200a0: 2063 6f6d 6d69 7420 7b63 6f6d 6d69 745f   commit {commit_",
            "+000200b0: 746f 5f72 6576 6572 742e 6964 7d20 2d20  to_revert.id} - ",
            "+000200c0: 6e6f 7420 7965 7420 696d 706c 656d 656e  not yet implemen",
            "+000200d0: 7465 6422 0a20 2020 2020 2020 2020 2020  ted\".           ",
            "+000200e0: 2020 2020 2029 0a0a 2020 2020 2020 2020       )..        ",
            "+000200f0: 2020 2020 7061 7265 6e74 5f63 6f6d 6d69      parent_commi",
            "+00020100: 7420 3d20 725b 636f 6d6d 6974 5f74 6f5f  t = r[commit_to_",
            "+00020110: 7265 7665 7274 2e70 6172 656e 7473 5b30  revert.parents[0",
            "+00020120: 5d5d 0a0a 2020 2020 2020 2020 2020 2020  ]]..            ",
            "+00020130: 2320 5065 7266 6f72 6d20 7468 7265 652d  # Perform three-",
            "+00020140: 7761 7920 6d65 7267 653a 0a20 2020 2020  way merge:.     ",
            "+00020150: 2020 2020 2020 2023 202d 2062 6173 653a         # - base:",
            "+00020160: 2074 6865 2063 6f6d 6d69 7420 7765 2772   the commit we'r",
            "+00020170: 6520 7265 7665 7274 696e 6720 2877 6861  e reverting (wha",
            "+00020180: 7420 7765 2077 616e 7420 746f 2072 656d  t we want to rem",
            "+00020190: 6f76 6529 0a20 2020 2020 2020 2020 2020  ove).           ",
            "+000201a0: 2023 202d 206f 7572 733a 2063 7572 7265   # - ours: curre",
            "+000201b0: 6e74 2048 4541 4420 2877 6861 7420 7765  nt HEAD (what we",
            "+000201c0: 2068 6176 6520 6e6f 7729 0a20 2020 2020   have now).     ",
            "+000201d0: 2020 2020 2020 2023 202d 2074 6865 6972         # - their",
            "+000201e0: 733a 2070 6172 656e 7420 6f66 2063 6f6d  s: parent of com",
            "+000201f0: 6d69 7420 6265 696e 6720 7265 7665 7274  mit being revert",
            "+00020200: 6564 2028 7768 6174 2077 6520 7761 6e74  ed (what we want",
            "+00020210: 2074 6f20 676f 2062 6163 6b20 746f 290a   to go back to).",
            "+00020220: 2020 2020 2020 2020 2020 2020 6d65 7267              merg",
            "+00020230: 6564 5f74 7265 652c 2063 6f6e 666c 6963  ed_tree, conflic",
            "+00020240: 7473 203d 2074 6872 6565 5f77 6179 5f6d  ts = three_way_m",
            "+00020250: 6572 6765 280a 2020 2020 2020 2020 2020  erge(.          ",
            "+00020260: 2020 2020 2020 722e 6f62 6a65 6374 5f73        r.object_s",
            "+00020270: 746f 7265 2c0a 2020 2020 2020 2020 2020  tore,.          ",
            "+00020280: 2020 2020 2020 636f 6d6d 6974 5f74 6f5f        commit_to_",
            "+00020290: 7265 7665 7274 2c20 2023 2062 6173 650a  revert,  # base.",
            "+000202a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000202b0: 725b 6865 6164 5f63 6f6d 6d69 745f 6964  r[head_commit_id",
            "+000202c0: 5d2c 2020 2320 6f75 7273 0a20 2020 2020  ],  # ours.     ",
            "+000202d0: 2020 2020 2020 2020 2020 2070 6172 656e             paren",
            "+000202e0: 745f 636f 6d6d 6974 2c20 2023 2074 6865  t_commit,  # the",
            "+000202f0: 6972 730a 2020 2020 2020 2020 2020 2020  irs.            ",
            "+00020300: 290a 0a20 2020 2020 2020 2020 2020 2069  )..            i",
            "+00020310: 6620 636f 6e66 6c69 6374 733a 0a20 2020  f conflicts:.   ",
            "+00020320: 2020 2020 2020 2020 2020 2020 2023 2055               # U",
            "+00020330: 7064 6174 6520 776f 726b 696e 6720 7472  pdate working tr",
            "+00020340: 6565 2077 6974 6820 636f 6e66 6c69 6374  ee with conflict",
            "+00020350: 730a 2020 2020 2020 2020 2020 2020 2020  s.              ",
            "+00020360: 2020 7570 6461 7465 5f77 6f72 6b69 6e67    update_working",
            "+00020370: 5f74 7265 6528 722c 2063 7572 7265 6e74  _tree(r, current",
            "+00020380: 5f74 7265 652c 206d 6572 6765 645f 7472  _tree, merged_tr",
            "+00020390: 6565 2e69 6429 0a20 2020 2020 2020 2020  ee.id).         ",
            "+000203a0: 2020 2020 2020 2063 6f6e 666c 6963 7465         conflicte",
            "+000203b0: 645f 7061 7468 7320 3d20 5b63 2e64 6563  d_paths = [c.dec",
            "+000203c0: 6f64 6528 2275 7466 2d38 222c 2022 7265  ode(\"utf-8\", \"re",
            "+000203d0: 706c 6163 6522 2920 666f 7220 6320 696e  place\") for c in",
            "+000203e0: 2063 6f6e 666c 6963 7473 5d0a 2020 2020   conflicts].    ",
            "+000203f0: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+00020400: 6520 4572 726f 7228 6622 436f 6e66 6c69  e Error(f\"Confli",
            "+00020410: 6374 7320 7768 696c 6520 7265 7665 7274  cts while revert",
            "+00020420: 696e 673a 207b 272c 2027 2e6a 6f69 6e28  ing: {', '.join(",
            "+00020430: 636f 6e66 6c69 6374 6564 5f70 6174 6873  conflicted_paths",
            "+00020440: 297d 2229 0a0a 2020 2020 2020 2020 2020  )}\")..          ",
            "+00020450: 2020 2320 4164 6420 6d65 7267 6564 2074    # Add merged t",
            "+00020460: 7265 6520 746f 206f 626a 6563 7420 7374  ree to object st",
            "+00020470: 6f72 650a 2020 2020 2020 2020 2020 2020  ore.            ",
            "+00020480: 722e 6f62 6a65 6374 5f73 746f 7265 2e61  r.object_store.a",
            "+00020490: 6464 5f6f 626a 6563 7428 6d65 7267 6564  dd_object(merged",
            "+000204a0: 5f74 7265 6529 0a0a 2020 2020 2020 2020  _tree)..        ",
            "+000204b0: 2020 2020 2320 5570 6461 7465 2077 6f72      # Update wor",
            "+000204c0: 6b69 6e67 2074 7265 650a 2020 2020 2020  king tree.      ",
            "+000204d0: 2020 2020 2020 7570 6461 7465 5f77 6f72        update_wor",
            "+000204e0: 6b69 6e67 5f74 7265 6528 722c 2063 7572  king_tree(r, cur",
            "+000204f0: 7265 6e74 5f74 7265 652c 206d 6572 6765  rent_tree, merge",
            "+00020500: 645f 7472 6565 2e69 6429 0a20 2020 2020  d_tree.id).     ",
            "+00020510: 2020 2020 2020 2063 7572 7265 6e74 5f74         current_t",
            "+00020520: 7265 6520 3d20 6d65 7267 6564 5f74 7265  ree = merged_tre",
            "+00020530: 652e 6964 0a0a 2020 2020 2020 2020 2020  e.id..          ",
            "+00020540: 2020 6966 206e 6f74 206e 6f5f 636f 6d6d    if not no_comm",
            "+00020550: 6974 3a0a 2020 2020 2020 2020 2020 2020  it:.            ",
            "+00020560: 2020 2020 2320 4372 6561 7465 2072 6576      # Create rev",
            "+00020570: 6572 7420 636f 6d6d 6974 0a20 2020 2020  ert commit.     ",
            "+00020580: 2020 2020 2020 2020 2020 2072 6576 6572             rever",
            "+00020590: 745f 636f 6d6d 6974 203d 2043 6f6d 6d69  t_commit = Commi",
            "+000205a0: 7428 290a 2020 2020 2020 2020 2020 2020  t().            ",
            "+000205b0: 2020 2020 7265 7665 7274 5f63 6f6d 6d69      revert_commi",
            "+000205c0: 742e 7472 6565 203d 206d 6572 6765 645f  t.tree = merged_",
            "+000205d0: 7472 6565 2e69 640a 2020 2020 2020 2020  tree.id.        ",
            "+000205e0: 2020 2020 2020 2020 7265 7665 7274 5f63          revert_c",
            "+000205f0: 6f6d 6d69 742e 7061 7265 6e74 7320 3d20  ommit.parents = ",
            "+00020600: 5b68 6561 645f 636f 6d6d 6974 5f69 645d  [head_commit_id]",
            "+00020610: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              ",
            "+00020620: 2020 2320 5365 7420 6175 7468 6f72 2f63    # Set author/c",
            "+00020630: 6f6d 6d69 7474 6572 0a20 2020 2020 2020  ommitter.       ",
            "+00020640: 2020 2020 2020 2020 2069 6620 6175 7468           if auth",
            "+00020650: 6f72 2069 7320 4e6f 6e65 3a0a 2020 2020  or is None:.    ",
            "+00020660: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00020670: 6175 7468 6f72 203d 2067 6574 5f75 7365  author = get_use",
            "+00020680: 725f 6964 656e 7469 7479 2872 2e67 6574  r_identity(r.get",
            "+00020690: 5f63 6f6e 6669 675f 7374 6163 6b28 2929  _config_stack())",
            "+000206a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000206b0: 2069 6620 636f 6d6d 6974 7465 7220 6973   if committer is",
            "+000206c0: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         ",
            "+000206d0: 2020 2020 2020 2020 2020 2063 6f6d 6d69             commi",
            "+000206e0: 7474 6572 203d 2061 7574 686f 720a 0a20  tter = author.. ",
            "+000206f0: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+00020700: 6576 6572 745f 636f 6d6d 6974 2e61 7574  evert_commit.aut",
            "+00020710: 686f 7220 3d20 6175 7468 6f72 0a20 2020  hor = author.   ",
            "+00020720: 2020 2020 2020 2020 2020 2020 2072 6576               rev",
            "+00020730: 6572 745f 636f 6d6d 6974 2e63 6f6d 6d69  ert_commit.commi",
            "+00020740: 7474 6572 203d 2063 6f6d 6d69 7474 6572  tter = committer",
            "+00020750: 0a0a 2020 2020 2020 2020 2020 2020 2020  ..              ",
            "+00020760: 2020 2320 5365 7420 7469 6d65 7374 616d    # Set timestam",
            "+00020770: 7073 0a20 2020 2020 2020 2020 2020 2020  ps.             ",
            "+00020780: 2020 2074 696d 6573 7461 6d70 203d 2069     timestamp = i",
            "+00020790: 6e74 2874 696d 652e 7469 6d65 2829 290a  nt(time.time()).",
            "+000207a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000207b0: 7469 6d65 7a6f 6e65 203d 2030 2020 2320  timezone = 0  # ",
            "+000207c0: 5554 430a 2020 2020 2020 2020 2020 2020  UTC.            ",
            "+000207d0: 2020 2020 7265 7665 7274 5f63 6f6d 6d69      revert_commi",
            "+000207e0: 742e 6175 7468 6f72 5f74 696d 6520 3d20  t.author_time = ",
            "+000207f0: 7469 6d65 7374 616d 700a 2020 2020 2020  timestamp.      ",
            "+00020800: 2020 2020 2020 2020 2020 7265 7665 7274            revert",
            "+00020810: 5f63 6f6d 6d69 742e 6175 7468 6f72 5f74  _commit.author_t",
            "+00020820: 696d 657a 6f6e 6520 3d20 7469 6d65 7a6f  imezone = timezo",
            "+00020830: 6e65 0a20 2020 2020 2020 2020 2020 2020  ne.             ",
            "+00020840: 2020 2072 6576 6572 745f 636f 6d6d 6974     revert_commit",
            "+00020850: 2e63 6f6d 6d69 745f 7469 6d65 203d 2074  .commit_time = t",
            "+00020860: 696d 6573 7461 6d70 0a20 2020 2020 2020  imestamp.       ",
            "+00020870: 2020 2020 2020 2020 2072 6576 6572 745f           revert_",
            "+00020880: 636f 6d6d 6974 2e63 6f6d 6d69 745f 7469  commit.commit_ti",
            "+00020890: 6d65 7a6f 6e65 203d 2074 696d 657a 6f6e  mezone = timezon",
            "+000208a0: 650a 0a20 2020 2020 2020 2020 2020 2020  e..             ",
            "+000208b0: 2020 2023 2053 6574 206d 6573 7361 6765     # Set message",
            "+000208c0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000208d0: 2069 6620 6d65 7373 6167 6520 6973 204e   if message is N",
            "+000208e0: 6f6e 653a 0a20 2020 2020 2020 2020 2020  one:.           ",
            "+000208f0: 2020 2020 2020 2020 2023 2045 7874 7261           # Extra",
            "+00020900: 6374 206f 7269 6769 6e61 6c20 636f 6d6d  ct original comm",
            "+00020910: 6974 2073 7562 6a65 6374 0a20 2020 2020  it subject.     ",
            "+00020920: 2020 2020 2020 2020 2020 2020 2020 206f                 o",
            "+00020930: 7269 6769 6e61 6c5f 6d65 7373 6167 6520  riginal_message ",
            "+00020940: 3d20 636f 6d6d 6974 5f74 6f5f 7265 7665  = commit_to_reve",
            "+00020950: 7274 2e6d 6573 7361 6765 0a20 2020 2020  rt.message.     ",
            "+00020960: 2020 2020 2020 2020 2020 2020 2020 2069                 i",
            "+00020970: 6620 6973 696e 7374 616e 6365 286f 7269  f isinstance(ori",
            "+00020980: 6769 6e61 6c5f 6d65 7373 6167 652c 2062  ginal_message, b",
            "+00020990: 7974 6573 293a 0a20 2020 2020 2020 2020  ytes):.         ",
            "+000209a0: 2020 2020 2020 2020 2020 2020 2020 206f                 o",
            "+000209b0: 7269 6769 6e61 6c5f 6d65 7373 6167 6520  riginal_message ",
            "+000209c0: 3d20 6f72 6967 696e 616c 5f6d 6573 7361  = original_messa",
            "+000209d0: 6765 2e64 6563 6f64 6528 2275 7466 2d38  ge.decode(\"utf-8",
            "+000209e0: 222c 2022 7265 706c 6163 6522 290a 2020  \", \"replace\").  ",
            "+000209f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00020a00: 2020 7375 626a 6563 7420 3d20 6f72 6967    subject = orig",
            "+00020a10: 696e 616c 5f6d 6573 7361 6765 2e73 706c  inal_message.spl",
            "+00020a20: 6974 2822 5c6e 2229 5b30 5d0a 2020 2020  it(\"\\n\")[0].    ",
            "+00020a30: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00020a40: 6d65 7373 6167 6520 3d20 6627 5265 7665  message = f'Reve",
            "+00020a50: 7274 2022 7b73 7562 6a65 6374 7d22 5c6e  rt \"{subject}\"\\n",
            "+00020a60: 5c6e 5468 6973 2072 6576 6572 7473 2063  \\nThis reverts c",
            "+00020a70: 6f6d 6d69 7420 7b63 6f6d 6d69 745f 746f  ommit {commit_to",
            "+00020a80: 5f72 6576 6572 742e 6964 2e64 6563 6f64  _revert.id.decod",
            "+00020a90: 6528 2261 7363 6969 2229 7d2e 272e 656e  e(\"ascii\")}.'.en",
            "+00020aa0: 636f 6465 2829 0a20 2020 2020 2020 2020  code().         ",
            "+00020ab0: 2020 2020 2020 2065 6c69 6620 6973 696e         elif isin",
            "+00020ac0: 7374 616e 6365 286d 6573 7361 6765 2c20  stance(message, ",
            "+00020ad0: 7374 7229 3a0a 2020 2020 2020 2020 2020  str):.          ",
            "+00020ae0: 2020 2020 2020 2020 2020 6d65 7373 6167            messag",
            "+00020af0: 6520 3d20 6d65 7373 6167 652e 656e 636f  e = message.enco",
            "+00020b00: 6465 2822 7574 662d 3822 290a 0a20 2020  de(\"utf-8\")..   ",
            "+00020b10: 2020 2020 2020 2020 2020 2020 2072 6576               rev",
            "+00020b20: 6572 745f 636f 6d6d 6974 2e6d 6573 7361  ert_commit.messa",
            "+00020b30: 6765 203d 206d 6573 7361 6765 0a0a 2020  ge = message..  ",
            "+00020b40: 2020 2020 2020 2020 2020 2020 2020 2320                # ",
            "+00020b50: 4164 6420 636f 6d6d 6974 2074 6f20 6f62  Add commit to ob",
            "+00020b60: 6a65 6374 2073 746f 7265 0a20 2020 2020  ject store.     ",
            "+00020b70: 2020 2020 2020 2020 2020 2072 2e6f 626a             r.obj",
            "+00020b80: 6563 745f 7374 6f72 652e 6164 645f 6f62  ect_store.add_ob",
            "+00020b90: 6a65 6374 2872 6576 6572 745f 636f 6d6d  ject(revert_comm",
            "+00020ba0: 6974 290a 0a20 2020 2020 2020 2020 2020  it)..           ",
            "+00020bb0: 2020 2020 2023 2055 7064 6174 6520 4845       # Update HE",
            "+00020bc0: 4144 0a20 2020 2020 2020 2020 2020 2020  AD.             ",
            "+00020bd0: 2020 2072 2e72 6566 735b 6222 4845 4144     r.refs[b\"HEAD",
            "+00020be0: 225d 203d 2072 6576 6572 745f 636f 6d6d  \"] = revert_comm",
            "+00020bf0: 6974 2e69 640a 2020 2020 2020 2020 2020  it.id.          ",
            "+00020c00: 2020 2020 2020 6865 6164 5f63 6f6d 6d69        head_commi",
            "+00020c10: 745f 6964 203d 2072 6576 6572 745f 636f  t_id = revert_co",
            "+00020c20: 6d6d 6974 2e69 640a 0a20 2020 2020 2020  mmit.id..       ",
            "+00020c30: 2072 6574 7572 6e20 6865 6164 5f63 6f6d   return head_com",
            "+00020c40: 6d69 745f 6964 2069 6620 6e6f 7420 6e6f  mit_id if not no",
            "+00020c50: 5f63 6f6d 6d69 7420 656c 7365 204e 6f6e  _commit else Non",
            "+00020c60: 650a 0a0a 6465 6620 6763 280a 2020 2020  e...def gc(.    ",
            "+00020c70: 7265 706f 2c0a 2020 2020 6175 746f 3a20  repo,.    auto: ",
            "+00020c80: 626f 6f6c 203d 2046 616c 7365 2c0a 2020  bool = False,.  ",
            "+00020c90: 2020 6167 6772 6573 7369 7665 3a20 626f    aggressive: bo",
            "+00020ca0: 6f6c 203d 2046 616c 7365 2c0a 2020 2020  ol = False,.    ",
            "+00020cb0: 7072 756e 653a 2062 6f6f 6c20 3d20 5472  prune: bool = Tr",
            "+00020cc0: 7565 2c0a 2020 2020 6772 6163 655f 7065  ue,.    grace_pe",
            "+00020cd0: 7269 6f64 3a20 4f70 7469 6f6e 616c 5b69  riod: Optional[i",
            "+00020ce0: 6e74 5d20 3d20 3132 3039 3630 302c 2020  nt] = 1209600,  ",
            "+00020cf0: 2320 3220 7765 656b 7320 6465 6661 756c  # 2 weeks defaul",
            "+00020d00: 740a 2020 2020 6472 795f 7275 6e3a 2062  t.    dry_run: b",
            "+00020d10: 6f6f 6c20 3d20 4661 6c73 652c 0a20 2020  ool = False,.   ",
            "+00020d20: 2070 726f 6772 6573 733d 4e6f 6e65 2c0a   progress=None,.",
            "+00020d30: 293a 0a20 2020 2022 2222 5275 6e20 6761  ):.    \"\"\"Run ga",
            "+00020d40: 7262 6167 6520 636f 6c6c 6563 7469 6f6e  rbage collection",
            "+00020d50: 206f 6e20 6120 7265 706f 7369 746f 7279   on a repository",
            "+00020d60: 2e0a 0a20 2020 2041 7267 733a 0a20 2020  ...    Args:.   ",
            "+00020d70: 2020 2072 6570 6f3a 2050 6174 6820 746f     repo: Path to",
            "+00020d80: 2074 6865 2072 6570 6f73 6974 6f72 7920   the repository ",
            "+00020d90: 6f72 2061 2052 6570 6f20 6f62 6a65 6374  or a Repo object",
            "+00020da0: 0a20 2020 2020 2061 7574 6f3a 2049 6620  .      auto: If ",
            "+00020db0: 5472 7565 2c20 6f6e 6c79 2072 756e 2067  True, only run g",
            "+00020dc0: 6320 6966 206e 6565 6465 640a 2020 2020  c if needed.    ",
            "+00020dd0: 2020 6167 6772 6573 7369 7665 3a20 4966    aggressive: If",
            "+00020de0: 2054 7275 652c 2075 7365 206d 6f72 6520   True, use more ",
            "+00020df0: 6167 6772 6573 7369 7665 2073 6574 7469  aggressive setti",
            "+00020e00: 6e67 730a 2020 2020 2020 7072 756e 653a  ngs.      prune:",
            "+00020e10: 2049 6620 5472 7565 2c20 7072 756e 6520   If True, prune ",
            "+00020e20: 756e 7265 6163 6861 626c 6520 6f62 6a65  unreachable obje",
            "+00020e30: 6374 730a 2020 2020 2020 6772 6163 655f  cts.      grace_",
            "+00020e40: 7065 7269 6f64 3a20 4772 6163 6520 7065  period: Grace pe",
            "+00020e50: 7269 6f64 2069 6e20 7365 636f 6e64 7320  riod in seconds ",
            "+00020e60: 666f 7220 7072 756e 696e 6720 2864 6566  for pruning (def",
            "+00020e70: 6175 6c74 2032 2077 6565 6b73 290a 2020  ault 2 weeks).  ",
            "+00020e80: 2020 2020 6472 795f 7275 6e3a 2049 6620      dry_run: If ",
            "+00020e90: 5472 7565 2c20 6f6e 6c79 2072 6570 6f72  True, only repor",
            "+00020ea0: 7420 7768 6174 2077 6f75 6c64 2062 6520  t what would be ",
            "+00020eb0: 646f 6e65 0a20 2020 2020 2070 726f 6772  done.      progr",
            "+00020ec0: 6573 733a 204f 7074 696f 6e61 6c20 7072  ess: Optional pr",
            "+00020ed0: 6f67 7265 7373 2063 616c 6c62 6163 6b0a  ogress callback.",
            "+00020ee0: 0a20 2020 2052 6574 7572 6e73 3a0a 2020  .    Returns:.  ",
            "+00020ef0: 2020 2020 4743 5374 6174 7320 6f62 6a65      GCStats obje",
            "+00020f00: 6374 2077 6974 6820 6761 7262 6167 6520  ct with garbage ",
            "+00020f10: 636f 6c6c 6563 7469 6f6e 2073 7461 7469  collection stati",
            "+00020f20: 7374 6963 730a 2020 2020 2222 220a 2020  stics.    \"\"\".  ",
            "+00020f30: 2020 6672 6f6d 202e 6763 2069 6d70 6f72    from .gc impor",
            "+00020f40: 7420 6761 7262 6167 655f 636f 6c6c 6563  t garbage_collec",
            "+00020f50: 740a 0a20 2020 2077 6974 6820 6f70 656e  t..    with open",
            "+00020f60: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "+00020f70: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "+00020f80: 2020 7265 7475 726e 2067 6172 6261 6765    return garbage",
            "+00020f90: 5f63 6f6c 6c65 6374 280a 2020 2020 2020  _collect(.      ",
            "+00020fa0: 2020 2020 2020 722c 0a20 2020 2020 2020        r,.       ",
            "+00020fb0: 2020 2020 2061 7574 6f3d 6175 746f 2c0a       auto=auto,.",
            "+00020fc0: 2020 2020 2020 2020 2020 2020 6167 6772              aggr",
            "+00020fd0: 6573 7369 7665 3d61 6767 7265 7373 6976  essive=aggressiv",
            "+00020fe0: 652c 0a20 2020 2020 2020 2020 2020 2070  e,.            p",
            "+00020ff0: 7275 6e65 3d70 7275 6e65 2c0a 2020 2020  rune=prune,.    ",
            "+00021000: 2020 2020 2020 2020 6772 6163 655f 7065          grace_pe",
            "+00021010: 7269 6f64 3d67 7261 6365 5f70 6572 696f  riod=grace_perio",
            "+00021020: 642c 0a20 2020 2020 2020 2020 2020 2064  d,.            d",
            "+00021030: 7279 5f72 756e 3d64 7279 5f72 756e 2c0a  ry_run=dry_run,.",
            "+00021040: 2020 2020 2020 2020 2020 2020 7072 6f67              prog",
            "+00021050: 7265 7373 3d70 726f 6772 6573 732c 0a20  ress=progress,. ",
            "+00021060: 2020 2020 2020 2029 0a0a 0a64 6566 2070         )...def p",
            "+00021070: 7275 6e65 280a 2020 2020 7265 706f 2c0a  rune(.    repo,.",
            "+00021080: 2020 2020 6772 6163 655f 7065 7269 6f64      grace_period",
            "+00021090: 3a20 4f70 7469 6f6e 616c 5b69 6e74 5d20  : Optional[int] ",
            "+000210a0: 3d20 4e6f 6e65 2c0a 2020 2020 6472 795f  = None,.    dry_",
            "+000210b0: 7275 6e3a 2062 6f6f 6c20 3d20 4661 6c73  run: bool = Fals",
            "+000210c0: 652c 0a20 2020 2070 726f 6772 6573 733d  e,.    progress=",
            "+000210d0: 4e6f 6e65 2c0a 293a 0a20 2020 2022 2222  None,.):.    \"\"\"",
            "+000210e0: 5072 756e 652f 636c 6561 6e20 7570 2061  Prune/clean up a",
            "+000210f0: 2072 6570 6f73 6974 6f72 7927 7320 6f62   repository's ob",
            "+00021100: 6a65 6374 2073 746f 7265 2e0a 0a20 2020  ject store...   ",
            "+00021110: 2054 6869 7320 7265 6d6f 7665 7320 7465   This removes te",
            "+00021120: 6d70 6f72 6172 7920 6669 6c65 7320 7468  mporary files th",
            "+00021130: 6174 2077 6572 6520 6c65 6674 2062 6568  at were left beh",
            "+00021140: 696e 6420 6279 2069 6e74 6572 7275 7074  ind by interrupt",
            "+00021150: 6564 0a20 2020 2070 6163 6b20 6f70 6572  ed.    pack oper",
            "+00021160: 6174 696f 6e73 2e0a 0a20 2020 2041 7267  ations...    Arg",
            "+00021170: 733a 0a20 2020 2020 2072 6570 6f3a 2050  s:.      repo: P",
            "+00021180: 6174 6820 746f 2074 6865 2072 6570 6f73  ath to the repos",
            "+00021190: 6974 6f72 7920 6f72 2061 2052 6570 6f20  itory or a Repo ",
            "+000211a0: 6f62 6a65 6374 0a20 2020 2020 2067 7261  object.      gra",
            "+000211b0: 6365 5f70 6572 696f 643a 2047 7261 6365  ce_period: Grace",
            "+000211c0: 2070 6572 696f 6420 696e 2073 6563 6f6e   period in secon",
            "+000211d0: 6473 2066 6f72 2072 656d 6f76 696e 6720  ds for removing ",
            "+000211e0: 7465 6d70 6f72 6172 7920 6669 6c65 730a  temporary files.",
            "+000211f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00021200: 2020 2020 2864 6566 6175 6c74 2032 2077      (default 2 w",
            "+00021210: 6565 6b73 290a 2020 2020 2020 6472 795f  eeks).      dry_",
            "+00021220: 7275 6e3a 2049 6620 5472 7565 2c20 6f6e  run: If True, on",
            "+00021230: 6c79 2072 6570 6f72 7420 7768 6174 2077  ly report what w",
            "+00021240: 6f75 6c64 2062 6520 646f 6e65 0a20 2020  ould be done.   ",
            "+00021250: 2020 2070 726f 6772 6573 733a 204f 7074     progress: Opt",
            "+00021260: 696f 6e61 6c20 7072 6f67 7265 7373 2063  ional progress c",
            "+00021270: 616c 6c62 6163 6b0a 2020 2020 2222 220a  allback.    \"\"\".",
            "+00021280: 2020 2020 7769 7468 206f 7065 6e5f 7265      with open_re",
            "+00021290: 706f 5f63 6c6f 7369 6e67 2872 6570 6f29  po_closing(repo)",
            "+000212a0: 2061 7320 723a 0a20 2020 2020 2020 2069   as r:.        i",
            "+000212b0: 6620 7072 6f67 7265 7373 3a0a 2020 2020  f progress:.    ",
            "+000212c0: 2020 2020 2020 2020 7072 6f67 7265 7373          progress",
            "+000212d0: 2822 5072 756e 696e 6720 7465 6d70 6f72  (\"Pruning tempor",
            "+000212e0: 6172 7920 6669 6c65 7322 290a 2020 2020  ary files\").    ",
            "+000212f0: 2020 2020 6966 206e 6f74 2064 7279 5f72      if not dry_r",
            "+00021300: 756e 3a0a 2020 2020 2020 2020 2020 2020  un:.            ",
            "+00021310: 722e 6f62 6a65 6374 5f73 746f 7265 2e70  r.object_store.p",
            "+00021320: 7275 6e65 2867 7261 6365 5f70 6572 696f  rune(grace_perio",
            "+00021330: 643d 6772 6163 655f 7065 7269 6f64 290a  d=grace_period).",
            "+00021340: 0a0a 6465 6620 636f 756e 745f 6f62 6a65  ..def count_obje",
            "+00021350: 6374 7328 7265 706f 3d22 2e22 2c20 7665  cts(repo=\".\", ve",
            "+00021360: 7262 6f73 653d 4661 6c73 6529 202d 3e20  rbose=False) -> ",
            "+00021370: 436f 756e 744f 626a 6563 7473 5265 7375  CountObjectsResu",
            "+00021380: 6c74 3a0a 2020 2020 2222 2243 6f75 6e74  lt:.    \"\"\"Count",
            "+00021390: 2075 6e70 6163 6b65 6420 6f62 6a65 6374   unpacked object",
            "+000213a0: 7320 616e 6420 7468 6569 7220 6469 736b  s and their disk",
            "+000213b0: 2075 7361 6765 2e0a 0a20 2020 2041 7267   usage...    Arg",
            "+000213c0: 733a 0a20 2020 2020 2072 6570 6f3a 2050  s:.      repo: P",
            "+000213d0: 6174 6820 746f 2072 6570 6f73 6974 6f72  ath to repositor",
            "+000213e0: 7920 6f72 2072 6570 6f73 6974 6f72 7920  y or repository ",
            "+000213f0: 6f62 6a65 6374 0a20 2020 2020 2076 6572  object.      ver",
            "+00021400: 626f 7365 3a20 5768 6574 6865 7220 746f  bose: Whether to",
            "+00021410: 2072 6574 7572 6e20 7665 7262 6f73 6520   return verbose ",
            "+00021420: 696e 666f 726d 6174 696f 6e0a 0a20 2020  information..   ",
            "+00021430: 2052 6574 7572 6e73 3a0a 2020 2020 2020   Returns:.      ",
            "+00021440: 436f 756e 744f 626a 6563 7473 5265 7375  CountObjectsResu",
            "+00021450: 6c74 206f 626a 6563 7420 7769 7468 2064  lt object with d",
            "+00021460: 6574 6169 6c65 6420 7374 6174 6973 7469  etailed statisti",
            "+00021470: 6373 0a20 2020 2022 2222 0a20 2020 2077  cs.    \"\"\".    w",
            "+00021480: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "+00021490: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "+000214a0: 3a0a 2020 2020 2020 2020 6f62 6a65 6374  :.        object",
            "+000214b0: 5f73 746f 7265 203d 2072 2e6f 626a 6563  _store = r.objec",
            "+000214c0: 745f 7374 6f72 650a 0a20 2020 2020 2020  t_store..       ",
            "+000214d0: 2023 2043 6f75 6e74 206c 6f6f 7365 206f   # Count loose o",
            "+000214e0: 626a 6563 7473 0a20 2020 2020 2020 206c  bjects.        l",
            "+000214f0: 6f6f 7365 5f63 6f75 6e74 203d 2030 0a20  oose_count = 0. ",
            "+00021500: 2020 2020 2020 206c 6f6f 7365 5f73 697a         loose_siz",
            "+00021510: 6520 3d20 300a 2020 2020 2020 2020 666f  e = 0.        fo",
            "+00021520: 7220 7368 6120 696e 206f 626a 6563 745f  r sha in object_",
            "+00021530: 7374 6f72 652e 5f69 7465 725f 6c6f 6f73  store._iter_loos",
            "+00021540: 655f 6f62 6a65 6374 7328 293a 0a20 2020  e_objects():.   ",
            "+00021550: 2020 2020 2020 2020 206c 6f6f 7365 5f63           loose_c",
            "+00021560: 6f75 6e74 202b 3d20 310a 2020 2020 2020  ount += 1.      ",
            "+00021570: 2020 2020 2020 7061 7468 203d 206f 626a        path = obj",
            "+00021580: 6563 745f 7374 6f72 652e 5f67 6574 5f73  ect_store._get_s",
            "+00021590: 6861 6669 6c65 5f70 6174 6828 7368 6129  hafile_path(sha)",
            "+000215a0: 0a20 2020 2020 2020 2020 2020 2074 7279  .            try",
            "+000215b0: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "+000215c0: 2020 7374 6174 5f69 6e66 6f20 3d20 6f73    stat_info = os",
            "+000215d0: 2e73 7461 7428 7061 7468 290a 2020 2020  .stat(path).    ",
            "+000215e0: 2020 2020 2020 2020 2020 2020 2320 4769              # Gi",
            "+000215f0: 7420 7573 6573 2064 6973 6b20 7573 6167  t uses disk usag",
            "+00021600: 652c 206e 6f74 2066 696c 6520 7369 7a65  e, not file size",
            "+00021610: 2e20 7374 5f62 6c6f 636b 7320 6973 2061  . st_blocks is a",
            "+00021620: 6c77 6179 7320 696e 0a20 2020 2020 2020  lways in.       ",
            "+00021630: 2020 2020 2020 2020 2023 2035 3132 2d62           # 512-b",
            "+00021640: 7974 6520 626c 6f63 6b73 2070 6572 2050  yte blocks per P",
            "+00021650: 4f53 4958 2073 7461 6e64 6172 640a 2020  OSIX standard.  ",
            "+00021660: 2020 2020 2020 2020 2020 2020 2020 6966                if",
            "+00021670: 2068 6173 6174 7472 2873 7461 745f 696e   hasattr(stat_in",
            "+00021680: 666f 2c20 2273 745f 626c 6f63 6b73 2229  fo, \"st_blocks\")",
            "+00021690: 3a0a 2020 2020 2020 2020 2020 2020 2020  :.              ",
            "+000216a0: 2020 2020 2020 2320 4176 6169 6c61 626c        # Availabl",
            "+000216b0: 6520 6f6e 204c 696e 7578 2061 6e64 206d  e on Linux and m",
            "+000216c0: 6163 4f53 0a20 2020 2020 2020 2020 2020  acOS.           ",
            "+000216d0: 2020 2020 2020 2020 206c 6f6f 7365 5f73           loose_s",
            "+000216e0: 697a 6520 2b3d 2073 7461 745f 696e 666f  ize += stat_info",
            "+000216f0: 2e73 745f 626c 6f63 6b73 202a 2035 3132  .st_blocks * 512",
            "+00021700: 2020 2320 7479 7065 3a20 6967 6e6f 7265    # type: ignore",
            "+00021710: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00021720: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         ",
            "+00021730: 2020 2020 2020 2020 2020 2023 2046 616c             # Fal",
            "+00021740: 6c62 6163 6b20 666f 7220 5769 6e64 6f77  lback for Window",
            "+00021750: 730a 2020 2020 2020 2020 2020 2020 2020  s.              ",
            "+00021760: 2020 2020 2020 6c6f 6f73 655f 7369 7a65        loose_size",
            "+00021770: 202b 3d20 7374 6174 5f69 6e66 6f2e 7374   += stat_info.st",
            "+00021780: 5f73 697a 650a 2020 2020 2020 2020 2020  _size.          ",
            "+00021790: 2020 6578 6365 7074 2046 696c 654e 6f74    except FileNot",
            "+000217a0: 466f 756e 6445 7272 6f72 3a0a 2020 2020  FoundError:.    ",
            "+000217b0: 2020 2020 2020 2020 2020 2020 2320 4f62              # Ob",
            "+000217c0: 6a65 6374 206d 6179 2068 6176 6520 6265  ject may have be",
            "+000217d0: 656e 2072 656d 6f76 6564 2062 6574 7765  en removed betwe",
            "+000217e0: 656e 2069 7465 7261 7469 6f6e 2061 6e64  en iteration and",
            "+000217f0: 2073 7461 740a 2020 2020 2020 2020 2020   stat.          ",
            "+00021800: 2020 2020 2020 7061 7373 0a0a 2020 2020        pass..    ",
            "+00021810: 2020 2020 6966 206e 6f74 2076 6572 626f      if not verbo",
            "+00021820: 7365 3a0a 2020 2020 2020 2020 2020 2020  se:.            ",
            "+00021830: 7265 7475 726e 2043 6f75 6e74 4f62 6a65  return CountObje",
            "+00021840: 6374 7352 6573 756c 7428 636f 756e 743d  ctsResult(count=",
            "+00021850: 6c6f 6f73 655f 636f 756e 742c 2073 697a  loose_count, siz",
            "+00021860: 653d 6c6f 6f73 655f 7369 7a65 290a 0a20  e=loose_size).. ",
            "+00021870: 2020 2020 2020 2023 2043 6f75 6e74 2070         # Count p",
            "+00021880: 6163 6b20 696e 666f 726d 6174 696f 6e0a  ack information.",
            "+00021890: 2020 2020 2020 2020 7061 636b 5f63 6f75          pack_cou",
            "+000218a0: 6e74 203d 206c 656e 286f 626a 6563 745f  nt = len(object_",
            "+000218b0: 7374 6f72 652e 7061 636b 7329 0a20 2020  store.packs).   ",
            "+000218c0: 2020 2020 2069 6e5f 7061 636b 5f63 6f75       in_pack_cou",
            "+000218d0: 6e74 203d 2030 0a20 2020 2020 2020 2070  nt = 0.        p",
            "+000218e0: 6163 6b5f 7369 7a65 203d 2030 0a0a 2020  ack_size = 0..  ",
            "+000218f0: 2020 2020 2020 666f 7220 7061 636b 2069        for pack i",
            "+00021900: 6e20 6f62 6a65 6374 5f73 746f 7265 2e70  n object_store.p",
            "+00021910: 6163 6b73 3a0a 2020 2020 2020 2020 2020  acks:.          ",
            "+00021920: 2020 696e 5f70 6163 6b5f 636f 756e 7420    in_pack_count ",
            "+00021930: 2b3d 206c 656e 2870 6163 6b29 0a20 2020  += len(pack).   ",
            "+00021940: 2020 2020 2020 2020 2023 2047 6574 2070           # Get p",
            "+00021950: 6163 6b20 6669 6c65 2073 697a 650a 2020  ack file size.  ",
            "+00021960: 2020 2020 2020 2020 2020 7061 636b 5f70            pack_p",
            "+00021970: 6174 6820 3d20 7061 636b 2e5f 6461 7461  ath = pack._data",
            "+00021980: 5f70 6174 680a 2020 2020 2020 2020 2020  _path.          ",
            "+00021990: 2020 7472 793a 0a20 2020 2020 2020 2020    try:.         ",
            "+000219a0: 2020 2020 2020 2070 6163 6b5f 7369 7a65         pack_size",
            "+000219b0: 202b 3d20 6f73 2e70 6174 682e 6765 7473   += os.path.gets",
            "+000219c0: 697a 6528 7061 636b 5f70 6174 6829 0a20  ize(pack_path). ",
            "+000219d0: 2020 2020 2020 2020 2020 2065 7863 6570             excep",
            "+000219e0: 7420 4669 6c65 4e6f 7446 6f75 6e64 4572  t FileNotFoundEr",
            "+000219f0: 726f 723a 0a20 2020 2020 2020 2020 2020  ror:.           ",
            "+00021a00: 2020 2020 2070 6173 730a 2020 2020 2020       pass.      ",
            "+00021a10: 2020 2020 2020 2320 4765 7420 696e 6465        # Get inde",
            "+00021a20: 7820 6669 6c65 2073 697a 650a 2020 2020  x file size.    ",
            "+00021a30: 2020 2020 2020 2020 6964 785f 7061 7468          idx_path",
            "+00021a40: 203d 2070 6163 6b2e 5f69 6478 5f70 6174   = pack._idx_pat",
            "+00021a50: 680a 2020 2020 2020 2020 2020 2020 7472  h.            tr",
            "+00021a60: 793a 0a20 2020 2020 2020 2020 2020 2020  y:.             ",
            "+00021a70: 2020 2070 6163 6b5f 7369 7a65 202b 3d20     pack_size += ",
            "+00021a80: 6f73 2e70 6174 682e 6765 7473 697a 6528  os.path.getsize(",
            "+00021a90: 6964 785f 7061 7468 290a 2020 2020 2020  idx_path).      ",
            "+00021aa0: 2020 2020 2020 6578 6365 7074 2046 696c        except Fil",
            "+00021ab0: 654e 6f74 466f 756e 6445 7272 6f72 3a0a  eNotFoundError:.",
            "+00021ac0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00021ad0: 7061 7373 0a0a 2020 2020 2020 2020 7265  pass..        re",
            "+00021ae0: 7475 726e 2043 6f75 6e74 4f62 6a65 6374  turn CountObject",
            "+00021af0: 7352 6573 756c 7428 0a20 2020 2020 2020  sResult(.       ",
            "+00021b00: 2020 2020 2063 6f75 6e74 3d6c 6f6f 7365       count=loose",
            "+00021b10: 5f63 6f75 6e74 2c0a 2020 2020 2020 2020  _count,.        ",
            "+00021b20: 2020 2020 7369 7a65 3d6c 6f6f 7365 5f73      size=loose_s",
            "+00021b30: 697a 652c 0a20 2020 2020 2020 2020 2020  ize,.           ",
            "+00021b40: 2069 6e5f 7061 636b 3d69 6e5f 7061 636b   in_pack=in_pack",
            "+00021b50: 5f63 6f75 6e74 2c0a 2020 2020 2020 2020  _count,.        ",
            "+00021b60: 2020 2020 7061 636b 733d 7061 636b 5f63      packs=pack_c",
            "+00021b70: 6f75 6e74 2c0a 2020 2020 2020 2020 2020  ount,.          ",
            "+00021b80: 2020 7369 7a65 5f70 6163 6b3d 7061 636b    size_pack=pack",
            "+00021b90: 5f73 697a 652c 0a20 2020 2020 2020 2029  _size,.        )",
            "+00021ba0: 0a0a 0a64 6566 2072 6562 6173 6528 0a20  ...def rebase(. ",
            "+00021bb0: 2020 2072 6570 6f3a 2055 6e69 6f6e 5b52     repo: Union[R",
            "+00021bc0: 6570 6f2c 2073 7472 5d2c 0a20 2020 2075  epo, str],.    u",
            "+00021bd0: 7073 7472 6561 6d3a 2055 6e69 6f6e 5b62  pstream: Union[b",
            "+00021be0: 7974 6573 2c20 7374 725d 2c0a 2020 2020  ytes, str],.    ",
            "+00021bf0: 6f6e 746f 3a20 4f70 7469 6f6e 616c 5b55  onto: Optional[U",
            "+00021c00: 6e69 6f6e 5b62 7974 6573 2c20 7374 725d  nion[bytes, str]",
            "+00021c10: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6272  ] = None,.    br",
            "+00021c20: 616e 6368 3a20 4f70 7469 6f6e 616c 5b55  anch: Optional[U",
            "+00021c30: 6e69 6f6e 5b62 7974 6573 2c20 7374 725d  nion[bytes, str]",
            "+00021c40: 5d20 3d20 4e6f 6e65 2c0a 2020 2020 6162  ] = None,.    ab",
            "+00021c50: 6f72 743a 2062 6f6f 6c20 3d20 4661 6c73  ort: bool = Fals",
            "+00021c60: 652c 0a20 2020 2063 6f6e 7469 6e75 655f  e,.    continue_",
            "+00021c70: 7265 6261 7365 3a20 626f 6f6c 203d 2046  rebase: bool = F",
            "+00021c80: 616c 7365 2c0a 2020 2020 736b 6970 3a20  alse,.    skip: ",
            "+00021c90: 626f 6f6c 203d 2046 616c 7365 2c0a 2920  bool = False,.) ",
            "+00021ca0: 2d3e 206c 6973 745b 6279 7465 735d 3a0a  -> list[bytes]:.",
            "+00021cb0: 2020 2020 2222 2252 6562 6173 6520 636f      \"\"\"Rebase co",
            "+00021cc0: 6d6d 6974 7320 6f6e 746f 2061 6e6f 7468  mmits onto anoth",
            "+00021cd0: 6572 2062 7261 6e63 682e 0a0a 2020 2020  er branch...    ",
            "+00021ce0: 4172 6773 3a0a 2020 2020 2020 7265 706f  Args:.      repo",
            "+00021cf0: 3a20 5265 706f 7369 746f 7279 2074 6f20  : Repository to ",
            "+00021d00: 7265 6261 7365 2069 6e0a 2020 2020 2020  rebase in.      ",
            "+00021d10: 7570 7374 7265 616d 3a20 5570 7374 7265  upstream: Upstre",
            "+00021d20: 616d 2062 7261 6e63 682f 636f 6d6d 6974  am branch/commit",
            "+00021d30: 2074 6f20 7265 6261 7365 206f 6e74 6f0a   to rebase onto.",
            "+00021d40: 2020 2020 2020 6f6e 746f 3a20 5370 6563        onto: Spec",
            "+00021d50: 6966 6963 2063 6f6d 6d69 7420 746f 2072  ific commit to r",
            "+00021d60: 6562 6173 6520 6f6e 746f 2028 6465 6661  ebase onto (defa",
            "+00021d70: 756c 7473 2074 6f20 7570 7374 7265 616d  ults to upstream",
            "+00021d80: 290a 2020 2020 2020 6272 616e 6368 3a20  ).      branch: ",
            "+00021d90: 4272 616e 6368 2074 6f20 7265 6261 7365  Branch to rebase",
            "+00021da0: 2028 6465 6661 756c 7473 2074 6f20 6375   (defaults to cu",
            "+00021db0: 7272 656e 7420 6272 616e 6368 290a 2020  rrent branch).  ",
            "+00021dc0: 2020 2020 6162 6f72 743a 2041 626f 7274      abort: Abort",
            "+00021dd0: 2061 6e20 696e 2d70 726f 6772 6573 7320   an in-progress ",
            "+00021de0: 7265 6261 7365 0a20 2020 2020 2063 6f6e  rebase.      con",
            "+00021df0: 7469 6e75 655f 7265 6261 7365 3a20 436f  tinue_rebase: Co",
            "+00021e00: 6e74 696e 7565 2061 6e20 696e 2d70 726f  ntinue an in-pro",
            "+00021e10: 6772 6573 7320 7265 6261 7365 0a20 2020  gress rebase.   ",
            "+00021e20: 2020 2073 6b69 703a 2053 6b69 7020 6375     skip: Skip cu",
            "+00021e30: 7272 656e 7420 636f 6d6d 6974 2061 6e64  rrent commit and",
            "+00021e40: 2063 6f6e 7469 6e75 6520 7265 6261 7365   continue rebase",
            "+00021e50: 0a0a 2020 2020 5265 7475 726e 733a 0a20  ..    Returns:. ",
            "+00021e60: 2020 2020 204c 6973 7420 6f66 206e 6577       List of new",
            "+00021e70: 2063 6f6d 6d69 7420 5348 4173 2063 7265   commit SHAs cre",
            "+00021e80: 6174 6564 2062 7920 7265 6261 7365 0a0a  ated by rebase..",
            "+00021e90: 2020 2020 5261 6973 6573 3a0a 2020 2020      Raises:.    ",
            "+00021ea0: 2020 4572 726f 723a 2049 6620 7265 6261    Error: If reba",
            "+00021eb0: 7365 2066 6169 6c73 206f 7220 636f 6e66  se fails or conf",
            "+00021ec0: 6c69 6374 7320 6f63 6375 720a 2020 2020  licts occur.    ",
            "+00021ed0: 2222 220a 2020 2020 6672 6f6d 202e 7265  \"\"\".    from .re",
            "+00021ee0: 6261 7365 2069 6d70 6f72 7420 5265 6261  base import Reba",
            "+00021ef0: 7365 436f 6e66 6c69 6374 2c20 5265 6261  seConflict, Reba",
            "+00021f00: 7365 4572 726f 722c 2052 6562 6173 6572  seError, Rebaser",
            "+00021f10: 0a0a 2020 2020 7769 7468 206f 7065 6e5f  ..    with open_",
            "+00021f20: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+00021f30: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "+00021f40: 2072 6562 6173 6572 203d 2052 6562 6173   rebaser = Rebas",
            "+00021f50: 6572 2872 290a 0a20 2020 2020 2020 2069  er(r)..        i",
            "+00021f60: 6620 6162 6f72 743a 0a20 2020 2020 2020  f abort:.       ",
            "+00021f70: 2020 2020 2074 7279 3a0a 2020 2020 2020       try:.      ",
            "+00021f80: 2020 2020 2020 2020 2020 7265 6261 7365            rebase",
            "+00021f90: 722e 6162 6f72 7428 290a 2020 2020 2020  r.abort().      ",
            "+00021fa0: 2020 2020 2020 2020 2020 7265 7475 726e            return",
            "+00021fb0: 205b 5d0a 2020 2020 2020 2020 2020 2020   [].            ",
            "+00021fc0: 6578 6365 7074 2052 6562 6173 6545 7272  except RebaseErr",
            "+00021fd0: 6f72 2061 7320 653a 0a20 2020 2020 2020  or as e:.       ",
            "+00021fe0: 2020 2020 2020 2020 2072 6169 7365 2045           raise E",
            "+00021ff0: 7272 6f72 2873 7472 2865 2929 0a0a 2020  rror(str(e))..  ",
            "+00022000: 2020 2020 2020 6966 2063 6f6e 7469 6e75        if continu",
            "+00022010: 655f 7265 6261 7365 3a0a 2020 2020 2020  e_rebase:.      ",
            "+00022020: 2020 2020 2020 7472 793a 0a20 2020 2020        try:.     ",
            "+00022030: 2020 2020 2020 2020 2020 2072 6573 756c             resul",
            "+00022040: 7420 3d20 7265 6261 7365 722e 636f 6e74  t = rebaser.cont",
            "+00022050: 696e 7565 5f28 290a 2020 2020 2020 2020  inue_().        ",
            "+00022060: 2020 2020 2020 2020 6966 2072 6573 756c          if resul",
            "+00022070: 7420 6973 204e 6f6e 653a 0a20 2020 2020  t is None:.     ",
            "+00022080: 2020 2020 2020 2020 2020 2020 2020 2023                 #",
            "+00022090: 2052 6562 6173 6520 636f 6d70 6c65 7465   Rebase complete",
            "+000220a0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000220b0: 2020 2020 2072 6574 7572 6e20 5b5d 0a20       return []. ",
            "+000220c0: 2020 2020 2020 2020 2020 2020 2020 2065                 e",
            "+000220d0: 6c69 6620 6973 696e 7374 616e 6365 2872  lif isinstance(r",
            "+000220e0: 6573 756c 742c 2074 7570 6c65 2920 616e  esult, tuple) an",
            "+000220f0: 6420 7265 7375 6c74 5b31 5d3a 0a20 2020  d result[1]:.   ",
            "+00022100: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00022110: 2023 2053 7469 6c6c 2068 6176 6520 636f   # Still have co",
            "+00022120: 6e66 6c69 6374 730a 2020 2020 2020 2020  nflicts.        ",
            "+00022130: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+00022140: 6520 4572 726f 7228 0a20 2020 2020 2020  e Error(.       ",
            "+00022150: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00022160: 2066 2243 6f6e 666c 6963 7473 2069 6e3a   f\"Conflicts in:",
            "+00022170: 207b 272c 2027 2e6a 6f69 6e28 662e 6465   {', '.join(f.de",
            "+00022180: 636f 6465 2827 7574 662d 3827 2c20 2772  code('utf-8', 'r",
            "+00022190: 6570 6c61 6365 2729 2066 6f72 2066 2069  eplace') for f i",
            "+000221a0: 6e20 7265 7375 6c74 5b31 5d29 7d22 0a20  n result[1])}\". ",
            "+000221b0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000221c0: 2020 2029 0a20 2020 2020 2020 2020 2020     ).           ",
            "+000221d0: 2065 7863 6570 7420 5265 6261 7365 4572   except RebaseEr",
            "+000221e0: 726f 7220 6173 2065 3a0a 2020 2020 2020  ror as e:.      ",
            "+000221f0: 2020 2020 2020 2020 2020 7261 6973 6520            raise ",
            "+00022200: 4572 726f 7228 7374 7228 6529 290a 0a20  Error(str(e)).. ",
            "+00022210: 2020 2020 2020 2023 2043 6f6e 7665 7274         # Convert",
            "+00022220: 2073 7472 696e 6720 7265 6673 2074 6f20   string refs to ",
            "+00022230: 6279 7465 730a 2020 2020 2020 2020 6966  bytes.        if",
            "+00022240: 2069 7369 6e73 7461 6e63 6528 7570 7374   isinstance(upst",
            "+00022250: 7265 616d 2c20 7374 7229 3a0a 2020 2020  ream, str):.    ",
            "+00022260: 2020 2020 2020 2020 7570 7374 7265 616d          upstream",
            "+00022270: 203d 2075 7073 7472 6561 6d2e 656e 636f   = upstream.enco",
            "+00022280: 6465 2822 7574 662d 3822 290a 2020 2020  de(\"utf-8\").    ",
            "+00022290: 2020 2020 6966 2069 7369 6e73 7461 6e63      if isinstanc",
            "+000222a0: 6528 6f6e 746f 2c20 7374 7229 3a0a 2020  e(onto, str):.  ",
            "+000222b0: 2020 2020 2020 2020 2020 6f6e 746f 203d            onto =",
            "+000222c0: 206f 6e74 6f2e 656e 636f 6465 2822 7574   onto.encode(\"ut",
            "+000222d0: 662d 3822 2920 6966 206f 6e74 6f20 656c  f-8\") if onto el",
            "+000222e0: 7365 204e 6f6e 650a 2020 2020 2020 2020  se None.        ",
            "+000222f0: 6966 2069 7369 6e73 7461 6e63 6528 6272  if isinstance(br",
            "+00022300: 616e 6368 2c20 7374 7229 3a0a 2020 2020  anch, str):.    ",
            "+00022310: 2020 2020 2020 2020 6272 616e 6368 203d          branch =",
            "+00022320: 2062 7261 6e63 682e 656e 636f 6465 2822   branch.encode(\"",
            "+00022330: 7574 662d 3822 2920 6966 2062 7261 6e63  utf-8\") if branc",
            "+00022340: 6820 656c 7365 204e 6f6e 650a 0a20 2020  h else None..   ",
            "+00022350: 2020 2020 2074 7279 3a0a 2020 2020 2020       try:.      ",
            "+00022360: 2020 2020 2020 2320 5374 6172 7420 7265        # Start re",
            "+00022370: 6261 7365 0a20 2020 2020 2020 2020 2020  base.           ",
            "+00022380: 2072 6562 6173 6572 2e73 7461 7274 2875   rebaser.start(u",
            "+00022390: 7073 7472 6561 6d2c 206f 6e74 6f2c 2062  pstream, onto, b",
            "+000223a0: 7261 6e63 6829 0a0a 2020 2020 2020 2020  ranch)..        ",
            "+000223b0: 2020 2020 2320 436f 6e74 696e 7565 2072      # Continue r",
            "+000223c0: 6562 6173 6520 6175 746f 6d61 7469 6361  ebase automatica",
            "+000223d0: 6c6c 790a 2020 2020 2020 2020 2020 2020  lly.            ",
            "+000223e0: 7265 7375 6c74 203d 2072 6562 6173 6572  result = rebaser",
            "+000223f0: 2e63 6f6e 7469 6e75 655f 2829 0a20 2020  .continue_().   ",
            "+00022400: 2020 2020 2020 2020 2069 6620 7265 7375           if resu",
            "+00022410: 6c74 2069 7320 6e6f 7420 4e6f 6e65 3a0a  lt is not None:.",
            "+00022420: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00022430: 2320 436f 6e66 6c69 6374 730a 2020 2020  # Conflicts.    ",
            "+00022440: 2020 2020 2020 2020 2020 2020 7261 6973              rais",
            "+00022450: 6520 5265 6261 7365 436f 6e66 6c69 6374  e RebaseConflict",
            "+00022460: 2872 6573 756c 745b 315d 290a 0a20 2020  (result[1])..   ",
            "+00022470: 2020 2020 2020 2020 2023 2052 6574 7572           # Retur",
            "+00022480: 6e20 7468 6520 5348 4173 206f 6620 7468  n the SHAs of th",
            "+00022490: 6520 7265 6261 7365 6420 636f 6d6d 6974  e rebased commit",
            "+000224a0: 730a 2020 2020 2020 2020 2020 2020 7265  s.            re",
            "+000224b0: 7475 726e 205b 632e 6964 2066 6f72 2063  turn [c.id for c",
            "+000224c0: 2069 6e20 7265 6261 7365 722e 5f64 6f6e   in rebaser._don",
            "+000224d0: 655d 0a0a 2020 2020 2020 2020 6578 6365  e]..        exce",
            "+000224e0: 7074 2052 6562 6173 6543 6f6e 666c 6963  pt RebaseConflic",
            "+000224f0: 7420 6173 2065 3a0a 2020 2020 2020 2020  t as e:.        ",
            "+00022500: 2020 2020 7261 6973 6520 4572 726f 7228      raise Error(",
            "+00022510: 7374 7228 6529 290a 2020 2020 2020 2020  str(e)).        ",
            "+00022520: 6578 6365 7074 2052 6562 6173 6545 7272  except RebaseErr",
            "+00022530: 6f72 2061 7320 653a 0a20 2020 2020 2020  or as e:.       ",
            "+00022540: 2020 2020 2072 6169 7365 2045 7272 6f72       raise Error",
            "+00022550: 2873 7472 2865 2929 0a0a 0a64 6566 2061  (str(e))...def a",
            "+00022560: 6e6e 6f74 6174 6528 7265 706f 2c20 7061  nnotate(repo, pa",
            "+00022570: 7468 2c20 636f 6d6d 6974 7469 7368 3d4e  th, committish=N",
            "+00022580: 6f6e 6529 3a0a 2020 2020 2222 2241 6e6e  one):.    \"\"\"Ann",
            "+00022590: 6f74 6174 6520 7468 6520 6869 7374 6f72  otate the histor",
            "+000225a0: 7920 6f66 2061 2066 696c 652e 0a0a 2020  y of a file...  ",
            "+000225b0: 2020 3a70 6172 616d 2072 6570 6f3a 2050    :param repo: P",
            "+000225c0: 6174 6820 746f 2074 6865 2072 6570 6f73  ath to the repos",
            "+000225d0: 6974 6f72 790a 2020 2020 3a70 6172 616d  itory.    :param",
            "+000225e0: 2070 6174 683a 2050 6174 6820 746f 2061   path: Path to a",
            "+000225f0: 6e6e 6f74 6174 650a 2020 2020 3a70 6172  nnotate.    :par",
            "+00022600: 616d 2063 6f6d 6d69 7474 6973 683a 2043  am committish: C",
            "+00022610: 6f6d 6d69 7420 6964 2074 6f20 6669 6e64  ommit id to find",
            "+00022620: 2070 6174 6820 696e 0a20 2020 203a 7265   path in.    :re",
            "+00022630: 7475 726e 3a20 4c69 7374 206f 6620 2828  turn: List of ((",
            "+00022640: 436f 6d6d 6974 2c20 5472 6565 4368 616e  Commit, TreeChan",
            "+00022650: 6765 292c 206c 696e 6529 2074 7570 6c65  ge), line) tuple",
            "+00022660: 730a 2020 2020 2222 220a 2020 2020 6966  s.    \"\"\".    if",
            "+00022670: 2063 6f6d 6d69 7474 6973 6820 6973 204e   committish is N",
            "+00022680: 6f6e 653a 0a20 2020 2020 2020 2063 6f6d  one:.        com",
            "+00022690: 6d69 7474 6973 6820 3d20 2248 4541 4422  mittish = \"HEAD\"",
            "+000226a0: 0a20 2020 2066 726f 6d20 6475 6c77 6963  .    from dulwic",
            "+000226b0: 682e 616e 6e6f 7461 7465 2069 6d70 6f72  h.annotate impor",
            "+000226c0: 7420 616e 6e6f 7461 7465 5f6c 696e 6573  t annotate_lines",
            "+000226d0: 0a0a 2020 2020 7769 7468 206f 7065 6e5f  ..    with open_",
            "+000226e0: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+000226f0: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "+00022700: 2063 6f6d 6d69 745f 6964 203d 2070 6172   commit_id = par",
            "+00022710: 7365 5f63 6f6d 6d69 7428 722c 2063 6f6d  se_commit(r, com",
            "+00022720: 6d69 7474 6973 6829 2e69 640a 2020 2020  mittish).id.    ",
            "+00022730: 2020 2020 2320 456e 7375 7265 2070 6174      # Ensure pat",
            "+00022740: 6820 6973 2062 7974 6573 0a20 2020 2020  h is bytes.     ",
            "+00022750: 2020 2069 6620 6973 696e 7374 616e 6365     if isinstance",
            "+00022760: 2870 6174 682c 2073 7472 293a 0a20 2020  (path, str):.   ",
            "+00022770: 2020 2020 2020 2020 2070 6174 6820 3d20           path = ",
            "+00022780: 7061 7468 2e65 6e63 6f64 6528 290a 2020  path.encode().  ",
            "+00022790: 2020 2020 2020 7265 7475 726e 2061 6e6e        return ann",
            "+000227a0: 6f74 6174 655f 6c69 6e65 7328 722e 6f62  otate_lines(r.ob",
            "+000227b0: 6a65 6374 5f73 746f 7265 2c20 636f 6d6d  ject_store, comm",
            "+000227c0: 6974 5f69 642c 2070 6174 6829 0a0a 0a62  it_id, path)...b",
            "+000227d0: 6c61 6d65 203d 2061 6e6e 6f74 6174 650a  lame = annotate.",
            "+000227e0: 0a0a 6465 6620 6669 6c74 6572 5f62 7261  ..def filter_bra",
            "+000227f0: 6e63 6828 0a20 2020 2072 6570 6f3d 222e  nch(.    repo=\".",
            "+00022800: 222c 0a20 2020 2062 7261 6e63 683d 2248  \",.    branch=\"H",
            "+00022810: 4541 4422 2c0a 2020 2020 2a2c 0a20 2020  EAD\",.    *,.   ",
            "+00022820: 2066 696c 7465 725f 666e 3d4e 6f6e 652c   filter_fn=None,",
            "+00022830: 0a20 2020 2066 696c 7465 725f 6175 7468  .    filter_auth",
            "+00022840: 6f72 3d4e 6f6e 652c 0a20 2020 2066 696c  or=None,.    fil",
            "+00022850: 7465 725f 636f 6d6d 6974 7465 723d 4e6f  ter_committer=No",
            "+00022860: 6e65 2c0a 2020 2020 6669 6c74 6572 5f6d  ne,.    filter_m",
            "+00022870: 6573 7361 6765 3d4e 6f6e 652c 0a20 2020  essage=None,.   ",
            "+00022880: 2074 7265 655f 6669 6c74 6572 3d4e 6f6e   tree_filter=Non",
            "+00022890: 652c 0a20 2020 2069 6e64 6578 5f66 696c  e,.    index_fil",
            "+000228a0: 7465 723d 4e6f 6e65 2c0a 2020 2020 7061  ter=None,.    pa",
            "+000228b0: 7265 6e74 5f66 696c 7465 723d 4e6f 6e65  rent_filter=None",
            "+000228c0: 2c0a 2020 2020 636f 6d6d 6974 5f66 696c  ,.    commit_fil",
            "+000228d0: 7465 723d 4e6f 6e65 2c0a 2020 2020 7375  ter=None,.    su",
            "+000228e0: 6264 6972 6563 746f 7279 5f66 696c 7465  bdirectory_filte",
            "+000228f0: 723d 4e6f 6e65 2c0a 2020 2020 7072 756e  r=None,.    prun",
            "+00022900: 655f 656d 7074 793d 4661 6c73 652c 0a20  e_empty=False,. ",
            "+00022910: 2020 2074 6167 5f6e 616d 655f 6669 6c74     tag_name_filt",
            "+00022920: 6572 3d4e 6f6e 652c 0a20 2020 2066 6f72  er=None,.    for",
            "+00022930: 6365 3d46 616c 7365 2c0a 2020 2020 6b65  ce=False,.    ke",
            "+00022940: 6570 5f6f 7269 6769 6e61 6c3d 5472 7565  ep_original=True",
            "+00022950: 2c0a 2020 2020 7265 6673 3d4e 6f6e 652c  ,.    refs=None,",
            "+00022960: 0a29 3a0a 2020 2020 2222 2252 6577 7269  .):.    \"\"\"Rewri",
            "+00022970: 7465 2062 7261 6e63 6820 6869 7374 6f72  te branch histor",
            "+00022980: 7920 6279 2063 7265 6174 696e 6720 6e65  y by creating ne",
            "+00022990: 7720 636f 6d6d 6974 7320 7769 7468 2066  w commits with f",
            "+000229a0: 696c 7465 7265 6420 7072 6f70 6572 7469  iltered properti",
            "+000229b0: 6573 2e0a 0a20 2020 2054 6869 7320 6973  es...    This is",
            "+000229c0: 2073 696d 696c 6172 2074 6f20 6769 7420   similar to git ",
            "+000229d0: 6669 6c74 6572 2d62 7261 6e63 682c 2061  filter-branch, a",
            "+000229e0: 6c6c 6f77 696e 6720 796f 7520 746f 2072  llowing you to r",
            "+000229f0: 6577 7269 7465 2063 6f6d 6d69 740a 2020  ewrite commit.  ",
            "+00022a00: 2020 6869 7374 6f72 7920 6279 206d 6f64    history by mod",
            "+00022a10: 6966 7969 6e67 2074 7265 6573 2c20 7061  ifying trees, pa",
            "+00022a20: 7265 6e74 732c 2061 7574 686f 722c 2063  rents, author, c",
            "+00022a30: 6f6d 6d69 7474 6572 2c20 6f72 2063 6f6d  ommitter, or com",
            "+00022a40: 6d69 7420 6d65 7373 6167 6573 2e0a 0a20  mit messages... ",
            "+00022a50: 2020 2041 7267 733a 0a20 2020 2020 2072     Args:.      r",
            "+00022a60: 6570 6f3a 2050 6174 6820 746f 2072 6570  epo: Path to rep",
            "+00022a70: 6f73 6974 6f72 790a 2020 2020 2020 6272  ository.      br",
            "+00022a80: 616e 6368 3a20 4272 616e 6368 2074 6f20  anch: Branch to ",
            "+00022a90: 7265 7772 6974 6520 2864 6566 6175 6c74  rewrite (default",
            "+00022aa0: 7320 746f 2048 4541 4429 0a20 2020 2020  s to HEAD).     ",
            "+00022ab0: 2066 696c 7465 725f 666e 3a20 4f70 7469   filter_fn: Opti",
            "+00022ac0: 6f6e 616c 2063 616c 6c61 626c 6520 7468  onal callable th",
            "+00022ad0: 6174 2074 616b 6573 2061 2043 6f6d 6d69  at takes a Commi",
            "+00022ae0: 7420 6f62 6a65 6374 2061 6e64 2072 6574  t object and ret",
            "+00022af0: 7572 6e73 0a20 2020 2020 2020 2061 2064  urns.        a d",
            "+00022b00: 6963 7420 6f66 2075 7064 6174 6564 2066  ict of updated f",
            "+00022b10: 6965 6c64 7320 2861 7574 686f 722c 2063  ields (author, c",
            "+00022b20: 6f6d 6d69 7474 6572 2c20 6d65 7373 6167  ommitter, messag",
            "+00022b30: 652c 2065 7463 2e29 0a20 2020 2020 2066  e, etc.).      f",
            "+00022b40: 696c 7465 725f 6175 7468 6f72 3a20 4f70  ilter_author: Op",
            "+00022b50: 7469 6f6e 616c 2063 616c 6c61 626c 6520  tional callable ",
            "+00022b60: 7468 6174 2074 616b 6573 2061 7574 686f  that takes autho",
            "+00022b70: 7220 6279 7465 7320 616e 6420 7265 7475  r bytes and retu",
            "+00022b80: 726e 730a 2020 2020 2020 2020 7570 6461  rns.        upda",
            "+00022b90: 7465 6420 6175 7468 6f72 2062 7974 6573  ted author bytes",
            "+00022ba0: 206f 7220 4e6f 6e65 2074 6f20 6b65 6570   or None to keep",
            "+00022bb0: 2075 6e63 6861 6e67 6564 0a20 2020 2020   unchanged.     ",
            "+00022bc0: 2066 696c 7465 725f 636f 6d6d 6974 7465   filter_committe",
            "+00022bd0: 723a 204f 7074 696f 6e61 6c20 6361 6c6c  r: Optional call",
            "+00022be0: 6162 6c65 2074 6861 7420 7461 6b65 7320  able that takes ",
            "+00022bf0: 636f 6d6d 6974 7465 7220 6279 7465 7320  committer bytes ",
            "+00022c00: 616e 6420 7265 7475 726e 730a 2020 2020  and returns.    ",
            "+00022c10: 2020 2020 7570 6461 7465 6420 636f 6d6d      updated comm",
            "+00022c20: 6974 7465 7220 6279 7465 7320 6f72 204e  itter bytes or N",
            "+00022c30: 6f6e 6520 746f 206b 6565 7020 756e 6368  one to keep unch",
            "+00022c40: 616e 6765 640a 2020 2020 2020 6669 6c74  anged.      filt",
            "+00022c50: 6572 5f6d 6573 7361 6765 3a20 4f70 7469  er_message: Opti",
            "+00022c60: 6f6e 616c 2063 616c 6c61 626c 6520 7468  onal callable th",
            "+00022c70: 6174 2074 616b 6573 2063 6f6d 6d69 7420  at takes commit ",
            "+00022c80: 6d65 7373 6167 6520 6279 7465 730a 2020  message bytes.  ",
            "+00022c90: 2020 2020 2020 616e 6420 7265 7475 726e        and return",
            "+00022ca0: 7320 7570 6461 7465 6420 6d65 7373 6167  s updated messag",
            "+00022cb0: 6520 6279 7465 730a 2020 2020 2020 7472  e bytes.      tr",
            "+00022cc0: 6565 5f66 696c 7465 723a 204f 7074 696f  ee_filter: Optio",
            "+00022cd0: 6e61 6c20 6361 6c6c 6162 6c65 2074 6861  nal callable tha",
            "+00022ce0: 7420 7461 6b65 7320 2874 7265 655f 7368  t takes (tree_sh",
            "+00022cf0: 612c 2074 656d 705f 6469 7229 2061 6e64  a, temp_dir) and",
            "+00022d00: 2072 6574 7572 6e73 0a20 2020 2020 2020   returns.       ",
            "+00022d10: 206e 6577 2074 7265 6520 5348 4120 6166   new tree SHA af",
            "+00022d20: 7465 7220 6d6f 6469 6679 696e 6720 776f  ter modifying wo",
            "+00022d30: 726b 696e 6720 6469 7265 6374 6f72 790a  rking directory.",
            "+00022d40: 2020 2020 2020 696e 6465 785f 6669 6c74        index_filt",
            "+00022d50: 6572 3a20 4f70 7469 6f6e 616c 2063 616c  er: Optional cal",
            "+00022d60: 6c61 626c 6520 7468 6174 2074 616b 6573  lable that takes",
            "+00022d70: 2028 7472 6565 5f73 6861 2c20 7465 6d70   (tree_sha, temp",
            "+00022d80: 5f69 6e64 6578 5f70 6174 6829 2061 6e64  _index_path) and",
            "+00022d90: 0a20 2020 2020 2020 2072 6574 7572 6e73  .        returns",
            "+00022da0: 206e 6577 2074 7265 6520 5348 4120 6166   new tree SHA af",
            "+00022db0: 7465 7220 6d6f 6469 6679 696e 6720 696e  ter modifying in",
            "+00022dc0: 6465 780a 2020 2020 2020 7061 7265 6e74  dex.      parent",
            "+00022dd0: 5f66 696c 7465 723a 204f 7074 696f 6e61  _filter: Optiona",
            "+00022de0: 6c20 6361 6c6c 6162 6c65 2074 6861 7420  l callable that ",
            "+00022df0: 7461 6b65 7320 7061 7265 6e74 206c 6973  takes parent lis",
            "+00022e00: 7420 616e 6420 7265 7475 726e 730a 2020  t and returns.  ",
            "+00022e10: 2020 2020 2020 6d6f 6469 6669 6564 2070        modified p",
            "+00022e20: 6172 656e 7420 6c69 7374 0a20 2020 2020  arent list.     ",
            "+00022e30: 2063 6f6d 6d69 745f 6669 6c74 6572 3a20   commit_filter: ",
            "+00022e40: 4f70 7469 6f6e 616c 2063 616c 6c61 626c  Optional callabl",
            "+00022e50: 6520 7468 6174 2074 616b 6573 2028 436f  e that takes (Co",
            "+00022e60: 6d6d 6974 2c20 7472 6565 5f73 6861 2920  mmit, tree_sha) ",
            "+00022e70: 616e 6420 7265 7475 726e 730a 2020 2020  and returns.    ",
            "+00022e80: 2020 2020 6e65 7720 636f 6d6d 6974 2053      new commit S",
            "+00022e90: 4841 206f 7220 4e6f 6e65 2074 6f20 736b  HA or None to sk",
            "+00022ea0: 6970 2063 6f6d 6d69 740a 2020 2020 2020  ip commit.      ",
            "+00022eb0: 7375 6264 6972 6563 746f 7279 5f66 696c  subdirectory_fil",
            "+00022ec0: 7465 723a 204f 7074 696f 6e61 6c20 7375  ter: Optional su",
            "+00022ed0: 6264 6972 6563 746f 7279 2070 6174 6820  bdirectory path ",
            "+00022ee0: 746f 2065 7874 7261 6374 2061 7320 6e65  to extract as ne",
            "+00022ef0: 7720 726f 6f74 0a20 2020 2020 2070 7275  w root.      pru",
            "+00022f00: 6e65 5f65 6d70 7479 3a20 5768 6574 6865  ne_empty: Whethe",
            "+00022f10: 7220 746f 2070 7275 6e65 2063 6f6d 6d69  r to prune commi",
            "+00022f20: 7473 2074 6861 7420 6265 636f 6d65 2065  ts that become e",
            "+00022f30: 6d70 7479 0a20 2020 2020 2074 6167 5f6e  mpty.      tag_n",
            "+00022f40: 616d 655f 6669 6c74 6572 3a20 4f70 7469  ame_filter: Opti",
            "+00022f50: 6f6e 616c 2063 616c 6c61 626c 6520 746f  onal callable to",
            "+00022f60: 2072 656e 616d 6520 7461 6773 0a20 2020   rename tags.   ",
            "+00022f70: 2020 2066 6f72 6365 3a20 466f 7263 6520     force: Force ",
            "+00022f80: 6f70 6572 6174 696f 6e20 6576 656e 2069  operation even i",
            "+00022f90: 6620 6272 616e 6368 2068 6173 2062 6565  f branch has bee",
            "+00022fa0: 6e20 6669 6c74 6572 6564 2062 6566 6f72  n filtered befor",
            "+00022fb0: 650a 2020 2020 2020 6b65 6570 5f6f 7269  e.      keep_ori",
            "+00022fc0: 6769 6e61 6c3a 204b 6565 7020 6f72 6967  ginal: Keep orig",
            "+00022fd0: 696e 616c 2072 6566 7320 756e 6465 7220  inal refs under ",
            "+00022fe0: 7265 6673 2f6f 7269 6769 6e61 6c2f 0a20  refs/original/. ",
            "+00022ff0: 2020 2020 2072 6566 733a 204c 6973 7420       refs: List ",
            "+00023000: 6f66 2072 6566 7320 746f 2072 6577 7269  of refs to rewri",
            "+00023010: 7465 2028 6465 6661 756c 7473 2074 6f20  te (defaults to ",
            "+00023020: 5b62 7261 6e63 685d 290a 0a20 2020 2052  [branch])..    R",
            "+00023030: 6574 7572 6e73 3a0a 2020 2020 2020 4469  eturns:.      Di",
            "+00023040: 6374 206d 6170 7069 6e67 206f 6c64 2063  ct mapping old c",
            "+00023050: 6f6d 6d69 7420 5348 4173 2074 6f20 6e65  ommit SHAs to ne",
            "+00023060: 7720 636f 6d6d 6974 2053 4841 730a 0a20  w commit SHAs.. ",
            "+00023070: 2020 2052 6169 7365 733a 0a20 2020 2020     Raises:.     ",
            "+00023080: 2045 7272 6f72 3a20 4966 2062 7261 6e63   Error: If branc",
            "+00023090: 6820 6973 2061 6c72 6561 6479 2066 696c  h is already fil",
            "+000230a0: 7465 7265 6420 616e 6420 666f 7263 6520  tered and force ",
            "+000230b0: 6973 2046 616c 7365 0a20 2020 2022 2222  is False.    \"\"\"",
            "+000230c0: 0a20 2020 2066 726f 6d20 2e66 696c 7465  .    from .filte",
            "+000230d0: 725f 6272 616e 6368 2069 6d70 6f72 7420  r_branch import ",
            "+000230e0: 436f 6d6d 6974 4669 6c74 6572 2c20 6669  CommitFilter, fi",
            "+000230f0: 6c74 6572 5f72 6566 730a 0a20 2020 2077  lter_refs..    w",
            "+00023100: 6974 6820 6f70 656e 5f72 6570 6f5f 636c  ith open_repo_cl",
            "+00023110: 6f73 696e 6728 7265 706f 2920 6173 2072  osing(repo) as r",
            "+00023120: 3a0a 2020 2020 2020 2020 2320 5061 7273  :.        # Pars",
            "+00023130: 6520 6272 616e 6368 2f63 6f6d 6d69 7474  e branch/committ",
            "+00023140: 6973 680a 2020 2020 2020 2020 6966 2069  ish.        if i",
            "+00023150: 7369 6e73 7461 6e63 6528 6272 616e 6368  sinstance(branch",
            "+00023160: 2c20 7374 7229 3a0a 2020 2020 2020 2020  , str):.        ",
            "+00023170: 2020 2020 6272 616e 6368 203d 2062 7261      branch = bra",
            "+00023180: 6e63 682e 656e 636f 6465 2829 0a0a 2020  nch.encode()..  ",
            "+00023190: 2020 2020 2020 2320 4465 7465 726d 696e        # Determin",
            "+000231a0: 6520 7768 6963 6820 7265 6673 2074 6f20  e which refs to ",
            "+000231b0: 7072 6f63 6573 730a 2020 2020 2020 2020  process.        ",
            "+000231c0: 6966 2072 6566 7320 6973 204e 6f6e 653a  if refs is None:",
            "+000231d0: 0a20 2020 2020 2020 2020 2020 2069 6620  .            if ",
            "+000231e0: 6272 616e 6368 203d 3d20 6222 4845 4144  branch == b\"HEAD",
            "+000231f0: 223a 0a20 2020 2020 2020 2020 2020 2020  \":.             ",
            "+00023200: 2020 2023 2052 6573 6f6c 7665 2048 4541     # Resolve HEA",
            "+00023210: 4420 746f 2061 6374 7561 6c20 6272 616e  D to actual bran",
            "+00023220: 6368 0a20 2020 2020 2020 2020 2020 2020  ch.             ",
            "+00023230: 2020 2074 7279 3a0a 2020 2020 2020 2020     try:.        ",
            "+00023240: 2020 2020 2020 2020 2020 2020 7265 736f              reso",
            "+00023250: 6c76 6564 203d 2072 2e72 6566 732e 666f  lved = r.refs.fo",
            "+00023260: 6c6c 6f77 2862 2248 4541 4422 290a 2020  llow(b\"HEAD\").  ",
            "+00023270: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00023280: 2020 6966 2072 6573 6f6c 7665 6420 616e    if resolved an",
            "+00023290: 6420 7265 736f 6c76 6564 5b30 5d3a 0a20  d resolved[0]:. ",
            "+000232a0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+000232b0: 2020 2020 2020 2023 2072 6573 6f6c 7665         # resolve",
            "+000232c0: 6420 6973 2061 206c 6973 7420 6f66 2028  d is a list of (",
            "+000232d0: 7265 666e 616d 652c 2073 6861 2920 7475  refname, sha) tu",
            "+000232e0: 706c 6573 0a20 2020 2020 2020 2020 2020  ples.           ",
            "+000232f0: 2020 2020 2020 2020 2020 2020 2072 6573               res",
            "+00023300: 6f6c 7665 645f 7265 6620 3d20 7265 736f  olved_ref = reso",
            "+00023310: 6c76 6564 5b30 5d5b 2d31 5d0a 2020 2020  lved[0][-1].    ",
            "+00023320: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00023330: 2020 2020 6966 2072 6573 6f6c 7665 645f      if resolved_",
            "+00023340: 7265 6620 616e 6420 7265 736f 6c76 6564  ref and resolved",
            "+00023350: 5f72 6566 2021 3d20 6222 4845 4144 223a  _ref != b\"HEAD\":",
            "+00023360: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+00023370: 2020 2020 2020 2020 2020 2020 2072 6566               ref",
            "+00023380: 7320 3d20 5b72 6573 6f6c 7665 645f 7265  s = [resolved_re",
            "+00023390: 665d 0a20 2020 2020 2020 2020 2020 2020  f].             ",
            "+000233a0: 2020 2020 2020 2020 2020 2065 6c73 653a             else:",
            "+000233b0: 0a20 2020 2020 2020 2020 2020 2020 2020  .               ",
            "+000233c0: 2020 2020 2020 2020 2020 2020 2023 2048               # H",
            "+000233d0: 4541 4420 706f 696e 7473 2064 6972 6563  EAD points direc",
            "+000233e0: 746c 7920 746f 2061 2063 6f6d 6d69 740a  tly to a commit.",
            "+000233f0: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00023400: 2020 2020 2020 2020 2020 2020 7265 6673              refs",
            "+00023410: 203d 205b 6222 4845 4144 225d 0a20 2020   = [b\"HEAD\"].   ",
            "+00023420: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00023430: 2065 6c73 653a 0a20 2020 2020 2020 2020   else:.         ",
            "+00023440: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+00023450: 6566 7320 3d20 5b62 2248 4541 4422 5d0a  efs = [b\"HEAD\"].",
            "+00023460: 2020 2020 2020 2020 2020 2020 2020 2020                  ",
            "+00023470: 6578 6365 7074 2053 796d 7265 664c 6f6f  except SymrefLoo",
            "+00023480: 703a 0a20 2020 2020 2020 2020 2020 2020  p:.             ",
            "+00023490: 2020 2020 2020 2072 6566 7320 3d20 5b62         refs = [b",
            "+000234a0: 2248 4541 4422 5d0a 2020 2020 2020 2020  \"HEAD\"].        ",
            "+000234b0: 2020 2020 656c 7365 3a0a 2020 2020 2020      else:.      ",
            "+000234c0: 2020 2020 2020 2020 2020 2320 436f 6e76            # Conv",
            "+000234d0: 6572 7420 6272 616e 6368 206e 616d 6520  ert branch name ",
            "+000234e0: 746f 2066 756c 6c20 7265 6620 6966 206e  to full ref if n",
            "+000234f0: 6565 6465 640a 2020 2020 2020 2020 2020  eeded.          ",
            "+00023500: 2020 2020 2020 6966 206e 6f74 2062 7261        if not bra",
            "+00023510: 6e63 682e 7374 6172 7473 7769 7468 2862  nch.startswith(b",
            "+00023520: 2272 6566 732f 2229 3a0a 2020 2020 2020  \"refs/\"):.      ",
            "+00023530: 2020 2020 2020 2020 2020 2020 2020 6272                br",
            "+00023540: 616e 6368 203d 2062 2272 6566 732f 6865  anch = b\"refs/he",
            "+00023550: 6164 732f 2220 2b20 6272 616e 6368 0a20  ads/\" + branch. ",
            "+00023560: 2020 2020 2020 2020 2020 2020 2020 2072                 r",
            "+00023570: 6566 7320 3d20 5b62 7261 6e63 685d 0a0a  efs = [branch]..",
            "+00023580: 2020 2020 2020 2020 2320 436f 6e76 6572          # Conver",
            "+00023590: 7420 7375 6264 6972 6563 746f 7279 2066  t subdirectory f",
            "+000235a0: 696c 7465 7220 746f 2062 7974 6573 2069  ilter to bytes i",
            "+000235b0: 6620 6e65 6564 6564 0a20 2020 2020 2020  f needed.       ",
            "+000235c0: 2069 6620 7375 6264 6972 6563 746f 7279   if subdirectory",
            "+000235d0: 5f66 696c 7465 7220 616e 6420 6973 696e  _filter and isin",
            "+000235e0: 7374 616e 6365 2873 7562 6469 7265 6374  stance(subdirect",
            "+000235f0: 6f72 795f 6669 6c74 6572 2c20 7374 7229  ory_filter, str)",
            "+00023600: 3a0a 2020 2020 2020 2020 2020 2020 7375  :.            su",
            "+00023610: 6264 6972 6563 746f 7279 5f66 696c 7465  bdirectory_filte",
            "+00023620: 7220 3d20 7375 6264 6972 6563 746f 7279  r = subdirectory",
            "+00023630: 5f66 696c 7465 722e 656e 636f 6465 2829  _filter.encode()",
            "+00023640: 0a0a 2020 2020 2020 2020 2320 4372 6561  ..        # Crea",
            "+00023650: 7465 2063 6f6d 6d69 7420 6669 6c74 6572  te commit filter",
            "+00023660: 0a20 2020 2020 2020 2063 6f6d 6d69 745f  .        commit_",
            "+00023670: 6669 6c74 6572 203d 2043 6f6d 6d69 7446  filter = CommitF",
            "+00023680: 696c 7465 7228 0a20 2020 2020 2020 2020  ilter(.         ",
            "+00023690: 2020 2072 2e6f 626a 6563 745f 7374 6f72     r.object_stor",
            "+000236a0: 652c 0a20 2020 2020 2020 2020 2020 2066  e,.            f",
            "+000236b0: 696c 7465 725f 666e 3d66 696c 7465 725f  ilter_fn=filter_",
            "+000236c0: 666e 2c0a 2020 2020 2020 2020 2020 2020  fn,.            ",
            "+000236d0: 6669 6c74 6572 5f61 7574 686f 723d 6669  filter_author=fi",
            "+000236e0: 6c74 6572 5f61 7574 686f 722c 0a20 2020  lter_author,.   ",
            "+000236f0: 2020 2020 2020 2020 2066 696c 7465 725f           filter_",
            "+00023700: 636f 6d6d 6974 7465 723d 6669 6c74 6572  committer=filter",
            "+00023710: 5f63 6f6d 6d69 7474 6572 2c0a 2020 2020  _committer,.    ",
            "+00023720: 2020 2020 2020 2020 6669 6c74 6572 5f6d          filter_m",
            "+00023730: 6573 7361 6765 3d66 696c 7465 725f 6d65  essage=filter_me",
            "+00023740: 7373 6167 652c 0a20 2020 2020 2020 2020  ssage,.         ",
            "+00023750: 2020 2074 7265 655f 6669 6c74 6572 3d74     tree_filter=t",
            "+00023760: 7265 655f 6669 6c74 6572 2c0a 2020 2020  ree_filter,.    ",
            "+00023770: 2020 2020 2020 2020 696e 6465 785f 6669          index_fi",
            "+00023780: 6c74 6572 3d69 6e64 6578 5f66 696c 7465  lter=index_filte",
            "+00023790: 722c 0a20 2020 2020 2020 2020 2020 2070  r,.            p",
            "+000237a0: 6172 656e 745f 6669 6c74 6572 3d70 6172  arent_filter=par",
            "+000237b0: 656e 745f 6669 6c74 6572 2c0a 2020 2020  ent_filter,.    ",
            "+000237c0: 2020 2020 2020 2020 636f 6d6d 6974 5f66          commit_f",
            "+000237d0: 696c 7465 723d 636f 6d6d 6974 5f66 696c  ilter=commit_fil",
            "+000237e0: 7465 722c 0a20 2020 2020 2020 2020 2020  ter,.           ",
            "+000237f0: 2073 7562 6469 7265 6374 6f72 795f 6669   subdirectory_fi",
            "+00023800: 6c74 6572 3d73 7562 6469 7265 6374 6f72  lter=subdirector",
            "+00023810: 795f 6669 6c74 6572 2c0a 2020 2020 2020  y_filter,.      ",
            "+00023820: 2020 2020 2020 7072 756e 655f 656d 7074        prune_empt",
            "+00023830: 793d 7072 756e 655f 656d 7074 792c 0a20  y=prune_empty,. ",
            "+00023840: 2020 2020 2020 2020 2020 2074 6167 5f6e             tag_n",
            "+00023850: 616d 655f 6669 6c74 6572 3d74 6167 5f6e  ame_filter=tag_n",
            "+00023860: 616d 655f 6669 6c74 6572 2c0a 2020 2020  ame_filter,.    ",
            "+00023870: 2020 2020 290a 0a20 2020 2020 2020 2023      )..        #",
            "+00023880: 2054 6167 2063 616c 6c62 6163 6b20 666f   Tag callback fo",
            "+00023890: 7220 7265 6e61 6d69 6e67 2074 6167 730a  r renaming tags.",
            "+000238a0: 2020 2020 2020 2020 6465 6620 7265 6e61          def rena",
            "+000238b0: 6d65 5f74 6167 286f 6c64 5f72 6566 2c20  me_tag(old_ref, ",
            "+000238c0: 6e65 775f 7265 6629 3a0a 2020 2020 2020  new_ref):.      ",
            "+000238d0: 2020 2020 2020 2320 436f 7079 2074 6167        # Copy tag",
            "+000238e0: 2074 6f20 6e65 7720 6e61 6d65 0a20 2020   to new name.   ",
            "+000238f0: 2020 2020 2020 2020 2072 2e72 6566 735b           r.refs[",
            "+00023900: 6e65 775f 7265 665d 203d 2072 2e72 6566  new_ref] = r.ref",
            "+00023910: 735b 6f6c 645f 7265 665d 0a20 2020 2020  s[old_ref].     ",
            "+00023920: 2020 2020 2020 2023 2044 656c 6574 6520         # Delete ",
            "+00023930: 6f6c 6420 7461 670a 2020 2020 2020 2020  old tag.        ",
            "+00023940: 2020 2020 6465 6c20 722e 7265 6673 5b6f      del r.refs[o",
            "+00023950: 6c64 5f72 6566 5d0a 0a20 2020 2020 2020  ld_ref]..       ",
            "+00023960: 2023 2046 696c 7465 7220 7265 6673 0a20   # Filter refs. ",
            "+00023970: 2020 2020 2020 2074 7279 3a0a 2020 2020         try:.    ",
            "+00023980: 2020 2020 2020 2020 7265 7475 726e 2066          return f",
            "+00023990: 696c 7465 725f 7265 6673 280a 2020 2020  ilter_refs(.    ",
            "+000239a0: 2020 2020 2020 2020 2020 2020 722e 7265              r.re",
            "+000239b0: 6673 2c0a 2020 2020 2020 2020 2020 2020  fs,.            ",
            "+000239c0: 2020 2020 722e 6f62 6a65 6374 5f73 746f      r.object_sto",
            "+000239d0: 7265 2c0a 2020 2020 2020 2020 2020 2020  re,.            ",
            "+000239e0: 2020 2020 7265 6673 2c0a 2020 2020 2020      refs,.      ",
            "+000239f0: 2020 2020 2020 2020 2020 636f 6d6d 6974            commit",
            "+00023a00: 5f66 696c 7465 722c 0a20 2020 2020 2020  _filter,.       ",
            "+00023a10: 2020 2020 2020 2020 206b 6565 705f 6f72           keep_or",
            "+00023a20: 6967 696e 616c 3d6b 6565 705f 6f72 6967  iginal=keep_orig",
            "+00023a30: 696e 616c 2c0a 2020 2020 2020 2020 2020  inal,.          ",
            "+00023a40: 2020 2020 2020 666f 7263 653d 666f 7263        force=forc",
            "+00023a50: 652c 0a20 2020 2020 2020 2020 2020 2020  e,.             ",
            "+00023a60: 2020 2074 6167 5f63 616c 6c62 6163 6b3d     tag_callback=",
            "+00023a70: 7265 6e61 6d65 5f74 6167 2069 6620 7461  rename_tag if ta",
            "+00023a80: 675f 6e61 6d65 5f66 696c 7465 7220 656c  g_name_filter el",
            "+00023a90: 7365 204e 6f6e 652c 0a20 2020 2020 2020  se None,.       ",
            "+00023aa0: 2020 2020 2029 0a20 2020 2020 2020 2065       ).        e",
            "+00023ab0: 7863 6570 7420 5661 6c75 6545 7272 6f72  xcept ValueError",
            "+00023ac0: 2061 7320 653a 0a20 2020 2020 2020 2020   as e:.         ",
            "+00023ad0: 2020 2072 6169 7365 2045 7272 6f72 2873     raise Error(s",
            "+00023ae0: 7472 2865 2929 2066 726f 6d20 650a 0a0a  tr(e)) from e...",
            "+00023af0: 6465 6620 6269 7365 6374 5f73 7461 7274  def bisect_start",
            "+00023b00: 280a 2020 2020 7265 706f 3d22 2e22 2c0a  (.    repo=\".\",.",
            "+00023b10: 2020 2020 6261 643d 4e6f 6e65 2c0a 2020      bad=None,.  ",
            "+00023b20: 2020 676f 6f64 3d4e 6f6e 652c 0a20 2020    good=None,.   ",
            "+00023b30: 2070 6174 6873 3d4e 6f6e 652c 0a20 2020   paths=None,.   ",
            "+00023b40: 206e 6f5f 6368 6563 6b6f 7574 3d46 616c   no_checkout=Fal",
            "+00023b50: 7365 2c0a 2020 2020 7465 726d 5f62 6164  se,.    term_bad",
            "+00023b60: 3d22 6261 6422 2c0a 2020 2020 7465 726d  =\"bad\",.    term",
            "+00023b70: 5f67 6f6f 643d 2267 6f6f 6422 2c0a 293a  _good=\"good\",.):",
            "+00023b80: 0a20 2020 2022 2222 5374 6172 7420 6120  .    \"\"\"Start a ",
            "+00023b90: 6e65 7720 6269 7365 6374 2073 6573 7369  new bisect sessi",
            "+00023ba0: 6f6e 2e0a 0a20 2020 2041 7267 733a 0a20  on...    Args:. ",
            "+00023bb0: 2020 2020 2020 2072 6570 6f3a 2050 6174         repo: Pat",
            "+00023bc0: 6820 746f 2072 6570 6f73 6974 6f72 7920  h to repository ",
            "+00023bd0: 6f72 2061 2052 6570 6f20 6f62 6a65 6374  or a Repo object",
            "+00023be0: 0a20 2020 2020 2020 2062 6164 3a20 5468  .        bad: Th",
            "+00023bf0: 6520 6261 6420 636f 6d6d 6974 2028 6465  e bad commit (de",
            "+00023c00: 6661 756c 7473 2074 6f20 4845 4144 290a  faults to HEAD).",
            "+00023c10: 2020 2020 2020 2020 676f 6f64 3a20 4c69          good: Li",
            "+00023c20: 7374 206f 6620 676f 6f64 2063 6f6d 6d69  st of good commi",
            "+00023c30: 7473 206f 7220 6120 7369 6e67 6c65 2067  ts or a single g",
            "+00023c40: 6f6f 6420 636f 6d6d 6974 0a20 2020 2020  ood commit.     ",
            "+00023c50: 2020 2070 6174 6873 3a20 4f70 7469 6f6e     paths: Option",
            "+00023c60: 616c 2070 6174 6873 2074 6f20 6c69 6d69  al paths to limi",
            "+00023c70: 7420 6269 7365 6374 2074 6f0a 2020 2020  t bisect to.    ",
            "+00023c80: 2020 2020 6e6f 5f63 6865 636b 6f75 743a      no_checkout:",
            "+00023c90: 2049 6620 5472 7565 2c20 646f 6e27 7420   If True, don't ",
            "+00023ca0: 6368 6563 6b6f 7574 2063 6f6d 6d69 7473  checkout commits",
            "+00023cb0: 2064 7572 696e 6720 6269 7365 6374 0a20   during bisect. ",
            "+00023cc0: 2020 2020 2020 2074 6572 6d5f 6261 643a         term_bad:",
            "+00023cd0: 2054 6572 6d20 746f 2075 7365 2066 6f72   Term to use for",
            "+00023ce0: 2062 6164 2063 6f6d 6d69 7473 2028 6465   bad commits (de",
            "+00023cf0: 6661 756c 743a 2022 6261 6422 290a 2020  fault: \"bad\").  ",
            "+00023d00: 2020 2020 2020 7465 726d 5f67 6f6f 643a        term_good:",
            "+00023d10: 2054 6572 6d20 746f 2075 7365 2066 6f72   Term to use for",
            "+00023d20: 2067 6f6f 6420 636f 6d6d 6974 7320 2864   good commits (d",
            "+00023d30: 6566 6175 6c74 3a20 2267 6f6f 6422 290a  efault: \"good\").",
            "+00023d40: 2020 2020 2222 220a 2020 2020 7769 7468      \"\"\".    with",
            "+00023d50: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "+00023d60: 6e67 2872 6570 6f29 2061 7320 723a 0a20  ng(repo) as r:. ",
            "+00023d70: 2020 2020 2020 2073 7461 7465 203d 2042         state = B",
            "+00023d80: 6973 6563 7453 7461 7465 2872 290a 0a20  isectState(r).. ",
            "+00023d90: 2020 2020 2020 2023 2043 6f6e 7665 7274         # Convert",
            "+00023da0: 2073 696e 676c 6520 676f 6f64 2063 6f6d   single good com",
            "+00023db0: 6d69 7420 746f 206c 6973 740a 2020 2020  mit to list.    ",
            "+00023dc0: 2020 2020 6966 2067 6f6f 6420 6973 206e      if good is n",
            "+00023dd0: 6f74 204e 6f6e 6520 616e 6420 6e6f 7420  ot None and not ",
            "+00023de0: 6973 696e 7374 616e 6365 2867 6f6f 642c  isinstance(good,",
            "+00023df0: 206c 6973 7429 3a0a 2020 2020 2020 2020   list):.        ",
            "+00023e00: 2020 2020 676f 6f64 203d 205b 676f 6f64      good = [good",
            "+00023e10: 5d0a 0a20 2020 2020 2020 2023 2050 6172  ]..        # Par",
            "+00023e20: 7365 2063 6f6d 6d69 7473 0a20 2020 2020  se commits.     ",
            "+00023e30: 2020 2062 6164 5f73 6861 203d 2070 6172     bad_sha = par",
            "+00023e40: 7365 5f63 6f6d 6d69 7428 722c 2062 6164  se_commit(r, bad",
            "+00023e50: 292e 6964 2069 6620 6261 6420 656c 7365  ).id if bad else",
            "+00023e60: 204e 6f6e 650a 2020 2020 2020 2020 676f   None.        go",
            "+00023e70: 6f64 5f73 6861 7320 3d20 5b70 6172 7365  od_shas = [parse",
            "+00023e80: 5f63 6f6d 6d69 7428 722c 2067 292e 6964  _commit(r, g).id",
            "+00023e90: 2066 6f72 2067 2069 6e20 676f 6f64 5d20   for g in good] ",
            "+00023ea0: 6966 2067 6f6f 6420 656c 7365 204e 6f6e  if good else Non",
            "+00023eb0: 650a 0a20 2020 2020 2020 2073 7461 7465  e..        state",
            "+00023ec0: 2e73 7461 7274 2862 6164 5f73 6861 2c20  .start(bad_sha, ",
            "+00023ed0: 676f 6f64 5f73 6861 732c 2070 6174 6873  good_shas, paths",
            "+00023ee0: 2c20 6e6f 5f63 6865 636b 6f75 742c 2074  , no_checkout, t",
            "+00023ef0: 6572 6d5f 6261 642c 2074 6572 6d5f 676f  erm_bad, term_go",
            "+00023f00: 6f64 290a 0a20 2020 2020 2020 2023 2052  od)..        # R",
            "+00023f10: 6574 7572 6e20 7468 6520 6e65 7874 2063  eturn the next c",
            "+00023f20: 6f6d 6d69 7420 746f 2074 6573 7420 6966  ommit to test if",
            "+00023f30: 2077 6520 6861 7665 2062 6f74 6820 676f   we have both go",
            "+00023f40: 6f64 2061 6e64 2062 6164 0a20 2020 2020  od and bad.     ",
            "+00023f50: 2020 2069 6620 6261 645f 7368 6120 616e     if bad_sha an",
            "+00023f60: 6420 676f 6f64 5f73 6861 733a 0a20 2020  d good_shas:.   ",
            "+00023f70: 2020 2020 2020 2020 206e 6578 745f 7368           next_sh",
            "+00023f80: 6120 3d20 7374 6174 652e 5f66 696e 645f  a = state._find_",
            "+00023f90: 6e65 7874 5f63 6f6d 6d69 7428 290a 2020  next_commit().  ",
            "+00023fa0: 2020 2020 2020 2020 2020 6966 206e 6578            if nex",
            "+00023fb0: 745f 7368 6120 616e 6420 6e6f 7420 6e6f  t_sha and not no",
            "+00023fc0: 5f63 6865 636b 6f75 743a 0a20 2020 2020  _checkout:.     ",
            "+00023fd0: 2020 2020 2020 2020 2020 2023 2043 6865             # Che",
            "+00023fe0: 636b 6f75 7420 7468 6520 6e65 7874 2063  ckout the next c",
            "+00023ff0: 6f6d 6d69 740a 2020 2020 2020 2020 2020  ommit.          ",
            "+00024000: 2020 2020 2020 6f6c 645f 7472 6565 203d        old_tree =",
            "+00024010: 2072 5b72 2e68 6561 6428 295d 2e74 7265   r[r.head()].tre",
            "+00024020: 6520 6966 2072 2e68 6561 6428 2920 656c  e if r.head() el",
            "+00024030: 7365 204e 6f6e 650a 2020 2020 2020 2020  se None.        ",
            "+00024040: 2020 2020 2020 2020 722e 7265 6673 5b62          r.refs[b",
            "+00024050: 2248 4541 4422 5d20 3d20 6e65 7874 5f73  \"HEAD\"] = next_s",
            "+00024060: 6861 0a20 2020 2020 2020 2020 2020 2020  ha.             ",
            "+00024070: 2020 2063 6f6d 6d69 7420 3d20 725b 6e65     commit = r[ne",
            "+00024080: 7874 5f73 6861 5d0a 2020 2020 2020 2020  xt_sha].        ",
            "+00024090: 2020 2020 2020 2020 7570 6461 7465 5f77          update_w",
            "+000240a0: 6f72 6b69 6e67 5f74 7265 6528 722c 206f  orking_tree(r, o",
            "+000240b0: 6c64 5f74 7265 652c 2063 6f6d 6d69 742e  ld_tree, commit.",
            "+000240c0: 7472 6565 290a 2020 2020 2020 2020 2020  tree).          ",
            "+000240d0: 2020 7265 7475 726e 206e 6578 745f 7368    return next_sh",
            "+000240e0: 610a 0a0a 6465 6620 6269 7365 6374 5f62  a...def bisect_b",
            "+000240f0: 6164 2872 6570 6f3d 222e 222c 2072 6576  ad(repo=\".\", rev",
            "+00024100: 3d4e 6f6e 6529 3a0a 2020 2020 2222 224d  =None):.    \"\"\"M",
            "+00024110: 6172 6b20 6120 636f 6d6d 6974 2061 7320  ark a commit as ",
            "+00024120: 6261 642e 0a0a 2020 2020 4172 6773 3a0a  bad...    Args:.",
            "+00024130: 2020 2020 2020 2020 7265 706f 3a20 5061          repo: Pa",
            "+00024140: 7468 2074 6f20 7265 706f 7369 746f 7279  th to repository",
            "+00024150: 206f 7220 6120 5265 706f 206f 626a 6563   or a Repo objec",
            "+00024160: 740a 2020 2020 2020 2020 7265 763a 2043  t.        rev: C",
            "+00024170: 6f6d 6d69 7420 746f 206d 6172 6b20 6173  ommit to mark as",
            "+00024180: 2062 6164 2028 6465 6661 756c 7473 2074   bad (defaults t",
            "+00024190: 6f20 4845 4144 290a 0a20 2020 2052 6574  o HEAD)..    Ret",
            "+000241a0: 7572 6e73 3a0a 2020 2020 2020 2020 5468  urns:.        Th",
            "+000241b0: 6520 5348 4120 6f66 2074 6865 206e 6578  e SHA of the nex",
            "+000241c0: 7420 636f 6d6d 6974 2074 6f20 7465 7374  t commit to test",
            "+000241d0: 2c20 6f72 204e 6f6e 6520 6966 2062 6973  , or None if bis",
            "+000241e0: 6563 7420 6973 2063 6f6d 706c 6574 650a  ect is complete.",
            "+000241f0: 2020 2020 2222 220a 2020 2020 7769 7468      \"\"\".    with",
            "+00024200: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "+00024210: 6e67 2872 6570 6f29 2061 7320 723a 0a20  ng(repo) as r:. ",
            "+00024220: 2020 2020 2020 2073 7461 7465 203d 2042         state = B",
            "+00024230: 6973 6563 7453 7461 7465 2872 290a 2020  isectState(r).  ",
            "+00024240: 2020 2020 2020 7265 765f 7368 6120 3d20        rev_sha = ",
            "+00024250: 7061 7273 655f 636f 6d6d 6974 2872 2c20  parse_commit(r, ",
            "+00024260: 7265 7629 2e69 6420 6966 2072 6576 2065  rev).id if rev e",
            "+00024270: 6c73 6520 4e6f 6e65 0a20 2020 2020 2020  lse None.       ",
            "+00024280: 206e 6578 745f 7368 6120 3d20 7374 6174   next_sha = stat",
            "+00024290: 652e 6d61 726b 5f62 6164 2872 6576 5f73  e.mark_bad(rev_s",
            "+000242a0: 6861 290a 0a20 2020 2020 2020 2069 6620  ha)..        if ",
            "+000242b0: 6e65 7874 5f73 6861 3a0a 2020 2020 2020  next_sha:.      ",
            "+000242c0: 2020 2020 2020 2320 4368 6563 6b6f 7574        # Checkout",
            "+000242d0: 2074 6865 206e 6578 7420 636f 6d6d 6974   the next commit",
            "+000242e0: 0a20 2020 2020 2020 2020 2020 206f 6c64  .            old",
            "+000242f0: 5f74 7265 6520 3d20 725b 722e 6865 6164  _tree = r[r.head",
            "+00024300: 2829 5d2e 7472 6565 2069 6620 722e 6865  ()].tree if r.he",
            "+00024310: 6164 2829 2065 6c73 6520 4e6f 6e65 0a20  ad() else None. ",
            "+00024320: 2020 2020 2020 2020 2020 2072 2e72 6566             r.ref",
            "+00024330: 735b 6222 4845 4144 225d 203d 206e 6578  s[b\"HEAD\"] = nex",
            "+00024340: 745f 7368 610a 2020 2020 2020 2020 2020  t_sha.          ",
            "+00024350: 2020 636f 6d6d 6974 203d 2072 5b6e 6578    commit = r[nex",
            "+00024360: 745f 7368 615d 0a20 2020 2020 2020 2020  t_sha].         ",
            "+00024370: 2020 2075 7064 6174 655f 776f 726b 696e     update_workin",
            "+00024380: 675f 7472 6565 2872 2c20 6f6c 645f 7472  g_tree(r, old_tr",
            "+00024390: 6565 2c20 636f 6d6d 6974 2e74 7265 6529  ee, commit.tree)",
            "+000243a0: 0a0a 2020 2020 2020 2020 7265 7475 726e  ..        return",
            "+000243b0: 206e 6578 745f 7368 610a 0a0a 6465 6620   next_sha...def ",
            "+000243c0: 6269 7365 6374 5f67 6f6f 6428 7265 706f  bisect_good(repo",
            "+000243d0: 3d22 2e22 2c20 7265 763d 4e6f 6e65 293a  =\".\", rev=None):",
            "+000243e0: 0a20 2020 2022 2222 4d61 726b 2061 2063  .    \"\"\"Mark a c",
            "+000243f0: 6f6d 6d69 7420 6173 2067 6f6f 642e 0a0a  ommit as good...",
            "+00024400: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+00024410: 2020 7265 706f 3a20 5061 7468 2074 6f20    repo: Path to ",
            "+00024420: 7265 706f 7369 746f 7279 206f 7220 6120  repository or a ",
            "+00024430: 5265 706f 206f 626a 6563 740a 2020 2020  Repo object.    ",
            "+00024440: 2020 2020 7265 763a 2043 6f6d 6d69 7420      rev: Commit ",
            "+00024450: 746f 206d 6172 6b20 6173 2067 6f6f 6420  to mark as good ",
            "+00024460: 2864 6566 6175 6c74 7320 746f 2048 4541  (defaults to HEA",
            "+00024470: 4429 0a0a 2020 2020 5265 7475 726e 733a  D)..    Returns:",
            "+00024480: 0a20 2020 2020 2020 2054 6865 2053 4841  .        The SHA",
            "+00024490: 206f 6620 7468 6520 6e65 7874 2063 6f6d   of the next com",
            "+000244a0: 6d69 7420 746f 2074 6573 742c 206f 7220  mit to test, or ",
            "+000244b0: 4e6f 6e65 2069 6620 6269 7365 6374 2069  None if bisect i",
            "+000244c0: 7320 636f 6d70 6c65 7465 0a20 2020 2022  s complete.    \"",
            "+000244d0: 2222 0a20 2020 2077 6974 6820 6f70 656e  \"\".    with open",
            "+000244e0: 5f72 6570 6f5f 636c 6f73 696e 6728 7265  _repo_closing(re",
            "+000244f0: 706f 2920 6173 2072 3a0a 2020 2020 2020  po) as r:.      ",
            "+00024500: 2020 7374 6174 6520 3d20 4269 7365 6374    state = Bisect",
            "+00024510: 5374 6174 6528 7229 0a20 2020 2020 2020  State(r).       ",
            "+00024520: 2072 6576 5f73 6861 203d 2070 6172 7365   rev_sha = parse",
            "+00024530: 5f63 6f6d 6d69 7428 722c 2072 6576 292e  _commit(r, rev).",
            "+00024540: 6964 2069 6620 7265 7620 656c 7365 204e  id if rev else N",
            "+00024550: 6f6e 650a 2020 2020 2020 2020 6e65 7874  one.        next",
            "+00024560: 5f73 6861 203d 2073 7461 7465 2e6d 6172  _sha = state.mar",
            "+00024570: 6b5f 676f 6f64 2872 6576 5f73 6861 290a  k_good(rev_sha).",
            "+00024580: 0a20 2020 2020 2020 2069 6620 6e65 7874  .        if next",
            "+00024590: 5f73 6861 3a0a 2020 2020 2020 2020 2020  _sha:.          ",
            "+000245a0: 2020 2320 4368 6563 6b6f 7574 2074 6865    # Checkout the",
            "+000245b0: 206e 6578 7420 636f 6d6d 6974 0a20 2020   next commit.   ",
            "+000245c0: 2020 2020 2020 2020 206f 6c64 5f74 7265           old_tre",
            "+000245d0: 6520 3d20 725b 722e 6865 6164 2829 5d2e  e = r[r.head()].",
            "+000245e0: 7472 6565 2069 6620 722e 6865 6164 2829  tree if r.head()",
            "+000245f0: 2065 6c73 6520 4e6f 6e65 0a20 2020 2020   else None.     ",
            "+00024600: 2020 2020 2020 2072 2e72 6566 735b 6222         r.refs[b\"",
            "+00024610: 4845 4144 225d 203d 206e 6578 745f 7368  HEAD\"] = next_sh",
            "+00024620: 610a 2020 2020 2020 2020 2020 2020 636f  a.            co",
            "+00024630: 6d6d 6974 203d 2072 5b6e 6578 745f 7368  mmit = r[next_sh",
            "+00024640: 615d 0a20 2020 2020 2020 2020 2020 2075  a].            u",
            "+00024650: 7064 6174 655f 776f 726b 696e 675f 7472  pdate_working_tr",
            "+00024660: 6565 2872 2c20 6f6c 645f 7472 6565 2c20  ee(r, old_tree, ",
            "+00024670: 636f 6d6d 6974 2e74 7265 6529 0a0a 2020  commit.tree)..  ",
            "+00024680: 2020 2020 2020 7265 7475 726e 206e 6578        return nex",
            "+00024690: 745f 7368 610a 0a0a 6465 6620 6269 7365  t_sha...def bise",
            "+000246a0: 6374 5f73 6b69 7028 7265 706f 3d22 2e22  ct_skip(repo=\".\"",
            "+000246b0: 2c20 7265 7673 3d4e 6f6e 6529 3a0a 2020  , revs=None):.  ",
            "+000246c0: 2020 2222 2253 6b69 7020 6f6e 6520 6f72    \"\"\"Skip one or",
            "+000246d0: 206d 6f72 6520 636f 6d6d 6974 732e 0a0a   more commits...",
            "+000246e0: 2020 2020 4172 6773 3a0a 2020 2020 2020      Args:.      ",
            "+000246f0: 2020 7265 706f 3a20 5061 7468 2074 6f20    repo: Path to ",
            "+00024700: 7265 706f 7369 746f 7279 206f 7220 6120  repository or a ",
            "+00024710: 5265 706f 206f 626a 6563 740a 2020 2020  Repo object.    ",
            "+00024720: 2020 2020 7265 7673 3a20 4c69 7374 206f      revs: List o",
            "+00024730: 6620 636f 6d6d 6974 7320 746f 2073 6b69  f commits to ski",
            "+00024740: 7020 2864 6566 6175 6c74 7320 746f 205b  p (defaults to [",
            "+00024750: 4845 4144 5d29 0a0a 2020 2020 5265 7475  HEAD])..    Retu",
            "+00024760: 726e 733a 0a20 2020 2020 2020 2054 6865  rns:.        The",
            "+00024770: 2053 4841 206f 6620 7468 6520 6e65 7874   SHA of the next",
            "+00024780: 2063 6f6d 6d69 7420 746f 2074 6573 742c   commit to test,",
            "+00024790: 206f 7220 4e6f 6e65 2069 6620 6269 7365   or None if bise",
            "+000247a0: 6374 2069 7320 636f 6d70 6c65 7465 0a20  ct is complete. ",
            "+000247b0: 2020 2022 2222 0a20 2020 2077 6974 6820     \"\"\".    with ",
            "+000247c0: 6f70 656e 5f72 6570 6f5f 636c 6f73 696e  open_repo_closin",
            "+000247d0: 6728 7265 706f 2920 6173 2072 3a0a 2020  g(repo) as r:.  ",
            "+000247e0: 2020 2020 2020 7374 6174 6520 3d20 4269        state = Bi",
            "+000247f0: 7365 6374 5374 6174 6528 7229 0a0a 2020  sectState(r)..  ",
            "+00024800: 2020 2020 2020 6966 2072 6576 7320 6973        if revs is",
            "+00024810: 204e 6f6e 653a 0a20 2020 2020 2020 2020   None:.         ",
            "+00024820: 2020 2072 6576 5f73 6861 7320 3d20 4e6f     rev_shas = No",
            "+00024830: 6e65 0a20 2020 2020 2020 2065 6c73 653a  ne.        else:",
            "+00024840: 0a20 2020 2020 2020 2020 2020 2023 2043  .            # C",
            "+00024850: 6f6e 7665 7274 2073 696e 676c 6520 7265  onvert single re",
            "+00024860: 7620 746f 206c 6973 740a 2020 2020 2020  v to list.      ",
            "+00024870: 2020 2020 2020 6966 206e 6f74 2069 7369        if not isi",
            "+00024880: 6e73 7461 6e63 6528 7265 7673 2c20 6c69  nstance(revs, li",
            "+00024890: 7374 293a 0a20 2020 2020 2020 2020 2020  st):.           ",
            "+000248a0: 2020 2020 2072 6576 7320 3d20 5b72 6576       revs = [rev",
            "+000248b0: 735d 0a20 2020 2020 2020 2020 2020 2072  s].            r",
            "+000248c0: 6576 5f73 6861 7320 3d20 5b70 6172 7365  ev_shas = [parse",
            "+000248d0: 5f63 6f6d 6d69 7428 722c 2072 6576 292e  _commit(r, rev).",
            "+000248e0: 6964 2066 6f72 2072 6576 2069 6e20 7265  id for rev in re",
            "+000248f0: 7673 5d0a 0a20 2020 2020 2020 206e 6578  vs]..        nex",
            "+00024900: 745f 7368 6120 3d20 7374 6174 652e 736b  t_sha = state.sk",
            "+00024910: 6970 2872 6576 5f73 6861 7329 0a0a 2020  ip(rev_shas)..  ",
            "+00024920: 2020 2020 2020 6966 206e 6578 745f 7368        if next_sh",
            "+00024930: 613a 0a20 2020 2020 2020 2020 2020 2023  a:.            #",
            "+00024940: 2043 6865 636b 6f75 7420 7468 6520 6e65   Checkout the ne",
            "+00024950: 7874 2063 6f6d 6d69 740a 2020 2020 2020  xt commit.      ",
            "+00024960: 2020 2020 2020 6f6c 645f 7472 6565 203d        old_tree =",
            "+00024970: 2072 5b72 2e68 6561 6428 295d 2e74 7265   r[r.head()].tre",
            "+00024980: 6520 6966 2072 2e68 6561 6428 2920 656c  e if r.head() el",
            "+00024990: 7365 204e 6f6e 650a 2020 2020 2020 2020  se None.        ",
            "+000249a0: 2020 2020 722e 7265 6673 5b62 2248 4541      r.refs[b\"HEA",
            "+000249b0: 4422 5d20 3d20 6e65 7874 5f73 6861 0a20  D\"] = next_sha. ",
            "+000249c0: 2020 2020 2020 2020 2020 2063 6f6d 6d69             commi",
            "+000249d0: 7420 3d20 725b 6e65 7874 5f73 6861 5d0a  t = r[next_sha].",
            "+000249e0: 2020 2020 2020 2020 2020 2020 7570 6461              upda",
            "+000249f0: 7465 5f77 6f72 6b69 6e67 5f74 7265 6528  te_working_tree(",
            "+00024a00: 722c 206f 6c64 5f74 7265 652c 2063 6f6d  r, old_tree, com",
            "+00024a10: 6d69 742e 7472 6565 290a 0a20 2020 2020  mit.tree)..     ",
            "+00024a20: 2020 2072 6574 7572 6e20 6e65 7874 5f73     return next_s",
            "+00024a30: 6861 0a0a 0a64 6566 2062 6973 6563 745f  ha...def bisect_",
            "+00024a40: 7265 7365 7428 7265 706f 3d22 2e22 2c20  reset(repo=\".\", ",
            "+00024a50: 636f 6d6d 6974 3d4e 6f6e 6529 3a0a 2020  commit=None):.  ",
            "+00024a60: 2020 2222 2252 6573 6574 2062 6973 6563    \"\"\"Reset bisec",
            "+00024a70: 7420 7374 6174 6520 616e 6420 7265 7475  t state and retu",
            "+00024a80: 726e 2074 6f20 6f72 6967 696e 616c 2062  rn to original b",
            "+00024a90: 7261 6e63 682f 636f 6d6d 6974 2e0a 0a20  ranch/commit... ",
            "+00024aa0: 2020 2041 7267 733a 0a20 2020 2020 2020     Args:.       ",
            "+00024ab0: 2072 6570 6f3a 2050 6174 6820 746f 2072   repo: Path to r",
            "+00024ac0: 6570 6f73 6974 6f72 7920 6f72 2061 2052  epository or a R",
            "+00024ad0: 6570 6f20 6f62 6a65 6374 0a20 2020 2020  epo object.     ",
            "+00024ae0: 2020 2063 6f6d 6d69 743a 204f 7074 696f     commit: Optio",
            "+00024af0: 6e61 6c20 636f 6d6d 6974 2074 6f20 7265  nal commit to re",
            "+00024b00: 7365 7420 746f 2028 6465 6661 756c 7473  set to (defaults",
            "+00024b10: 2074 6f20 6f72 6967 696e 616c 2062 7261   to original bra",
            "+00024b20: 6e63 682f 636f 6d6d 6974 290a 2020 2020  nch/commit).    ",
            "+00024b30: 2222 220a 2020 2020 7769 7468 206f 7065  \"\"\".    with ope",
            "+00024b40: 6e5f 7265 706f 5f63 6c6f 7369 6e67 2872  n_repo_closing(r",
            "+00024b50: 6570 6f29 2061 7320 723a 0a20 2020 2020  epo) as r:.     ",
            "+00024b60: 2020 2073 7461 7465 203d 2042 6973 6563     state = Bisec",
            "+00024b70: 7453 7461 7465 2872 290a 2020 2020 2020  tState(r).      ",
            "+00024b80: 2020 2320 4765 7420 6f6c 6420 7472 6565    # Get old tree",
            "+00024b90: 2062 6566 6f72 6520 7265 7365 740a 2020   before reset.  ",
            "+00024ba0: 2020 2020 2020 7472 793a 0a20 2020 2020        try:.     ",
            "+00024bb0: 2020 2020 2020 206f 6c64 5f74 7265 6520         old_tree ",
            "+00024bc0: 3d20 725b 722e 6865 6164 2829 5d2e 7472  = r[r.head()].tr",
            "+00024bd0: 6565 0a20 2020 2020 2020 2065 7863 6570  ee.        excep",
            "+00024be0: 7420 4b65 7945 7272 6f72 3a0a 2020 2020  t KeyError:.    ",
            "+00024bf0: 2020 2020 2020 2020 6f6c 645f 7472 6565          old_tree",
            "+00024c00: 203d 204e 6f6e 650a 0a20 2020 2020 2020   = None..       ",
            "+00024c10: 2063 6f6d 6d69 745f 7368 6120 3d20 7061   commit_sha = pa",
            "+00024c20: 7273 655f 636f 6d6d 6974 2872 2c20 636f  rse_commit(r, co",
            "+00024c30: 6d6d 6974 292e 6964 2069 6620 636f 6d6d  mmit).id if comm",
            "+00024c40: 6974 2065 6c73 6520 4e6f 6e65 0a20 2020  it else None.   ",
            "+00024c50: 2020 2020 2073 7461 7465 2e72 6573 6574       state.reset",
            "+00024c60: 2863 6f6d 6d69 745f 7368 6129 0a0a 2020  (commit_sha)..  ",
            "+00024c70: 2020 2020 2020 2320 5570 6461 7465 2077        # Update w",
            "+00024c80: 6f72 6b69 6e67 2074 7265 6520 746f 206e  orking tree to n",
            "+00024c90: 6577 2048 4541 440a 2020 2020 2020 2020  ew HEAD.        ",
            "+00024ca0: 7472 793a 0a20 2020 2020 2020 2020 2020  try:.           ",
            "+00024cb0: 206e 6577 5f68 6561 6420 3d20 722e 6865   new_head = r.he",
            "+00024cc0: 6164 2829 0a20 2020 2020 2020 2020 2020  ad().           ",
            "+00024cd0: 2069 6620 6e65 775f 6865 6164 3a0a 2020   if new_head:.  ",
            "+00024ce0: 2020 2020 2020 2020 2020 2020 2020 6e65                ne",
            "+00024cf0: 775f 636f 6d6d 6974 203d 2072 5b6e 6577  w_commit = r[new",
            "+00024d00: 5f68 6561 645d 0a20 2020 2020 2020 2020  _head].         ",
            "+00024d10: 2020 2020 2020 2075 7064 6174 655f 776f         update_wo",
            "+00024d20: 726b 696e 675f 7472 6565 2872 2c20 6f6c  rking_tree(r, ol",
            "+00024d30: 645f 7472 6565 2c20 6e65 775f 636f 6d6d  d_tree, new_comm",
            "+00024d40: 6974 2e74 7265 6529 0a20 2020 2020 2020  it.tree).       ",
            "+00024d50: 2065 7863 6570 7420 4b65 7945 7272 6f72   except KeyError",
            "+00024d60: 3a0a 2020 2020 2020 2020 2020 2020 2320  :.            # ",
            "+00024d70: 4e6f 2048 4541 4420 6166 7465 7220 7265  No HEAD after re",
            "+00024d80: 7365 740a 2020 2020 2020 2020 2020 2020  set.            ",
            "+00024d90: 7061 7373 0a0a 0a64 6566 2062 6973 6563  pass...def bisec",
            "+00024da0: 745f 6c6f 6728 7265 706f 3d22 2e22 293a  t_log(repo=\".\"):",
            "+00024db0: 0a20 2020 2022 2222 4765 7420 7468 6520  .    \"\"\"Get the ",
            "+00024dc0: 6269 7365 6374 206c 6f67 2e0a 0a20 2020  bisect log...   ",
            "+00024dd0: 2041 7267 733a 0a20 2020 2020 2020 2072   Args:.        r",
            "+00024de0: 6570 6f3a 2050 6174 6820 746f 2072 6570  epo: Path to rep",
            "+00024df0: 6f73 6974 6f72 7920 6f72 2061 2052 6570  ository or a Rep",
            "+00024e00: 6f20 6f62 6a65 6374 0a0a 2020 2020 5265  o object..    Re",
            "+00024e10: 7475 726e 733a 0a20 2020 2020 2020 2054  turns:.        T",
            "+00024e20: 6865 2062 6973 6563 7420 6c6f 6720 6173  he bisect log as",
            "+00024e30: 2061 2073 7472 696e 670a 2020 2020 2222   a string.    \"\"",
            "+00024e40: 220a 2020 2020 7769 7468 206f 7065 6e5f  \".    with open_",
            "+00024e50: 7265 706f 5f63 6c6f 7369 6e67 2872 6570  repo_closing(rep",
            "+00024e60: 6f29 2061 7320 723a 0a20 2020 2020 2020  o) as r:.       ",
            "+00024e70: 2073 7461 7465 203d 2042 6973 6563 7453   state = BisectS",
            "+00024e80: 7461 7465 2872 290a 2020 2020 2020 2020  tate(r).        ",
            "+00024e90: 7265 7475 726e 2073 7461 7465 2e67 6574  return state.get",
            "+00024ea0: 5f6c 6f67 2829 0a0a 0a64 6566 2062 6973  _log()...def bis",
            "+00024eb0: 6563 745f 7265 706c 6179 2872 6570 6f2c  ect_replay(repo,",
            "+00024ec0: 206c 6f67 5f66 696c 6529 3a0a 2020 2020   log_file):.    ",
            "+00024ed0: 2222 2252 6570 6c61 7920 6120 6269 7365  \"\"\"Replay a bise",
            "+00024ee0: 6374 206c 6f67 2e0a 0a20 2020 2041 7267  ct log...    Arg",
            "+00024ef0: 733a 0a20 2020 2020 2020 2072 6570 6f3a  s:.        repo:",
            "+00024f00: 2050 6174 6820 746f 2072 6570 6f73 6974   Path to reposit",
            "+00024f10: 6f72 7920 6f72 2061 2052 6570 6f20 6f62  ory or a Repo ob",
            "+00024f20: 6a65 6374 0a20 2020 2020 2020 206c 6f67  ject.        log",
            "+00024f30: 5f66 696c 653a 2050 6174 6820 746f 2074  _file: Path to t",
            "+00024f40: 6865 206c 6f67 2066 696c 6520 6f72 2066  he log file or f",
            "+00024f50: 696c 652d 6c69 6b65 206f 626a 6563 740a  ile-like object.",
            "+00024f60: 2020 2020 2222 220a 2020 2020 7769 7468      \"\"\".    with",
            "+00024f70: 206f 7065 6e5f 7265 706f 5f63 6c6f 7369   open_repo_closi",
            "+00024f80: 6e67 2872 6570 6f29 2061 7320 723a 0a20  ng(repo) as r:. ",
            "+00024f90: 2020 2020 2020 2073 7461 7465 203d 2042         state = B",
            "+00024fa0: 6973 6563 7453 7461 7465 2872 290a 0a20  isectState(r).. ",
            "+00024fb0: 2020 2020 2020 2069 6620 6973 696e 7374         if isinst",
            "+00024fc0: 616e 6365 286c 6f67 5f66 696c 652c 2073  ance(log_file, s",
            "+00024fd0: 7472 293a 0a20 2020 2020 2020 2020 2020  tr):.           ",
            "+00024fe0: 2077 6974 6820 6f70 656e 286c 6f67 5f66   with open(log_f",
            "+00024ff0: 696c 6529 2061 7320 663a 0a20 2020 2020  ile) as f:.     ",
            "+00025000: 2020 2020 2020 2020 2020 206c 6f67 5f63             log_c",
            "+00025010: 6f6e 7465 6e74 203d 2066 2e72 6561 6428  ontent = f.read(",
            "+00025020: 290a 2020 2020 2020 2020 656c 7365 3a0a  ).        else:.",
            "+00025030: 2020 2020 2020 2020 2020 2020 6c6f 675f              log_",
            "+00025040: 636f 6e74 656e 7420 3d20 6c6f 675f 6669  content = log_fi",
            "+00025050: 6c65 2e72 6561 6428 290a 0a20 2020 2020  le.read()..     ",
            "+00025060: 2020 2073 7461 7465 2e72 6570 6c61 7928     state.replay(",
            "+00025070: 6c6f 675f 636f 6e74 656e 7429 0a         log_content)."
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/protocol.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/protocol.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/protocol.py",
            "@@ -18,17 +18,19 @@",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Generic functions for talking the git smart server protocol.\"\"\"",
            " ",
            "+import types",
            "+from collections.abc import Iterable",
            " from io import BytesIO",
            " from os import SEEK_END",
            "-from typing import Optional",
            "+from typing import Callable, Optional",
            " ",
            " import dulwich",
            " ",
            " from .errors import GitProtocolError, HangupException",
            " ",
            " TCP_GIT_PORT = 9418",
            " ",
            "@@ -124,88 +126,90 @@",
            " )",
            " ",
            " DEPTH_INFINITE = 0x7FFFFFFF",
            " ",
            " NAK_LINE = b\"NAK\\n\"",
            " ",
            " ",
            "-def agent_string():",
            "+def agent_string() -> bytes:",
            "     return (\"dulwich/\" + \".\".join(map(str, dulwich.__version__))).encode(\"ascii\")",
            " ",
            " ",
            "-def capability_agent():",
            "+def capability_agent() -> bytes:",
            "     return CAPABILITY_AGENT + b\"=\" + agent_string()",
            " ",
            " ",
            "-def capability_symref(from_ref, to_ref):",
            "+def capability_symref(from_ref: bytes, to_ref: bytes) -> bytes:",
            "     return CAPABILITY_SYMREF + b\"=\" + from_ref + b\":\" + to_ref",
            " ",
            " ",
            "-def extract_capability_names(capabilities):",
            "+def extract_capability_names(capabilities: Iterable[bytes]) -> set[bytes]:",
            "     return {parse_capability(c)[0] for c in capabilities}",
            " ",
            " ",
            "-def parse_capability(capability):",
            "+def parse_capability(capability: bytes) -> tuple[bytes, Optional[bytes]]:",
            "     parts = capability.split(b\"=\", 1)",
            "     if len(parts) == 1:",
            "         return (parts[0], None)",
            "-    return tuple(parts)",
            "+    return (parts[0], parts[1])",
            " ",
            " ",
            "-def symref_capabilities(symrefs):",
            "+def symref_capabilities(symrefs: Iterable[tuple[bytes, bytes]]) -> list[bytes]:",
            "     return [capability_symref(*k) for k in symrefs]",
            " ",
            " ",
            " COMMAND_DEEPEN = b\"deepen\"",
            " COMMAND_SHALLOW = b\"shallow\"",
            " COMMAND_UNSHALLOW = b\"unshallow\"",
            " COMMAND_DONE = b\"done\"",
            " COMMAND_WANT = b\"want\"",
            " COMMAND_HAVE = b\"have\"",
            " ",
            " ",
            "-def format_cmd_pkt(cmd, *args):",
            "+def format_cmd_pkt(cmd: bytes, *args: bytes) -> bytes:",
            "     return cmd + b\" \" + b\"\".join([(a + b\"\\0\") for a in args])",
            " ",
            " ",
            "-def parse_cmd_pkt(line):",
            "+def parse_cmd_pkt(line: bytes) -> tuple[bytes, list[bytes]]:",
            "     splice_at = line.find(b\" \")",
            "     cmd, args = line[:splice_at], line[splice_at + 1 :]",
            "     assert args[-1:] == b\"\\x00\"",
            "     return cmd, args[:-1].split(b\"\\0\")",
            " ",
            " ",
            "-def pkt_line(data):",
            "+def pkt_line(data: Optional[bytes]) -> bytes:",
            "     \"\"\"Wrap data in a pkt-line.",
            " ",
            "     Args:",
            "       data: The data to wrap, as a str or None.",
            "     Returns: The data prefixed with its length in pkt-line format; if data was",
            "         None, returns the flush-pkt ('0000').",
            "     \"\"\"",
            "     if data is None:",
            "         return b\"0000\"",
            "     return (\"%04x\" % (len(data) + 4)).encode(\"ascii\") + data",
            " ",
            " ",
            "-def pkt_seq(*seq):",
            "+def pkt_seq(*seq: Optional[bytes]) -> bytes:",
            "     \"\"\"Wrap a sequence of data in pkt-lines.",
            " ",
            "     Args:",
            "       seq: An iterable of strings to wrap.",
            "     \"\"\"",
            "     return b\"\".join([pkt_line(s) for s in seq]) + pkt_line(None)",
            " ",
            " ",
            "-def filter_ref_prefix(refs, prefixes):",
            "+def filter_ref_prefix(",
            "+    refs: dict[bytes, bytes], prefixes: Iterable[bytes]",
            "+) -> dict[bytes, bytes]:",
            "     \"\"\"Filter refs to only include those with a given prefix.",
            " ",
            "     Args:",
            "       refs: A list of refs.",
            "-      prefix: The prefix to filter by.",
            "+      prefixes: The prefixes to filter by.",
            "     \"\"\"",
            "     return {k: v for k, v in refs.items() if any(k.startswith(p) for p in prefixes)}",
            " ",
            " ",
            " class Protocol:",
            "     \"\"\"Class for interacting with a remote git process over the wire.",
            " ",
            "@@ -214,32 +218,43 @@",
            "     payload data. The length includes the 4-byte header. The special line",
            "     '0000' indicates the end of a section of input and is called a 'flush-pkt'.",
            " ",
            "     For details on the pkt-line format, see the cgit distribution:",
            "         Documentation/technical/protocol-common.txt",
            "     \"\"\"",
            " ",
            "-    def __init__(self, read, write, close=None, report_activity=None) -> None:",
            "+    def __init__(",
            "+        self,",
            "+        read: Callable[[int], bytes],",
            "+        write: Callable[[bytes], Optional[int]],",
            "+        close: Optional[Callable[[], None]] = None,",
            "+        report_activity: Optional[Callable[[int, str], None]] = None,",
            "+    ) -> None:",
            "         self.read = read",
            "         self.write = write",
            "         self._close = close",
            "         self.report_activity = report_activity",
            "         self._readahead: Optional[BytesIO] = None",
            " ",
            "     def close(self) -> None:",
            "         if self._close:",
            "             self._close()",
            " ",
            "-    def __enter__(self):",
            "+    def __enter__(self) -> \"Protocol\":",
            "         return self",
            " ",
            "-    def __exit__(self, exc_type, exc_val, exc_tb):",
            "+    def __exit__(",
            "+        self,",
            "+        exc_type: Optional[type[BaseException]],",
            "+        exc_val: Optional[BaseException],",
            "+        exc_tb: Optional[types.TracebackType],",
            "+    ) -> None:",
            "         self.close()",
            " ",
            "-    def read_pkt_line(self):",
            "+    def read_pkt_line(self) -> Optional[bytes]:",
            "         \"\"\"Reads a pkt-line from the remote git process.",
            " ",
            "         This method may read from the readahead buffer; see unread_pkt_line.",
            " ",
            "         Returns: The next string from the stream, without the length prefix, or",
            "             None for a flush-pkt ('0000') or delim-pkt ('0001').",
            "         \"\"\"",
            "@@ -283,15 +298,15 @@",
            "         try:",
            "             next_line = self.read_pkt_line()",
            "         except HangupException:",
            "             return True",
            "         self.unread_pkt_line(next_line)",
            "         return False",
            " ",
            "-    def unread_pkt_line(self, data) -> None:",
            "+    def unread_pkt_line(self, data: Optional[bytes]) -> None:",
            "         \"\"\"Unread a single line of data into the readahead buffer.",
            " ",
            "         This method can be used to unread a single pkt-line into a fixed",
            "         readahead buffer.",
            " ",
            "         Args:",
            "           data: The data to unread, without the length prefix.",
            "@@ -299,77 +314,79 @@",
            "         Raises:",
            "           ValueError: If more than one pkt-line is unread.",
            "         \"\"\"",
            "         if self._readahead is not None:",
            "             raise ValueError(\"Attempted to unread multiple pkt-lines.\")",
            "         self._readahead = BytesIO(pkt_line(data))",
            " ",
            "-    def read_pkt_seq(self):",
            "+    def read_pkt_seq(self) -> Iterable[bytes]:",
            "         \"\"\"Read a sequence of pkt-lines from the remote git process.",
            " ",
            "         Returns: Yields each line of data up to but not including the next",
            "             flush-pkt.",
            "         \"\"\"",
            "         pkt = self.read_pkt_line()",
            "         while pkt:",
            "             yield pkt",
            "             pkt = self.read_pkt_line()",
            " ",
            "-    def write_pkt_line(self, line) -> None:",
            "+    def write_pkt_line(self, line: Optional[bytes]) -> None:",
            "         \"\"\"Sends a pkt-line to the remote git process.",
            " ",
            "         Args:",
            "           line: A string containing the data to send, without the length",
            "             prefix.",
            "         \"\"\"",
            "         try:",
            "             line = pkt_line(line)",
            "             self.write(line)",
            "             if self.report_activity:",
            "                 self.report_activity(len(line), \"write\")",
            "         except OSError as exc:",
            "             raise GitProtocolError(str(exc)) from exc",
            " ",
            "-    def write_sideband(self, channel, blob) -> None:",
            "+    def write_sideband(self, channel: int, blob: bytes) -> None:",
            "         \"\"\"Write multiplexed data to the sideband.",
            " ",
            "         Args:",
            "           channel: An int specifying the channel to write to.",
            "           blob: A blob of data (as a string) to send on this channel.",
            "         \"\"\"",
            "         # a pktline can be a max of 65520. a sideband line can therefore be",
            "         # 65520-5 = 65515",
            "         # WTF: Why have the len in ASCII, but the channel in binary.",
            "         while blob:",
            "             self.write_pkt_line(bytes(bytearray([channel])) + blob[:65515])",
            "             blob = blob[65515:]",
            " ",
            "-    def send_cmd(self, cmd, *args) -> None:",
            "+    def send_cmd(self, cmd: bytes, *args: bytes) -> None:",
            "         \"\"\"Send a command and some arguments to a git server.",
            " ",
            "         Only used for the TCP git protocol (git://).",
            " ",
            "         Args:",
            "           cmd: The remote service to access.",
            "           args: List of arguments to send to remove service.",
            "         \"\"\"",
            "         self.write_pkt_line(format_cmd_pkt(cmd, *args))",
            " ",
            "-    def read_cmd(self):",
            "+    def read_cmd(self) -> tuple[bytes, list[bytes]]:",
            "         \"\"\"Read a command and some arguments from the git client.",
            " ",
            "         Only used for the TCP git protocol (git://).",
            " ",
            "         Returns: A tuple of (command, [list of arguments]).",
            "         \"\"\"",
            "         line = self.read_pkt_line()",
            "+        if line is None:",
            "+            raise GitProtocolError(\"Expected command, got flush packet\")",
            "         return parse_cmd_pkt(line)",
            " ",
            " ",
            "-_RBUFSIZE = 8192  # Default read buffer size.",
            "+_RBUFSIZE = 65536  # 64KB buffer for better network I/O performance",
            " ",
            " ",
            " class ReceivableProtocol(Protocol):",
            "     \"\"\"Variant of Protocol that allows reading up to a size without blocking.",
            " ",
            "     This class has a recv() method that behaves like socket.recv() in addition",
            "     to a read() method.",
            "@@ -377,22 +394,27 @@",
            "     If you want to read n bytes from the wire and block until exactly n bytes",
            "     (or EOF) are read, use read(n). If you want to read at most n bytes from",
            "     the wire but don't care if you get less, use recv(n). Note that recv(n)",
            "     will still block until at least one byte is read.",
            "     \"\"\"",
            " ",
            "     def __init__(",
            "-        self, recv, write, close=None, report_activity=None, rbufsize=_RBUFSIZE",
            "+        self,",
            "+        recv: Callable[[int], bytes],",
            "+        write: Callable[[bytes], Optional[int]],",
            "+        close: Optional[Callable[[], None]] = None,",
            "+        report_activity: Optional[Callable[[int, str], None]] = None,",
            "+        rbufsize: int = _RBUFSIZE,",
            "     ) -> None:",
            "         super().__init__(self.read, write, close=close, report_activity=report_activity)",
            "         self._recv = recv",
            "         self._rbuf = BytesIO()",
            "         self._rbufsize = rbufsize",
            " ",
            "-    def read(self, size):",
            "+    def read(self, size: int) -> bytes:",
            "         # From _fileobj.read in socket.py in the Python 2.6.5 standard library,",
            "         # with the following modifications:",
            "         #  - omit the size <= 0 branch",
            "         #  - seek back to start rather than 0 in case some buffer has been",
            "         #    consumed.",
            "         #  - use SEEK_END instead of the magic number.",
            "         # Copyright (c) 2001-2010 Python Software Foundation; All Rights",
            "@@ -445,15 +467,15 @@",
            "             buf.write(data)",
            "             buf_len += n",
            "             del data  # explicit free",
            "             # assert buf_len == buf.tell()",
            "         buf.seek(start)",
            "         return buf.read()",
            " ",
            "-    def recv(self, size):",
            "+    def recv(self, size: int) -> bytes:",
            "         assert size > 0",
            " ",
            "         buf = self._rbuf",
            "         start = buf.tell()",
            "         buf.seek(0, SEEK_END)",
            "         buf_len = buf.tell()",
            "         buf.seek(start)",
            "@@ -469,28 +491,28 @@",
            "             buf.write(data)",
            "             buf.seek(0)",
            "             del data  # explicit free",
            "             self._rbuf = buf",
            "         return buf.read(size)",
            " ",
            " ",
            "-def extract_capabilities(text):",
            "+def extract_capabilities(text: bytes) -> tuple[bytes, list[bytes]]:",
            "     \"\"\"Extract a capabilities list from a string, if present.",
            " ",
            "     Args:",
            "       text: String to extract from",
            "     Returns: Tuple with text with capabilities removed and list of capabilities",
            "     \"\"\"",
            "     if b\"\\0\" not in text:",
            "         return text, []",
            "     text, capabilities = text.rstrip().split(b\"\\0\")",
            "     return (text, capabilities.strip().split(b\" \"))",
            " ",
            " ",
            "-def extract_want_line_capabilities(text):",
            "+def extract_want_line_capabilities(text: bytes) -> tuple[bytes, list[bytes]]:",
            "     \"\"\"Extract a capabilities list from a want line, if present.",
            " ",
            "     Note that want lines have capabilities separated from the rest of the line",
            "     by a space instead of a null byte. Thus want lines have the form:",
            " ",
            "         want obj-id cap1 cap2 ...",
            " ",
            "@@ -500,15 +522,15 @@",
            "     \"\"\"",
            "     split_text = text.rstrip().split(b\" \")",
            "     if len(split_text) < 3:",
            "         return text, []",
            "     return (b\" \".join(split_text[:2]), split_text[2:])",
            " ",
            " ",
            "-def ack_type(capabilities):",
            "+def ack_type(capabilities: Iterable[bytes]) -> int:",
            "     \"\"\"Extract the ack type from a capabilities list.\"\"\"",
            "     if b\"multi_ack_detailed\" in capabilities:",
            "         return MULTI_ACK_DETAILED",
            "     elif b\"multi_ack\" in capabilities:",
            "         return MULTI_ACK",
            "     return SINGLE_ACK",
            " ",
            "@@ -517,27 +539,29 @@",
            "     \"\"\"Writer that wraps its data in pkt-lines and has an independent buffer.",
            " ",
            "     Consecutive calls to write() wrap the data in a pkt-line and then buffers",
            "     it until enough lines have been written such that their total length",
            "     (including length prefix) reach the buffer size.",
            "     \"\"\"",
            " ",
            "-    def __init__(self, write, bufsize=65515) -> None:",
            "+    def __init__(",
            "+        self, write: Callable[[bytes], Optional[int]], bufsize: int = 65515",
            "+    ) -> None:",
            "         \"\"\"Initialize the BufferedPktLineWriter.",
            " ",
            "         Args:",
            "           write: A write callback for the underlying writer.",
            "           bufsize: The internal buffer size, including length prefixes.",
            "         \"\"\"",
            "         self._write = write",
            "         self._bufsize = bufsize",
            "         self._wbuf = BytesIO()",
            "         self._buflen = 0",
            " ",
            "-    def write(self, data) -> None:",
            "+    def write(self, data: bytes) -> None:",
            "         \"\"\"Write data, wrapping it in a pkt-line.\"\"\"",
            "         line = pkt_line(data)",
            "         line_len = len(line)",
            "         over = self._buflen + line_len - self._bufsize",
            "         if over >= 0:",
            "             start = line_len - over",
            "             self._wbuf.write(line[:start])",
            "@@ -556,19 +580,19 @@",
            "         self._len = 0",
            "         self._wbuf = BytesIO()",
            " ",
            " ",
            " class PktLineParser:",
            "     \"\"\"Packet line parser that hands completed packets off to a callback.\"\"\"",
            " ",
            "-    def __init__(self, handle_pkt) -> None:",
            "+    def __init__(self, handle_pkt: Callable[[Optional[bytes]], None]) -> None:",
            "         self.handle_pkt = handle_pkt",
            "         self._readahead = BytesIO()",
            " ",
            "-    def parse(self, data) -> None:",
            "+    def parse(self, data: bytes) -> None:",
            "         \"\"\"Parse a fragment of data and call back for any completed packets.\"\"\"",
            "         self._readahead.write(data)",
            "         buf = self._readahead.getvalue()",
            "         if len(buf) < 4:",
            "             return",
            "         while len(buf) >= 4:",
            "             size = int(buf[:4], 16)",
            "@@ -579,35 +603,37 @@",
            "                 self.handle_pkt(buf[4:size])",
            "                 buf = buf[size:]",
            "             else:",
            "                 break",
            "         self._readahead = BytesIO()",
            "         self._readahead.write(buf)",
            " ",
            "-    def get_tail(self):",
            "+    def get_tail(self) -> bytes:",
            "         \"\"\"Read back any unused data.\"\"\"",
            "         return self._readahead.getvalue()",
            " ",
            " ",
            "-def format_capability_line(capabilities):",
            "+def format_capability_line(capabilities: Iterable[bytes]) -> bytes:",
            "     return b\"\".join([b\" \" + c for c in capabilities])",
            " ",
            " ",
            "-def format_ref_line(ref, sha, capabilities=None):",
            "+def format_ref_line(",
            "+    ref: bytes, sha: bytes, capabilities: Optional[list[bytes]] = None",
            "+) -> bytes:",
            "     if capabilities is None:",
            "         return sha + b\" \" + ref + b\"\\n\"",
            "     else:",
            "         return sha + b\" \" + ref + b\"\\0\" + format_capability_line(capabilities) + b\"\\n\"",
            " ",
            " ",
            "-def format_shallow_line(sha):",
            "+def format_shallow_line(sha: bytes) -> bytes:",
            "     return COMMAND_SHALLOW + b\" \" + sha",
            " ",
            " ",
            "-def format_unshallow_line(sha):",
            "+def format_unshallow_line(sha: bytes) -> bytes:",
            "     return COMMAND_UNSHALLOW + b\" \" + sha",
            " ",
            " ",
            "-def format_ack_line(sha, ack_type=b\"\"):",
            "+def format_ack_line(sha: bytes, ack_type: bytes = b\"\") -> bytes:",
            "     if ack_type:",
            "         ack_type = b\" \" + ack_type",
            "     return b\"ACK \" + sha + ack_type + b\"\\n\""
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/reflog.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/reflog.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/reflog.py",
            "@@ -18,24 +18,33 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Utilities for reading and generating reflogs.\"\"\"",
            " ",
            " import collections",
            "+from collections.abc import Generator",
            "+from typing import BinaryIO, Optional, Union",
            " ",
            " from .objects import ZERO_SHA, format_timezone, parse_timezone",
            " ",
            " Entry = collections.namedtuple(",
            "     \"Entry\",",
            "     [\"old_sha\", \"new_sha\", \"committer\", \"timestamp\", \"timezone\", \"message\"],",
            " )",
            " ",
            " ",
            "-def format_reflog_line(old_sha, new_sha, committer, timestamp, timezone, message):",
            "+def format_reflog_line(",
            "+    old_sha: Optional[bytes],",
            "+    new_sha: bytes,",
            "+    committer: bytes,",
            "+    timestamp: Union[int, float],",
            "+    timezone: int,",
            "+    message: bytes,",
            "+) -> bytes:",
            "     \"\"\"Generate a single reflog line.",
            " ",
            "     Args:",
            "       old_sha: Old Commit SHA",
            "       new_sha: New Commit SHA",
            "       committer: Committer name and e-mail",
            "       timestamp: Timestamp",
            "@@ -55,15 +64,15 @@",
            "         + b\" \"",
            "         + format_timezone(timezone)",
            "         + b\"\\t\"",
            "         + message",
            "     )",
            " ",
            " ",
            "-def parse_reflog_line(line):",
            "+def parse_reflog_line(line: bytes) -> Entry:",
            "     \"\"\"Parse a reflog line.",
            " ",
            "     Args:",
            "       line: Line to parse",
            "     Returns: Tuple of (old_sha, new_sha, committer, timestamp, timezone,",
            "         message)",
            "     \"\"\"",
            "@@ -76,26 +85,26 @@",
            "         committer,",
            "         int(timestamp_str),",
            "         parse_timezone(timezone_str)[0],",
            "         message,",
            "     )",
            " ",
            " ",
            "-def read_reflog(f):",
            "+def read_reflog(f: BinaryIO) -> Generator[Entry, None, None]:",
            "     \"\"\"Read reflog.",
            " ",
            "     Args:",
            "       f: File-like object",
            "     Returns: Iterator over Entry objects",
            "     \"\"\"",
            "     for line in f:",
            "         yield parse_reflog_line(line)",
            " ",
            " ",
            "-def drop_reflog_entry(f, index, rewrite=False) -> None:",
            "+def drop_reflog_entry(f: BinaryIO, index: int, rewrite: bool = False) -> None:",
            "     \"\"\"Drop the specified reflog entry.",
            " ",
            "     Args:",
            "         f: File-like object",
            "         index: Reflog entry index (in Git reflog reverse 0-indexed order)",
            "         rewrite: If a reflog entry's predecessor is removed, set its",
            "             old SHA to the new SHA of the entry that now precedes it"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/refs.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/refs.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/refs.py",
            "@@ -22,28 +22,29 @@",
            " ",
            " \"\"\"Ref handling.\"\"\"",
            " ",
            " import os",
            " import warnings",
            " from collections.abc import Iterator",
            " from contextlib import suppress",
            "-from typing import Any, Optional",
            "+from typing import Any, Optional, Union",
            " ",
            " from .errors import PackedRefsException, RefFormatError",
            " from .file import GitFile, ensure_dir_exists",
            " from .objects import ZERO_SHA, ObjectID, Tag, git_line, valid_hexsha",
            " from .pack import ObjectContainer",
            " ",
            " Ref = bytes",
            " ",
            " HEADREF = b\"HEAD\"",
            " SYMREF = b\"ref: \"",
            " LOCAL_BRANCH_PREFIX = b\"refs/heads/\"",
            " LOCAL_TAG_PREFIX = b\"refs/tags/\"",
            " LOCAL_REMOTE_PREFIX = b\"refs/remotes/\"",
            "+LOCAL_NOTES_PREFIX = b\"refs/notes/\"",
            " BAD_REF_CHARS = set(b\"\\177 ~^:?*[\")",
            " PEELED_TAG_SUFFIX = b\"^{}\"",
            " ",
            " # For backwards compatibility",
            " ANNOTATED_TAG_SUFFIX = PEELED_TAG_SUFFIX",
            " ",
            " ",
            "@@ -97,14 +98,41 @@",
            "     if b\"@{\" in refname:",
            "         return False",
            "     if b\"\\\\\" in refname:",
            "         return False",
            "     return True",
            " ",
            " ",
            "+def parse_remote_ref(ref: bytes) -> tuple[bytes, bytes]:",
            "+    \"\"\"Parse a remote ref into remote name and branch name.",
            "+",
            "+    Args:",
            "+      ref: Remote ref like b\"refs/remotes/origin/main\"",
            "+",
            "+    Returns:",
            "+      Tuple of (remote_name, branch_name)",
            "+",
            "+    Raises:",
            "+      ValueError: If ref is not a valid remote ref",
            "+    \"\"\"",
            "+    if not ref.startswith(LOCAL_REMOTE_PREFIX):",
            "+        raise ValueError(f\"Not a remote ref: {ref!r}\")",
            "+",
            "+    # Remove the prefix",
            "+    remainder = ref[len(LOCAL_REMOTE_PREFIX) :]",
            "+",
            "+    # Split into remote name and branch name",
            "+    parts = remainder.split(b\"/\", 1)",
            "+    if len(parts) != 2:",
            "+        raise ValueError(f\"Invalid remote ref format: {ref!r}\")",
            "+",
            "+    remote_name, branch_name = parts",
            "+    return (remote_name, branch_name)",
            "+",
            "+",
            " class RefsContainer:",
            "     \"\"\"A container for refs.\"\"\"",
            " ",
            "     def __init__(self, logger=None) -> None:",
            "         self._logger = logger",
            " ",
            "     def _log(",
            "@@ -433,14 +461,22 @@",
            "                 dst = parse_symref_value(self.read_ref(src))",
            "             except ValueError:",
            "                 pass",
            "             else:",
            "                 ret[src] = dst",
            "         return ret",
            " ",
            "+    def pack_refs(self, all: bool = False) -> None:",
            "+        \"\"\"Pack loose refs into packed-refs file.",
            "+",
            "+        Args:",
            "+            all: If True, pack all refs. If False, only pack tags.",
            "+        \"\"\"",
            "+        raise NotImplementedError(self.pack_refs)",
            "+",
            " ",
            " class DictRefsContainer(RefsContainer):",
            "     \"\"\"RefsContainer backed by a simple dict.",
            " ",
            "     This container does not support symbolic or packed references and is not",
            "     threadsafe.",
            "     \"\"\"",
            "@@ -495,29 +531,28 @@",
            "         committer=None,",
            "         timestamp=None,",
            "         timezone=None,",
            "         message=None,",
            "     ) -> bool:",
            "         if old_ref is not None and self._refs.get(name, ZERO_SHA) != old_ref:",
            "             return False",
            "-        realnames, _ = self.follow(name)",
            "-        for realname in realnames:",
            "-            self._check_refname(realname)",
            "-            old = self._refs.get(realname)",
            "-            self._refs[realname] = new_ref",
            "-            self._notify(realname, new_ref)",
            "-            self._log(",
            "-                realname,",
            "-                old,",
            "-                new_ref,",
            "-                committer=committer,",
            "-                timestamp=timestamp,",
            "-                timezone=timezone,",
            "-                message=message,",
            "-            )",
            "+        # Only update the specific ref requested, not the whole chain",
            "+        self._check_refname(name)",
            "+        old = self._refs.get(name)",
            "+        self._refs[name] = new_ref",
            "+        self._notify(name, new_ref)",
            "+        self._log(",
            "+            name,",
            "+            old,",
            "+            new_ref,",
            "+            committer=committer,",
            "+            timestamp=timestamp,",
            "+            timezone=timezone,",
            "+            message=message,",
            "+        )",
            "         return True",
            " ",
            "     def add_if_new(",
            "         self,",
            "         name: Ref,",
            "         ref: ObjectID,",
            "         committer=None,",
            "@@ -607,24 +642,27 @@",
            "         except KeyError:",
            "             return self._refs[name]",
            " ",
            " ",
            " class DiskRefsContainer(RefsContainer):",
            "     \"\"\"Refs container that reads refs from disk.\"\"\"",
            " ",
            "-    def __init__(self, path, worktree_path=None, logger=None) -> None:",
            "+    def __init__(",
            "+        self,",
            "+        path: Union[str, bytes, os.PathLike],",
            "+        worktree_path: Optional[Union[str, bytes, os.PathLike]] = None,",
            "+        logger=None,",
            "+    ) -> None:",
            "         super().__init__(logger=logger)",
            "-        if getattr(path, \"encode\", None) is not None:",
            "-            path = os.fsencode(path)",
            "-        self.path = path",
            "+        # Convert path-like objects to strings, then to bytes for Git compatibility",
            "+        self.path = os.fsencode(os.fspath(path))",
            "         if worktree_path is None:",
            "-            worktree_path = path",
            "-        if getattr(worktree_path, \"encode\", None) is not None:",
            "-            worktree_path = os.fsencode(worktree_path)",
            "-        self.worktree_path = worktree_path",
            "+            self.worktree_path = self.path",
            "+        else:",
            "+            self.worktree_path = os.fsencode(os.fspath(worktree_path))",
            "         self._packed_refs = None",
            "         self._peeled_refs = None",
            " ",
            "     def __repr__(self) -> str:",
            "         return f\"{self.__class__.__name__}({self.path!r})\"",
            " ",
            "     def subkeys(self, base):",
            "@@ -1047,14 +1085,37 @@",
            "                 # removed by another process, being not empty, etc.",
            "                 # in any case, this is non fatal because we already",
            "                 # removed the reference, just ignore it",
            "                 break",
            " ",
            "         return True",
            " ",
            "+    def pack_refs(self, all: bool = False) -> None:",
            "+        \"\"\"Pack loose refs into packed-refs file.",
            "+",
            "+        Args:",
            "+            all: If True, pack all refs. If False, only pack tags.",
            "+        \"\"\"",
            "+        refs_to_pack: dict[Ref, Optional[ObjectID]] = {}",
            "+        for ref in self.allkeys():",
            "+            if ref == HEADREF:",
            "+                # Never pack HEAD",
            "+                continue",
            "+            if all or ref.startswith(LOCAL_TAG_PREFIX):",
            "+                try:",
            "+                    sha = self[ref]",
            "+                    if sha:",
            "+                        refs_to_pack[ref] = sha",
            "+                except KeyError:",
            "+                    # Broken ref, skip it",
            "+                    pass",
            "+",
            "+        if refs_to_pack:",
            "+            self.add_packed_refs(refs_to_pack)",
            "+",
            " ",
            " def _split_ref_line(line):",
            "     \"\"\"Split a single ref line into a tuple of SHA1 and name.\"\"\"",
            "     fields = line.rstrip(b\"\\n\\r\").split(b\" \")",
            "     if len(fields) != 2:",
            "         raise PackedRefsException(f\"invalid ref line {line!r}\")",
            "     sha, name = fields"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/repo.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/repo.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/repo.py",
            "@@ -45,16 +45,18 @@",
            "     Union,",
            " )",
            " ",
            " if TYPE_CHECKING:",
            "     # There are no circular imports here, but we try to defer imports as long",
            "     # as possible to reduce start-up time for anything that doesn't need",
            "     # these imports.",
            "-    from .config import ConfigFile, StackedConfig",
            "+    from .attrs import GitAttributes",
            "+    from .config import ConditionMatcher, ConfigFile, StackedConfig",
            "     from .index import Index",
            "+    from .notes import Notes",
            " ",
            " from .errors import (",
            "     CommitError,",
            "     HookError,",
            "     NoIndexPresent,",
            "     NotBlobError,",
            "     NotCommitError,",
            "@@ -74,14 +76,15 @@",
            " from .line_ending import BlobNormalizer, TreeBlobNormalizer",
            " from .object_store import (",
            "     DiskObjectStore,",
            "     MemoryObjectStore,",
            "     MissingObjectFinder,",
            "     ObjectStoreGraphWalker,",
            "     PackBasedObjectStore,",
            "+    find_shallow,",
            "     peel_sha,",
            " )",
            " from .objects import (",
            "     Blob,",
            "     Commit,",
            "     ObjectID,",
            "     ShaFile,",
            "@@ -330,21 +333,32 @@",
            " ",
            " class ParentsProvider:",
            "     def __init__(self, store, grafts={}, shallows=[]) -> None:",
            "         self.store = store",
            "         self.grafts = grafts",
            "         self.shallows = set(shallows)",
            " ",
            "+        # Get commit graph once at initialization for performance",
            "+        self.commit_graph = store.get_commit_graph()",
            "+",
            "     def get_parents(self, commit_id, commit=None):",
            "         try:",
            "             return self.grafts[commit_id]",
            "         except KeyError:",
            "             pass",
            "         if commit_id in self.shallows:",
            "             return []",
            "+",
            "+        # Try to use commit graph for faster parent lookup",
            "+        if self.commit_graph:",
            "+            parents = self.commit_graph.get_parents(commit_id)",
            "+            if parents is not None:",
            "+                return parents",
            "+",
            "+        # Fallback to reading the commit object",
            "         if commit is None:",
            "             commit = self.store[commit_id]",
            "         return commit.parents",
            " ",
            " ",
            " class BaseRepo:",
            "     \"\"\"Base class for a git repository.",
            "@@ -386,22 +400,28 @@",
            "         \"\"\"Probe the filesystem to determine whether symlinks can be created.",
            " ",
            "         Returns: True if symlinks can be created, False otherwise.",
            "         \"\"\"",
            "         # For now, just mimic the old behaviour",
            "         return sys.platform != \"win32\"",
            " ",
            "-    def _init_files(self, bare: bool, symlinks: Optional[bool] = None) -> None:",
            "+    def _init_files(",
            "+        self, bare: bool, symlinks: Optional[bool] = None, format: Optional[int] = None",
            "+    ) -> None:",
            "         \"\"\"Initialize a default set of named files.\"\"\"",
            "         from .config import ConfigFile",
            " ",
            "         self._put_named_file(\"description\", b\"Unnamed repository\")",
            "         f = BytesIO()",
            "         cf = ConfigFile()",
            "-        cf.set(\"core\", \"repositoryformatversion\", \"0\")",
            "+        if format is None:",
            "+            format = 0",
            "+        if format not in (0, 1):",
            "+            raise ValueError(f\"Unsupported repository format version: {format}\")",
            "+        cf.set(\"core\", \"repositoryformatversion\", str(format))",
            "         if self._determine_file_mode():",
            "             cf.set(\"core\", \"filemode\", True)",
            "         else:",
            "             cf.set(\"core\", \"filemode\", False)",
            " ",
            "         if symlinks is None and not bare:",
            "             symlinks = self._determine_symlinks()",
            "@@ -446,15 +466,17 @@",
            " ",
            "         Raises:",
            "           NoIndexPresent: If no index is present",
            "         Returns: The matching `Index`",
            "         \"\"\"",
            "         raise NotImplementedError(self.open_index)",
            " ",
            "-    def fetch(self, target, determine_wants=None, progress=None, depth=None):",
            "+    def fetch(",
            "+        self, target, determine_wants=None, progress=None, depth: Optional[int] = None",
            "+    ):",
            "         \"\"\"Fetch objects into another repository.",
            " ",
            "         Args:",
            "           target: The target repository",
            "           determine_wants: Optional function to determine what refs to",
            "             fetch.",
            "           progress: Optional progress function",
            "@@ -473,16 +495,17 @@",
            "         return self.get_refs()",
            " ",
            "     def fetch_pack_data(",
            "         self,",
            "         determine_wants,",
            "         graph_walker,",
            "         progress,",
            "+        *,",
            "         get_tagged=None,",
            "-        depth=None,",
            "+        depth: Optional[int] = None,",
            "     ):",
            "         \"\"\"Fetch the pack data required for a set of revisions.",
            " ",
            "         Args:",
            "           determine_wants: Function that takes a dictionary with heads",
            "             and returns the list of heads to fetch.",
            "           graph_walker: Object that can iterate over the list of revisions",
            "@@ -492,29 +515,32 @@",
            "             updated progress strings.",
            "           get_tagged: Function that returns a dict of pointed-to sha ->",
            "             tag sha for including tags.",
            "           depth: Shallow fetch depth",
            "         Returns: count and iterator over pack data",
            "         \"\"\"",
            "         missing_objects = self.find_missing_objects(",
            "-            determine_wants, graph_walker, progress, get_tagged, depth=depth",
            "+            determine_wants, graph_walker, progress, get_tagged=get_tagged, depth=depth",
            "         )",
            "+        if missing_objects is None:",
            "+            return 0, iter([])",
            "         remote_has = missing_objects.get_remote_has()",
            "         object_ids = list(missing_objects)",
            "         return len(object_ids), generate_unpacked_objects(",
            "             self.object_store, object_ids, progress=progress, other_haves=remote_has",
            "         )",
            " ",
            "     def find_missing_objects(",
            "         self,",
            "         determine_wants,",
            "         graph_walker,",
            "         progress,",
            "+        *,",
            "         get_tagged=None,",
            "-        depth=None,",
            "+        depth: Optional[int] = None,",
            "     ) -> Optional[MissingObjectFinder]:",
            "         \"\"\"Fetch the missing objects required for a set of revisions.",
            " ",
            "         Args:",
            "           determine_wants: Function that takes a dictionary with heads",
            "             and returns the list of heads to fetch.",
            "           graph_walker: Object that can iterate over the list of revisions",
            "@@ -523,33 +549,39 @@",
            "           progress: Simple progress function that will be called with",
            "             updated progress strings.",
            "           get_tagged: Function that returns a dict of pointed-to sha ->",
            "             tag sha for including tags.",
            "           depth: Shallow fetch depth",
            "         Returns: iterator over objects, with __len__ implemented",
            "         \"\"\"",
            "-        if depth not in (None, 0):",
            "-            raise NotImplementedError(\"depth not supported yet\")",
            "-",
            "         refs = serialize_refs(self.object_store, self.get_refs())",
            " ",
            "         wants = determine_wants(refs)",
            "         if not isinstance(wants, list):",
            "             raise TypeError(\"determine_wants() did not return a list\")",
            " ",
            "-        shallows: frozenset[ObjectID] = getattr(graph_walker, \"shallow\", frozenset())",
            "-        unshallows: frozenset[ObjectID] = getattr(",
            "-            graph_walker, \"unshallow\", frozenset()",
            "-        )",
            "+        current_shallow = set(getattr(graph_walker, \"shallow\", set()))",
            "+",
            "+        if depth not in (None, 0):",
            "+            shallow, not_shallow = find_shallow(self.object_store, wants, depth)",
            "+            # Only update if graph_walker has shallow attribute",
            "+            if hasattr(graph_walker, \"shallow\"):",
            "+                graph_walker.shallow.update(shallow - not_shallow)",
            "+                new_shallow = graph_walker.shallow - current_shallow",
            "+                unshallow = graph_walker.unshallow = not_shallow & current_shallow",
            "+                if hasattr(graph_walker, \"update_shallow\"):",
            "+                    graph_walker.update_shallow(new_shallow, unshallow)",
            "+        else:",
            "+            unshallow = getattr(graph_walker, \"unshallow\", frozenset())",
            " ",
            "         if wants == []:",
            "             # TODO(dborowitz): find a way to short-circuit that doesn't change",
            "             # this interface.",
            " ",
            "-            if shallows or unshallows:",
            "+            if getattr(graph_walker, \"shallow\", set()) or unshallow:",
            "                 # Do not send a pack in shallow short-circuit path",
            "                 return None",
            " ",
            "             class DummyMissingObjectFinder:",
            "                 def get_remote_has(self) -> None:",
            "                     return None",
            " ",
            "@@ -564,29 +596,29 @@",
            "         # If the graph walker is set up with an implementation that can",
            "         # ACK/NAK to the wire, it will write data to the client through",
            "         # this call as a side-effect.",
            "         haves = self.object_store.find_common_revisions(graph_walker)",
            " ",
            "         # Deal with shallow requests separately because the haves do",
            "         # not reflect what objects are missing",
            "-        if shallows or unshallows:",
            "+        if getattr(graph_walker, \"shallow\", set()) or unshallow:",
            "             # TODO: filter the haves commits from iter_shas. the specific",
            "             # commits aren't missing.",
            "             haves = []",
            " ",
            "-        parents_provider = ParentsProvider(self.object_store, shallows=shallows)",
            "+        parents_provider = ParentsProvider(self.object_store, shallows=current_shallow)",
            " ",
            "         def get_parents(commit):",
            "             return parents_provider.get_parents(commit.id, commit)",
            " ",
            "         return MissingObjectFinder(",
            "             self.object_store,",
            "             haves=haves,",
            "             wants=wants,",
            "-            shallow=self.get_shallow(),",
            "+            shallow=getattr(graph_walker, \"shallow\", set()),",
            "             progress=progress,",
            "             get_tagged=get_tagged,",
            "             get_parents=get_parents,",
            "         )",
            " ",
            "     def generate_pack_data(",
            "         self,",
            "@@ -627,15 +659,18 @@",
            "             heads = [",
            "                 sha",
            "                 for sha in self.refs.as_dict(b\"refs/heads\").values()",
            "                 if sha in self.object_store",
            "             ]",
            "         parents_provider = ParentsProvider(self.object_store)",
            "         return ObjectStoreGraphWalker(",
            "-            heads, parents_provider.get_parents, shallow=self.get_shallow()",
            "+            heads,",
            "+            parents_provider.get_parents,",
            "+            shallow=self.get_shallow(),",
            "+            update_shallow=self.update_shallow,",
            "         )",
            " ",
            "     def get_refs(self) -> dict[bytes, bytes]:",
            "         \"\"\"Get dictionary with all refs.",
            " ",
            "         Returns: A ``dict`` mapping ref names to SHA1s",
            "         \"\"\"",
            "@@ -715,14 +750,21 @@",
            "         \"\"\"Set the description for this repository.",
            " ",
            "         Args:",
            "           description: Text to set as description for this repository.",
            "         \"\"\"",
            "         raise NotImplementedError(self.set_description)",
            " ",
            "+    def get_rebase_state_manager(self):",
            "+        \"\"\"Get the appropriate rebase state manager for this repository.",
            "+",
            "+        Returns: RebaseStateManager instance",
            "+        \"\"\"",
            "+        raise NotImplementedError(self.get_rebase_state_manager)",
            "+",
            "     def get_config_stack(self) -> \"StackedConfig\":",
            "         \"\"\"Return a config stack for this repository.",
            " ",
            "         This stack accesses the configuration for both this repository",
            "         itself (.git/config) and the global configuration, which usually",
            "         lives in ~/.gitconfig.",
            " ",
            "@@ -776,20 +818,33 @@",
            "             this will equal the original SHA1.",
            "         \"\"\"",
            "         cached = self.refs.get_peeled(ref)",
            "         if cached is not None:",
            "             return cached",
            "         return peel_sha(self.object_store, self.refs[ref])[1].id",
            " ",
            "-    def get_walker(self, include: Optional[list[bytes]] = None, *args, **kwargs):",
            "+    @property",
            "+    def notes(self) -> \"Notes\":",
            "+        \"\"\"Access notes functionality for this repository.",
            "+",
            "+        Returns:",
            "+            Notes object for accessing notes",
            "+        \"\"\"",
            "+        from .notes import Notes",
            "+",
            "+        return Notes(self.object_store, self.refs)",
            "+",
            "+    def get_walker(self, include: Optional[list[bytes]] = None, **kwargs):",
            "         \"\"\"Obtain a walker for this repository.",
            " ",
            "         Args:",
            "           include: Iterable of SHAs of commits to include along with their",
            "             ancestors. Defaults to [HEAD]",
            "+",
            "+        Keyword Args:",
            "           exclude: Iterable of SHAs of commits to exclude along with their",
            "             ancestors, overriding includes.",
            "           order: ORDER_* constant specifying the order of results.",
            "             Anything other than ORDER_DATE may result in O(n) memory usage.",
            "           reverse: If True, reverse the order of output, requiring O(n)",
            "             memory.",
            "           max_entries: The maximum number of entries to yield, or None for",
            "@@ -800,24 +855,25 @@",
            "           follow: If True, follow path across renames/copies. Forces a",
            "             default rename_detector.",
            "           since: Timestamp to list commits after.",
            "           until: Timestamp to list commits before.",
            "           queue_cls: A class to use for a queue of commits, supporting the",
            "             iterator protocol. The constructor takes a single argument, the",
            "             Walker.",
            "+",
            "         Returns: A `Walker` object",
            "         \"\"\"",
            "         from .walk import Walker",
            " ",
            "         if include is None:",
            "             include = [self.head()]",
            " ",
            "         kwargs[\"get_parents\"] = lambda commit: self.get_parents(commit.id, commit)",
            " ",
            "-        return Walker(self.object_store, include, *args, **kwargs)",
            "+        return Walker(self.object_store, include, **kwargs)",
            " ",
            "     def __getitem__(self, name: Union[ObjectID, Ref]):",
            "         \"\"\"Retrieve a Git object by SHA1 or ref.",
            " ",
            "         Args:",
            "           name: A Git object SHA1 or a ref name",
            "         Returns: A `ShaFile` object, such as a Commit or Blob",
            "@@ -921,15 +977,15 @@",
            "         author: Optional[bytes] = None,",
            "         commit_timestamp=None,",
            "         commit_timezone=None,",
            "         author_timestamp=None,",
            "         author_timezone=None,",
            "         tree: Optional[ObjectID] = None,",
            "         encoding: Optional[bytes] = None,",
            "-        ref: Ref = b\"HEAD\",",
            "+        ref: Optional[Ref] = b\"HEAD\",",
            "         merge_heads: Optional[list[ObjectID]] = None,",
            "         no_verify: bool = False,",
            "         sign: bool = False,",
            "     ):",
            "         \"\"\"Create a new commit.",
            " ",
            "         If not specified, committer and author default to",
            "@@ -945,15 +1001,16 @@",
            "           author_timestamp: Author timestamp (defaults to commit",
            "             timestamp)",
            "           author_timezone: Author timestamp timezone",
            "             (defaults to commit timestamp timezone)",
            "           tree: SHA1 of the tree root to use (if not specified the",
            "             current index will be committed).",
            "           encoding: Encoding",
            "-          ref: Optional ref to commit to (defaults to current branch)",
            "+          ref: Optional ref to commit to (defaults to current branch).",
            "+            If None, creates a dangling commit without updating any ref.",
            "           merge_heads: Merge heads (defaults to .git/MERGE_HEAD)",
            "           no_verify: Skip pre-commit and commit-msg hooks",
            "           sign: GPG Sign the commit (bool, defaults to False,",
            "             pass True to use default GPG key,",
            "             pass a str containing Key ID to use a specific GPG key)",
            " ",
            "         Returns:",
            "@@ -1021,41 +1078,50 @@",
            "                 if c.message is None:",
            "                     c.message = message",
            "         except HookError as exc:",
            "             raise CommitError(exc) from exc",
            "         except KeyError:  # no hook defined, message not modified",
            "             c.message = message",
            " ",
            "+        # Check if we should sign the commit",
            "+        should_sign = sign",
            "+        if sign is None:",
            "+            # Check commit.gpgSign configuration when sign is not explicitly set",
            "+            config = self.get_config_stack()",
            "+            try:",
            "+                should_sign = config.get_boolean((b\"commit\",), b\"gpgSign\")",
            "+            except KeyError:",
            "+                should_sign = False  # Default to not signing if no config",
            "         keyid = sign if isinstance(sign, str) else None",
            " ",
            "         if ref is None:",
            "             # Create a dangling commit",
            "             c.parents = merge_heads",
            "-            if sign:",
            "+            if should_sign:",
            "                 c.sign(keyid)",
            "             self.object_store.add_object(c)",
            "         else:",
            "             try:",
            "                 old_head = self.refs[ref]",
            "                 c.parents = [old_head, *merge_heads]",
            "-                if sign:",
            "+                if should_sign:",
            "                     c.sign(keyid)",
            "                 self.object_store.add_object(c)",
            "                 ok = self.refs.set_if_equals(",
            "                     ref,",
            "                     old_head,",
            "                     c.id,",
            "                     message=b\"commit: \" + message,",
            "                     committer=committer,",
            "                     timestamp=commit_timestamp,",
            "                     timezone=commit_timezone,",
            "                 )",
            "             except KeyError:",
            "                 c.parents = merge_heads",
            "-                if sign:",
            "+                if should_sign:",
            "                     c.sign(keyid)",
            "                 self.object_store.add_object(c)",
            "                 ok = self.refs.add_if_new(",
            "                     ref,",
            "                     c.id,",
            "                     message=b\"commit: \" + message,",
            "                     committer=committer,",
            "@@ -1072,14 +1138,19 @@",
            "         try:",
            "             self.hooks[\"post-commit\"].execute()",
            "         except HookError as e:  # silent failure",
            "             warnings.warn(f\"post-commit hook failed: {e}\", UserWarning)",
            "         except KeyError:  # no hook defined, silent fallthrough",
            "             pass",
            " ",
            "+        # Trigger auto GC if needed",
            "+        from .gc import maybe_auto_gc",
            "+",
            "+        maybe_auto_gc(self)",
            "+",
            "         return c.id",
            " ",
            " ",
            " def read_gitfile(f):",
            "     \"\"\"Read a ``.git`` file.",
            " ",
            "     The first line of the file should start with \"gitdir: \"",
            "@@ -1127,18 +1198,29 @@",
            "     \"\"\"",
            " ",
            "     path: str",
            "     bare: bool",
            " ",
            "     def __init__(",
            "         self,",
            "-        root: str,",
            "+        root: Union[str, bytes, os.PathLike],",
            "         object_store: Optional[PackBasedObjectStore] = None,",
            "         bare: Optional[bool] = None,",
            "     ) -> None:",
            "+        \"\"\"Open a repository on disk.",
            "+",
            "+        Args:",
            "+          root: Path to the repository's root.",
            "+          object_store: ObjectStore to use; if omitted, we use the",
            "+            repository's default object store",
            "+          bare: True if this is a bare repository.",
            "+        \"\"\"",
            "+        root = os.fspath(root)",
            "+        if isinstance(root, bytes):",
            "+            root = os.fsdecode(root)",
            "         hidden_path = os.path.join(root, CONTROLDIR)",
            "         if bare is None:",
            "             if os.path.isfile(hidden_path) or os.path.isdir(",
            "                 os.path.join(hidden_path, OBJECTDIR)",
            "             ):",
            "                 bare = False",
            "             elif os.path.isdir(os.path.join(root, OBJECTDIR)) and os.path.isdir(",
            "@@ -1166,40 +1248,56 @@",
            "                 self._commondir = os.path.join(",
            "                     self.controldir(),",
            "                     os.fsdecode(commondir.read().rstrip(b\"\\r\\n\")),",
            "                 )",
            "         else:",
            "             self._commondir = self._controldir",
            "         self.path = root",
            "+",
            "+        # Initialize refs early so they're available for config condition matchers",
            "+        self.refs = DiskRefsContainer(",
            "+            self.commondir(), self._controldir, logger=self._write_reflog",
            "+        )",
            "+",
            "         config = self.get_config()",
            "         try:",
            "             repository_format_version = config.get(\"core\", \"repositoryformatversion\")",
            "             format_version = (",
            "                 0",
            "                 if repository_format_version is None",
            "                 else int(repository_format_version)",
            "             )",
            "         except KeyError:",
            "             format_version = 0",
            " ",
            "         if format_version not in (0, 1):",
            "             raise UnsupportedVersion(format_version)",
            " ",
            "-        for extension, _value in config.items((b\"extensions\",)):",
            "-            if extension.lower() not in (b\"worktreeconfig\",):",
            "+        # Track extensions we encounter",
            "+        has_reftable_extension = False",
            "+        for extension, value in config.items((b\"extensions\",)):",
            "+            if extension.lower() == b\"refstorage\":",
            "+                if value == b\"reftable\":",
            "+                    has_reftable_extension = True",
            "+                else:",
            "+                    raise UnsupportedExtension(f\"refStorage = {value.decode()}\")",
            "+            elif extension.lower() not in (b\"worktreeconfig\",):",
            "                 raise UnsupportedExtension(extension)",
            " ",
            "         if object_store is None:",
            "             object_store = DiskObjectStore.from_config(",
            "                 os.path.join(self.commondir(), OBJECTDIR), config",
            "             )",
            "-        refs = DiskRefsContainer(",
            "-            self.commondir(), self._controldir, logger=self._write_reflog",
            "-        )",
            "-        BaseRepo.__init__(self, object_store, refs)",
            "+",
            "+        # Use reftable if extension is configured",
            "+        if has_reftable_extension:",
            "+            from .reftable import ReftableRefsContainer",
            "+",
            "+            self.refs = ReftableRefsContainer(self.commondir())",
            "+        BaseRepo.__init__(self, object_store, self.refs)",
            " ",
            "         self._graftpoints = {}",
            "         graft_file = self.get_named_file(",
            "             os.path.join(\"info\", \"grafts\"), basedir=self.commondir()",
            "         )",
            "         if graft_file:",
            "             with graft_file:",
            "@@ -1357,15 +1455,39 @@",
            "           NoIndexPresent: If no index is present",
            "         Returns: The matching `Index`",
            "         \"\"\"",
            "         from .index import Index",
            " ",
            "         if not self.has_index():",
            "             raise NoIndexPresent",
            "-        return Index(self.index_path())",
            "+",
            "+        # Check for manyFiles feature configuration",
            "+        config = self.get_config_stack()",
            "+        many_files = config.get_boolean(b\"feature\", b\"manyFiles\", False)",
            "+        skip_hash = False",
            "+        index_version = None",
            "+",
            "+        if many_files:",
            "+            # When feature.manyFiles is enabled, set index.version=4 and index.skipHash=true",
            "+            try:",
            "+                index_version_str = config.get(b\"index\", b\"version\")",
            "+                index_version = int(index_version_str)",
            "+            except KeyError:",
            "+                index_version = 4  # Default to version 4 for manyFiles",
            "+            skip_hash = config.get_boolean(b\"index\", b\"skipHash\", True)",
            "+        else:",
            "+            # Check for explicit index settings",
            "+            try:",
            "+                index_version_str = config.get(b\"index\", b\"version\")",
            "+                index_version = int(index_version_str)",
            "+            except KeyError:",
            "+                index_version = None",
            "+            skip_hash = config.get_boolean(b\"index\", b\"skipHash\", False)",
            "+",
            "+        return Index(self.index_path(), skip_hash=skip_hash, version=index_version)",
            " ",
            "     def has_index(self) -> bool:",
            "         \"\"\"Check if an index is present.\"\"\"",
            "         # Bare repos must never have index files; non-bare repos may have a",
            "         # missing index file, which is treated as empty.",
            "         return not self.bare",
            " ",
            "@@ -1481,14 +1603,16 @@",
            "                 dev=st.st_dev if st else 0,",
            "                 ino=st.st_ino if st else 0,",
            "                 mode=tree_entry[0],",
            "                 uid=st.st_uid if st else 0,",
            "                 gid=st.st_gid if st else 0,",
            "                 size=len(self[tree_entry[1]].data),",
            "                 sha=tree_entry[1],",
            "+                flags=0,",
            "+                extended_flags=0,",
            "             )",
            " ",
            "             index[tree_path] = index_entry",
            "         index.write()",
            " ",
            "     def clone(",
            "         self,",
            "@@ -1496,15 +1620,15 @@",
            "         *,",
            "         mkdir=True,",
            "         bare=False,",
            "         origin=b\"origin\",",
            "         checkout=None,",
            "         branch=None,",
            "         progress=None,",
            "-        depth=None,",
            "+        depth: Optional[int] = None,",
            "         symlinks=None,",
            "     ) -> \"Repo\":",
            "         \"\"\"Clone this repository.",
            " ",
            "         Args:",
            "           target_path: Target path",
            "           mkdir: Create the target directory",
            "@@ -1591,75 +1715,165 @@",
            "         Args:",
            "           tree: Tree SHA to reset to, None for current HEAD tree.",
            "         \"\"\"",
            "         from .index import (",
            "             build_index_from_tree,",
            "             symlink,",
            "             validate_path_element_default,",
            "+            validate_path_element_hfs,",
            "             validate_path_element_ntfs,",
            "         )",
            " ",
            "         if tree is None:",
            "             head = self[b\"HEAD\"]",
            "             if isinstance(head, Tag):",
            "                 _cls, obj = head.object",
            "                 head = self.get_object(obj)",
            "             tree = head.tree",
            "         config = self.get_config()",
            "         honor_filemode = config.get_boolean(b\"core\", b\"filemode\", os.name != \"nt\")",
            "         if config.get_boolean(b\"core\", b\"core.protectNTFS\", os.name == \"nt\"):",
            "             validate_path_element = validate_path_element_ntfs",
            "+        elif config.get_boolean(b\"core\", b\"core.protectHFS\", sys.platform == \"darwin\"):",
            "+            validate_path_element = validate_path_element_hfs",
            "         else:",
            "             validate_path_element = validate_path_element_default",
            "         if config.get_boolean(b\"core\", b\"symlinks\", True):",
            "             symlink_fn = symlink",
            "         else:",
            " ",
            "             def symlink_fn(source, target) -> None:  # type: ignore",
            "                 with open(",
            "                     target, \"w\" + (\"b\" if isinstance(source, bytes) else \"\")",
            "                 ) as f:",
            "                     f.write(source)",
            " ",
            "+        blob_normalizer = self.get_blob_normalizer()",
            "         return build_index_from_tree(",
            "             self.path,",
            "             self.index_path(),",
            "             self.object_store,",
            "             tree,",
            "             honor_filemode=honor_filemode,",
            "             validate_path_element=validate_path_element,",
            "             symlink_fn=symlink_fn,",
            "+            blob_normalizer=blob_normalizer,",
            "         )",
            " ",
            "+    def _get_config_condition_matchers(self) -> dict[str, \"ConditionMatcher\"]:",
            "+        \"\"\"Get condition matchers for includeIf conditions.",
            "+",
            "+        Returns a dict of condition prefix to matcher function.",
            "+        \"\"\"",
            "+        from pathlib import Path",
            "+",
            "+        from .config import ConditionMatcher, match_glob_pattern",
            "+",
            "+        # Add gitdir matchers",
            "+        def match_gitdir(pattern: str, case_sensitive: bool = True) -> bool:",
            "+            # Handle relative patterns (starting with ./)",
            "+            if pattern.startswith(\"./\"):",
            "+                # Can't handle relative patterns without config directory context",
            "+                return False",
            "+",
            "+            # Normalize repository path",
            "+            try:",
            "+                repo_path = str(Path(self._controldir).resolve())",
            "+            except (OSError, ValueError):",
            "+                return False",
            "+",
            "+            # Expand ~ in pattern and normalize",
            "+            pattern = os.path.expanduser(pattern)",
            "+",
            "+            # Normalize pattern following Git's rules",
            "+            pattern = pattern.replace(\"\\\\\", \"/\")",
            "+            if not pattern.startswith((\"~/\", \"./\", \"/\", \"**\")):",
            "+                # Check for Windows absolute path",
            "+                if len(pattern) >= 2 and pattern[1] == \":\":",
            "+                    pass",
            "+                else:",
            "+                    pattern = \"**/\" + pattern",
            "+            if pattern.endswith(\"/\"):",
            "+                pattern = pattern + \"**\"",
            "+",
            "+            # Use the existing _match_gitdir_pattern function",
            "+            from .config import _match_gitdir_pattern",
            "+",
            "+            pattern_bytes = pattern.encode(\"utf-8\", errors=\"replace\")",
            "+            repo_path_bytes = repo_path.encode(\"utf-8\", errors=\"replace\")",
            "+",
            "+            return _match_gitdir_pattern(",
            "+                repo_path_bytes, pattern_bytes, ignorecase=not case_sensitive",
            "+            )",
            "+",
            "+        # Add onbranch matcher",
            "+        def match_onbranch(pattern: str) -> bool:",
            "+            try:",
            "+                # Get the current branch using refs",
            "+                ref_chain, _ = self.refs.follow(b\"HEAD\")",
            "+                head_ref = ref_chain[-1]  # Get the final resolved ref",
            "+            except KeyError:",
            "+                pass",
            "+            else:",
            "+                if head_ref and head_ref.startswith(b\"refs/heads/\"):",
            "+                    # Extract branch name from ref",
            "+                    branch = head_ref[11:].decode(\"utf-8\", errors=\"replace\")",
            "+                    return match_glob_pattern(branch, pattern)",
            "+            return False",
            "+",
            "+        matchers: dict[str, ConditionMatcher] = {",
            "+            \"onbranch:\": match_onbranch,",
            "+            \"gitdir:\": lambda pattern: match_gitdir(pattern, True),",
            "+            \"gitdir/i:\": lambda pattern: match_gitdir(pattern, False),",
            "+        }",
            "+",
            "+        return matchers",
            "+",
            "     def get_worktree_config(self) -> \"ConfigFile\":",
            "         from .config import ConfigFile",
            " ",
            "         path = os.path.join(self.commondir(), \"config.worktree\")",
            "         try:",
            "-            return ConfigFile.from_path(path)",
            "+            # Pass condition matchers for includeIf evaluation",
            "+            condition_matchers = self._get_config_condition_matchers()",
            "+            return ConfigFile.from_path(path, condition_matchers=condition_matchers)",
            "         except FileNotFoundError:",
            "             cf = ConfigFile()",
            "             cf.path = path",
            "             return cf",
            " ",
            "     def get_config(self) -> \"ConfigFile\":",
            "         \"\"\"Retrieve the config object.",
            " ",
            "         Returns: `ConfigFile` object for the ``.git/config`` file.",
            "         \"\"\"",
            "         from .config import ConfigFile",
            " ",
            "         path = os.path.join(self._commondir, \"config\")",
            "         try:",
            "-            return ConfigFile.from_path(path)",
            "+            # Pass condition matchers for includeIf evaluation",
            "+            condition_matchers = self._get_config_condition_matchers()",
            "+            return ConfigFile.from_path(path, condition_matchers=condition_matchers)",
            "         except FileNotFoundError:",
            "             ret = ConfigFile()",
            "             ret.path = path",
            "             return ret",
            " ",
            "+    def get_rebase_state_manager(self):",
            "+        \"\"\"Get the appropriate rebase state manager for this repository.",
            "+",
            "+        Returns: DiskRebaseStateManager instance",
            "+        \"\"\"",
            "+        import os",
            "+",
            "+        from .rebase import DiskRebaseStateManager",
            "+",
            "+        path = os.path.join(self.controldir(), \"rebase-merge\")",
            "+        return DiskRebaseStateManager(path)",
            "+",
            "     def get_description(self):",
            "         \"\"\"Retrieve the description of this repository.",
            " ",
            "         Returns: A string describing the repository or None.",
            "         \"\"\"",
            "         path = os.path.join(self._controldir, \"description\")",
            "         try:",
            "@@ -1678,22 +1892,29 @@",
            "           description: Text to set as description for this repository.",
            "         \"\"\"",
            "         self._put_named_file(\"description\", description)",
            " ",
            "     @classmethod",
            "     def _init_maybe_bare(",
            "         cls,",
            "-        path,",
            "-        controldir,",
            "+        path: Union[str, bytes, os.PathLike],",
            "+        controldir: Union[str, bytes, os.PathLike],",
            "         bare,",
            "         object_store=None,",
            "         config=None,",
            "         default_branch=None,",
            "         symlinks: Optional[bool] = None,",
            "+        format: Optional[int] = None,",
            "     ):",
            "+        path = os.fspath(path)",
            "+        if isinstance(path, bytes):",
            "+            path = os.fsdecode(path)",
            "+        controldir = os.fspath(controldir)",
            "+        if isinstance(controldir, bytes):",
            "+            controldir = os.fsdecode(controldir)",
            "         for d in BASE_DIRECTORIES:",
            "             os.mkdir(os.path.join(controldir, *d))",
            "         if object_store is None:",
            "             object_store = DiskObjectStore.init(os.path.join(controldir, OBJECTDIR))",
            "         ret = cls(path, bare=bare, object_store=object_store)",
            "         if default_branch is None:",
            "             if config is None:",
            "@@ -1701,59 +1922,74 @@",
            " ",
            "                 config = StackedConfig.default()",
            "             try:",
            "                 default_branch = config.get(\"init\", \"defaultBranch\")",
            "             except KeyError:",
            "                 default_branch = DEFAULT_BRANCH",
            "         ret.refs.set_symbolic_ref(b\"HEAD\", LOCAL_BRANCH_PREFIX + default_branch)",
            "-        ret._init_files(bare=bare, symlinks=symlinks)",
            "+        ret._init_files(bare=bare, symlinks=symlinks, format=format)",
            "         return ret",
            " ",
            "     @classmethod",
            "     def init(",
            "         cls,",
            "-        path: str,",
            "+        path: Union[str, bytes, os.PathLike],",
            "         *,",
            "         mkdir: bool = False,",
            "         config=None,",
            "         default_branch=None,",
            "         symlinks: Optional[bool] = None,",
            "+        format: Optional[int] = None,",
            "     ) -> \"Repo\":",
            "         \"\"\"Create a new repository.",
            " ",
            "         Args:",
            "           path: Path in which to create the repository",
            "           mkdir: Whether to create the directory",
            "+          format: Repository format version (defaults to 0)",
            "         Returns: `Repo` instance",
            "         \"\"\"",
            "+        path = os.fspath(path)",
            "+        if isinstance(path, bytes):",
            "+            path = os.fsdecode(path)",
            "         if mkdir:",
            "             os.mkdir(path)",
            "         controldir = os.path.join(path, CONTROLDIR)",
            "         os.mkdir(controldir)",
            "         _set_filesystem_hidden(controldir)",
            "         return cls._init_maybe_bare(",
            "             path,",
            "             controldir,",
            "             False,",
            "             config=config,",
            "             default_branch=default_branch,",
            "             symlinks=symlinks,",
            "+            format=format,",
            "         )",
            " ",
            "     @classmethod",
            "-    def _init_new_working_directory(cls, path, main_repo, identifier=None, mkdir=False):",
            "+    def _init_new_working_directory(",
            "+        cls,",
            "+        path: Union[str, bytes, os.PathLike],",
            "+        main_repo,",
            "+        identifier=None,",
            "+        mkdir=False,",
            "+    ):",
            "         \"\"\"Create a new working directory linked to a repository.",
            " ",
            "         Args:",
            "           path: Path in which to create the working tree.",
            "           main_repo: Main repository to reference",
            "           identifier: Worktree identifier",
            "           mkdir: Whether to create the directory",
            "         Returns: `Repo` instance",
            "         \"\"\"",
            "+        path = os.fspath(path)",
            "+        if isinstance(path, bytes):",
            "+            path = os.fsdecode(path)",
            "         if mkdir:",
            "             os.mkdir(path)",
            "         if identifier is None:",
            "             identifier = os.path.basename(path)",
            "         main_worktreesdir = os.path.join(main_repo.controldir(), WORKTREES)",
            "         worktree_controldir = os.path.join(main_worktreesdir, identifier)",
            "         gitdirfile = os.path.join(path, CONTROLDIR)",
            "@@ -1775,33 +2011,45 @@",
            "             f.write(main_repo.head() + b\"\\n\")",
            "         r = cls(path)",
            "         r.reset_index()",
            "         return r",
            " ",
            "     @classmethod",
            "     def init_bare(",
            "-        cls, path, *, mkdir=False, object_store=None, config=None, default_branch=None",
            "+        cls,",
            "+        path: Union[str, bytes, os.PathLike],",
            "+        *,",
            "+        mkdir=False,",
            "+        object_store=None,",
            "+        config=None,",
            "+        default_branch=None,",
            "+        format: Optional[int] = None,",
            "     ):",
            "         \"\"\"Create a new bare repository.",
            " ",
            "         ``path`` should already exist and be an empty directory.",
            " ",
            "         Args:",
            "           path: Path to create bare repository in",
            "+          format: Repository format version (defaults to 0)",
            "         Returns: a `Repo` instance",
            "         \"\"\"",
            "+        path = os.fspath(path)",
            "+        if isinstance(path, bytes):",
            "+            path = os.fsdecode(path)",
            "         if mkdir:",
            "             os.mkdir(path)",
            "         return cls._init_maybe_bare(",
            "             path,",
            "             path,",
            "             True,",
            "             object_store=object_store,",
            "             config=config,",
            "             default_branch=default_branch,",
            "+            format=format,",
            "         )",
            " ",
            "     create = init_bare",
            " ",
            "     def close(self) -> None:",
            "         \"\"\"Close any files opened by this repository.\"\"\"",
            "         self.object_store.close()",
            "@@ -1814,33 +2062,160 @@",
            " ",
            "     def get_blob_normalizer(self):",
            "         \"\"\"Return a BlobNormalizer object.\"\"\"",
            "         # TODO Parse the git attributes files",
            "         git_attributes = {}",
            "         config_stack = self.get_config_stack()",
            "         try:",
            "-            tree = self.object_store[self.refs[b\"HEAD\"]].tree",
            "+            head_sha = self.refs[b\"HEAD\"]",
            "+            # Peel tags to get the underlying commit",
            "+            _, obj = peel_sha(self.object_store, head_sha)",
            "+            tree = obj.tree",
            "             return TreeBlobNormalizer(",
            "                 config_stack,",
            "                 git_attributes,",
            "                 self.object_store,",
            "                 tree,",
            "             )",
            "         except KeyError:",
            "             return BlobNormalizer(config_stack, git_attributes)",
            " ",
            "+    def get_gitattributes(self, tree: Optional[bytes] = None) -> \"GitAttributes\":",
            "+        \"\"\"Read gitattributes for the repository.",
            "+",
            "+        Args:",
            "+            tree: Tree SHA to read .gitattributes from (defaults to HEAD)",
            "+",
            "+        Returns:",
            "+            GitAttributes object that can be used to match paths",
            "+        \"\"\"",
            "+        from .attrs import (",
            "+            GitAttributes,",
            "+            Pattern,",
            "+            parse_git_attributes,",
            "+        )",
            "+",
            "+        patterns = []",
            "+",
            "+        # Read system gitattributes (TODO: implement this)",
            "+        # Read global gitattributes (TODO: implement this)",
            "+",
            "+        # Read repository .gitattributes from index/tree",
            "+        if tree is None:",
            "+            try:",
            "+                # Try to get from HEAD",
            "+                head = self[b\"HEAD\"]",
            "+                if isinstance(head, Tag):",
            "+                    _cls, obj = head.object",
            "+                    head = self.get_object(obj)",
            "+                tree = head.tree",
            "+            except KeyError:",
            "+                # No HEAD, no attributes from tree",
            "+                pass",
            "+",
            "+        if tree is not None:",
            "+            try:",
            "+                tree_obj = self[tree]",
            "+                if b\".gitattributes\" in tree_obj:",
            "+                    _, attrs_sha = tree_obj[b\".gitattributes\"]",
            "+                    attrs_blob = self[attrs_sha]",
            "+                    if isinstance(attrs_blob, Blob):",
            "+                        attrs_data = BytesIO(attrs_blob.data)",
            "+                        for pattern_bytes, attrs in parse_git_attributes(attrs_data):",
            "+                            pattern = Pattern(pattern_bytes)",
            "+                            patterns.append((pattern, attrs))",
            "+            except (KeyError, NotTreeError):",
            "+                pass",
            "+",
            "+        # Read .git/info/attributes",
            "+        info_attrs_path = os.path.join(self.controldir(), \"info\", \"attributes\")",
            "+        if os.path.exists(info_attrs_path):",
            "+            with open(info_attrs_path, \"rb\") as f:",
            "+                for pattern_bytes, attrs in parse_git_attributes(f):",
            "+                    pattern = Pattern(pattern_bytes)",
            "+                    patterns.append((pattern, attrs))",
            "+",
            "+        return GitAttributes(patterns)",
            "+",
            "+    def _sparse_checkout_file_path(self) -> str:",
            "+        \"\"\"Return the path of the sparse-checkout file in this repo's control dir.\"\"\"",
            "+        return os.path.join(self.controldir(), \"info\", \"sparse-checkout\")",
            "+",
            "+    def configure_for_cone_mode(self) -> None:",
            "+        \"\"\"Ensure the repository is configured for cone-mode sparse-checkout.\"\"\"",
            "+        config = self.get_config()",
            "+        config.set((b\"core\",), b\"sparseCheckout\", b\"true\")",
            "+        config.set((b\"core\",), b\"sparseCheckoutCone\", b\"true\")",
            "+        config.write_to_path()",
            "+",
            "+    def infer_cone_mode(self) -> bool:",
            "+        \"\"\"Return True if 'core.sparseCheckoutCone' is set to 'true' in config, else False.\"\"\"",
            "+        config = self.get_config()",
            "+        try:",
            "+            sc_cone = config.get((b\"core\",), b\"sparseCheckoutCone\")",
            "+            return sc_cone == b\"true\"",
            "+        except KeyError:",
            "+            # If core.sparseCheckoutCone is not set, default to False",
            "+            return False",
            "+",
            "+    def get_sparse_checkout_patterns(self) -> list[str]:",
            "+        \"\"\"Return a list of sparse-checkout patterns from info/sparse-checkout.",
            "+",
            "+        Returns:",
            "+            A list of patterns. Returns an empty list if the file is missing.",
            "+        \"\"\"",
            "+        path = self._sparse_checkout_file_path()",
            "+        try:",
            "+            with open(path, encoding=\"utf-8\") as f:",
            "+                return [line.strip() for line in f if line.strip()]",
            "+        except FileNotFoundError:",
            "+            return []",
            "+",
            "+    def set_sparse_checkout_patterns(self, patterns: list[str]) -> None:",
            "+        \"\"\"Write the given sparse-checkout patterns into info/sparse-checkout.",
            "+",
            "+        Creates the info/ directory if it does not exist.",
            "+",
            "+        Args:",
            "+            patterns: A list of gitignore-style patterns to store.",
            "+        \"\"\"",
            "+        info_dir = os.path.join(self.controldir(), \"info\")",
            "+        os.makedirs(info_dir, exist_ok=True)",
            "+",
            "+        path = self._sparse_checkout_file_path()",
            "+        with open(path, \"w\", encoding=\"utf-8\") as f:",
            "+            for pat in patterns:",
            "+                f.write(pat + \"\\n\")",
            "+",
            "+    def set_cone_mode_patterns(self, dirs: Union[list[str], None] = None) -> None:",
            "+        \"\"\"Write the given cone-mode directory patterns into info/sparse-checkout.",
            "+",
            "+        For each directory to include, add an inclusion line that \"undoes\" the prior",
            "+        ``!/*/`` 'exclude' that re-includes that directory and everything under it.",
            "+        Never add the same line twice.",
            "+        \"\"\"",
            "+        patterns = [\"/*\", \"!/*/\"]",
            "+        if dirs:",
            "+            for d in dirs:",
            "+                d = d.strip(\"/\")",
            "+                line = f\"/{d}/\"",
            "+                if d and line not in patterns:",
            "+                    patterns.append(line)",
            "+        self.set_sparse_checkout_patterns(patterns)",
            "+",
            " ",
            " class MemoryRepo(BaseRepo):",
            "     \"\"\"Repo that stores refs, objects, and named files in memory.",
            " ",
            "     MemoryRepos are always bare: they have no working tree and no index, since",
            "     those have a stronger dependency on the filesystem.",
            "     \"\"\"",
            " ",
            "     def __init__(self) -> None:",
            "+        \"\"\"Create a new repository in memory.\"\"\"",
            "         from .config import ConfigFile",
            " ",
            "         self._reflog: list[Any] = []",
            "         refs_container = DictRefsContainer({}, logger=self._append_reflog)",
            "         BaseRepo.__init__(self, MemoryObjectStore(), refs_container)  # type: ignore",
            "         self._named_files: dict[str, bytes] = {}",
            "         self.bare = True",
            "@@ -1912,24 +2287,34 @@",
            "     def get_config(self):",
            "         \"\"\"Retrieve the config object.",
            " ",
            "         Returns: `ConfigFile` object.",
            "         \"\"\"",
            "         return self._config",
            " ",
            "+    def get_rebase_state_manager(self):",
            "+        \"\"\"Get the appropriate rebase state manager for this repository.",
            "+",
            "+        Returns: MemoryRebaseStateManager instance",
            "+        \"\"\"",
            "+        from .rebase import MemoryRebaseStateManager",
            "+",
            "+        return MemoryRebaseStateManager(self)",
            "+",
            "     @classmethod",
            "-    def init_bare(cls, objects, refs):",
            "+    def init_bare(cls, objects, refs, format: Optional[int] = None):",
            "         \"\"\"Create a new bare repository in memory.",
            " ",
            "         Args:",
            "           objects: Objects for the new repository,",
            "             as iterable",
            "           refs: Refs as dictionary, mapping names",
            "             to object SHA1s",
            "+          format: Repository format version (defaults to 0)",
            "         \"\"\"",
            "         ret = cls()",
            "         for obj in objects:",
            "             ret.object_store.add_object(obj)",
            "         for refname, sha in refs.items():",
            "             ret.refs.add_if_new(refname, sha)",
            "-        ret._init_files(bare=True)",
            "+        ret._init_files(bare=True, format=format)",
            "         return ret"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/server.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/server.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/server.py",
            "@@ -48,31 +48,34 @@",
            " import socket",
            " import socketserver",
            " import sys",
            " import time",
            " import zlib",
            " from collections.abc import Iterable, Iterator",
            " from functools import partial",
            "-from typing import Optional, cast",
            "+from typing import TYPE_CHECKING, Optional, cast",
            " from typing import Protocol as TypingProtocol",
            " ",
            "+if TYPE_CHECKING:",
            "+    from .object_store import BaseObjectStore",
            "+",
            " from dulwich import log_utils",
            " ",
            " from .archive import tar_stream",
            " from .errors import (",
            "     ApplyDeltaError,",
            "     ChecksumMismatch,",
            "     GitProtocolError,",
            "     HookError,",
            "     NotGitRepository,",
            "     ObjectFormatException,",
            "     UnexpectedCommandError,",
            " )",
            "-from .object_store import peel_sha",
            "-from .objects import Commit, ObjectID, valid_hexsha",
            "+from .object_store import find_shallow",
            "+from .objects import Commit, ObjectID, Tree, valid_hexsha",
            " from .pack import ObjectContainer, PackedObjectContainer, write_pack_from_container",
            " from .protocol import (",
            "     CAPABILITIES_REF,",
            "     CAPABILITY_AGENT,",
            "     CAPABILITY_DELETE_REFS,",
            "     CAPABILITY_INCLUDE_TAG,",
            "     CAPABILITY_MULTI_ACK,",
            "@@ -109,15 +112,15 @@",
            "     extract_want_line_capabilities,",
            "     format_ack_line,",
            "     format_ref_line,",
            "     format_shallow_line,",
            "     format_unshallow_line,",
            "     symref_capabilities,",
            " )",
            "-from .refs import PEELED_TAG_SUFFIX, RefsContainer, write_info_refs",
            "+from .refs import PEELED_TAG_SUFFIX, Ref, RefsContainer, write_info_refs",
            " from .repo import Repo",
            " ",
            " logger = log_utils.getLogger(__name__)",
            " ",
            " ",
            " class Backend:",
            "     \"\"\"A backend for the Git smart server implementation.\"\"\"",
            "@@ -197,14 +200,18 @@",
            " ",
            "     def __init__(self, root=os.sep) -> None:",
            "         super().__init__()",
            "         self.root = (os.path.abspath(root) + os.sep).replace(os.sep * 2, os.sep)",
            " ",
            "     def open_repository(self, path):",
            "         logger.debug(\"opening repository at %s\", path)",
            "+        # Ensure path is a string to avoid TypeError when joining with self.root",
            "+        path = os.fspath(path)",
            "+        if isinstance(path, bytes):",
            "+            path = os.fsdecode(path)",
            "         abspath = os.path.abspath(os.path.join(self.root, path)) + os.sep",
            "         normcase_abspath = os.path.normcase(abspath)",
            "         normcase_root = os.path.normcase(self.root)",
            "         if not normcase_abspath.startswith(normcase_root):",
            "             raise NotGitRepository(f\"Path {path!r} not inside root {self.root!r}\")",
            "         return Repo(abspath)",
            " ",
            "@@ -253,28 +260,28 @@",
            "         allowable_caps = set(self.innocuous_capabilities())",
            "         allowable_caps.update(self.capabilities())",
            "         for cap in caps:",
            "             if cap.startswith(CAPABILITY_AGENT + b\"=\"):",
            "                 continue",
            "             if cap not in allowable_caps:",
            "                 raise GitProtocolError(",
            "-                    f\"Client asked for capability {cap!r} that \" \"was not advertised.\"",
            "+                    f\"Client asked for capability {cap!r} that was not advertised.\"",
            "                 )",
            "         for cap in self.required_capabilities():",
            "             if cap not in caps:",
            "                 raise GitProtocolError(",
            "-                    \"Client does not support required \" f\"capability {cap!r}.\"",
            "+                    f\"Client does not support required capability {cap!r}.\"",
            "                 )",
            "         self._client_capabilities = set(caps)",
            "         logger.info(\"Client capabilities: %s\", caps)",
            " ",
            "     def has_capability(self, cap: bytes) -> bool:",
            "         if self._client_capabilities is None:",
            "             raise GitProtocolError(",
            "-                f\"Server attempted to access capability {cap!r} \" \"before asking client\"",
            "+                f\"Server attempted to access capability {cap!r} before asking client\"",
            "             )",
            "         return cap in self._client_capabilities",
            " ",
            "     def notify_done(self) -> None:",
            "         self._done_received = True",
            " ",
            " ",
            "@@ -455,55 +462,14 @@",
            "                 raise GitProtocolError(\"Invalid sha\")",
            "             return tuple(fields)",
            "         elif command == COMMAND_DEEPEN:",
            "             return command, int(fields[1])",
            "     raise GitProtocolError(f\"Received invalid line from client: {line!r}\")",
            " ",
            " ",
            "-def _find_shallow(store: ObjectContainer, heads, depth):",
            "-    \"\"\"Find shallow commits according to a given depth.",
            "-",
            "-    Args:",
            "-      store: An ObjectStore for looking up objects.",
            "-      heads: Iterable of head SHAs to start walking from.",
            "-      depth: The depth of ancestors to include. A depth of one includes",
            "-        only the heads themselves.",
            "-    Returns: A tuple of (shallow, not_shallow), sets of SHAs that should be",
            "-        considered shallow and unshallow according to the arguments. Note that",
            "-        these sets may overlap if a commit is reachable along multiple paths.",
            "-    \"\"\"",
            "-    parents: dict[bytes, list[bytes]] = {}",
            "-",
            "-    def get_parents(sha):",
            "-        result = parents.get(sha, None)",
            "-        if not result:",
            "-            result = store[sha].parents",
            "-            parents[sha] = result",
            "-        return result",
            "-",
            "-    todo = []  # stack of (sha, depth)",
            "-    for head_sha in heads:",
            "-        _unpeeled, peeled = peel_sha(store, head_sha)",
            "-        if isinstance(peeled, Commit):",
            "-            todo.append((peeled.id, 1))",
            "-",
            "-    not_shallow = set()",
            "-    shallow = set()",
            "-    while todo:",
            "-        sha, cur_depth = todo.pop()",
            "-        if cur_depth < depth:",
            "-            not_shallow.add(sha)",
            "-            new_depth = cur_depth + 1",
            "-            todo.extend((p, new_depth) for p in get_parents(sha))",
            "-        else:",
            "-            shallow.add(sha)",
            "-",
            "-    return shallow, not_shallow",
            "-",
            "-",
            " def _want_satisfied(store: ObjectContainer, haves, want, earliest) -> bool:",
            "     o = store[want]",
            "     pending = collections.deque([o])",
            "     known = {want}",
            "     while pending:",
            "         commit = pending.popleft()",
            "         if commit.id in haves:",
            "@@ -715,22 +681,25 @@",
            "             command, val = self.read_proto_line((COMMAND_DEEPEN, COMMAND_SHALLOW))",
            "             if command == COMMAND_DEEPEN:",
            "                 depth = val",
            "                 break",
            "             self.client_shallow.add(val)",
            "         self.read_proto_line((None,))  # consume client's flush-pkt",
            " ",
            "-        shallow, not_shallow = _find_shallow(self.store, wants, depth)",
            "+        shallow, not_shallow = find_shallow(self.store, wants, depth)",
            " ",
            "         # Update self.shallow instead of reassigning it since we passed a",
            "         # reference to it before this method was called.",
            "         self.shallow.update(shallow - not_shallow)",
            "         new_shallow = self.shallow - self.client_shallow",
            "         unshallow = self.unshallow = not_shallow & self.client_shallow",
            " ",
            "+        self.update_shallow(new_shallow, unshallow)",
            "+",
            "+    def update_shallow(self, new_shallow, unshallow):",
            "         for sha in sorted(new_shallow):",
            "             self.proto.write_pkt_line(format_shallow_line(sha))",
            "         for sha in sorted(unshallow):",
            "             self.proto.write_pkt_line(format_unshallow_line(sha))",
            " ",
            "         self.proto.write_pkt_line(None)",
            " ",
            "@@ -959,72 +928,68 @@",
            "             CAPABILITY_QUIET,",
            "             CAPABILITY_OFS_DELTA,",
            "             CAPABILITY_SIDE_BAND_64K,",
            "             CAPABILITY_NO_DONE,",
            "         ]",
            " ",
            "     def _apply_pack(",
            "-        self, refs: list[tuple[bytes, bytes, bytes]]",
            "-    ) -> list[tuple[bytes, bytes]]:",
            "+        self, refs: list[tuple[ObjectID, ObjectID, Ref]]",
            "+    ) -> Iterator[tuple[bytes, bytes]]:",
            "         all_exceptions = (",
            "             IOError,",
            "             OSError,",
            "             ChecksumMismatch,",
            "             ApplyDeltaError,",
            "             AssertionError,",
            "             socket.error,",
            "             zlib.error,",
            "             ObjectFormatException,",
            "         )",
            "-        status = []",
            "         will_send_pack = False",
            " ",
            "         for command in refs:",
            "             if command[1] != ZERO_SHA:",
            "                 will_send_pack = True",
            " ",
            "         if will_send_pack:",
            "             # TODO: more informative error messages than just the exception",
            "             # string",
            "             try:",
            "                 recv = getattr(self.proto, \"recv\", None)",
            "                 self.repo.object_store.add_thin_pack(self.proto.read, recv)",
            "-                status.append((b\"unpack\", b\"ok\"))",
            "+                yield (b\"unpack\", b\"ok\")",
            "             except all_exceptions as e:",
            "-                status.append((b\"unpack\", str(e).replace(\"\\n\", \"\").encode(\"utf-8\")))",
            "+                yield (b\"unpack\", str(e).replace(\"\\n\", \"\").encode(\"utf-8\"))",
            "                 # The pack may still have been moved in, but it may contain",
            "                 # broken objects. We trust a later GC to clean it up.",
            "         else:",
            "             # The git protocol want to find a status entry related to unpack",
            "             # process even if no pack data has been sent.",
            "-            status.append((b\"unpack\", b\"ok\"))",
            "+            yield (b\"unpack\", b\"ok\")",
            " ",
            "         for oldsha, sha, ref in refs:",
            "             ref_status = b\"ok\"",
            "             try:",
            "                 if sha == ZERO_SHA:",
            "                     if CAPABILITY_DELETE_REFS not in self.capabilities():",
            "                         raise GitProtocolError(",
            "-                            \"Attempted to delete refs without delete-refs \"",
            "-                            \"capability.\"",
            "+                            \"Attempted to delete refs without delete-refs capability.\"",
            "                         )",
            "                     try:",
            "                         self.repo.refs.remove_if_equals(ref, oldsha)",
            "                     except all_exceptions:",
            "                         ref_status = b\"failed to delete\"",
            "                 else:",
            "                     try:",
            "                         self.repo.refs.set_if_equals(ref, oldsha, sha)",
            "                     except all_exceptions:",
            "                         ref_status = b\"failed to write\"",
            "             except KeyError:",
            "                 ref_status = b\"bad ref\"",
            "-            status.append((ref, ref_status))",
            "-",
            "-        return status",
            "+            yield (ref, ref_status)",
            " ",
            "     def _report_status(self, status: list[tuple[bytes, bytes]]) -> None:",
            "         if self.has_capability(CAPABILITY_SIDE_BAND_64K):",
            "             writer = BufferedPktLineWriter(",
            "                 lambda d: self.proto.write_sideband(SIDE_BAND_CHANNEL_DATA, d)",
            "             )",
            "             write = writer.write",
            "@@ -1042,15 +1007,15 @@",
            "         for name, msg in status:",
            "             if name == b\"unpack\":",
            "                 write(b\"unpack \" + msg + b\"\\n\")",
            "             elif msg == b\"ok\":",
            "                 write(b\"ok \" + name + b\"\\n\")",
            "             else:",
            "                 write(b\"ng \" + name + b\" \" + msg + b\"\\n\")",
            "-        write(None)",
            "+        write(None)  # type: ignore",
            "         flush()",
            " ",
            "     def _on_post_receive(self, client_refs) -> None:",
            "         hook = self.repo.hooks.get(\"post-receive\", None)",
            "         if not hook:",
            "             return",
            "         try:",
            "@@ -1068,15 +1033,15 @@",
            "             if not refs:",
            "                 refs = [(CAPABILITIES_REF, ZERO_SHA)]",
            "             logger.info(\"Sending capabilities: %s\", self.capabilities())",
            "             self.proto.write_pkt_line(",
            "                 format_ref_line(",
            "                     refs[0][0],",
            "                     refs[0][1],",
            "-                    self.capabilities() + symref_capabilities(symrefs),",
            "+                    list(self.capabilities()) + symref_capabilities(symrefs),",
            "                 )",
            "             )",
            "             for i in range(1, len(refs)):",
            "                 ref = refs[i]",
            "                 self.proto.write_pkt_line(format_ref_line(ref[0], ref[1]))",
            " ",
            "             self.proto.write_pkt_line(None)",
            "@@ -1091,19 +1056,20 @@",
            "             return",
            " ",
            "         ref, caps = extract_capabilities(ref)",
            "         self.set_client_capabilities(caps)",
            " ",
            "         # client will now send us a list of (oldsha, newsha, ref)",
            "         while ref:",
            "-            client_refs.append(ref.split())",
            "+            (oldsha, newsha, ref) = ref.split()",
            "+            client_refs.append((oldsha, newsha, ref))",
            "             ref = self.proto.read_pkt_line()",
            " ",
            "         # backend can now deal with this refs and read a pack using self.read",
            "-        status = self._apply_pack(client_refs)",
            "+        status = list(self._apply_pack(client_refs))",
            " ",
            "         self._on_post_receive(client_refs)",
            " ",
            "         # when we have read all the pack from the client, send a status report",
            "         # if the client asked for it",
            "         if self.has_capability(CAPABILITY_REPORT_STATUS):",
            "             self._report_status(status)",
            "@@ -1123,31 +1089,35 @@",
            "             (key, value) = pkt.split(b\" \", 1)",
            "             if key != b\"argument\":",
            "                 raise GitProtocolError(f\"unknown command {key}\")",
            "             arguments.append(value.rstrip(b\"\\n\"))",
            "         prefix = b\"\"",
            "         format = \"tar\"",
            "         i = 0",
            "-        store: ObjectContainer = self.repo.object_store",
            "+        store: BaseObjectStore = self.repo.object_store",
            "         while i < len(arguments):",
            "             argument = arguments[i]",
            "             if argument == b\"--prefix\":",
            "                 i += 1",
            "                 prefix = arguments[i]",
            "             elif argument == b\"--format\":",
            "                 i += 1",
            "                 format = arguments[i].decode(\"ascii\")",
            "             else:",
            "                 commit_sha = self.repo.refs[argument]",
            "-                tree = store[cast(Commit, store[commit_sha]).tree]",
            "+                tree = cast(Tree, store[cast(Commit, store[commit_sha]).tree])",
            "             i += 1",
            "         self.proto.write_pkt_line(b\"ACK\")",
            "         self.proto.write_pkt_line(None)",
            "         for chunk in tar_stream(",
            "-            store, tree, mtime=time.time(), prefix=prefix, format=format",
            "+            store,",
            "+            tree,",
            "+            mtime=int(time.time()),",
            "+            prefix=prefix,",
            "+            format=format,  # type: ignore",
            "         ):",
            "             write(chunk)",
            "         self.proto.write_pkt_line(None)",
            " ",
            " ",
            " # Default handler classes for git services.",
            " DEFAULT_HANDLERS = {",
            "@@ -1165,15 +1135,15 @@",
            "     def handle(self) -> None:",
            "         proto = ReceivableProtocol(self.connection.recv, self.wfile.write)",
            "         command, args = proto.read_cmd()",
            "         logger.info(\"Handling %s request, args=%s\", command, args)",
            " ",
            "         cls = self.handlers.get(command, None)",
            "         if not callable(cls):",
            "-            raise GitProtocolError(f\"Invalid service {command}\")",
            "+            raise GitProtocolError(f\"Invalid service {command!r}\")",
            "         h = cls(self.server.backend, args, proto)  # type: ignore",
            "         h.handle()",
            " ",
            " ",
            " class TCPGitServer(socketserver.TCPServer):",
            "     allow_reuse_address = True",
            "     serve = socketserver.TCPServer.serve_forever",
            "@@ -1191,15 +1161,15 @@",
            " ",
            "     def verify_request(self, request, client_address) -> bool:",
            "         logger.info(\"Handling request from %s\", client_address)",
            "         return True",
            " ",
            "     def handle_error(self, request, client_address) -> None:",
            "         logger.exception(",
            "-            \"Exception happened during processing of request \" \"from %s\",",
            "+            \"Exception happened during processing of request from %s\",",
            "             client_address,",
            "         )",
            " ",
            " ",
            " def main(argv=sys.argv) -> None:",
            "     \"\"\"Entry point for starting a TCP git server.\"\"\"",
            "     import optparse"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/stash.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/stash.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/stash.py",
            "@@ -18,96 +18,282 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Stash handling.\"\"\"",
            " ",
            " import os",
            "+import sys",
            "+from typing import TYPE_CHECKING, Optional, TypedDict",
            " ",
            " from .file import GitFile",
            "-from .index import commit_tree, iter_fresh_objects",
            "+from .index import (",
            "+    IndexEntry,",
            "+    _tree_to_fs_path,",
            "+    build_file_from_blob,",
            "+    commit_tree,",
            "+    index_entry_from_stat,",
            "+    iter_fresh_objects,",
            "+    iter_tree_contents,",
            "+    symlink,",
            "+    update_working_tree,",
            "+    validate_path,",
            "+    validate_path_element_default,",
            "+    validate_path_element_hfs,",
            "+    validate_path_element_ntfs,",
            "+)",
            "+from .objects import S_IFGITLINK, Blob, Commit, ObjectID",
            " from .reflog import drop_reflog_entry, read_reflog",
            "+from .refs import Ref",
            "+",
            "+if TYPE_CHECKING:",
            "+    from .reflog import Entry",
            "+    from .repo import Repo",
            "+",
            "+",
            "+class CommitKwargs(TypedDict, total=False):",
            "+    \"\"\"Keyword arguments for do_commit.\"\"\"",
            "+",
            "+    committer: bytes",
            "+    author: bytes",
            "+",
            " ",
            " DEFAULT_STASH_REF = b\"refs/stash\"",
            " ",
            " ",
            " class Stash:",
            "     \"\"\"A Git stash.",
            " ",
            "     Note that this doesn't currently update the working tree.",
            "     \"\"\"",
            " ",
            "-    def __init__(self, repo, ref=DEFAULT_STASH_REF) -> None:",
            "+    def __init__(self, repo: \"Repo\", ref: Ref = DEFAULT_STASH_REF) -> None:",
            "         self._ref = ref",
            "         self._repo = repo",
            " ",
            "     @property",
            "-    def _reflog_path(self):",
            "+    def _reflog_path(self) -> str:",
            "         return os.path.join(self._repo.commondir(), \"logs\", os.fsdecode(self._ref))",
            " ",
            "-    def stashes(self):",
            "+    def stashes(self) -> list[\"Entry\"]:",
            "         try:",
            "             with GitFile(self._reflog_path, \"rb\") as f:",
            "-                return reversed(list(read_reflog(f)))",
            "+                return list(reversed(list(read_reflog(f))))",
            "         except FileNotFoundError:",
            "             return []",
            " ",
            "     @classmethod",
            "-    def from_repo(cls, repo):",
            "+    def from_repo(cls, repo: \"Repo\") -> \"Stash\":",
            "         \"\"\"Create a new stash from a Repo object.\"\"\"",
            "         return cls(repo)",
            " ",
            "-    def drop(self, index) -> None:",
            "+    def drop(self, index: int) -> None:",
            "         \"\"\"Drop entry with specified index.\"\"\"",
            "         with open(self._reflog_path, \"rb+\") as f:",
            "             drop_reflog_entry(f, index, rewrite=True)",
            "         if len(self) == 0:",
            "             os.remove(self._reflog_path)",
            "             del self._repo.refs[self._ref]",
            "             return",
            "         if index == 0:",
            "             self._repo.refs[self._ref] = self[0].new_sha",
            " ",
            "-    def pop(self, index):",
            "-        raise NotImplementedError(self.pop)",
            "+    def pop(self, index: int) -> \"Entry\":",
            "+        \"\"\"Pop a stash entry and apply its changes.",
            "+",
            "+        Args:",
            "+          index: Index of the stash entry to pop (0 is the most recent)",
            "+",
            "+        Returns:",
            "+          The stash entry that was popped",
            "+        \"\"\"",
            "+        # Get the stash entry before removing it",
            "+        entry = self[index]",
            "+",
            "+        # Get the stash commit",
            "+        stash_commit = self._repo.get_object(entry.new_sha)",
            "+        assert isinstance(stash_commit, Commit)",
            "+",
            "+        # The stash commit has the working tree changes",
            "+        # Its first parent is the commit the stash was based on",
            "+        # Its second parent is the index commit",
            "+        if len(stash_commit.parents) < 1:",
            "+            raise ValueError(\"Invalid stash entry: no parent commits\")",
            "+",
            "+        base_commit_sha = stash_commit.parents[0]",
            " ",
            "-    def push(self, committer=None, author=None, message=None):",
            "+        # Get current HEAD to determine if we can apply cleanly",
            "+        try:",
            "+            current_head = self._repo.refs[b\"HEAD\"]",
            "+        except KeyError:",
            "+            raise ValueError(\"Cannot pop stash: no HEAD\")",
            "+",
            "+        # Check if we're at the same commit where the stash was created",
            "+        # If not, we need to do a three-way merge",
            "+        if current_head != base_commit_sha:",
            "+            # For now, we'll apply changes directly but this could cause conflicts",
            "+            # A full implementation would do a three-way merge",
            "+            pass",
            "+",
            "+        # Apply the stash changes to the working tree and index",
            "+        # Get config for working directory update",
            "+        config = self._repo.get_config()",
            "+        honor_filemode = config.get_boolean(b\"core\", b\"filemode\", os.name != \"nt\")",
            "+",
            "+        if config.get_boolean(b\"core\", b\"core.protectNTFS\", os.name == \"nt\"):",
            "+            validate_path_element = validate_path_element_ntfs",
            "+        elif config.get_boolean(b\"core\", b\"core.protectHFS\", sys.platform == \"darwin\"):",
            "+            validate_path_element = validate_path_element_hfs",
            "+        else:",
            "+            validate_path_element = validate_path_element_default",
            "+",
            "+        if config.get_boolean(b\"core\", b\"symlinks\", True):",
            "+            symlink_fn = symlink",
            "+        else:",
            "+",
            "+            def symlink_fn(source, target) -> None:  # type: ignore",
            "+                mode = \"w\" + (\"b\" if isinstance(source, bytes) else \"\")",
            "+                with open(target, mode) as f:",
            "+                    f.write(source)",
            "+",
            "+        # Get blob normalizer for line ending conversion",
            "+        blob_normalizer = self._repo.get_blob_normalizer()",
            "+",
            "+        # Open the index",
            "+        repo_index = self._repo.open_index()",
            "+",
            "+        # Apply working tree changes",
            "+        stash_tree_id = stash_commit.tree",
            "+        repo_path = os.fsencode(self._repo.path)",
            "+",
            "+        # First, if we have index changes (second parent), restore the index state",
            "+        if len(stash_commit.parents) >= 2:",
            "+            index_commit_sha = stash_commit.parents[1]",
            "+            index_commit = self._repo.get_object(index_commit_sha)",
            "+            assert isinstance(index_commit, Commit)",
            "+            index_tree_id = index_commit.tree",
            "+",
            "+            # Update index entries from the stashed index tree",
            "+            for entry in iter_tree_contents(self._repo.object_store, index_tree_id):",
            "+                if not validate_path(entry.path, validate_path_element):",
            "+                    continue",
            "+",
            "+                # Add to index with stage 0 (normal)",
            "+                # Get file stats for the entry",
            "+                full_path = _tree_to_fs_path(repo_path, entry.path)",
            "+                try:",
            "+                    st = os.lstat(full_path)",
            "+                except FileNotFoundError:",
            "+                    # File doesn't exist yet, use dummy stats",
            "+                    st = os.stat_result((entry.mode, 0, 0, 0, 0, 0, 0, 0, 0, 0))",
            "+                repo_index[entry.path] = index_entry_from_stat(st, entry.sha)",
            "+",
            "+        # Apply working tree changes from the stash",
            "+        for entry in iter_tree_contents(self._repo.object_store, stash_tree_id):",
            "+            if not validate_path(entry.path, validate_path_element):",
            "+                continue",
            "+",
            "+            full_path = _tree_to_fs_path(repo_path, entry.path)",
            "+",
            "+            # Create parent directories if needed",
            "+            parent_dir = os.path.dirname(full_path)",
            "+            if parent_dir and not os.path.exists(parent_dir):",
            "+                os.makedirs(parent_dir)",
            "+",
            "+            # Write the file",
            "+            if entry.mode == S_IFGITLINK:",
            "+                # Submodule - just create directory",
            "+                if not os.path.isdir(full_path):",
            "+                    os.mkdir(full_path)",
            "+                st = os.lstat(full_path)",
            "+            else:",
            "+                obj = self._repo.object_store[entry.sha]",
            "+                assert isinstance(obj, Blob)",
            "+                # Apply blob normalization for checkout if normalizer is provided",
            "+                if blob_normalizer is not None:",
            "+                    obj = blob_normalizer.checkout_normalize(obj, entry.path)",
            "+                st = build_file_from_blob(",
            "+                    obj,",
            "+                    entry.mode,",
            "+                    full_path,",
            "+                    honor_filemode=honor_filemode,",
            "+                    symlink_fn=symlink_fn,",
            "+                )",
            "+",
            "+            # Update index if the file wasn't already staged",
            "+            if entry.path not in repo_index:",
            "+                # Update with file stats from disk",
            "+                repo_index[entry.path] = index_entry_from_stat(st, entry.sha)",
            "+            else:",
            "+                existing_entry = repo_index[entry.path]",
            "+",
            "+                if (",
            "+                    isinstance(existing_entry, IndexEntry)",
            "+                    and existing_entry.mode == entry.mode",
            "+                    and existing_entry.sha == entry.sha",
            "+                ):",
            "+                    # Update with file stats from disk",
            "+                    repo_index[entry.path] = index_entry_from_stat(st, entry.sha)",
            "+",
            "+        # Write the updated index",
            "+        repo_index.write()",
            "+",
            "+        # Remove the stash entry",
            "+        self.drop(index)",
            "+",
            "+        return entry",
            "+",
            "+    def push(",
            "+        self,",
            "+        committer: Optional[bytes] = None,",
            "+        author: Optional[bytes] = None,",
            "+        message: Optional[bytes] = None,",
            "+    ) -> ObjectID:",
            "         \"\"\"Create a new stash.",
            " ",
            "         Args:",
            "           committer: Optional committer name to use",
            "           author: Optional author name to use",
            "           message: Optional commit message",
            "         \"\"\"",
            "         # First, create the index commit.",
            "-        commit_kwargs = {}",
            "+        commit_kwargs = CommitKwargs()",
            "         if committer is not None:",
            "             commit_kwargs[\"committer\"] = committer",
            "         if author is not None:",
            "             commit_kwargs[\"author\"] = author",
            " ",
            "         index = self._repo.open_index()",
            "         index_tree_id = index.commit(self._repo.object_store)",
            "+        # Create a dangling commit for the index state",
            "+        # Note: We pass ref=None which is handled specially in do_commit",
            "+        # to create a commit without updating any reference",
            "         index_commit_id = self._repo.do_commit(",
            "-            ref=None,",
            "             tree=index_tree_id,",
            "             message=b\"Index stash\",",
            "             merge_heads=[self._repo.head()],",
            "             no_verify=True,",
            "+            ref=None,  # Don't update any ref",
            "             **commit_kwargs,",
            "         )",
            " ",
            "         # Then, the working tree one.",
            "-        stash_tree_id = commit_tree(",
            "-            self._repo.object_store,",
            "-            iter_fresh_objects(",
            "+        # Filter out entries with None values since commit_tree expects non-None values",
            "+        fresh_objects = [",
            "+            (path, sha, mode)",
            "+            for path, sha, mode in iter_fresh_objects(",
            "                 index,",
            "                 os.fsencode(self._repo.path),",
            "                 object_store=self._repo.object_store,",
            "-            ),",
            "+            )",
            "+            if sha is not None and mode is not None",
            "+        ]",
            "+        stash_tree_id = commit_tree(",
            "+            self._repo.object_store,",
            "+            fresh_objects,",
            "         )",
            " ",
            "         if message is None:",
            "             message = b\"A stash on \" + self._repo.head()",
            " ",
            "         # TODO(jelmer): Just pass parents into do_commit()?",
            "         self._repo.refs[self._ref] = self._repo.head()",
            "@@ -117,14 +303,30 @@",
            "             tree=stash_tree_id,",
            "             message=message,",
            "             merge_heads=[index_commit_id],",
            "             no_verify=True,",
            "             **commit_kwargs,",
            "         )",
            " ",
            "+        # Reset working tree and index to HEAD to match git's behavior",
            "+        # Use update_working_tree to reset from stash tree to HEAD tree",
            "+        # Get HEAD tree",
            "+        head_commit = self._repo.get_object(self._repo.head())",
            "+        assert isinstance(head_commit, Commit)",
            "+        head_tree_id = head_commit.tree",
            "+",
            "+        # Update from stash tree to HEAD tree",
            "+        # This will remove files that were in stash but not in HEAD,",
            "+        # and restore files to their HEAD versions",
            "+        update_working_tree(",
            "+            self._repo,",
            "+            old_tree_id=stash_tree_id,",
            "+            new_tree_id=head_tree_id,",
            "+        )",
            "+",
            "         return cid",
            " ",
            "-    def __getitem__(self, index):",
            "+    def __getitem__(self, index: int) -> \"Entry\":",
            "         return list(self.stashes())[index]",
            " ",
            "     def __len__(self) -> int:",
            "         return len(list(self.stashes()))"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/submodule.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/submodule.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/submodule.py",
            "@@ -17,26 +17,76 @@",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Working with Git submodules.\"\"\"",
            " ",
            "+import os",
            " from collections.abc import Iterator",
            "+from typing import TYPE_CHECKING, Union",
            " ",
            " from .object_store import iter_tree_contents",
            " from .objects import S_ISGITLINK",
            " ",
            "+if TYPE_CHECKING:",
            "+    from .object_store import ObjectContainer",
            "+    from .repo import Repo",
            " ",
            "-def iter_cached_submodules(store, root_tree_id: bytes) -> Iterator[tuple[str, bytes]]:",
            "+",
            "+def iter_cached_submodules(",
            "+    store: \"ObjectContainer\", root_tree_id: bytes",
            "+) -> Iterator[tuple[str, bytes]]:",
            "     \"\"\"Iterate over cached submodules.",
            " ",
            "     Args:",
            "       store: Object store to iterate",
            "       root_tree_id: SHA of root tree",
            " ",
            "     Returns:",
            "       Iterator over over (path, sha) tuples",
            "     \"\"\"",
            "     for entry in iter_tree_contents(store, root_tree_id):",
            "         if S_ISGITLINK(entry.mode):",
            "             yield entry.path, entry.sha",
            "+",
            "+",
            "+def ensure_submodule_placeholder(",
            "+    repo: \"Repo\",",
            "+    submodule_path: Union[str, bytes],",
            "+) -> None:",
            "+    \"\"\"Create a submodule placeholder directory with .git file.",
            "+",
            "+    This creates the minimal structure needed for a submodule:",
            "+    - The submodule directory",
            "+    - A .git file pointing to the submodule's git directory",
            "+",
            "+    Args:",
            "+      repo: Parent repository",
            "+      submodule_path: Path to the submodule relative to repo root",
            "+    \"\"\"",
            "+    # Ensure path is bytes",
            "+    if isinstance(submodule_path, str):",
            "+        submodule_path = submodule_path.encode()",
            "+",
            "+    # Get repo path as bytes",
            "+    repo_path = repo.path if isinstance(repo.path, bytes) else repo.path.encode()",
            "+",
            "+    # Create full path to submodule",
            "+    full_path = os.path.join(repo_path, submodule_path)",
            "+",
            "+    # Create submodule directory if it doesn't exist",
            "+    if not os.path.exists(full_path):",
            "+        os.makedirs(full_path)",
            "+",
            "+    # Create .git file pointing to the submodule's git directory",
            "+    git_filename = b\".git\" if isinstance(full_path, bytes) else \".git\"",
            "+    git_file_path = os.path.join(full_path, git_filename)",
            "+    if not os.path.exists(git_file_path):",
            "+        # Submodule git directories are typically stored in .git/modules/<name>",
            "+        # The relative path from the submodule to the parent's .git directory",
            "+        # depends on the submodule's depth",
            "+        depth = submodule_path.count(b\"/\") + 1",
            "+        relative_git_dir = b\"../\" * depth + b\".git/modules/\" + submodule_path",
            "+",
            "+        with open(git_file_path, \"wb\") as f:",
            "+            f.write(b\"gitdir: \" + relative_git_dir + b\"\\n\")"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/tests/test_object_store.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/tests/test_object_store.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/tests/test_object_store.py",
            "@@ -18,64 +18,64 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Tests for the object store interface.\"\"\"",
            " ",
            " from typing import TYPE_CHECKING, Any, Callable",
            "-from unittest import skipUnless",
            "+from unittest import TestCase",
            "+from unittest.mock import patch",
            " ",
            " from dulwich.index import commit_tree",
            " from dulwich.object_store import (",
            "+    MemoryObjectStore,",
            "     PackBasedObjectStore,",
            "+    find_shallow,",
            "     iter_tree_contents,",
            "     peel_sha,",
            " )",
            " from dulwich.objects import (",
            "     Blob,",
            "     Tree,",
            "     TreeEntry,",
            " )",
            " from dulwich.protocol import DEPTH_INFINITE",
            " ",
            "-from .utils import make_object, make_tag",
            "+from .utils import make_commit, make_object, make_tag",
            " ",
            " if TYPE_CHECKING:",
            "     from dulwich.object_store import BaseObjectStore",
            " ",
            "-try:",
            "-    from unittest.mock import patch",
            "-except ImportError:",
            "-    patch = None  # type: ignore",
            "-",
            " ",
            " testobject = make_object(Blob, data=b\"yummy data\")",
            " ",
            " ",
            " class ObjectStoreTests:",
            "     store: \"BaseObjectStore\"",
            " ",
            "     assertEqual: Callable[[object, object], None]",
            "-    assertRaises: Callable[[type[Exception], Callable[[], Any]], None]",
            "+    # For type checker purposes - actual implementation supports both styles",
            "+    assertRaises: Callable[..., Any]",
            "     assertNotIn: Callable[[object, object], None]",
            "     assertNotEqual: Callable[[object, object], None]",
            "     assertIn: Callable[[object, object], None]",
            "+    assertTrue: Callable[[bool], None]",
            "+    assertFalse: Callable[[bool], None]",
            " ",
            "     def test_determine_wants_all(self) -> None:",
            "         self.assertEqual(",
            "             [b\"1\" * 40],",
            "             self.store.determine_wants_all({b\"refs/heads/foo\": b\"1\" * 40}),",
            "         )",
            " ",
            "     def test_determine_wants_all_zero(self) -> None:",
            "         self.assertEqual(",
            "             [], self.store.determine_wants_all({b\"refs/heads/foo\": b\"0\" * 40})",
            "         )",
            " ",
            "-    @skipUnless(patch, \"Required mock.patch\")",
            "     def test_determine_wants_all_depth(self) -> None:",
            "         self.store.add_object(testobject)",
            "         refs = {b\"refs/heads/foo\": testobject.id}",
            "         with patch.object(self.store, \"_get_depth\", return_value=1) as m:",
            "             self.assertEqual([], self.store.determine_wants_all(refs, depth=0))",
            "             self.assertEqual(",
            "                 [testobject.id],",
            "@@ -255,18 +255,49 @@",
            " ",
            "     def test_iter_prefix(self) -> None:",
            "         self.store.add_object(testobject)",
            "         self.assertEqual([testobject.id], list(self.store.iter_prefix(testobject.id)))",
            "         self.assertEqual(",
            "             [testobject.id], list(self.store.iter_prefix(testobject.id[:10]))",
            "         )",
            "-        self.assertEqual(",
            "-            [testobject.id], list(self.store.iter_prefix(testobject.id[:4]))",
            "+",
            "+    def test_iterobjects_subset_all_present(self) -> None:",
            "+        \"\"\"Test iterating over a subset of objects that all exist.\"\"\"",
            "+        blob1 = make_object(Blob, data=b\"blob 1 data\")",
            "+        blob2 = make_object(Blob, data=b\"blob 2 data\")",
            "+        self.store.add_object(blob1)",
            "+        self.store.add_object(blob2)",
            "+",
            "+        objects = list(self.store.iterobjects_subset([blob1.id, blob2.id]))",
            "+        self.assertEqual(2, len(objects))",
            "+        object_ids = set(o.id for o in objects)",
            "+        self.assertEqual(set([blob1.id, blob2.id]), object_ids)",
            "+",
            "+    def test_iterobjects_subset_missing_not_allowed(self) -> None:",
            "+        \"\"\"Test iterating with missing objects when not allowed.\"\"\"",
            "+        blob1 = make_object(Blob, data=b\"blob 1 data\")",
            "+        self.store.add_object(blob1)",
            "+        missing_sha = b\"1\" * 40",
            "+",
            "+        self.assertRaises(",
            "+            KeyError,",
            "+            lambda: list(self.store.iterobjects_subset([blob1.id, missing_sha])),",
            "+        )",
            "+",
            "+    def test_iterobjects_subset_missing_allowed(self) -> None:",
            "+        \"\"\"Test iterating with missing objects when allowed.\"\"\"",
            "+        blob1 = make_object(Blob, data=b\"blob 1 data\")",
            "+        self.store.add_object(blob1)",
            "+        missing_sha = b\"1\" * 40",
            "+",
            "+        objects = list(",
            "+            self.store.iterobjects_subset([blob1.id, missing_sha], allow_missing=True)",
            "         )",
            "-        self.assertEqual([testobject.id], list(self.store.iter_prefix(b\"\")))",
            "+        self.assertEqual(1, len(objects))",
            "+        self.assertEqual(blob1.id, objects[0].id)",
            " ",
            "     def test_iter_prefix_not_found(self) -> None:",
            "         self.assertEqual([], list(self.store.iter_prefix(b\"1\" * 40)))",
            " ",
            " ",
            " class PackBasedObjectStoreTests(ObjectStoreTests):",
            "     store: PackBasedObjectStore",
            "@@ -323,7 +354,132 @@",
            "         self.assertEqual(0, self.store.pack_loose_objects())",
            " ",
            "         self.assertEqual({b1.id, b2.id}, set(self.store))",
            "         self.assertEqual(1, len(self.store.packs))",
            "         self.assertEqual(2, self.store.repack())",
            "         self.assertEqual(1, len(self.store.packs))",
            "         self.assertEqual(0, self.store.pack_loose_objects())",
            "+",
            "+    def test_repack_with_exclude(self) -> None:",
            "+        \"\"\"Test repacking while excluding specific objects.\"\"\"",
            "+        b1 = make_object(Blob, data=b\"yummy data\")",
            "+        self.store.add_object(b1)",
            "+        b2 = make_object(Blob, data=b\"more yummy data\")",
            "+        self.store.add_object(b2)",
            "+        b3 = make_object(Blob, data=b\"even more yummy data\")",
            "+        b4 = make_object(Blob, data=b\"and more yummy data\")",
            "+        self.store.add_objects([(b3, None), (b4, None)])",
            "+",
            "+        self.assertEqual({b1.id, b2.id, b3.id, b4.id}, set(self.store))",
            "+        self.assertEqual(1, len(self.store.packs))",
            "+",
            "+        # Repack, excluding b2 and b3",
            "+        excluded = {b2.id, b3.id}",
            "+        self.assertEqual(2, self.store.repack(exclude=excluded))",
            "+",
            "+        # Should have repacked only b1 and b4",
            "+        self.assertEqual(1, len(self.store.packs))",
            "+        self.assertIn(b1.id, self.store)",
            "+        self.assertNotIn(b2.id, self.store)",
            "+        self.assertNotIn(b3.id, self.store)",
            "+        self.assertIn(b4.id, self.store)",
            "+",
            "+    def test_delete_loose_object(self) -> None:",
            "+        \"\"\"Test deleting loose objects.\"\"\"",
            "+        b1 = make_object(Blob, data=b\"test data\")",
            "+        self.store.add_object(b1)",
            "+",
            "+        # Verify it's loose",
            "+        self.assertTrue(self.store.contains_loose(b1.id))",
            "+        self.assertIn(b1.id, self.store)",
            "+",
            "+        # Delete it",
            "+        self.store.delete_loose_object(b1.id)",
            "+",
            "+        # Verify it's gone",
            "+        self.assertFalse(self.store.contains_loose(b1.id))",
            "+        self.assertNotIn(b1.id, self.store)",
            "+",
            "+",
            "+class FindShallowTests(TestCase):",
            "+    def setUp(self):",
            "+        super().setUp()",
            "+        self._store = MemoryObjectStore()",
            "+",
            "+    def make_commit(self, **attrs):",
            "+        commit = make_commit(**attrs)",
            "+        self._store.add_object(commit)",
            "+        return commit",
            "+",
            "+    def make_linear_commits(self, n, message=b\"\"):",
            "+        commits = []",
            "+        parents = []",
            "+        for _ in range(n):",
            "+            commits.append(self.make_commit(parents=parents, message=message))",
            "+            parents = [commits[-1].id]",
            "+        return commits",
            "+",
            "+    def assertSameElements(self, expected, actual):",
            "+        self.assertEqual(set(expected), set(actual))",
            "+",
            "+    def test_linear(self):",
            "+        c1, c2, c3 = self.make_linear_commits(3)",
            "+",
            "+        self.assertEqual((set([c3.id]), set([])), find_shallow(self._store, [c3.id], 1))",
            "+        self.assertEqual(",
            "+            (set([c2.id]), set([c3.id])),",
            "+            find_shallow(self._store, [c3.id], 2),",
            "+        )",
            "+        self.assertEqual(",
            "+            (set([c1.id]), set([c2.id, c3.id])),",
            "+            find_shallow(self._store, [c3.id], 3),",
            "+        )",
            "+        self.assertEqual(",
            "+            (set([]), set([c1.id, c2.id, c3.id])),",
            "+            find_shallow(self._store, [c3.id], 4),",
            "+        )",
            "+",
            "+    def test_multiple_independent(self):",
            "+        a = self.make_linear_commits(2, message=b\"a\")",
            "+        b = self.make_linear_commits(2, message=b\"b\")",
            "+        c = self.make_linear_commits(2, message=b\"c\")",
            "+        heads = [a[1].id, b[1].id, c[1].id]",
            "+",
            "+        self.assertEqual(",
            "+            (set([a[0].id, b[0].id, c[0].id]), set(heads)),",
            "+            find_shallow(self._store, heads, 2),",
            "+        )",
            "+",
            "+    def test_multiple_overlapping(self):",
            "+        # Create the following commit tree:",
            "+        # 1--2",
            "+        #  \\",
            "+        #   3--4",
            "+        c1, c2 = self.make_linear_commits(2)",
            "+        c3 = self.make_commit(parents=[c1.id])",
            "+        c4 = self.make_commit(parents=[c3.id])",
            "+",
            "+        # 1 is shallow along the path from 4, but not along the path from 2.",
            "+        self.assertEqual(",
            "+            (set([c1.id]), set([c1.id, c2.id, c3.id, c4.id])),",
            "+            find_shallow(self._store, [c2.id, c4.id], 3),",
            "+        )",
            "+",
            "+    def test_merge(self):",
            "+        c1 = self.make_commit()",
            "+        c2 = self.make_commit()",
            "+        c3 = self.make_commit(parents=[c1.id, c2.id])",
            "+",
            "+        self.assertEqual(",
            "+            (set([c1.id, c2.id]), set([c3.id])),",
            "+            find_shallow(self._store, [c3.id], 2),",
            "+        )",
            "+",
            "+    def test_tag(self):",
            "+        c1, c2 = self.make_linear_commits(2)",
            "+        tag = make_tag(c2, name=b\"tag\")",
            "+        self._store.add_object(tag)",
            "+",
            "+        self.assertEqual(",
            "+            (set([c1.id]), set([c2.id])),",
            "+            find_shallow(self._store, [tag.id], 2),",
            "+        )"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/walk.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/walk.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/walk.py",
            "@@ -19,16 +19,20 @@",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"General implementation of walking commits and their contents.\"\"\"",
            " ",
            " import collections",
            " import heapq",
            "+from collections.abc import Iterator",
            " from itertools import chain",
            "-from typing import Optional",
            "+from typing import TYPE_CHECKING, Any, Callable, Optional, Union, cast",
            "+",
            "+if TYPE_CHECKING:",
            "+    from .object_store import BaseObjectStore",
            " ",
            " from .diff_tree import (",
            "     RENAME_CHANGE_TYPES,",
            "     RenameDetector,",
            "     TreeChange,",
            "     tree_changes,",
            "     tree_changes_for_merge,",
            "@@ -44,67 +48,87 @@",
            " # Maximum number of commits to walk past a commit time boundary.",
            " _MAX_EXTRA_COMMITS = 5",
            " ",
            " ",
            " class WalkEntry:",
            "     \"\"\"Object encapsulating a single result from a walk.\"\"\"",
            " ",
            "-    def __init__(self, walker, commit) -> None:",
            "+    def __init__(self, walker: \"Walker\", commit: Commit) -> None:",
            "         self.commit = commit",
            "         self._store = walker.store",
            "         self._get_parents = walker.get_parents",
            "-        self._changes: dict[str, list[TreeChange]] = {}",
            "+        self._changes: dict[Optional[bytes], list[TreeChange]] = {}",
            "         self._rename_detector = walker.rename_detector",
            " ",
            "-    def changes(self, path_prefix=None):",
            "+    def changes(",
            "+        self, path_prefix: Optional[bytes] = None",
            "+    ) -> Union[list[TreeChange], list[list[TreeChange]]]:",
            "         \"\"\"Get the tree changes for this entry.",
            " ",
            "         Args:",
            "           path_prefix: Portion of the path in the repository to",
            "             use to filter changes. Must be a directory name. Must be",
            "             a full, valid, path reference (no partial names or wildcards).",
            "         Returns: For commits with up to one parent, a list of TreeChange",
            "             objects; if the commit has no parents, these will be relative to",
            "             the empty tree. For merge commits, a list of lists of TreeChange",
            "-            objects; see dulwich.diff.tree_changes_for_merge.",
            "+            objects; see dulwich.diff_tree.tree_changes_for_merge.",
            "         \"\"\"",
            "         cached = self._changes.get(path_prefix)",
            "         if cached is None:",
            "             commit = self.commit",
            "             if not self._get_parents(commit):",
            "                 changes_func = tree_changes",
            "                 parent = None",
            "             elif len(self._get_parents(commit)) == 1:",
            "                 changes_func = tree_changes",
            "-                parent = self._store[self._get_parents(commit)[0]].tree",
            "+                parent = cast(Commit, self._store[self._get_parents(commit)[0]]).tree",
            "                 if path_prefix:",
            "                     mode, subtree_sha = parent.lookup_path(",
            "                         self._store.__getitem__,",
            "                         path_prefix,",
            "                     )",
            "                     parent = self._store[subtree_sha]",
            "             else:",
            "-                changes_func = tree_changes_for_merge",
            "-                parent = [self._store[p].tree for p in self._get_parents(commit)]",
            "+                # For merge commits, we need to handle multiple parents differently",
            "+                parent = [",
            "+                    cast(Commit, self._store[p]).tree for p in self._get_parents(commit)",
            "+                ]",
            "+                # Use a lambda to adapt the signature",
            "+                changes_func = cast(",
            "+                    Any,",
            "+                    lambda store,",
            "+                    parent_trees,",
            "+                    tree_id,",
            "+                    rename_detector=None: tree_changes_for_merge(",
            "+                        store, parent_trees, tree_id, rename_detector",
            "+                    ),",
            "+                )",
            "                 if path_prefix:",
            "                     parent_trees = [self._store[p] for p in parent]",
            "                     parent = []",
            "                     for p in parent_trees:",
            "                         try:",
            "+                            from .objects import Tree",
            "+",
            "+                            assert isinstance(p, Tree)",
            "                             mode, st = p.lookup_path(",
            "                                 self._store.__getitem__,",
            "                                 path_prefix,",
            "                             )",
            "                         except KeyError:",
            "                             pass",
            "                         else:",
            "                             parent.append(st)",
            "             commit_tree_sha = commit.tree",
            "             if path_prefix:",
            "                 commit_tree = self._store[commit_tree_sha]",
            "+                from .objects import Tree",
            "+",
            "+                assert isinstance(commit_tree, Tree)",
            "                 mode, commit_tree_sha = commit_tree.lookup_path(",
            "                     self._store.__getitem__,",
            "                     path_prefix,",
            "                 )",
            "             cached = list(",
            "                 changes_func(",
            "                     self._store,",
            "@@ -113,15 +137,15 @@",
            "                     rename_detector=self._rename_detector,",
            "                 )",
            "             )",
            "             self._changes[path_prefix] = cached",
            "         return self._changes[path_prefix]",
            " ",
            "     def __repr__(self) -> str:",
            "-        return f\"<WalkEntry commit={self.commit.id}, changes={self.changes()!r}>\"",
            "+        return f\"<WalkEntry commit={self.commit.id.decode('ascii')}, changes={self.changes()!r}>\"",
            " ",
            " ",
            " class _CommitTimeQueue:",
            "     \"\"\"Priority queue of WalkEntry objects by commit time.\"\"\"",
            " ",
            "     def __init__(self, walker: \"Walker\") -> None:",
            "         self._walker = walker",
            "@@ -129,52 +153,54 @@",
            "         self._get_parents = walker.get_parents",
            "         self._excluded = walker.excluded",
            "         self._pq: list[tuple[int, Commit]] = []",
            "         self._pq_set: set[ObjectID] = set()",
            "         self._seen: set[ObjectID] = set()",
            "         self._done: set[ObjectID] = set()",
            "         self._min_time = walker.since",
            "-        self._last = None",
            "+        self._last: Optional[Commit] = None",
            "         self._extra_commits_left = _MAX_EXTRA_COMMITS",
            "         self._is_finished = False",
            " ",
            "         for commit_id in chain(walker.include, walker.excluded):",
            "             self._push(commit_id)",
            " ",
            "-    def _push(self, object_id: bytes) -> None:",
            "+    def _push(self, object_id: ObjectID) -> None:",
            "         try:",
            "             obj = self._store[object_id]",
            "         except KeyError as exc:",
            "             raise MissingCommitError(object_id) from exc",
            "         if isinstance(obj, Tag):",
            "             self._push(obj.object[1])",
            "             return",
            "         # TODO(jelmer): What to do about non-Commit and non-Tag objects?",
            "+        if not isinstance(obj, Commit):",
            "+            return",
            "         commit = obj",
            "         if commit.id not in self._pq_set and commit.id not in self._done:",
            "             heapq.heappush(self._pq, (-commit.commit_time, commit))",
            "             self._pq_set.add(commit.id)",
            "             self._seen.add(commit.id)",
            " ",
            "-    def _exclude_parents(self, commit) -> None:",
            "+    def _exclude_parents(self, commit: Commit) -> None:",
            "         excluded = self._excluded",
            "         seen = self._seen",
            "         todo = [commit]",
            "         while todo:",
            "             commit = todo.pop()",
            "             for parent in self._get_parents(commit):",
            "                 if parent not in excluded and parent in seen:",
            "                     # TODO: This is inefficient unless the object store does",
            "                     # some caching (which DiskObjectStore currently does not).",
            "                     # We could either add caching in this class or pass around",
            "                     # parsed queue entry objects instead of commits.",
            "-                    todo.append(self._store[parent])",
            "+                    todo.append(cast(Commit, self._store[parent]))",
            "                 excluded.add(parent)",
            " ",
            "-    def next(self):",
            "+    def next(self) -> Optional[WalkEntry]:",
            "         if self._is_finished:",
            "             return None",
            "         while self._pq:",
            "             _, commit = heapq.heappop(self._pq)",
            "             sha = commit.id",
            "             self._pq_set.remove(sha)",
            "             if sha in self._done:",
            "@@ -229,27 +255,27 @@",
            " ",
            "     Walker objects are initialized with a store and other options and can then",
            "     be treated as iterators of Commit objects.",
            "     \"\"\"",
            " ",
            "     def __init__(",
            "         self,",
            "-        store,",
            "+        store: \"BaseObjectStore\",",
            "         include: list[bytes],",
            "         exclude: Optional[list[bytes]] = None,",
            "         order: str = \"date\",",
            "         reverse: bool = False,",
            "         max_entries: Optional[int] = None,",
            "         paths: Optional[list[bytes]] = None,",
            "         rename_detector: Optional[RenameDetector] = None,",
            "         follow: bool = False,",
            "         since: Optional[int] = None,",
            "         until: Optional[int] = None,",
            "-        get_parents=lambda commit: commit.parents,",
            "-        queue_cls=_CommitTimeQueue,",
            "+        get_parents: Callable[[Commit], list[bytes]] = lambda commit: commit.parents,",
            "+        queue_cls: type = _CommitTimeQueue,",
            "     ) -> None:",
            "         \"\"\"Constructor.",
            " ",
            "         Args:",
            "           store: ObjectStore instance for looking up objects.",
            "           include: Iterable of SHAs of commits to include along with their",
            "             ancestors.",
            "@@ -296,30 +322,30 @@",
            "         self.since = since",
            "         self.until = until",
            " ",
            "         self._num_entries = 0",
            "         self._queue = queue_cls(self)",
            "         self._out_queue: collections.deque[WalkEntry] = collections.deque()",
            " ",
            "-    def _path_matches(self, changed_path) -> bool:",
            "+    def _path_matches(self, changed_path: Optional[bytes]) -> bool:",
            "         if changed_path is None:",
            "             return False",
            "         if self.paths is None:",
            "             return True",
            "         for followed_path in self.paths:",
            "             if changed_path == followed_path:",
            "                 return True",
            "             if (",
            "                 changed_path.startswith(followed_path)",
            "                 and changed_path[len(followed_path)] == b\"/\"[0]",
            "             ):",
            "                 return True",
            "         return False",
            " ",
            "-    def _change_matches(self, change) -> bool:",
            "+    def _change_matches(self, change: TreeChange) -> bool:",
            "         assert self.paths",
            "         if not change:",
            "             return False",
            " ",
            "         old_path = change.old.path",
            "         new_path = change.new.path",
            "         if self._path_matches(new_path):",
            "@@ -327,15 +353,15 @@",
            "                 self.paths.add(old_path)",
            "                 self.paths.remove(new_path)",
            "             return True",
            "         elif self._path_matches(old_path):",
            "             return True",
            "         return False",
            " ",
            "-    def _should_return(self, entry) -> Optional[bool]:",
            "+    def _should_return(self, entry: WalkEntry) -> Optional[bool]:",
            "         \"\"\"Determine if a walk entry should be returned..",
            " ",
            "         Args:",
            "           entry: The WalkEntry to consider.",
            "         Returns: True if the WalkEntry should be returned by this walk, or",
            "             False otherwise (e.g. if it doesn't match any requested paths).",
            "         \"\"\"",
            "@@ -355,68 +381,85 @@",
            "                 # For merge commits, only include changes with conflicts for",
            "                 # this path. Since a rename conflict may include different",
            "                 # old.paths, we have to check all of them.",
            "                 for change in path_changes:",
            "                     if self._change_matches(change):",
            "                         return True",
            "         else:",
            "-            for change in entry.changes():",
            "-                if self._change_matches(change):",
            "-                    return True",
            "+            changes = entry.changes()",
            "+            # Handle both list[TreeChange] and list[list[TreeChange]]",
            "+            if changes and isinstance(changes[0], list):",
            "+                # It's list[list[TreeChange]], flatten it",
            "+                for change_list in changes:",
            "+                    for change in change_list:",
            "+                        if self._change_matches(change):",
            "+                            return True",
            "+            else:",
            "+                # It's list[TreeChange]",
            "+                from .diff_tree import TreeChange",
            "+",
            "+                for change in changes:",
            "+                    if isinstance(change, TreeChange) and self._change_matches(change):",
            "+                        return True",
            "         return None",
            " ",
            "-    def _next(self):",
            "+    def _next(self) -> Optional[WalkEntry]:",
            "         max_entries = self.max_entries",
            "         while max_entries is None or self._num_entries < max_entries:",
            "             entry = next(self._queue)",
            "             if entry is not None:",
            "                 self._out_queue.append(entry)",
            "             if entry is None or len(self._out_queue) > _MAX_EXTRA_COMMITS:",
            "                 if not self._out_queue:",
            "                     return None",
            "                 entry = self._out_queue.popleft()",
            "                 if self._should_return(entry):",
            "                     self._num_entries += 1",
            "                     return entry",
            "         return None",
            " ",
            "-    def _reorder(self, results):",
            "+    def _reorder(",
            "+        self, results: Iterator[WalkEntry]",
            "+    ) -> Union[Iterator[WalkEntry], list[WalkEntry]]:",
            "         \"\"\"Possibly reorder a results iterator.",
            " ",
            "         Args:",
            "           results: An iterator of WalkEntry objects, in the order returned",
            "             from the queue_cls.",
            "         Returns: An iterator or list of WalkEntry objects, in the order",
            "             required by the Walker.",
            "         \"\"\"",
            "         if self.order == ORDER_TOPO:",
            "             results = _topo_reorder(results, self.get_parents)",
            "         if self.reverse:",
            "             results = reversed(list(results))",
            "         return results",
            " ",
            "-    def __iter__(self):",
            "+    def __iter__(self) -> Iterator[WalkEntry]:",
            "         return iter(self._reorder(iter(self._next, None)))",
            " ",
            " ",
            "-def _topo_reorder(entries, get_parents=lambda commit: commit.parents):",
            "+def _topo_reorder(",
            "+    entries: Iterator[WalkEntry],",
            "+    get_parents: Callable[[Commit], list[bytes]] = lambda commit: commit.parents,",
            "+) -> Iterator[WalkEntry]:",
            "     \"\"\"Reorder an iterable of entries topologically.",
            " ",
            "     This works best assuming the entries are already in almost-topological",
            "     order, e.g. in commit time order.",
            " ",
            "     Args:",
            "       entries: An iterable of WalkEntry objects.",
            "       get_parents: Optional function for getting the parents of a commit.",
            "     Returns: iterator over WalkEntry objects from entries in FIFO order, except",
            "         where a parent would be yielded before any of its children.",
            "     \"\"\"",
            "-    todo = collections.deque()",
            "-    pending = {}",
            "-    num_children = collections.defaultdict(int)",
            "+    todo: collections.deque[WalkEntry] = collections.deque()",
            "+    pending: dict[bytes, WalkEntry] = {}",
            "+    num_children: dict[bytes, int] = collections.defaultdict(int)",
            "     for entry in entries:",
            "         todo.append(entry)",
            "         for p in get_parents(entry.commit):",
            "             num_children[p] += 1",
            " ",
            "     while todo:",
            "         entry = todo.popleft()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/dulwich/web.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/dulwich/web.py",
            "+++ /home/dulwich-dulwich-0.23.2/dulwich/web.py",
            "@@ -254,14 +254,16 @@",
            "         chunk = f.read(length + 2)",
            "         if length == 0:",
            "             break",
            "         yield chunk[:-2]",
            " ",
            " ",
            " class ChunkReader:",
            "+    \"\"\"Reader for chunked transfer encoding streams.\"\"\"",
            "+",
            "     def __init__(self, f) -> None:",
            "         self._iter = _chunk_iter(f)",
            "         self._buffer: list[bytes] = []",
            " ",
            "     def read(self, n):",
            "         while sum(map(len, self._buffer)) < n:",
            "             try:",
            "@@ -553,14 +555,16 @@",
            "             self.get_environ(),",
            "         )",
            "         handler.request_handler = self  # type: ignore  # backpointer for logging",
            "         handler.run(self.server.get_app())  # type: ignore",
            " ",
            " ",
            " class WSGIServerLogger(WSGIServer):",
            "+    \"\"\"WSGIServer that uses dulwich's logger for error handling.\"\"\"",
            "+",
            "     def handle_error(self, request, client_address) -> None:",
            "         \"\"\"Handle an error.\"\"\"",
            "         logger.exception(",
            "             f\"Exception happened during processing of request from {client_address!s}\"",
            "         )"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/examples/diff.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/examples/diff.py",
            "+++ /home/dulwich-dulwich-0.23.2/examples/diff.py",
            "@@ -15,9 +15,8 @@",
            " repo_path = \".\"",
            " commit_id = b\"a6602654997420bcfd0bee2a0563d9416afe34b4\"",
            " ",
            " r = Repo(repo_path)",
            " ",
            " commit = r[commit_id]",
            " parent_commit = r[commit.parents[0]]",
            "-outstream = getattr(sys.stdout, \"buffer\", sys.stdout)",
            "-write_tree_diff(outstream, r.object_store, parent_commit.tree, commit.tree)",
            "+write_tree_diff(sys.stdout.buffer, r.object_store, parent_commit.tree, commit.tree)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/examples/gcs.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/examples/gcs.py",
            "+++ /home/dulwich-dulwich-0.23.2/examples/gcs.py",
            "@@ -1,13 +1,13 @@",
            " #!/usr/bin/python3",
            " # SPDX-License-Identifier: Apache-2.0 OR GPL-2.0-or-later",
            " ",
            " import tempfile",
            " ",
            "-from google.cloud import storage",
            "+from google.cloud import storage  # type: ignore[attr-defined]",
            " ",
            " from dulwich.cloud.gcs import GcsObjectStore",
            " from dulwich.repo import Repo",
            " ",
            " client = storage.Client()",
            " bucket = client.get_bucket(\"mybucket\")"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/setup.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/setup.py",
            "+++ /home/dulwich-dulwich-0.23.2/setup.py",
            "@@ -36,15 +36,15 @@",
            " optional = os.environ.get(\"CIBUILDWHEEL\", \"0\") != \"1\"",
            " ",
            " # Ideally, setuptools would just provide a way to do this",
            " if \"PURE\" in os.environ or \"--pure\" in sys.argv:",
            "     if \"--pure\" in sys.argv:",
            "         sys.argv.remove(\"--pure\")",
            "     setup_requires = []",
            "-    rust_extensions = []",
            "+    rust_extensions = []  # type: list[\"RustExtension\"]",
            " else:",
            "     setup_requires = [\"setuptools_rust\"]",
            "     # We check for egg_info since that indicates we are running prepare_metadata_for_build_*",
            "     if \"egg_info\" in sys.argv:",
            "         rust_extensions = []",
            "     else:",
            "         from setuptools_rust import Binding, RustExtension",
            "@@ -71,9 +71,8 @@",
            "         ]",
            " ",
            " ",
            " setup(",
            "     package_data={\"\": [\"py.typed\"]},",
            "     rust_extensions=rust_extensions,",
            "     setup_requires=setup_requires,",
            "-    tests_require=tests_require,",
            " )"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/__init__.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/__init__.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/__init__.py",
            "@@ -49,21 +49,21 @@",
            "         self.overrideEnv(\"HOME\", \"/nonexistent\")",
            "         self.overrideEnv(\"GIT_CONFIG_NOSYSTEM\", \"1\")",
            " ",
            "     def overrideEnv(self, name, value) -> None:",
            "         def restore() -> None:",
            "             if oldval is not None:",
            "                 os.environ[name] = oldval",
            "-            else:",
            "+            elif name in os.environ:",
            "                 del os.environ[name]",
            " ",
            "         oldval = os.environ.get(name)",
            "         if value is not None:",
            "             os.environ[name] = value",
            "-        else:",
            "+        elif name in os.environ:",
            "             del os.environ[name]",
            "         self.addCleanup(restore)",
            " ",
            " ",
            " class BlackboxTestCase(TestCase):",
            "     \"\"\"Blackbox testing.\"\"\"",
            " ",
            "@@ -111,46 +111,66 @@",
            "             stderr=subprocess.PIPE,",
            "             env=env,",
            "         )",
            " ",
            " ",
            " def self_test_suite():",
            "     names = [",
            "+        \"annotate\",",
            "         \"archive\",",
            "+        \"attrs\",",
            "+        \"bisect\",",
            "         \"blackbox\",",
            "         \"bundle\",",
            "+        \"cli\",",
            "+        \"cli_cherry_pick\",",
            "+        \"cli_merge\",",
            "         \"client\",",
            "+        \"cloud_gcs\",",
            "+        \"commit_graph\",",
            "         \"config\",",
            "         \"credentials\",",
            "         \"diff_tree\",",
            "+        \"dumb\",",
            "         \"fastexport\",",
            "         \"file\",",
            "+        \"gc\",",
            "         \"grafts\",",
            "         \"graph\",",
            "         \"greenthreads\",",
            "         \"hooks\",",
            "         \"ignore\",",
            "         \"index\",",
            "         \"lfs\",",
            "         \"line_ending\",",
            "+        \"log_utils\",",
            "         \"lru_cache\",",
            "         \"mailmap\",",
            "+        \"merge\",",
            "+        \"missing_obj_finder\",",
            "+        \"notes\",",
            "         \"objects\",",
            "         \"objectspec\",",
            "         \"object_store\",",
            "-        \"missing_obj_finder\",",
            "         \"pack\",",
            "         \"patch\",",
            "         \"porcelain\",",
            "+        \"porcelain_cherry_pick\",",
            "+        \"porcelain_merge\",",
            "+        \"porcelain_notes\",",
            "         \"protocol\",",
            "+        \"rebase\",",
            "         \"reflog\",",
            "         \"refs\",",
            "+        \"reftable\",",
            "         \"repository\",",
            "         \"server\",",
            "+        \"sparse_patterns\",",
            "         \"stash\",",
            "+        \"submodule\",",
            "         \"utils\",",
            "         \"walk\",",
            "         \"web\",",
            "     ]",
            "     module_names = [\"tests.test_\" + name for name in names]",
            "     loader = unittest.TestLoader()",
            "     return loader.loadTestsFromNames(module_names)",
            "@@ -204,15 +224,15 @@",
            "     )",
            " ",
            " ",
            " def nocompat_test_suite():",
            "     result = unittest.TestSuite()",
            "     result.addTests(self_test_suite())",
            "     result.addTests(tutorial_test_suite())",
            "-    from dulwich.contrib import test_suite as contrib_test_suite",
            "+    from .contrib import test_suite as contrib_test_suite",
            " ",
            "     result.addTests(contrib_test_suite())",
            "     return result",
            " ",
            " ",
            " def compat_test_suite():",
            "     result = unittest.TestSuite()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/compat/__init__.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/compat/__init__.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/compat/__init__.py",
            "@@ -22,18 +22,23 @@",
            " \"\"\"Compatibility tests for Dulwich.\"\"\"",
            " ",
            " import unittest",
            " ",
            " ",
            " def test_suite():",
            "     names = [",
            "+        \"check_ignore\",",
            "         \"client\",",
            "+        \"commit_graph\",",
            "+        \"dumb\",",
            "+        \"index\",",
            "         \"pack\",",
            "         \"patch\",",
            "         \"porcelain\",",
            "+        \"reftable\",",
            "         \"repository\",",
            "         \"server\",",
            "         \"utils\",",
            "         \"web\",",
            "     ]",
            "     module_names = [\"tests.compat.test_\" + name for name in names]",
            "     result = unittest.TestSuite()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/compat/test_client.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/compat/test_client.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/compat/test_client.py",
            "@@ -265,28 +265,28 @@",
            "             [",
            "                 b\"HEAD\",",
            "                 b\"refs/heads/branch\",",
            "                 b\"refs/heads/master\",",
            "                 b\"refs/tags/v1.0\",",
            "                 b\"refs/tags/v1.0^{}\",",
            "             ],",
            "-            sorted(refs.keys()),",
            "+            sorted(refs.refs.keys()),",
            "         )",
            " ",
            "     def test_get_refs_with_ref_prefix(self) -> None:",
            "         c = self._client()",
            "         refs = c.get_refs(",
            "             self._build_path(\"/server_new.export\"), ref_prefix=[b\"refs/heads\"]",
            "         )",
            "         self.assertEqual(",
            "             [",
            "                 b\"refs/heads/branch\",",
            "                 b\"refs/heads/master\",",
            "             ],",
            "-            sorted(refs.keys()),",
            "+            sorted(refs.refs.keys()),",
            "         )",
            " ",
            "     def test_fetch_pack_depth(self) -> None:",
            "         c = self._client()",
            "         with repo.Repo(os.path.join(self.gitroot, \"dest\")) as dest:",
            "             result = c.fetch(self._build_path(\"/server_new.export\"), dest, depth=1)",
            "             for r in result.refs.items():",
            "@@ -401,15 +401,15 @@",
            " ",
            "     def test_get_refs(self) -> None:",
            "         c = self._client()",
            "         refs = c.get_refs(self._build_path(\"/server_new.export\"))",
            " ",
            "         repo_dir = os.path.join(self.gitroot, \"server_new.export\")",
            "         with repo.Repo(repo_dir) as dest:",
            "-            self.assertDictEqual(dest.refs.as_dict(), refs)",
            "+            self.assertDictEqual(dest.refs.as_dict(), refs.refs)",
            " ",
            " ",
            " class DulwichTCPClientTest(CompatTestCase, DulwichClientTestBase):",
            "     def setUp(self) -> None:",
            "         CompatTestCase.setUp(self)",
            "         DulwichClientTestBase.setUp(self)",
            "         if check_for_daemon(limit=1):"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/compat/test_pack.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/compat/test_pack.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/compat/test_pack.py",
            "@@ -24,20 +24,27 @@",
            " import binascii",
            " import os",
            " import re",
            " import shutil",
            " import tempfile",
            " from typing import NoReturn",
            " ",
            "+from dulwich.file import GitFile",
            " from dulwich.objects import Blob",
            "-from dulwich.pack import write_pack",
            "+from dulwich.pack import (",
            "+    PackData,",
            "+    PackIndex3,",
            "+    load_pack_index,",
            "+    write_pack,",
            "+    write_pack_index_v3,",
            "+)",
            " ",
            " from .. import SkipTest",
            " from ..test_pack import PackTests, a_sha, pack1_sha",
            "-from .utils import require_git_version, run_git_or_fail",
            "+from .utils import require_git_version, rmtree_ro, run_git_or_fail",
            " ",
            " _NON_DELTA_RE = re.compile(b\"non delta: (?P<non_delta>\\\\d+) objects\")",
            " ",
            " ",
            " def _git_verify_pack_object_list(output):",
            "     pack_shas = set()",
            "     for line in output.splitlines():",
            "@@ -159,7 +166,82 @@",
            "         # non-delta objects:",
            "         got_non_delta = int(_NON_DELTA_RE.search(output).group(\"non_delta\"))",
            "         self.assertEqual(",
            "             4,",
            "             got_non_delta,",
            "             f\"Expected 4 non-delta objects, got {got_non_delta}\",",
            "         )",
            "+",
            "+",
            "+class TestPackIndexCompat(PackTests):",
            "+    \"\"\"Compatibility tests for pack index formats.\"\"\"",
            "+",
            "+    def setUp(self) -> None:",
            "+        require_git_version((1, 5, 0))",
            "+        super().setUp()",
            "+        self._tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(rmtree_ro, self._tempdir)",
            "+",
            "+    def test_dulwich_create_index_git_readable(self) -> None:",
            "+        \"\"\"Test that git can read pack indexes created by dulwich.\"\"\"",
            "+        # Create a simple pack with objects",
            "+        blob = Blob()",
            "+        blob.data = b\"Test blob\"",
            "+",
            "+        pack_path = os.path.join(self._tempdir, \"test_pack\")",
            "+        entries = [(blob, None)]",
            "+        write_pack(pack_path, entries)",
            "+",
            "+        # Load the pack and create v2 index (most compatible)",
            "+        pack_data = PackData(pack_path + \".pack\")",
            "+        try:",
            "+            pack_data.create_index(pack_path + \".idx\", version=2)",
            "+        finally:",
            "+            pack_data.close()",
            "+",
            "+        # Verify git can read it",
            "+        output = run_git_or_fail([\"verify-pack\", \"-v\", pack_path + \".pack\"])",
            "+        self.assertIn(blob.id.decode(\"ascii\"), output.decode(\"ascii\"))",
            "+",
            "+    def test_dulwich_read_git_index(self) -> None:",
            "+        \"\"\"Test that dulwich can read pack indexes created by git.\"\"\"",
            "+        # Create a simple pack with objects",
            "+        blob = Blob()",
            "+        blob.data = b\"Test blob for git\"",
            "+",
            "+        pack_path = os.path.join(self._tempdir, \"git_pack\")",
            "+        entries = [(blob, None)]",
            "+        write_pack(pack_path, entries)",
            "+",
            "+        # Create index with git",
            "+        run_git_or_fail([\"index-pack\", pack_path + \".pack\"])",
            "+",
            "+        # Load with dulwich",
            "+        idx = load_pack_index(pack_path + \".idx\")",
            "+",
            "+        # Verify it works",
            "+        self.assertIn(blob.id, idx)",
            "+        self.assertEqual(len(idx), 1)",
            "+",
            "+    def test_index_format_v3_sha256_future(self) -> None:",
            "+        \"\"\"Test that v3 index format is ready for SHA-256 support.\"\"\"",
            "+        # This test verifies the v3 implementation structure is ready",
            "+        # for SHA-256, even though SHA-256 itself is not yet implemented",
            "+",
            "+        # Create a dummy v3 index to test the format",
            "+        entries = [(b\"a\" * 20, 100, 1234)]  # SHA-1 for now",
            "+",
            "+        v3_path = os.path.join(self._tempdir, \"v3_test.idx\")",
            "+        with GitFile(v3_path, \"wb\") as f:",
            "+            write_pack_index_v3(f, entries, b\"x\" * 20, hash_algorithm=1)",
            "+",
            "+        # Load and verify structure",
            "+        idx = load_pack_index(v3_path)",
            "+        self.assertIsInstance(idx, PackIndex3)",
            "+        self.assertEqual(idx.version, 3)",
            "+        self.assertEqual(idx.hash_algorithm, 1)  # SHA-1",
            "+        self.assertEqual(idx.hash_size, 20)",
            "+",
            "+        # Verify SHA-256 would raise NotImplementedError",
            "+        with self.assertRaises(NotImplementedError):",
            "+            with GitFile(v3_path + \".sha256\", \"wb\") as f:",
            "+                write_pack_index_v3(f, entries, b\"x\" * 32, hash_algorithm=2)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/compat/utils.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/compat/utils.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/compat/utils.py",
            "@@ -136,14 +136,27 @@",
            "     Raises:",
            "       OSError: if the git executable was not found.",
            "     \"\"\"",
            "     env = popen_kwargs.pop(\"env\", {})",
            "     env[\"LC_ALL\"] = env[\"LANG\"] = \"C\"",
            "     env[\"PATH\"] = os.getenv(\"PATH\")",
            " ",
            "+    # Preserve Git identity environment variables if they exist, otherwise set dummy values",
            "+    git_env_defaults = {",
            "+        \"GIT_AUTHOR_NAME\": \"Test User\",",
            "+        \"GIT_AUTHOR_EMAIL\": \"test@example.com\",",
            "+        \"GIT_COMMITTER_NAME\": \"Test User\",",
            "+        \"GIT_COMMITTER_EMAIL\": \"test@example.com\",",
            "+    }",
            "+",
            "+    for git_env_var, default_value in git_env_defaults.items():",
            "+        if git_env_var not in env:",
            "+            # If the environment variable is not set, use the default value",
            "+            env[git_env_var] = default_value",
            "+",
            "     args = [git_path, *args]",
            "     popen_kwargs[\"stdin\"] = subprocess.PIPE",
            "     if capture_stdout:",
            "         popen_kwargs[\"stdout\"] = subprocess.PIPE",
            "     else:",
            "         popen_kwargs.pop(\"stdout\", None)",
            "     if capture_stderr:",
            "@@ -270,16 +283,19 @@",
            "             repo.close()",
            "             rmtree_ro(os.path.dirname(path.rstrip(os.sep)))",
            " ",
            "         self.addCleanup(cleanup)",
            "         return repo",
            " ",
            " ",
            "-if sys.platform == \"win32\":",
            "+def remove_ro(path: str) -> None:",
            "+    \"\"\"Remove a read-only file.\"\"\"",
            "+    os.chmod(path, stat.S_IWRITE)",
            "+    os.remove(path)",
            " ",
            "-    def remove_ro(action, name, exc) -> None:",
            "-        os.chmod(name, stat.S_IWRITE)",
            "-        os.remove(name)",
            " ",
            "-    rmtree_ro = functools.partial(shutil.rmtree, onerror=remove_ro)",
            "+if sys.platform == \"win32\":",
            "+    rmtree_ro = functools.partial(",
            "+        shutil.rmtree, onerror=lambda action, name, exc: remove_ro(name)",
            "+    )",
            " else:",
            "     rmtree_ro = shutil.rmtree"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/contrib/__init__.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/contrib/__init__.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/contrib/__init__.py",
            "@@ -20,14 +20,15 @@",
            " #",
            " ",
            " ",
            " def test_suite():",
            "     import unittest",
            " ",
            "     names = [",
            "+        \"diffstat\",",
            "         \"paramiko_vendor\",",
            "         \"release_robot\",",
            "         \"swift\",",
            "     ]",
            "     module_names = [\"tests.contrib.test_\" + name for name in names]",
            "     loader = unittest.TestLoader()",
            "     return loader.loadTestsFromNames(module_names)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/contrib/test_paramiko_vendor.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/contrib/test_paramiko_vendor.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/contrib/test_paramiko_vendor.py",
            "@@ -16,19 +16,22 @@",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Tests for paramiko_vendor.\"\"\"",
            " ",
            "+import os",
            " import socket",
            "+import tempfile",
            " import threading",
            " from io import StringIO",
            " from typing import Optional",
            " from unittest import skipIf",
            "+from unittest.mock import patch",
            " ",
            " from .. import TestCase",
            " ",
            " try:",
            "     import paramiko",
            " except ImportError:",
            "     has_paramiko = False",
            "@@ -217,7 +220,50 @@",
            "         # Fixme: it's return false",
            "         # self.assertTrue(con.can_read())",
            " ",
            "         self.assertEqual(b\"stdout\\n\", con.read(4096))",
            " ",
            "         # Fixme: it's return empty string",
            "         # self.assertEqual(b'stderr\\n', con.read_stderr(4096))",
            "+",
            "+    def test_ssh_config_parsing(self) -> None:",
            "+        \"\"\"Test that SSH config is properly parsed and used by ParamikoSSHVendor.\"\"\"",
            "+        # Create a temporary SSH config file",
            "+        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".config\", delete=False) as f:",
            "+            f.write(",
            "+                f\"\"\"",
            "+Host testserver",
            "+    HostName 127.0.0.1",
            "+    User testuser",
            "+    Port {self.port}",
            "+    IdentityFile /path/to/key",
            "+\"\"\"",
            "+            )",
            "+            config_path = f.name",
            "+",
            "+        try:",
            "+            # Mock the config path",
            "+            with patch(",
            "+                \"dulwich.contrib.paramiko_vendor.os.path.expanduser\"",
            "+            ) as mock_expanduser:",
            "+",
            "+                def side_effect(path):",
            "+                    if path == \"~/.ssh/config\":",
            "+                        return config_path",
            "+                    return path",
            "+",
            "+                mock_expanduser.side_effect = side_effect",
            "+",
            "+                vendor = ParamikoSSHVendor(",
            "+                    allow_agent=False,",
            "+                    look_for_keys=False,",
            "+                )",
            "+",
            "+                # Test that SSH config values are loaded",
            "+                host_config = vendor.ssh_config.lookup(\"testserver\")",
            "+                self.assertEqual(host_config[\"hostname\"], \"127.0.0.1\")",
            "+                self.assertEqual(host_config[\"user\"], \"testuser\")",
            "+                self.assertEqual(host_config[\"port\"], str(self.port))",
            "+                self.assertIn(\"/path/to/key\", host_config[\"identityfile\"])",
            "+",
            "+        finally:",
            "+            os.unlink(config_path)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/contrib/test_release_robot.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/contrib/test_release_robot.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/contrib/test_release_robot.py",
            "@@ -17,23 +17,27 @@",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Tests for release_robot.\"\"\"",
            " ",
            " import datetime",
            "+import logging",
            " import os",
            " import re",
            " import shutil",
            "+import sys",
            " import tempfile",
            " import time",
            " import unittest",
            " from typing import ClassVar, Optional",
            "+from unittest.mock import MagicMock, patch",
            " ",
            " from dulwich.contrib import release_robot",
            "+from dulwich.objects import Commit, Tag",
            " from dulwich.repo import Repo",
            " from dulwich.tests.utils import make_commit, make_tag",
            " ",
            " BASEDIR = os.path.abspath(os.path.dirname(__file__))  # this directory",
            " ",
            " ",
            " def gmtime_to_datetime(gmt):",
            "@@ -56,16 +60,30 @@",
            "             \"version 0.3\": \"0.3\",",
            "             \"version_0.3_rc_1\": \"0.3_rc_1\",",
            "             \"v1\": \"1\",",
            "             \"0.3rc1\": \"0.3rc1\",",
            "         }",
            "         for testcase, version in test_cases.items():",
            "             matches = re.match(release_robot.PATTERN, testcase)",
            "+            assert matches is not None",
            "             self.assertEqual(matches.group(1), version)",
            " ",
            "+    def test_pattern_no_match(self) -> None:",
            "+        \"\"\"Test tags that don't match the pattern.\"\"\"",
            "+        test_cases = [",
            "+            \"master\",",
            "+            \"HEAD\",",
            "+            \"feature-branch\",",
            "+            \"no-numbers\",",
            "+            \"_\",",
            "+        ]",
            "+        for testcase in test_cases:",
            "+            matches = re.match(release_robot.PATTERN, testcase)",
            "+            self.assertIsNone(matches)",
            "+",
            " ",
            " class GetRecentTagsTest(unittest.TestCase):",
            "     \"\"\"test get recent tags.\"\"\"",
            " ",
            "     # Git repo for dulwich project",
            "     test_repo = os.path.join(BASEDIR, \"dulwich_test_repo.zip\")",
            "     committer = b\"Mark Mikofski <mark.mikofski@sunpowercorp.com>\"",
            "@@ -73,14 +91,22 @@",
            "     tag_test_data: ClassVar[",
            "         dict[bytes, tuple[int, bytes, Optional[tuple[int, bytes]]]]",
            "     ] = {",
            "         test_tags[0]: (1484788003, b\"3\" * 40, None),",
            "         test_tags[1]: (1484788314, b\"1\" * 40, (1484788401, b\"2\" * 40)),",
            "     }",
            " ",
            "+    # Class attributes set in setUpClass",
            "+    projdir: ClassVar[str]",
            "+    repo: ClassVar[Repo]",
            "+    c1: ClassVar[Commit]",
            "+    c2: ClassVar[Commit]",
            "+    t1: ClassVar[bytes]",
            "+    t2: ClassVar[Tag]",
            "+",
            "     @classmethod",
            "     def setUpClass(cls) -> None:",
            "         cls.projdir = tempfile.mkdtemp()  # temporary project directory",
            "         cls.repo = Repo.init(cls.projdir)  # test repo",
            "         obj_store = cls.repo.object_store  # test repo object store",
            "         # commit 1 ('2017-01-19T01:06:43')",
            "         cls.c1 = make_commit(",
            "@@ -99,40 +125,253 @@",
            "             commit_time=cls.tag_test_data[cls.test_tags[1]][0],",
            "             message=b\"annotated tag\",",
            "             parents=[cls.c1.id],",
            "             author=cls.committer,",
            "         )",
            "         obj_store.add_object(cls.c2)",
            "         # tag 2: annotated ('2017-01-19T01:13:21')",
            "+        tag_data = cls.tag_test_data[cls.test_tags[1]][2]",
            "+        if tag_data is None:",
            "+            raise AssertionError(\"test_tags[1] should have annotated tag data\")",
            "         cls.t2 = make_tag(",
            "             cls.c2,",
            "-            id=cls.tag_test_data[cls.test_tags[1]][2][1],",
            "+            id=tag_data[1],",
            "             name=cls.test_tags[1],",
            "-            tag_time=cls.tag_test_data[cls.test_tags[1]][2][0],",
            "+            tag_time=tag_data[0],",
            "         )",
            "         obj_store.add_object(cls.t2)",
            "         cls.repo[b\"refs/heads/master\"] = cls.c2.id",
            "         cls.repo[b\"refs/tags/\" + cls.t2.name] = cls.t2.id  # add annotated tag",
            " ",
            "     @classmethod",
            "     def tearDownClass(cls) -> None:",
            "         cls.repo.close()",
            "         shutil.rmtree(cls.projdir)",
            " ",
            "     def test_get_recent_tags(self) -> None:",
            "         \"\"\"Test get recent tags.\"\"\"",
            "         tags = release_robot.get_recent_tags(self.projdir)  # get test tags",
            "         for tag, metadata in tags:",
            "-            tag = tag.encode(\"utf-8\")",
            "-            test_data = self.tag_test_data[tag]  # test data tag",
            "+            tag_bytes = tag.encode(\"utf-8\")",
            "+            test_data = self.tag_test_data[tag_bytes]  # test data tag",
            "             # test commit date, id and author name",
            "             self.assertEqual(metadata[0], gmtime_to_datetime(test_data[0]))",
            "             self.assertEqual(metadata[1].encode(\"utf-8\"), test_data[1])",
            "             self.assertEqual(metadata[2].encode(\"utf-8\"), self.committer)",
            "             # skip unannotated tags",
            "             tag_obj = test_data[2]",
            "             if not tag_obj:",
            "                 continue",
            "             # tag date, id and name",
            "             self.assertEqual(metadata[3][0], gmtime_to_datetime(tag_obj[0]))",
            "             self.assertEqual(metadata[3][1].encode(\"utf-8\"), tag_obj[1])",
            "-            self.assertEqual(metadata[3][2].encode(\"utf-8\"), tag)",
            "+            self.assertEqual(metadata[3][2], tag)",
            "+",
            "+    def test_get_recent_tags_sorting(self) -> None:",
            "+        \"\"\"Test that tags are sorted by commit time from newest to oldest.\"\"\"",
            "+        tags = release_robot.get_recent_tags(self.projdir)",
            "+        # v0.1 should be first as it's newer",
            "+        self.assertEqual(tags[0][0], \"v0.1\")",
            "+        # v0.1a should be second as it's older",
            "+        self.assertEqual(tags[1][0], \"v0.1a\")",
            "+",
            "+    def test_get_recent_tags_non_tag_refs(self) -> None:",
            "+        \"\"\"Test that non-tag refs are ignored.\"\"\"",
            "+        # Create a commit on a branch to test that it's not included",
            "+        branch_commit = make_commit(",
            "+            message=b\"branch commit\",",
            "+            author=self.committer,",
            "+            commit_time=int(time.time()),",
            "+        )",
            "+        self.repo.object_store.add_object(branch_commit)",
            "+        self.repo[b\"refs/heads/test-branch\"] = branch_commit.id",
            "+",
            "+        # Get tags and ensure only the actual tags are returned",
            "+        tags = release_robot.get_recent_tags(self.projdir)",
            "+        self.assertEqual(len(tags), 2)  # Still only 2 tags",
            "+        tag_names = [tag[0] for tag in tags]",
            "+        self.assertIn(\"v0.1\", tag_names)",
            "+        self.assertIn(\"v0.1a\", tag_names)",
            "+        self.assertNotIn(\"test-branch\", tag_names)",
            "+",
            "+",
            "+class GetCurrentVersionTests(unittest.TestCase):",
            "+    \"\"\"Test get_current_version function.\"\"\"",
            "+",
            "+    def setUp(self):",
            "+        \"\"\"Set up a test repository for each test.\"\"\"",
            "+        self.projdir = tempfile.mkdtemp()",
            "+        self.repo = Repo.init(self.projdir)",
            "+        self.addCleanup(self.cleanup)",
            "+",
            "+    def cleanup(self):",
            "+        \"\"\"Clean up after test.\"\"\"",
            "+        self.repo.close()",
            "+        shutil.rmtree(self.projdir)",
            "+",
            "+    def test_no_tags(self):",
            "+        \"\"\"Test behavior when repo has no tags.\"\"\"",
            "+        # Create a repo with no tags",
            "+        result = release_robot.get_current_version(self.projdir)",
            "+        self.assertIsNone(result)",
            "+",
            "+    def test_tag_with_pattern_match(self):",
            "+        \"\"\"Test with a tag that matches the pattern.\"\"\"",
            "+        # Create a test commit and tag",
            "+        c = make_commit(message=b\"Test commit\")",
            "+        self.repo.object_store.add_object(c)",
            "+        self.repo[b\"refs/tags/v1.2.3\"] = c.id",
            "+        self.repo[b\"HEAD\"] = c.id",
            "+",
            "+        # Test that the version is extracted correctly",
            "+        result = release_robot.get_current_version(self.projdir)",
            "+        self.assertEqual(\"1.2.3\", result)",
            "+",
            "+    def test_tag_no_pattern_match(self):",
            "+        \"\"\"Test with a tag that doesn't match the pattern.\"\"\"",
            "+        # Create a test commit and tag that won't match the default pattern",
            "+        c = make_commit(message=b\"Test commit\")",
            "+        self.repo.object_store.add_object(c)",
            "+        self.repo[b\"refs/tags/no-version-tag\"] = c.id",
            "+        self.repo[b\"HEAD\"] = c.id",
            "+",
            "+        # Test that the full tag is returned when no match",
            "+        result = release_robot.get_current_version(self.projdir)",
            "+        self.assertEqual(\"no-version-tag\", result)",
            "+",
            "+    def test_with_logger(self):",
            "+        \"\"\"Test with a logger when regex match fails.\"\"\"",
            "+        # Create a test commit and tag that won't match the pattern",
            "+        c = make_commit(message=b\"Test commit\")",
            "+        self.repo.object_store.add_object(c)",
            "+        self.repo[b\"refs/tags/no-version-tag\"] = c.id",
            "+        self.repo[b\"HEAD\"] = c.id",
            "+",
            "+        # Create a logger",
            "+        logger = logging.getLogger(\"test_logger\")",
            "+",
            "+        # Test with the logger",
            "+        result = release_robot.get_current_version(self.projdir, logger=logger)",
            "+        self.assertEqual(\"no-version-tag\", result)",
            "+",
            "+    def test_custom_pattern(self):",
            "+        \"\"\"Test with a custom regex pattern.\"\"\"",
            "+        # Create a test commit and tag",
            "+        c = make_commit(message=b\"Test commit\")",
            "+        self.repo.object_store.add_object(c)",
            "+        self.repo[b\"refs/tags/CUSTOM-99.88.77\"] = c.id",
            "+        self.repo[b\"HEAD\"] = c.id",
            "+",
            "+        # Test with a custom pattern",
            "+        custom_pattern = r\"CUSTOM-([\\d\\.]+)\"",
            "+        result = release_robot.get_current_version(self.projdir, pattern=custom_pattern)",
            "+        self.assertEqual(\"99.88.77\", result)",
            "+",
            "+    def test_with_logger_debug_call(self):",
            "+        \"\"\"Test that the logger.debug method is actually called.\"\"\"",
            "+        # Create a test commit and tag that won't match the pattern",
            "+        c = make_commit(message=b\"Test commit\")",
            "+        self.repo.object_store.add_object(c)",
            "+        self.repo[b\"refs/tags/no-version-tag\"] = c.id",
            "+        self.repo[b\"HEAD\"] = c.id",
            "+",
            "+        # Create a mock logger",
            "+        mock_logger = MagicMock()",
            "+",
            "+        # Test with the mock logger",
            "+        result = release_robot.get_current_version(self.projdir, logger=mock_logger)",
            "+",
            "+        # Verify logger.debug was called",
            "+        mock_logger.debug.assert_called_once()",
            "+        # Check the tag name is in the debug message",
            "+        self.assertIn(\"no-version-tag\", mock_logger.debug.call_args[0][2])",
            "+",
            "+        # The result should still be the full tag",
            "+        self.assertEqual(\"no-version-tag\", result)",
            "+",
            "+    def test_multiple_tags(self):",
            "+        \"\"\"Test behavior with multiple tags to ensure we get the most recent.\"\"\"",
            "+        # Create multiple commits and tags with different timestamps",
            "+        c1 = make_commit(message=b\"First commit\", commit_time=1000)",
            "+        c2 = make_commit(message=b\"Second commit\", commit_time=2000, parents=[c1.id])",
            "+",
            "+        self.repo.object_store.add_object(c1)",
            "+        self.repo.object_store.add_object(c2)",
            "+",
            "+        # Add tags with older commit first",
            "+        self.repo[b\"refs/tags/v0.9.0\"] = c1.id",
            "+        self.repo[b\"refs/tags/v1.0.0\"] = c2.id",
            "+        self.repo[b\"HEAD\"] = c2.id",
            "+",
            "+        # Get the current version - should be from the most recent commit",
            "+        result = release_robot.get_current_version(self.projdir)",
            "+        self.assertEqual(\"1.0.0\", result)",
            "+",
            "+",
            "+class MainFunctionTests(unittest.TestCase):",
            "+    \"\"\"Test the __main__ block.\"\"\"",
            "+",
            "+    def setUp(self):",
            "+        \"\"\"Set up a test repository.\"\"\"",
            "+        self.projdir = tempfile.mkdtemp()",
            "+        self.repo = Repo.init(self.projdir)",
            "+        # Create a test commit and tag",
            "+        c = make_commit(message=b\"Test commit\")",
            "+        self.repo.object_store.add_object(c)",
            "+        self.repo[b\"refs/tags/v3.2.1\"] = c.id",
            "+        self.repo[b\"HEAD\"] = c.id",
            "+        self.addCleanup(self.cleanup)",
            "+",
            "+    def cleanup(self):",
            "+        \"\"\"Clean up after test.\"\"\"",
            "+        self.repo.close()",
            "+        shutil.rmtree(self.projdir)",
            "+",
            "+    @patch.object(sys, \"argv\", [\"release_robot.py\"])",
            "+    @patch(\"builtins.print\")",
            "+    def test_main_default_dir(self, mock_print):",
            "+        \"\"\"Test main function with default directory.\"\"\"",
            "+        # Run the __main__ block code with mocked environment",
            "+        module_globals = {",
            "+            \"__name__\": \"__main__\",",
            "+            \"sys\": sys,",
            "+            \"get_current_version\": lambda projdir: \"3.2.1\",",
            "+            \"PROJDIR\": \".\",",
            "+        }",
            "+        exec(",
            "+            compile(",
            "+                \"if __name__ == '__main__':\\n    if len(sys.argv) > 1:\\n        _PROJDIR = sys.argv[1]\\n    else:\\n        _PROJDIR = PROJDIR\\n    print(get_current_version(projdir=_PROJDIR))\",",
            "+                \"<string>\",",
            "+                \"exec\",",
            "+            ),",
            "+            module_globals,",
            "+        )",
            "+",
            "+        # Check that print was called with the version",
            "+        mock_print.assert_called_once_with(\"3.2.1\")",
            "+",
            "+    @patch.object(sys, \"argv\", [\"release_robot.py\", \"/custom/path\"])",
            "+    @patch(\"builtins.print\")",
            "+    @patch(\"dulwich.contrib.release_robot.get_current_version\")",
            "+    def test_main_custom_dir(self, mock_get_version, mock_print):",
            "+        \"\"\"Test main function with custom directory from command line.\"\"\"",
            "+        mock_get_version.return_value = \"4.5.6\"",
            "+",
            "+        # Run the __main__ block code with mocked environment",
            "+        module_globals = {",
            "+            \"__name__\": \"__main__\",",
            "+            \"sys\": sys,",
            "+            \"get_current_version\": mock_get_version,",
            "+            \"PROJDIR\": \".\",",
            "+        }",
            "+        exec(",
            "+            compile(",
            "+                \"if __name__ == '__main__':\\n    if len(sys.argv) > 1:\\n        _PROJDIR = sys.argv[1]\\n    else:\\n        _PROJDIR = PROJDIR\\n    print(get_current_version(projdir=_PROJDIR))\",",
            "+                \"<string>\",",
            "+                \"exec\",",
            "+            ),",
            "+            module_globals,",
            "+        )",
            "+",
            "+        # Check that get_current_version was called with the right arg",
            "+        mock_get_version.assert_called_once_with(projdir=\"/custom/path\")",
            "+        mock_print.assert_called_once_with(\"4.5.6\")"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/contrib/test_swift.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/contrib/test_swift.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/contrib/test_swift.py",
            "@@ -464,15 +464,15 @@",
            " ",
            "     def test_del_root(self) -> None:",
            "         with patch(",
            "             \"dulwich.contrib.swift.SwiftConnector.del_object\",",
            "             lambda *args: None,",
            "         ):",
            "             with patch(",
            "-                \"dulwich.contrib.swift.SwiftConnector.\" \"get_container_objects\",",
            "+                \"dulwich.contrib.swift.SwiftConnector.get_container_objects\",",
            "                 lambda *args: ({\"name\": \"a\"}, {\"name\": \"b\"}),",
            "             ):",
            "                 with patch(",
            "                     \"geventhttpclient.HTTPClient.request\",",
            "                     lambda *args: Response(),",
            "                 ):",
            "                     self.assertEqual(self.conn.del_root(), None)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_archive.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_archive.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_archive.py",
            "@@ -32,15 +32,15 @@",
            " from dulwich.tests.utils import build_commit_graph",
            " ",
            " from . import TestCase",
            " ",
            " try:",
            "     from unittest.mock import patch",
            " except ImportError:",
            "-    patch = None  # type: ignore",
            "+    patch = None",
            " ",
            " ",
            " class ArchiveTests(TestCase):",
            "     def test_empty(self) -> None:",
            "         store = MemoryObjectStore()",
            "         c1, c2, c3 = build_commit_graph(store, [[1], [2, 1], [3, 1, 2]])",
            "         tree = store[c3.tree]"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_bundle.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_bundle.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_bundle.py",
            "@@ -28,14 +28,278 @@",
            " from dulwich.bundle import Bundle, read_bundle, write_bundle",
            " from dulwich.pack import PackData, write_pack_objects",
            " ",
            " from . import TestCase",
            " ",
            " ",
            " class BundleTests(TestCase):",
            "+    def setUp(self):",
            "+        super().setUp()",
            "+        self.tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(os.rmdir, self.tempdir)",
            "+",
            "+    def test_bundle_repr(self) -> None:",
            "+        \"\"\"Test the Bundle.__repr__ method.\"\"\"",
            "+        bundle = Bundle()",
            "+        bundle.version = 3",
            "+        bundle.capabilities = {\"foo\": \"bar\"}",
            "+        bundle.prerequisites = [(b\"cc\" * 20, \"comment\")]",
            "+        bundle.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+",
            "+        # Create a simple pack data",
            "+        b = BytesIO()",
            "+        write_pack_objects(b.write, [])",
            "+        b.seek(0)",
            "+        bundle.pack_data = PackData.from_file(b)",
            "+",
            "+        # Check the repr output",
            "+        rep = repr(bundle)",
            "+        self.assertIn(\"Bundle(version=3\", rep)",
            "+        self.assertIn(\"capabilities={'foo': 'bar'}\", rep)",
            "+        self.assertIn(\"prerequisites=[(\", rep)",
            "+        self.assertIn(\"references={\", rep)",
            "+",
            "+    def test_bundle_equality(self) -> None:",
            "+        \"\"\"Test the Bundle.__eq__ method.\"\"\"",
            "+        # Create two identical bundles",
            "+        bundle1 = Bundle()",
            "+        bundle1.version = 3",
            "+        bundle1.capabilities = {\"foo\": \"bar\"}",
            "+        bundle1.prerequisites = [(b\"cc\" * 20, \"comment\")]",
            "+        bundle1.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+",
            "+        b1 = BytesIO()",
            "+        write_pack_objects(b1.write, [])",
            "+        b1.seek(0)",
            "+        bundle1.pack_data = PackData.from_file(b1)",
            "+",
            "+        bundle2 = Bundle()",
            "+        bundle2.version = 3",
            "+        bundle2.capabilities = {\"foo\": \"bar\"}",
            "+        bundle2.prerequisites = [(b\"cc\" * 20, \"comment\")]",
            "+        bundle2.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+",
            "+        b2 = BytesIO()",
            "+        write_pack_objects(b2.write, [])",
            "+        b2.seek(0)",
            "+        bundle2.pack_data = PackData.from_file(b2)",
            "+",
            "+        # Test equality",
            "+        self.assertEqual(bundle1, bundle2)",
            "+",
            "+        # Test inequality by changing different attributes",
            "+        bundle3 = Bundle()",
            "+        bundle3.version = 2  # Different version",
            "+        bundle3.capabilities = {\"foo\": \"bar\"}",
            "+        bundle3.prerequisites = [(b\"cc\" * 20, \"comment\")]",
            "+        bundle3.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+        b3 = BytesIO()",
            "+        write_pack_objects(b3.write, [])",
            "+        b3.seek(0)",
            "+        bundle3.pack_data = PackData.from_file(b3)",
            "+        self.assertNotEqual(bundle1, bundle3)",
            "+",
            "+        bundle4 = Bundle()",
            "+        bundle4.version = 3",
            "+        bundle4.capabilities = {\"different\": \"value\"}  # Different capabilities",
            "+        bundle4.prerequisites = [(b\"cc\" * 20, \"comment\")]",
            "+        bundle4.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+        b4 = BytesIO()",
            "+        write_pack_objects(b4.write, [])",
            "+        b4.seek(0)",
            "+        bundle4.pack_data = PackData.from_file(b4)",
            "+        self.assertNotEqual(bundle1, bundle4)",
            "+",
            "+        bundle5 = Bundle()",
            "+        bundle5.version = 3",
            "+        bundle5.capabilities = {\"foo\": \"bar\"}",
            "+        bundle5.prerequisites = [(b\"dd\" * 20, \"different\")]  # Different prerequisites",
            "+        bundle5.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+        b5 = BytesIO()",
            "+        write_pack_objects(b5.write, [])",
            "+        b5.seek(0)",
            "+        bundle5.pack_data = PackData.from_file(b5)",
            "+        self.assertNotEqual(bundle1, bundle5)",
            "+",
            "+        bundle6 = Bundle()",
            "+        bundle6.version = 3",
            "+        bundle6.capabilities = {\"foo\": \"bar\"}",
            "+        bundle6.prerequisites = [(b\"cc\" * 20, \"comment\")]",
            "+        bundle6.references = {",
            "+            b\"refs/heads/different\": b\"ab\" * 20",
            "+        }  # Different references",
            "+        b6 = BytesIO()",
            "+        write_pack_objects(b6.write, [])",
            "+        b6.seek(0)",
            "+        bundle6.pack_data = PackData.from_file(b6)",
            "+        self.assertNotEqual(bundle1, bundle6)",
            "+",
            "+        # Test inequality with different type",
            "+        self.assertNotEqual(bundle1, \"not a bundle\")",
            "+",
            "+    def test_read_bundle_v2(self) -> None:",
            "+        \"\"\"Test reading a v2 bundle.\"\"\"",
            "+        f = BytesIO()",
            "+        f.write(b\"# v2 git bundle\\n\")",
            "+        f.write(b\"-\" + b\"cc\" * 20 + b\" prerequisite comment\\n\")",
            "+        f.write(b\"ab\" * 20 + b\" refs/heads/master\\n\")",
            "+        f.write(b\"\\n\")",
            "+        # Add pack data",
            "+        b = BytesIO()",
            "+        write_pack_objects(b.write, [])",
            "+        f.write(b.getvalue())",
            "+        f.seek(0)",
            "+",
            "+        bundle = read_bundle(f)",
            "+        self.assertEqual(2, bundle.version)",
            "+        self.assertEqual({}, bundle.capabilities)",
            "+        self.assertEqual([(b\"cc\" * 20, \"prerequisite comment\")], bundle.prerequisites)",
            "+        self.assertEqual({b\"refs/heads/master\": b\"ab\" * 20}, bundle.references)",
            "+",
            "+    def test_read_bundle_v3(self) -> None:",
            "+        \"\"\"Test reading a v3 bundle with capabilities.\"\"\"",
            "+        f = BytesIO()",
            "+        f.write(b\"# v3 git bundle\\n\")",
            "+        f.write(b\"@capability1\\n\")",
            "+        f.write(b\"@capability2=value2\\n\")",
            "+        f.write(b\"-\" + b\"cc\" * 20 + b\" prerequisite comment\\n\")",
            "+        f.write(b\"ab\" * 20 + b\" refs/heads/master\\n\")",
            "+        f.write(b\"\\n\")",
            "+        # Add pack data",
            "+        b = BytesIO()",
            "+        write_pack_objects(b.write, [])",
            "+        f.write(b.getvalue())",
            "+        f.seek(0)",
            "+",
            "+        bundle = read_bundle(f)",
            "+        self.assertEqual(3, bundle.version)",
            "+        self.assertEqual(",
            "+            {\"capability1\": None, \"capability2\": \"value2\"}, bundle.capabilities",
            "+        )",
            "+        self.assertEqual([(b\"cc\" * 20, \"prerequisite comment\")], bundle.prerequisites)",
            "+        self.assertEqual({b\"refs/heads/master\": b\"ab\" * 20}, bundle.references)",
            "+",
            "+    def test_read_bundle_invalid_format(self) -> None:",
            "+        \"\"\"Test reading a bundle with invalid format.\"\"\"",
            "+        f = BytesIO()",
            "+        f.write(b\"invalid bundle format\\n\")",
            "+        f.seek(0)",
            "+",
            "+        with self.assertRaises(AssertionError):",
            "+            read_bundle(f)",
            "+",
            "+    def test_write_bundle_v2(self) -> None:",
            "+        \"\"\"Test writing a v2 bundle.\"\"\"",
            "+        bundle = Bundle()",
            "+        bundle.version = 2",
            "+        bundle.capabilities = {}",
            "+        bundle.prerequisites = [(b\"cc\" * 20, \"prerequisite comment\")]",
            "+        bundle.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+",
            "+        # Create a simple pack data",
            "+        b = BytesIO()",
            "+        write_pack_objects(b.write, [])",
            "+        b.seek(0)",
            "+        bundle.pack_data = PackData.from_file(b)",
            "+",
            "+        # Write the bundle",
            "+        f = BytesIO()",
            "+        write_bundle(f, bundle)",
            "+        f.seek(0)",
            "+",
            "+        # Verify the written content",
            "+        self.assertEqual(b\"# v2 git bundle\\n\", f.readline())",
            "+        self.assertEqual(b\"-\" + b\"cc\" * 20 + b\" prerequisite comment\\n\", f.readline())",
            "+        self.assertEqual(b\"ab\" * 20 + b\" refs/heads/master\\n\", f.readline())",
            "+        self.assertEqual(b\"\\n\", f.readline())",
            "+        # The rest is pack data which we don't validate in detail",
            "+",
            "+    def test_write_bundle_v3(self) -> None:",
            "+        \"\"\"Test writing a v3 bundle with capabilities.\"\"\"",
            "+        bundle = Bundle()",
            "+        bundle.version = 3",
            "+        bundle.capabilities = {\"capability1\": None, \"capability2\": \"value2\"}",
            "+        bundle.prerequisites = [(b\"cc\" * 20, \"prerequisite comment\")]",
            "+        bundle.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+",
            "+        # Create a simple pack data",
            "+        b = BytesIO()",
            "+        write_pack_objects(b.write, [])",
            "+        b.seek(0)",
            "+        bundle.pack_data = PackData.from_file(b)",
            "+",
            "+        # Write the bundle",
            "+        f = BytesIO()",
            "+        write_bundle(f, bundle)",
            "+        f.seek(0)",
            "+",
            "+        # Verify the written content",
            "+        self.assertEqual(b\"# v3 git bundle\\n\", f.readline())",
            "+        self.assertEqual(b\"@capability1\\n\", f.readline())",
            "+        self.assertEqual(b\"@capability2=value2\\n\", f.readline())",
            "+        self.assertEqual(b\"-\" + b\"cc\" * 20 + b\" prerequisite comment\\n\", f.readline())",
            "+        self.assertEqual(b\"ab\" * 20 + b\" refs/heads/master\\n\", f.readline())",
            "+        self.assertEqual(b\"\\n\", f.readline())",
            "+        # The rest is pack data which we don't validate in detail",
            "+",
            "+    def test_write_bundle_auto_version(self) -> None:",
            "+        \"\"\"Test writing a bundle with auto-detected version.\"\"\"",
            "+        # Create a bundle with no explicit version but capabilities",
            "+        bundle1 = Bundle()",
            "+        bundle1.version = None",
            "+        bundle1.capabilities = {\"capability1\": \"value1\"}",
            "+        bundle1.prerequisites = [(b\"cc\" * 20, \"prerequisite comment\")]",
            "+        bundle1.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+",
            "+        b1 = BytesIO()",
            "+        write_pack_objects(b1.write, [])",
            "+        b1.seek(0)",
            "+        bundle1.pack_data = PackData.from_file(b1)",
            "+",
            "+        f1 = BytesIO()",
            "+        write_bundle(f1, bundle1)",
            "+        f1.seek(0)",
            "+        # Should use v3 format since capabilities are present",
            "+        self.assertEqual(b\"# v3 git bundle\\n\", f1.readline())",
            "+",
            "+        # Create a bundle with no explicit version and no capabilities",
            "+        bundle2 = Bundle()",
            "+        bundle2.version = None",
            "+        bundle2.capabilities = {}",
            "+        bundle2.prerequisites = [(b\"cc\" * 20, \"prerequisite comment\")]",
            "+        bundle2.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "+",
            "+        b2 = BytesIO()",
            "+        write_pack_objects(b2.write, [])",
            "+        b2.seek(0)",
            "+        bundle2.pack_data = PackData.from_file(b2)",
            "+",
            "+        f2 = BytesIO()",
            "+        write_bundle(f2, bundle2)",
            "+        f2.seek(0)",
            "+        # Should use v2 format since no capabilities are present",
            "+        self.assertEqual(b\"# v2 git bundle\\n\", f2.readline())",
            "+",
            "+    def test_write_bundle_invalid_version(self) -> None:",
            "+        \"\"\"Test writing a bundle with an invalid version.\"\"\"",
            "+        bundle = Bundle()",
            "+        bundle.version = 4  # Invalid version",
            "+        bundle.capabilities = {}",
            "+        bundle.prerequisites = []",
            "+        bundle.references = {}",
            "+",
            "+        b = BytesIO()",
            "+        write_pack_objects(b.write, [])",
            "+        b.seek(0)",
            "+        bundle.pack_data = PackData.from_file(b)",
            "+",
            "+        f = BytesIO()",
            "+        with self.assertRaises(AssertionError):",
            "+            write_bundle(f, bundle)",
            "+",
            "     def test_roundtrip_bundle(self) -> None:",
            "         origbundle = Bundle()",
            "         origbundle.version = 3",
            "         origbundle.capabilities = {\"foo\": None}",
            "         origbundle.references = {b\"refs/heads/master\": b\"ab\" * 20}",
            "         origbundle.prerequisites = [(b\"cc\" * 20, \"comment\")]",
            "         b = BytesIO()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_client.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_client.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_client.py",
            "@@ -127,15 +127,15 @@",
            "                 b\"side-band-64k\",",
            "                 agent_cap,",
            "             },",
            "             set(self.client._send_capabilities),",
            "         )",
            " ",
            "     def test_archive_ack(self) -> None:",
            "-        self.rin.write(b\"0009NACK\\n\" b\"0000\")",
            "+        self.rin.write(b\"0009NACK\\n0000\")",
            "         self.rin.seek(0)",
            "         self.client.archive(b\"bla\", b\"HEAD\", None, None)",
            "         self.assertEqual(self.rout.getvalue(), b\"0011argument HEAD0000\")",
            " ",
            "     def test_fetch_empty(self) -> None:",
            "         self.rin.write(b\"0000\")",
            "         self.rin.seek(0)",
            "@@ -523,14 +523,38 @@",
            "         self.assertEqual(\"bar/baz\", path)",
            " ",
            "     def test_local(self) -> None:",
            "         c, path = get_transport_and_path(\"foo.bar/baz\")",
            "         self.assertIsInstance(c, LocalGitClient)",
            "         self.assertEqual(\"foo.bar/baz\", path)",
            " ",
            "+    def test_ssh_with_config(self) -> None:",
            "+        # Test that core.sshCommand from config is passed to SSHGitClient",
            "+        from dulwich.config import ConfigDict",
            "+",
            "+        config = ConfigDict()",
            "+        c, path = get_transport_and_path(",
            "+            \"ssh://git@github.com/user/repo.git\", config=config",
            "+        )",
            "+        self.assertIsInstance(c, SSHGitClient)",
            "+        self.assertIsNone(c.ssh_command)",
            "+",
            "+        config.set((b\"core\",), b\"sshCommand\", b\"custom-ssh -o CustomOption=yes\")",
            "+",
            "+        c, path = get_transport_and_path(",
            "+            \"ssh://git@github.com/user/repo.git\", config=config",
            "+        )",
            "+        self.assertIsInstance(c, SSHGitClient)",
            "+        self.assertEqual(\"custom-ssh -o CustomOption=yes\", c.ssh_command)",
            "+",
            "+        # Test rsync-style URL also gets the config",
            "+        c, path = get_transport_and_path(\"git@github.com:user/repo.git\", config=config)",
            "+        self.assertIsInstance(c, SSHGitClient)",
            "+        self.assertEqual(\"custom-ssh -o CustomOption=yes\", c.ssh_command)",
            "+",
            "     @skipIf(sys.platform != \"win32\", \"Behaviour only happens on windows.\")",
            "     def test_local_abs_windows_path(self) -> None:",
            "         c, path = get_transport_and_path(\"C:\\\\foo.bar\\\\baz\")",
            "         self.assertIsInstance(c, LocalGitClient)",
            "         self.assertEqual(\"C:\\\\foo.bar\\\\baz\", path)",
            " ",
            "     def test_error(self) -> None:",
            "@@ -814,14 +838,41 @@",
            "         self.overrideEnv(\"GIT_SSH_COMMAND\", \"/path/to/ssh -o Option=Value\")",
            "         test_client = SSHGitClient(\"git.samba.org\")",
            "         self.assertEqual(test_client.ssh_command, \"/path/to/ssh -o Option=Value\")",
            " ",
            "         test_client = SSHGitClient(\"git.samba.org\", ssh_command=\"ssh -o Option1=Value1\")",
            "         self.assertEqual(test_client.ssh_command, \"ssh -o Option1=Value1\")",
            " ",
            "+    def test_ssh_command_config(self) -> None:",
            "+        # Test core.sshCommand config setting",
            "+        from dulwich.config import ConfigDict",
            "+",
            "+        # No config, no environment - should be None",
            "+        self.overrideEnv(\"GIT_SSH\", None)",
            "+        self.overrideEnv(\"GIT_SSH_COMMAND\", None)",
            "+        test_client = SSHGitClient(\"git.samba.org\")",
            "+        self.assertIsNone(test_client.ssh_command)",
            "+",
            "+        # Config with core.sshCommand",
            "+        config = ConfigDict()",
            "+        config.set((b\"core\",), b\"sshCommand\", b\"ssh -o StrictHostKeyChecking=no\")",
            "+        test_client = SSHGitClient(\"git.samba.org\", config=config)",
            "+        self.assertEqual(test_client.ssh_command, \"ssh -o StrictHostKeyChecking=no\")",
            "+",
            "+        # ssh_command parameter takes precedence over config",
            "+        test_client = SSHGitClient(",
            "+            \"git.samba.org\", config=config, ssh_command=\"custom-ssh\"",
            "+        )",
            "+        self.assertEqual(test_client.ssh_command, \"custom-ssh\")",
            "+",
            "+        # Environment variables take precedence over config when no ssh_command parameter",
            "+        self.overrideEnv(\"GIT_SSH_COMMAND\", \"/usr/bin/ssh -v\")",
            "+        test_client = SSHGitClient(\"git.samba.org\", config=config)",
            "+        self.assertEqual(test_client.ssh_command, \"/usr/bin/ssh -v\")",
            "+",
            " ",
            " class ReportStatusParserTests(TestCase):",
            "     def test_invalid_pack(self) -> None:",
            "         parser = ReportStatusParser()",
            "         parser.handle_packet(b\"unpack error - foo bar\")",
            "         parser.handle_packet(b\"ok refs/foo/bar\")",
            "         parser.handle_packet(None)",
            "@@ -944,16 +995,18 @@",
            "             self.send_and_verify(b\"master\", local, target)",
            " ",
            "     def test_get_refs(self) -> None:",
            "         local = open_repo(\"refs.git\")",
            "         self.addCleanup(tear_down_repo, local)",
            " ",
            "         client = LocalGitClient()",
            "-        refs = client.get_refs(local.path)",
            "-        self.assertDictEqual(local.refs.as_dict(), refs)",
            "+        result = client.get_refs(local.path)",
            "+        self.assertDictEqual(local.refs.as_dict(), result.refs)",
            "+        # Check that symrefs are detected correctly",
            "+        self.assertIn(b\"HEAD\", result.symrefs)",
            " ",
            "     def send_and_verify(self, branch, local, target) -> None:",
            "         \"\"\"Send branch from local to remote repository and verify it worked.\"\"\"",
            "         client = LocalGitClient()",
            "         ref_name = b\"refs/heads/\" + branch",
            "         result = client.send_pack(",
            "             target.path,",
            "@@ -1239,14 +1292,174 @@",
            "             return []",
            " ",
            "         clone_url = \"https://git.example.org/user/project.git/\"",
            "         client = HttpGitClient(clone_url, pool_manager=PoolManagerMock(), config=None)",
            "         with self.assertRaises(GitProtocolError, msg=error_msg):",
            "             client.fetch_pack(b\"/\", check_heads, None, None)",
            " ",
            "+    def test_fetch_pack_dumb_http(self) -> None:",
            "+        import zlib",
            "+",
            "+        from urllib3.response import HTTPResponse",
            "+",
            "+        # Mock responses for dumb HTTP",
            "+        info_refs_content = (",
            "+            b\"0123456789abcdef0123456789abcdef01234567\\trefs/heads/master\\n\"",
            "+        )",
            "+        head_content = b\"ref: refs/heads/master\"",
            "+",
            "+        # Create a blob object for testing",
            "+        blob_content = b\"Hello, dumb HTTP!\"",
            "+        blob_sha = b\"0123456789abcdef0123456789abcdef01234567\"",
            "+        blob_hex = blob_sha.decode(\"ascii\")",
            "+        blob_obj_data = (",
            "+            b\"blob \" + str(len(blob_content)).encode() + b\"\\x00\" + blob_content",
            "+        )",
            "+        blob_compressed = zlib.compress(blob_obj_data)",
            "+",
            "+        responses = {",
            "+            \"/HEAD\": {",
            "+                \"status\": 200,",
            "+                \"content\": head_content,",
            "+                \"content_type\": \"text/plain\",",
            "+            },",
            "+            \"/git-upload-pack\": {",
            "+                \"status\": 404,",
            "+                \"content\": b\"Not Found\",",
            "+                \"content_type\": \"text/plain\",",
            "+            },",
            "+            \"/info/refs\": {",
            "+                \"status\": 200,",
            "+                \"content\": info_refs_content,",
            "+                \"content_type\": \"text/plain\",",
            "+            },",
            "+            f\"/objects/{blob_hex[:2]}/{blob_hex[2:]}\": {",
            "+                \"status\": 200,",
            "+                \"content\": blob_compressed,",
            "+                \"content_type\": \"application/octet-stream\",",
            "+            },",
            "+        }",
            "+",
            "+        class PoolManagerMock:",
            "+            def __init__(self) -> None:",
            "+                self.headers: dict[str, str] = {}",
            "+",
            "+            def request(",
            "+                self,",
            "+                method,",
            "+                url,",
            "+                fields=None,",
            "+                headers=None,",
            "+                redirect=True,",
            "+                preload_content=True,",
            "+            ):",
            "+                # Extract path from URL",
            "+                from urllib.parse import urlparse",
            "+",
            "+                parsed = urlparse(url)",
            "+                path = parsed.path.rstrip(\"/\")",
            "+",
            "+                # Find matching response",
            "+                for pattern, resp_data in responses.items():",
            "+                    if path.endswith(pattern):",
            "+                        return HTTPResponse(",
            "+                            body=BytesIO(resp_data[\"content\"]),",
            "+                            headers={",
            "+                                \"Content-Type\": resp_data.get(",
            "+                                    \"content_type\", \"text/plain\"",
            "+                                )",
            "+                            },",
            "+                            request_method=method,",
            "+                            request_url=url,",
            "+                            preload_content=preload_content,",
            "+                            status=resp_data[\"status\"],",
            "+                        )",
            "+",
            "+                # Default 404",
            "+                return HTTPResponse(",
            "+                    body=BytesIO(b\"Not Found\"),",
            "+                    headers={\"Content-Type\": \"text/plain\"},",
            "+                    request_method=method,",
            "+                    request_url=url,",
            "+                    preload_content=preload_content,",
            "+                    status=404,",
            "+                )",
            "+",
            "+        def determine_wants(heads, **kwargs):",
            "+            # heads contains the refs with SHA values, just return the SHA we want",
            "+            return [heads[b\"refs/heads/master\"]]",
            "+",
            "+        received_data = []",
            "+",
            "+        def pack_data_handler(data):",
            "+            # Collect pack data",
            "+            received_data.append(data)",
            "+",
            "+        clone_url = \"https://git.example.org/repo.git/\"",
            "+        client = HttpGitClient(clone_url, pool_manager=PoolManagerMock(), config=None)",
            "+",
            "+        # Mock graph walker that says we don't have anything",
            "+        class MockGraphWalker:",
            "+            def ack(self, sha):",
            "+                return []",
            "+",
            "+        graph_walker = MockGraphWalker()",
            "+",
            "+        result = client.fetch_pack(",
            "+            b\"/\", determine_wants, graph_walker, pack_data_handler",
            "+        )",
            "+",
            "+        # Verify we got the refs",
            "+        expected_sha = blob_hex.encode(\"ascii\")",
            "+        self.assertEqual({b\"refs/heads/master\": expected_sha}, result.refs)",
            "+",
            "+        # Verify we received pack data",
            "+        self.assertTrue(len(received_data) > 0)",
            "+        pack_data = b\"\".join(received_data)",
            "+        self.assertTrue(len(pack_data) > 0)",
            "+",
            "+        # The pack should be valid pack format",
            "+        self.assertTrue(pack_data.startswith(b\"PACK\"))",
            "+        # Pack header: PACK + version (4 bytes) + num objects (4 bytes)",
            "+        self.assertEqual(pack_data[4:8], b\"\\x00\\x00\\x00\\x02\")  # version 2",
            "+        self.assertEqual(pack_data[8:12], b\"\\x00\\x00\\x00\\x01\")  # 1 object",
            "+",
            "+    def test_timeout_configuration(self) -> None:",
            "+        \"\"\"Test that timeout parameter is properly configured.\"\"\"",
            "+        url = \"https://github.com/jelmer/dulwich\"",
            "+        timeout = 30",
            "+",
            "+        c = HttpGitClient(url, timeout=timeout)",
            "+        self.assertEqual(c._timeout, timeout)",
            "+",
            "+    def test_timeout_from_config(self) -> None:",
            "+        \"\"\"Test that timeout can be configured via git config.\"\"\"",
            "+        from dulwich.config import ConfigDict",
            "+",
            "+        url = \"https://github.com/jelmer/dulwich\"",
            "+        config = ConfigDict()",
            "+        config.set((b\"http\",), b\"timeout\", b\"25\")",
            "+",
            "+        c = HttpGitClient(url, config=config)",
            "+        # The timeout should be set on the pool manager",
            "+        # Since we can't easily access the timeout from the pool manager,",
            "+        # we just verify the client was created successfully",
            "+        self.assertIsNotNone(c.pool_manager)",
            "+",
            "+    def test_timeout_parameter_precedence(self) -> None:",
            "+        \"\"\"Test that explicit timeout parameter takes precedence over config.\"\"\"",
            "+        from dulwich.config import ConfigDict",
            "+",
            "+        url = \"https://github.com/jelmer/dulwich\"",
            "+        config = ConfigDict()",
            "+        config.set((b\"http\",), b\"timeout\", b\"25\")",
            "+",
            "+        c = HttpGitClient(url, config=config, timeout=15)",
            "+        self.assertEqual(c._timeout, 15)",
            "+",
            " ",
            " class TCPGitClientTests(TestCase):",
            "     def test_get_url(self) -> None:",
            "         host = \"github.com\"",
            "         path = \"/jelmer/dulwich\"",
            "         c = TCPGitClient(host)",
            " ",
            "@@ -1498,14 +1711,40 @@",
            "             manager.proxy_headers, {\"proxy-authorization\": \"Basic amVsbWVyOmV4YW1wbGU=\"}",
            "         )",
            " ",
            "     def test_config_no_verify_ssl(self) -> None:",
            "         manager = default_urllib3_manager(config=None, cert_reqs=\"CERT_NONE\")",
            "         self.assertEqual(manager.connection_pool_kw[\"cert_reqs\"], \"CERT_NONE\")",
            " ",
            "+    def test_timeout_parameter(self) -> None:",
            "+        \"\"\"Test that timeout parameter is passed to urllib3 manager.\"\"\"",
            "+        timeout = 30",
            "+        manager = default_urllib3_manager(config=None, timeout=timeout)",
            "+        self.assertEqual(manager.connection_pool_kw[\"timeout\"], timeout)",
            "+",
            "+    def test_timeout_from_config(self) -> None:",
            "+        \"\"\"Test that timeout can be configured via git config.\"\"\"",
            "+        from dulwich.config import ConfigDict",
            "+",
            "+        config = ConfigDict()",
            "+        config.set((b\"http\",), b\"timeout\", b\"25\")",
            "+",
            "+        manager = default_urllib3_manager(config=config)",
            "+        self.assertEqual(manager.connection_pool_kw[\"timeout\"], 25)",
            "+",
            "+    def test_timeout_parameter_precedence(self) -> None:",
            "+        \"\"\"Test that explicit timeout parameter takes precedence over config.\"\"\"",
            "+        from dulwich.config import ConfigDict",
            "+",
            "+        config = ConfigDict()",
            "+        config.set((b\"http\",), b\"timeout\", b\"25\")",
            "+",
            "+        manager = default_urllib3_manager(config=config, timeout=15)",
            "+        self.assertEqual(manager.connection_pool_kw[\"timeout\"], 15)",
            "+",
            " ",
            " class SubprocessSSHVendorTests(TestCase):",
            "     def setUp(self) -> None:",
            "         # Monkey Patch client subprocess popen",
            "         self._orig_popen = dulwich.client.subprocess.Popen",
            "         dulwich.client.subprocess.Popen = DummyPopen",
            " ",
            "@@ -1632,16 +1871,15 @@",
            "             \"host\",",
            "             \"git-clone-url\",",
            "             password=\"12345\",",
            "             key_filename=\"/tmp/id_rsa\",",
            "         )",
            " ",
            "         expected_warning = UserWarning(",
            "-            \"Invoking PLink with a password exposes the password in the \"",
            "-            \"process list.\"",
            "+            \"Invoking PLink with a password exposes the password in the process list.\"",
            "         )",
            " ",
            "         for w in warnings_list:",
            "             if type(w) is type(expected_warning) and w.args == expected_warning.args:",
            "                 break",
            "         else:",
            "             raise AssertionError(",
            "@@ -1678,16 +1916,15 @@",
            "         self.addCleanup(warnings.resetwarnings)",
            "         warnings_list, restore_warnings = setup_warning_catcher()",
            "         self.addCleanup(restore_warnings)",
            " ",
            "         command = vendor.run_command(\"host\", \"git-clone-url\", password=\"12345\")",
            " ",
            "         expected_warning = UserWarning(",
            "-            \"Invoking PLink with a password exposes the password in the \"",
            "-            \"process list.\"",
            "+            \"Invoking PLink with a password exposes the password in the process list.\"",
            "         )",
            " ",
            "         for w in warnings_list:",
            "             if type(w) is type(expected_warning) and w.args == expected_warning.args:",
            "                 break",
            "         else:",
            "             raise AssertionError("
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_config.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_config.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_config.py",
            "@@ -19,19 +19,21 @@",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Tests for reading and writing configuration files.\"\"\"",
            " ",
            " import os",
            " import sys",
            "+import tempfile",
            " from io import BytesIO",
            " from unittest import skipIf",
            " from unittest.mock import patch",
            " ",
            " from dulwich.config import (",
            "+    CaseInsensitiveOrderedMultiDict,",
            "     ConfigDict,",
            "     ConfigFile,",
            "     StackedConfig,",
            "     _check_section_name,",
            "     _check_variable_name,",
            "     _escape_value,",
            "     _format_string,",
            "@@ -140,36 +142,51 @@",
            "     def test_from_file_section_with_open_brackets(self) -> None:",
            "         self.assertRaises(ValueError, self.from_file, b\"[core\\nfoo = bar\\n\")",
            " ",
            "     def test_from_file_value_with_open_quoted(self) -> None:",
            "         self.assertRaises(ValueError, self.from_file, b'[core]\\nfoo = \"bar\\n')",
            " ",
            "     def test_from_file_with_quotes(self) -> None:",
            "-        cf = self.from_file(b\"[core]\\n\" b'foo = \" bar\"\\n')",
            "+        cf = self.from_file(b'[core]\\nfoo = \" bar\"\\n')",
            "         self.assertEqual(b\" bar\", cf.get((b\"core\",), b\"foo\"))",
            " ",
            "     def test_from_file_with_interrupted_line(self) -> None:",
            "-        cf = self.from_file(b\"[core]\\n\" b\"foo = bar\\\\\\n\" b\" la\\n\")",
            "+        cf = self.from_file(b\"[core]\\nfoo = bar\\\\\\n la\\n\")",
            "         self.assertEqual(b\"barla\", cf.get((b\"core\",), b\"foo\"))",
            " ",
            "     def test_from_file_with_boolean_setting(self) -> None:",
            "-        cf = self.from_file(b\"[core]\\n\" b\"foo\\n\")",
            "+        cf = self.from_file(b\"[core]\\nfoo\\n\")",
            "         self.assertEqual(b\"true\", cf.get((b\"core\",), b\"foo\"))",
            " ",
            "     def test_from_file_subsection(self) -> None:",
            "         cf = self.from_file(b'[branch \"foo\"]\\nfoo = bar\\n')",
            "         self.assertEqual(b\"bar\", cf.get((b\"branch\", b\"foo\"), b\"foo\"))",
            " ",
            "     def test_from_file_subsection_invalid(self) -> None:",
            "         self.assertRaises(ValueError, self.from_file, b'[branch \"foo]\\nfoo = bar\\n')",
            " ",
            "     def test_from_file_subsection_not_quoted(self) -> None:",
            "         cf = self.from_file(b\"[branch.foo]\\nfoo = bar\\n\")",
            "         self.assertEqual(b\"bar\", cf.get((b\"branch\", b\"foo\"), b\"foo\"))",
            " ",
            "+    def test_from_file_includeif_hasconfig(self) -> None:",
            "+        \"\"\"Test parsing includeIf sections with hasconfig conditions.\"\"\"",
            "+        # Test case from issue #1216",
            "+        cf = self.from_file(",
            "+            b'[includeIf \"hasconfig:remote.*.url:ssh://org-*@github.com/**\"]\\n'",
            "+            b\"    path = ~/.config/git/.work\\n\"",
            "+        )",
            "+        self.assertEqual(",
            "+            b\"~/.config/git/.work\",",
            "+            cf.get(",
            "+                (b\"includeIf\", b\"hasconfig:remote.*.url:ssh://org-*@github.com/**\"),",
            "+                b\"path\",",
            "+            ),",
            "+        )",
            "+",
            "     def test_write_preserve_multivar(self) -> None:",
            "         cf = self.from_file(b\"[core]\\nfoo = bar\\nfoo = blah\\n\")",
            "         f = BytesIO()",
            "         cf.write_to_file(f)",
            "         self.assertEqual(b\"[core]\\n\\tfoo = bar\\n\\tfoo = blah\\n\", f.getvalue())",
            " ",
            "     def test_write_to_file_empty(self) -> None:",
            "@@ -181,14 +198,22 @@",
            "     def test_write_to_file_section(self) -> None:",
            "         c = ConfigFile()",
            "         c.set((b\"core\",), b\"foo\", b\"bar\")",
            "         f = BytesIO()",
            "         c.write_to_file(f)",
            "         self.assertEqual(b\"[core]\\n\\tfoo = bar\\n\", f.getvalue())",
            " ",
            "+    def test_write_to_file_section_multiple(self) -> None:",
            "+        c = ConfigFile()",
            "+        c.set((b\"core\",), b\"foo\", b\"old\")",
            "+        c.set((b\"core\",), b\"foo\", b\"new\")",
            "+        f = BytesIO()",
            "+        c.write_to_file(f)",
            "+        self.assertEqual(b\"[core]\\n\\tfoo = new\\n\", f.getvalue())",
            "+",
            "     def test_write_to_file_subsection(self) -> None:",
            "         c = ConfigFile()",
            "         c.set((b\"branch\", b\"blie\"), b\"foo\", b\"bar\")",
            "         f = BytesIO()",
            "         c.write_to_file(f)",
            "         self.assertEqual(b'[branch \"blie\"]\\n\\tfoo = bar\\n', f.getvalue())",
            " ",
            "@@ -202,15 +227,15 @@",
            "             b\"c = '!f() { \\\\\\r\\n\"",
            "             b' printf \\'[git commit -m \\\\\"%s\\\\\"]\\\\n\\' \\\\\"$*\\\\\" && \\\\\\r\\n'",
            "             b' git commit -m \\\\\"$*\\\\\"; \\\\\\r\\n'",
            "             b\" }; f'\\r\\n\"",
            "         )",
            "         self.assertEqual(list(cf.sections()), [(b\"alias\",)])",
            "         self.assertEqual(",
            "-            b\"'!f() { printf '[git commit -m \\\"%s\\\"]\\n' \" b'\"$*\" && git commit -m \"$*\"',",
            "+            b'\\'!f() { printf \\'[git commit -m \"%s\"]\\n\\' \"$*\" && git commit -m \"$*\"',",
            "             cf.get((b\"alias\",), b\"c\"),",
            "         )",
            " ",
            "     def test_quoted(self) -> None:",
            "         cf = self.from_file(",
            "             b\"\"\"[gui]",
            " \\tfontdiff = -family \\\\\\\"Ubuntu Mono\\\\\\\" -size 11 -overstrike 0",
            "@@ -253,14 +278,477 @@",
            "     def test_set_hash_gets_quoted(self) -> None:",
            "         c = ConfigFile()",
            "         c.set(b\"xandikos\", b\"color\", b\"#665544\")",
            "         f = BytesIO()",
            "         c.write_to_file(f)",
            "         self.assertEqual(b'[xandikos]\\n\\tcolor = \"#665544\"\\n', f.getvalue())",
            " ",
            "+    def test_windows_path_with_trailing_backslash_unquoted(self) -> None:",
            "+        \"\"\"Test that Windows paths ending with escaped backslash are handled correctly.\"\"\"",
            "+        # This reproduces the issue from https://github.com/jelmer/dulwich/issues/1088",
            "+        # A single backslash at the end should actually be a line continuation in strict Git config",
            "+        # But we want to be more tolerant like Git itself",
            "+        cf = self.from_file(",
            "+            b'[core]\\n\\trepositoryformatversion = 0\\n[remote \"origin\"]\\n\\turl = C:/Users/test\\\\\\\\\\n\\tfetch = +refs/heads/*:refs/remotes/origin/*\\n'",
            "+        )",
            "+        self.assertEqual(b\"C:/Users/test\\\\\", cf.get((b\"remote\", b\"origin\"), b\"url\"))",
            "+        self.assertEqual(",
            "+            b\"+refs/heads/*:refs/remotes/origin/*\",",
            "+            cf.get((b\"remote\", b\"origin\"), b\"fetch\"),",
            "+        )",
            "+",
            "+    def test_windows_path_with_trailing_backslash_quoted(self) -> None:",
            "+        \"\"\"Test that quoted Windows paths with escaped backslashes work correctly.\"\"\"",
            "+        cf = self.from_file(",
            "+            b'[core]\\n\\trepositoryformatversion = 0\\n[remote \"origin\"]\\n\\turl = \"C:\\\\\\\\Users\\\\\\\\test\\\\\\\\\"\\n\\tfetch = +refs/heads/*:refs/remotes/origin/*\\n'",
            "+        )",
            "+        self.assertEqual(b\"C:\\\\Users\\\\test\\\\\", cf.get((b\"remote\", b\"origin\"), b\"url\"))",
            "+        self.assertEqual(",
            "+            b\"+refs/heads/*:refs/remotes/origin/*\",",
            "+            cf.get((b\"remote\", b\"origin\"), b\"fetch\"),",
            "+        )",
            "+",
            "+    def test_single_backslash_at_line_end_shows_proper_escaping_needed(self) -> None:",
            "+        \"\"\"Test that demonstrates proper escaping is needed for single backslashes.\"\"\"",
            "+        # This test documents the current behavior: a single backslash at the end of a line",
            "+        # is treated as a line continuation per Git config spec. Users should escape backslashes.",
            "+",
            "+        # This reproduces the original issue - single backslash causes line continuation",
            "+        cf = self.from_file(",
            "+            b'[remote \"origin\"]\\n\\turl = C:/Users/test\\\\\\n\\tfetch = +refs/heads/*:refs/remotes/origin/*\\n'",
            "+        )",
            "+        # The result shows that line continuation occurred",
            "+        self.assertEqual(",
            "+            b\"C:/Users/testfetch = +refs/heads/*:refs/remotes/origin/*\",",
            "+            cf.get((b\"remote\", b\"origin\"), b\"url\"),",
            "+        )",
            "+",
            "+        # The proper way to include a literal backslash is to escape it",
            "+        cf2 = self.from_file(",
            "+            b'[remote \"origin\"]\\n\\turl = C:/Users/test\\\\\\\\\\n\\tfetch = +refs/heads/*:refs/remotes/origin/*\\n'",
            "+        )",
            "+        self.assertEqual(b\"C:/Users/test\\\\\", cf2.get((b\"remote\", b\"origin\"), b\"url\"))",
            "+        self.assertEqual(",
            "+            b\"+refs/heads/*:refs/remotes/origin/*\",",
            "+            cf2.get((b\"remote\", b\"origin\"), b\"fetch\"),",
            "+        )",
            "+",
            "+    def test_from_path_pathlib(self) -> None:",
            "+        import tempfile",
            "+        from pathlib import Path",
            "+",
            "+        # Create a temporary config file",
            "+        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".config\", delete=False) as f:",
            "+            f.write(\"[core]\\n    filemode = true\\n\")",
            "+            temp_path = f.name",
            "+",
            "+        self.addCleanup(os.unlink, temp_path)",
            "+",
            "+        # Test with pathlib.Path",
            "+        path_obj = Path(temp_path)",
            "+        cf = ConfigFile.from_path(path_obj)",
            "+        self.assertEqual(cf.get((b\"core\",), b\"filemode\"), b\"true\")",
            "+",
            "+    def test_write_to_path_pathlib(self) -> None:",
            "+        import tempfile",
            "+        from pathlib import Path",
            "+",
            "+        # Create a config",
            "+        cf = ConfigFile()",
            "+        cf.set((b\"user\",), b\"name\", b\"Test User\")",
            "+",
            "+        # Write to pathlib.Path",
            "+        with tempfile.NamedTemporaryFile(suffix=\".config\", delete=False) as f:",
            "+            temp_path = f.name",
            "+",
            "+        try:",
            "+            path_obj = Path(temp_path)",
            "+            cf.write_to_path(path_obj)",
            "+",
            "+            # Read it back",
            "+            cf2 = ConfigFile.from_path(path_obj)",
            "+            self.assertEqual(cf2.get((b\"user\",), b\"name\"), b\"Test User\")",
            "+        finally:",
            "+            # Clean up",
            "+            os.unlink(temp_path)",
            "+",
            "+    def test_include_basic(self) -> None:",
            "+        \"\"\"Test basic include functionality.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create included config file",
            "+            included_path = os.path.join(tmpdir, \"included.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(",
            "+                    b\"[user]\\n    name = Included User\\n    email = included@example.com\\n\"",
            "+                )",
            "+",
            "+            # Create main config with include",
            "+            main_config = self.from_file(",
            "+                b\"[user]\\n    name = Main User\\n[include]\\n    path = included.config\\n\"",
            "+            )",
            "+",
            "+            # Should not include anything without proper directory context",
            "+            self.assertEqual(b\"Main User\", main_config.get((b\"user\",), b\"name\"))",
            "+            with self.assertRaises(KeyError):",
            "+                main_config.get((b\"user\",), b\"email\")",
            "+",
            "+            # Now test with proper file loading",
            "+            main_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_path, \"wb\") as f:",
            "+                f.write(",
            "+                    b\"[user]\\n    name = Main User\\n[include]\\n    path = included.config\\n\"",
            "+                )",
            "+",
            "+            # Load from path to get include functionality",
            "+            cf = ConfigFile.from_path(main_path)",
            "+            self.assertEqual(b\"Included User\", cf.get((b\"user\",), b\"name\"))",
            "+            self.assertEqual(b\"included@example.com\", cf.get((b\"user\",), b\"email\"))",
            "+",
            "+    def test_include_absolute_path(self) -> None:",
            "+        \"\"\"Test include with absolute path.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Use realpath to resolve any symlinks (important on macOS and Windows)",
            "+            tmpdir = os.path.realpath(tmpdir)",
            "+",
            "+            # Create included config file",
            "+            included_path = os.path.join(tmpdir, \"included.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[core]\\n    bare = true\\n\")",
            "+",
            "+            # Create main config with absolute include path",
            "+            main_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_path, \"wb\") as f:",
            "+                # Properly escape backslashes in Windows paths",
            "+                escaped_path = included_path.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"[include]\\n    path = {escaped_path}\\n\".encode())",
            "+",
            "+            cf = ConfigFile.from_path(main_path)",
            "+            self.assertEqual(b\"true\", cf.get((b\"core\",), b\"bare\"))",
            "+",
            "+    def test_includeif_hasconfig(self) -> None:",
            "+        \"\"\"Test includeIf with hasconfig conditions.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create included config file",
            "+            work_included_path = os.path.join(tmpdir, \"work.config\")",
            "+            with open(work_included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = work@company.com\\n\")",
            "+",
            "+            personal_included_path = os.path.join(tmpdir, \"personal.config\")",
            "+            with open(personal_included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = personal@example.com\\n\")",
            "+",
            "+            # Create main config with hasconfig conditions",
            "+            main_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_path, \"wb\") as f:",
            "+                f.write(",
            "+                    b'[remote \"origin\"]\\n'",
            "+                    b\"    url = ssh://org-work@github.com/company/project\\n\"",
            "+                    b'[includeIf \"hasconfig:remote.*.url:ssh://org-*@github.com/**\"]\\n'",
            "+                    b\"    path = work.config\\n\"",
            "+                    b'[includeIf \"hasconfig:remote.*.url:https://github.com/opensource/**\"]\\n'",
            "+                    b\"    path = personal.config\\n\"",
            "+                )",
            "+",
            "+            # Load config - should match the work config due to org-work remote",
            "+            # The second condition won't match since url doesn't have /opensource/ path",
            "+            cf = ConfigFile.from_path(main_path)",
            "+            self.assertEqual(b\"work@company.com\", cf.get((b\"user\",), b\"email\"))",
            "+",
            "+    def test_includeif_hasconfig_wildcard(self) -> None:",
            "+        \"\"\"Test includeIf hasconfig with wildcard patterns.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create included config",
            "+            included_path = os.path.join(tmpdir, \"included.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    name = IncludedUser\\n\")",
            "+",
            "+            # Create main config with hasconfig condition using wildcards",
            "+            main_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_path, \"wb\") as f:",
            "+                f.write(",
            "+                    b\"[core]\\n\"",
            "+                    b\"    autocrlf = true\\n\"",
            "+                    b'[includeIf \"hasconfig:core.autocrlf:true\"]\\n'",
            "+                    b\"    path = included.config\\n\"",
            "+                )",
            "+",
            "+            # Load config - should include based on core.autocrlf value",
            "+            cf = ConfigFile.from_path(main_path)",
            "+            self.assertEqual(b\"IncludedUser\", cf.get((b\"user\",), b\"name\"))",
            "+",
            "+    def test_includeif_hasconfig_no_match(self) -> None:",
            "+        \"\"\"Test includeIf hasconfig when condition doesn't match.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create included config",
            "+            included_path = os.path.join(tmpdir, \"included.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    name = IncludedUser\\n\")",
            "+",
            "+            # Create main config with non-matching hasconfig condition",
            "+            main_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_path, \"wb\") as f:",
            "+                f.write(",
            "+                    b\"[core]\\n\"",
            "+                    b\"    autocrlf = false\\n\"",
            "+                    b'[includeIf \"hasconfig:core.autocrlf:true\"]\\n'",
            "+                    b\"    path = included.config\\n\"",
            "+                )",
            "+",
            "+            # Load config - should NOT include since condition doesn't match",
            "+            cf = ConfigFile.from_path(main_path)",
            "+            with self.assertRaises(KeyError):",
            "+                cf.get((b\"user\",), b\"name\")",
            "+",
            "+    def test_include_circular(self) -> None:",
            "+        \"\"\"Test that circular includes are handled properly.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create two configs that include each other",
            "+            config1_path = os.path.join(tmpdir, \"config1\")",
            "+            config2_path = os.path.join(tmpdir, \"config2\")",
            "+",
            "+            with open(config1_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    name = User1\\n[include]\\n    path = config2\\n\")",
            "+",
            "+            with open(config2_path, \"wb\") as f:",
            "+                f.write(",
            "+                    b\"[user]\\n    email = user2@example.com\\n[include]\\n    path = config1\\n\"",
            "+                )",
            "+",
            "+            # Should handle circular includes gracefully",
            "+            cf = ConfigFile.from_path(config1_path)",
            "+            self.assertEqual(b\"User1\", cf.get((b\"user\",), b\"name\"))",
            "+            self.assertEqual(b\"user2@example.com\", cf.get((b\"user\",), b\"email\"))",
            "+",
            "+    def test_include_missing_file(self) -> None:",
            "+        \"\"\"Test that missing include files are ignored.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create config with include of non-existent file",
            "+            config_path = os.path.join(tmpdir, \"config\")",
            "+            with open(config_path, \"wb\") as f:",
            "+                f.write(",
            "+                    b\"[user]\\n    name = TestUser\\n[include]\\n    path = missing.config\\n\"",
            "+                )",
            "+",
            "+            # Should not fail, just ignore missing include",
            "+            cf = ConfigFile.from_path(config_path)",
            "+            self.assertEqual(b\"TestUser\", cf.get((b\"user\",), b\"name\"))",
            "+",
            "+    def test_include_depth_limit(self) -> None:",
            "+        \"\"\"Test that excessive include depth is prevented.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create a chain of includes that exceeds depth limit",
            "+            for i in range(15):",
            "+                config_path = os.path.join(tmpdir, f\"config{i}\")",
            "+                with open(config_path, \"wb\") as f:",
            "+                    if i == 0:",
            "+                        f.write(b\"[user]\\n    name = User0\\n\")",
            "+                    f.write(f\"[include]\\n    path = config{i + 1}\\n\".encode())",
            "+",
            "+            # Should raise error due to depth limit",
            "+            with self.assertRaises(ValueError) as cm:",
            "+                ConfigFile.from_path(os.path.join(tmpdir, \"config0\"))",
            "+            self.assertIn(\"include depth\", str(cm.exception))",
            "+",
            "+    def test_include_with_custom_file_opener(self) -> None:",
            "+        \"\"\"Test include functionality with a custom file opener for security.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create config files",
            "+            included_path = os.path.join(tmpdir, \"included.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = custom@example.com\\n\")",
            "+",
            "+            restricted_path = os.path.join(tmpdir, \"restricted.config\")",
            "+            with open(restricted_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = restricted@example.com\\n\")",
            "+",
            "+            main_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    name = Test User\\n\")",
            "+                f.write(b\"[include]\\n    path = included.config\\n\")",
            "+                f.write(b\"[include]\\n    path = restricted.config\\n\")",
            "+",
            "+            # Define a custom file opener that restricts access",
            "+            allowed_files = {included_path, main_path}",
            "+",
            "+            def secure_file_opener(path):",
            "+                path_str = os.fspath(path)",
            "+                if path_str not in allowed_files:",
            "+                    raise PermissionError(f\"Access denied to {path}\")",
            "+                return open(path_str, \"rb\")",
            "+",
            "+            # Load config with restricted file access",
            "+            cf = ConfigFile.from_path(main_path, file_opener=secure_file_opener)",
            "+",
            "+            # Should have the main config and included config, but not restricted",
            "+            self.assertEqual(b\"Test User\", cf.get((b\"user\",), b\"name\"))",
            "+            self.assertEqual(b\"custom@example.com\", cf.get((b\"user\",), b\"email\"))",
            "+            # Email from restricted.config should not be loaded",
            "+",
            "+    def test_unknown_includeif_condition(self) -> None:",
            "+        \"\"\"Test that unknown includeIf conditions are silently ignored (like Git).\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create included config file",
            "+            included_path = os.path.join(tmpdir, \"included.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = included@example.com\\n\")",
            "+",
            "+            # Create main config with unknown includeIf condition",
            "+            main_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    name = Main User\\n\")",
            "+                f.write(",
            "+                    b'[includeIf \"unknowncondition:foo\"]\\n    path = included.config\\n'",
            "+                )",
            "+",
            "+            # Should not fail, just ignore the unknown condition",
            "+            cf = ConfigFile.from_path(main_path)",
            "+            self.assertEqual(b\"Main User\", cf.get((b\"user\",), b\"name\"))",
            "+            # Email should not be included because condition is unknown",
            "+            with self.assertRaises(KeyError):",
            "+                cf.get((b\"user\",), b\"email\")",
            "+",
            "+    def test_missing_include_file_logging(self) -> None:",
            "+        \"\"\"Test that missing include files are logged but don't cause failure.\"\"\"",
            "+        import logging",
            "+        from io import StringIO",
            "+",
            "+        # Set up logging capture",
            "+        log_capture = StringIO()",
            "+        handler = logging.StreamHandler(log_capture)",
            "+        handler.setLevel(logging.DEBUG)",
            "+        logger = logging.getLogger(\"dulwich.config\")",
            "+        logger.addHandler(handler)",
            "+        logger.setLevel(logging.DEBUG)",
            "+",
            "+        try:",
            "+            with tempfile.TemporaryDirectory() as tmpdir:",
            "+                config_path = os.path.join(tmpdir, \"test.config\")",
            "+                with open(config_path, \"wb\") as f:",
            "+                    f.write(b\"[user]\\n    name = Test User\\n\")",
            "+                    f.write(b\"[include]\\n    path = nonexistent.config\\n\")",
            "+",
            "+                # Should not fail, just log",
            "+                cf = ConfigFile.from_path(config_path)",
            "+                self.assertEqual(b\"Test User\", cf.get((b\"user\",), b\"name\"))",
            "+",
            "+                # Check that it was logged",
            "+                log_output = log_capture.getvalue()",
            "+                self.assertIn(\"Invalid include path\", log_output)",
            "+                self.assertIn(\"nonexistent.config\", log_output)",
            "+        finally:",
            "+            logger.removeHandler(handler)",
            "+",
            "+    def test_invalid_include_path_logging(self) -> None:",
            "+        \"\"\"Test that invalid include paths are logged but don't cause failure.\"\"\"",
            "+        import logging",
            "+        from io import StringIO",
            "+",
            "+        # Set up logging capture",
            "+        log_capture = StringIO()",
            "+        handler = logging.StreamHandler(log_capture)",
            "+        handler.setLevel(logging.DEBUG)",
            "+        logger = logging.getLogger(\"dulwich.config\")",
            "+        logger.addHandler(handler)",
            "+        logger.setLevel(logging.DEBUG)",
            "+",
            "+        try:",
            "+            with tempfile.TemporaryDirectory() as tmpdir:",
            "+                config_path = os.path.join(tmpdir, \"test.config\")",
            "+                with open(config_path, \"wb\") as f:",
            "+                    f.write(b\"[user]\\n    name = Test User\\n\")",
            "+                    # Use null bytes which are invalid in paths",
            "+                    f.write(b\"[include]\\n    path = /invalid\\x00path/file.config\\n\")",
            "+",
            "+                # Should not fail, just log",
            "+                cf = ConfigFile.from_path(config_path)",
            "+                self.assertEqual(b\"Test User\", cf.get((b\"user\",), b\"name\"))",
            "+",
            "+                # Check that it was logged",
            "+                log_output = log_capture.getvalue()",
            "+                self.assertIn(\"Invalid include path\", log_output)",
            "+        finally:",
            "+            logger.removeHandler(handler)",
            "+",
            "+    def test_unknown_includeif_condition_logging(self) -> None:",
            "+        \"\"\"Test that unknown includeIf conditions are logged.\"\"\"",
            "+        import logging",
            "+        from io import StringIO",
            "+",
            "+        # Set up logging capture",
            "+        log_capture = StringIO()",
            "+        handler = logging.StreamHandler(log_capture)",
            "+        handler.setLevel(logging.DEBUG)",
            "+        logger = logging.getLogger(\"dulwich.config\")",
            "+        logger.addHandler(handler)",
            "+        logger.setLevel(logging.DEBUG)",
            "+",
            "+        try:",
            "+            with tempfile.TemporaryDirectory() as tmpdir:",
            "+                config_path = os.path.join(tmpdir, \"test.config\")",
            "+                with open(config_path, \"wb\") as f:",
            "+                    f.write(b\"[user]\\n    name = Test User\\n\")",
            "+                    f.write(",
            "+                        b'[includeIf \"futurefeature:value\"]\\n    path = other.config\\n'",
            "+                    )",
            "+",
            "+                # Should not fail, just log",
            "+                cf = ConfigFile.from_path(config_path)",
            "+                self.assertEqual(b\"Test User\", cf.get((b\"user\",), b\"name\"))",
            "+",
            "+                # Check that it was logged",
            "+                log_output = log_capture.getvalue()",
            "+                self.assertIn(\"Unknown includeIf condition\", log_output)",
            "+                self.assertIn(\"futurefeature:value\", log_output)",
            "+        finally:",
            "+            logger.removeHandler(handler)",
            "+",
            "+    def test_custom_file_opener_with_include_depth(self) -> None:",
            "+        \"\"\"Test that custom file opener is passed through include chain.\"\"\"",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Use realpath to resolve any symlinks",
            "+            tmpdir = os.path.realpath(tmpdir)",
            "+",
            "+            # Create a chain of includes",
            "+            final_config = os.path.join(tmpdir, \"final.config\")",
            "+            with open(final_config, \"wb\") as f:",
            "+                f.write(b\"[feature]\\n    enabled = true\\n\")",
            "+",
            "+            middle_config = os.path.join(tmpdir, \"middle.config\")",
            "+            with open(middle_config, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = test@example.com\\n\")",
            "+                escaped_final_config = final_config.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"[include]\\n    path = {escaped_final_config}\\n\".encode())",
            "+",
            "+            main_config = os.path.join(tmpdir, \"main.config\")",
            "+            with open(main_config, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    name = Test User\\n\")",
            "+                escaped_middle_config = middle_config.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"[include]\\n    path = {escaped_middle_config}\\n\".encode())",
            "+",
            "+            # Track file access order",
            "+            access_order = []",
            "+",
            "+            def ordering_file_opener(path):",
            "+                path_str = os.fspath(path)",
            "+                access_order.append(os.path.basename(path_str))",
            "+                return open(path_str, \"rb\")",
            "+",
            "+            # Load config",
            "+            cf = ConfigFile.from_path(main_config, file_opener=ordering_file_opener)",
            "+",
            "+            # Verify all values were loaded",
            "+            self.assertEqual(b\"Test User\", cf.get((b\"user\",), b\"name\"))",
            "+            self.assertEqual(b\"test@example.com\", cf.get((b\"user\",), b\"email\"))",
            "+            self.assertEqual(b\"true\", cf.get((b\"feature\",), b\"enabled\"))",
            "+",
            "+            # Verify access order",
            "+            self.assertEqual(",
            "+                [\"main.config\", \"middle.config\", \"final.config\"], access_order",
            "+            )",
            "+",
            " ",
            " class ConfigDictTests(TestCase):",
            "     def test_get_set(self) -> None:",
            "         cd = ConfigDict()",
            "         self.assertRaises(KeyError, cd.get, b\"foo\", b\"core\")",
            "         cd.set((b\"core\",), b\"foo\", b\"bla\")",
            "         self.assertEqual(b\"bla\", cd.get((b\"core\",), b\"foo\"))",
            "@@ -302,14 +790,28 @@",
            " ",
            "     def test_sections(self) -> None:",
            "         cd = ConfigDict()",
            "         cd.set((b\"core2\",), b\"foo\", b\"bloe\")",
            " ",
            "         self.assertEqual([(b\"core2\",)], list(cd.sections()))",
            " ",
            "+    def test_set_vs_add(self) -> None:",
            "+        cd = ConfigDict()",
            "+        # Test add() creates multivars",
            "+        cd.add((b\"core\",), b\"foo\", b\"value1\")",
            "+        cd.add((b\"core\",), b\"foo\", b\"value2\")",
            "+        self.assertEqual(",
            "+            [b\"value1\", b\"value2\"], list(cd.get_multivar((b\"core\",), b\"foo\"))",
            "+        )",
            "+",
            "+        # Test set() replaces values",
            "+        cd.set((b\"core\",), b\"foo\", b\"value3\")",
            "+        self.assertEqual([b\"value3\"], list(cd.get_multivar((b\"core\",), b\"foo\")))",
            "+        self.assertEqual(b\"value3\", cd.get((b\"core\",), b\"foo\"))",
            "+",
            " ",
            " class StackedConfigTests(TestCase):",
            "     def test_default_backends(self) -> None:",
            "         StackedConfig.default_backends()",
            " ",
            "     @skipIf(sys.platform != \"win32\", \"Windows specific config location.\")",
            "     def test_windows_config_from_path(self) -> None:",
            "@@ -478,16 +980,194 @@",
            "         config.set((\"url\", \"https://samba.org/\"), \"insteadOf\", \"https://example.com/\")",
            "         self.assertEqual(",
            "             \"https://samba.org/\", apply_instead_of(config, \"https://example.com/\")",
            "         )",
            " ",
            "     def test_apply_multiple(self) -> None:",
            "         config = ConfigDict()",
            "-        config.set((\"url\", \"https://samba.org/\"), \"insteadOf\", \"https://blah.com/\")",
            "-        config.set((\"url\", \"https://samba.org/\"), \"insteadOf\", \"https://example.com/\")",
            "+        config.add((\"url\", \"https://samba.org/\"), \"insteadOf\", \"https://blah.com/\")",
            "+        config.add((\"url\", \"https://samba.org/\"), \"insteadOf\", \"https://example.com/\")",
            "         self.assertEqual(",
            "             [b\"https://blah.com/\", b\"https://example.com/\"],",
            "             list(config.get_multivar((\"url\", \"https://samba.org/\"), \"insteadOf\")),",
            "         )",
            "         self.assertEqual(",
            "             \"https://samba.org/\", apply_instead_of(config, \"https://example.com/\")",
            "         )",
            "+",
            "+    def test_apply_preserves_case_in_subsection(self) -> None:",
            "+        \"\"\"Test that mixed-case URLs (like those with access tokens) are preserved.\"\"\"",
            "+        config = ConfigDict()",
            "+        # GitHub access tokens have mixed case that must be preserved",
            "+        url_with_token = \"https://ghp_AbCdEfGhIjKlMnOpQrStUvWxYz1234567890@github.com/\"",
            "+        config.set((\"url\", url_with_token), \"insteadOf\", \"https://github.com/\")",
            "+",
            "+        # Apply the substitution",
            "+        result = apply_instead_of(config, \"https://github.com/jelmer/dulwich.git\")",
            "+        expected = \"https://ghp_AbCdEfGhIjKlMnOpQrStUvWxYz1234567890@github.com/jelmer/dulwich.git\"",
            "+        self.assertEqual(expected, result)",
            "+",
            "+        # Verify the token case is preserved",
            "+        self.assertIn(\"ghp_AbCdEfGhIjKlMnOpQrStUvWxYz1234567890\", result)",
            "+",
            "+",
            "+class CaseInsensitiveConfigTests(TestCase):",
            "+    def test_case_insensitive(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"core\",)] = \"value\"",
            "+        self.assertEqual(\"value\", config[(\"CORE\",)])",
            "+        self.assertEqual(\"value\", config[(\"CoRe\",)])",
            "+        self.assertEqual([(\"core\",)], list(config.keys()))",
            "+",
            "+    def test_multiple_set(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"core\",)] = \"value1\"",
            "+        config[(\"core\",)] = \"value2\"",
            "+        # The second set overwrites the first one",
            "+        self.assertEqual(\"value2\", config[(\"core\",)])",
            "+        self.assertEqual(\"value2\", config[(\"CORE\",)])",
            "+",
            "+    def test_get_all(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"core\",)] = \"value1\"",
            "+        config[(\"CORE\",)] = \"value2\"",
            "+        config[(\"CoRe\",)] = \"value3\"",
            "+        self.assertEqual(",
            "+            [\"value1\", \"value2\", \"value3\"], list(config.get_all((\"core\",)))",
            "+        )",
            "+        self.assertEqual(",
            "+            [\"value1\", \"value2\", \"value3\"], list(config.get_all((\"CORE\",)))",
            "+        )",
            "+",
            "+    def test_delitem(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"core\",)] = \"value1\"",
            "+        config[(\"CORE\",)] = \"value2\"",
            "+        config[(\"other\",)] = \"value3\"",
            "+        del config[(\"core\",)]",
            "+        self.assertNotIn((\"core\",), config)",
            "+        self.assertNotIn((\"CORE\",), config)",
            "+        self.assertEqual(\"value3\", config[(\"other\",)])",
            "+        self.assertEqual(1, len(config))",
            "+",
            "+    def test_len(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        self.assertEqual(0, len(config))",
            "+        config[(\"core\",)] = \"value1\"",
            "+        self.assertEqual(1, len(config))",
            "+        config[(\"CORE\",)] = \"value2\"",
            "+        self.assertEqual(1, len(config))  # Same key, case insensitive",
            "+",
            "+    def test_subsection_case_preserved(self) -> None:",
            "+        \"\"\"Test that subsection names preserve their case.\"\"\"",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        # Section names should be case-insensitive, but subsection names should preserve case",
            "+        config[(\"url\", \"https://Example.COM/Path\")] = \"value1\"",
            "+",
            "+        # Can retrieve with different case section name",
            "+        self.assertEqual(\"value1\", config[(\"URL\", \"https://Example.COM/Path\")])",
            "+        self.assertEqual(\"value1\", config[(\"url\", \"https://Example.COM/Path\")])",
            "+",
            "+        # But not with different case subsection name",
            "+        with self.assertRaises(KeyError):",
            "+            config[(\"url\", \"https://example.com/path\")]",
            "+",
            "+        # Verify the stored key preserves subsection case",
            "+        stored_keys = list(config.keys())",
            "+        self.assertEqual(1, len(stored_keys))",
            "+        self.assertEqual((\"url\", \"https://Example.COM/Path\"), stored_keys[0])",
            "+        config[(\"other\",)] = \"value3\"",
            "+        self.assertEqual(2, len(config))",
            "+",
            "+    def test_make_from_dict(self) -> None:",
            "+        original = {(\"core\",): \"value1\", (\"other\",): \"value2\"}",
            "+        config = CaseInsensitiveOrderedMultiDict.make(original)",
            "+        self.assertEqual(\"value1\", config[(\"core\",)])",
            "+        self.assertEqual(\"value1\", config[(\"CORE\",)])",
            "+        self.assertEqual(\"value2\", config[(\"other\",)])",
            "+",
            "+    def test_make_from_self(self) -> None:",
            "+        config1 = CaseInsensitiveOrderedMultiDict()",
            "+        config1[(\"core\",)] = \"value\"",
            "+        config2 = CaseInsensitiveOrderedMultiDict.make(config1)",
            "+        self.assertIs(config1, config2)",
            "+",
            "+    def test_make_invalid_type(self) -> None:",
            "+        self.assertRaises(TypeError, CaseInsensitiveOrderedMultiDict.make, \"invalid\")",
            "+",
            "+    def test_get_with_default(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"core\",)] = \"value\"",
            "+        self.assertEqual(\"value\", config.get((\"core\",)))",
            "+        self.assertEqual(\"value\", config.get((\"CORE\",)))",
            "+        self.assertEqual(\"default\", config.get((\"missing\",), \"default\"))",
            "+        # Test default_factory behavior",
            "+        config_with_factory = CaseInsensitiveOrderedMultiDict(",
            "+            default_factory=CaseInsensitiveOrderedMultiDict",
            "+        )",
            "+        result = config_with_factory.get((\"missing\",))",
            "+        self.assertIsInstance(result, CaseInsensitiveOrderedMultiDict)",
            "+        self.assertEqual(0, len(result))",
            "+",
            "+    def test_setdefault(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        # Set new value",
            "+        result1 = config.setdefault((\"core\",), \"value1\")",
            "+        self.assertEqual(\"value1\", result1)",
            "+        self.assertEqual(\"value1\", config[(\"core\",)])",
            "+        # Try to set again with different case - should return existing",
            "+        result2 = config.setdefault((\"CORE\",), \"value2\")",
            "+        self.assertEqual(\"value1\", result2)",
            "+        self.assertEqual(\"value1\", config[(\"core\",)])",
            "+",
            "+    def test_values(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"core\",)] = \"value1\"",
            "+        config[(\"other\",)] = \"value2\"",
            "+        config[(\"CORE\",)] = \"value3\"  # Overwrites previous core value",
            "+        self.assertEqual({\"value3\", \"value2\"}, set(config.values()))",
            "+",
            "+    def test_items_iteration(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"core\",)] = \"value1\"",
            "+        config[(\"other\",)] = \"value2\"",
            "+        config[(\"CORE\",)] = \"value3\"",
            "+        items = list(config.items())",
            "+        self.assertEqual(3, len(items))",
            "+        self.assertEqual(((\"core\",), \"value1\"), items[0])",
            "+        self.assertEqual(((\"other\",), \"value2\"), items[1])",
            "+        self.assertEqual(((\"CORE\",), \"value3\"), items[2])",
            "+",
            "+    def test_str_keys(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[\"core\"] = \"value\"",
            "+        self.assertEqual(\"value\", config[\"CORE\"])",
            "+        self.assertEqual(\"value\", config[\"CoRe\"])",
            "+",
            "+    def test_nested_tuple_keys(self) -> None:",
            "+        config = CaseInsensitiveOrderedMultiDict()",
            "+        config[(\"branch\", \"master\")] = \"value\"",
            "+        # Section names are case-insensitive",
            "+        self.assertEqual(\"value\", config[(\"BRANCH\", \"master\")])",
            "+        self.assertEqual(\"value\", config[(\"Branch\", \"master\")])",
            "+        # But subsection names are case-sensitive",
            "+        with self.assertRaises(KeyError):",
            "+            config[(\"branch\", \"MASTER\")]",
            "+",
            "+",
            "+class ConfigFileSetTests(TestCase):",
            "+    def test_set_replaces_value(self) -> None:",
            "+        # Test that set() replaces the value instead of appending",
            "+        cf = ConfigFile()",
            "+        cf.set((b\"core\",), b\"sshCommand\", b\"ssh -i ~/.ssh/id_rsa1\")",
            "+        cf.set((b\"core\",), b\"sshCommand\", b\"ssh -i ~/.ssh/id_rsa2\")",
            "+",
            "+        # Should only have one value",
            "+        self.assertEqual(b\"ssh -i ~/.ssh/id_rsa2\", cf.get((b\"core\",), b\"sshCommand\"))",
            "+",
            "+        # When written to file, should only have one entry",
            "+        f = BytesIO()",
            "+        cf.write_to_file(f)",
            "+        content = f.getvalue()",
            "+        self.assertEqual(1, content.count(b\"sshCommand\"))",
            "+        self.assertIn(b\"sshCommand = ssh -i ~/.ssh/id_rsa2\", content)",
            "+        self.assertNotIn(b\"id_rsa1\", content)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_graph.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_graph.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_graph.py",
            "@@ -16,15 +16,21 @@",
            " # You should have received a copy of the licenses; if not, see",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " ",
            " \"\"\"Tests for dulwich.graph.\"\"\"",
            " ",
            "-from dulwich.graph import WorkList, _find_lcas, can_fast_forward",
            "+from dulwich.graph import (",
            "+    WorkList,",
            "+    _find_lcas,",
            "+    can_fast_forward,",
            "+    find_merge_base,",
            "+    find_octopus_base,",
            "+)",
            " from dulwich.repo import MemoryRepo",
            " from dulwich.tests.utils import make_commit",
            " ",
            " from . import TestCase",
            " ",
            " ",
            " class FindMergeBaseTests(TestCase):",
            "@@ -164,14 +170,119 @@",
            "             for ca in lcas:",
            "                 res = _find_lcas(lookup_parents, cmt, [ca], lookup_stamp)",
            "                 next_lcas.extend(res)",
            "             lcas = next_lcas[:]",
            "         self.assertEqual(set(lcas), {\"2\"})",
            " ",
            " ",
            "+class FindMergeBaseFunctionTests(TestCase):",
            "+    def test_find_merge_base_empty(self) -> None:",
            "+        r = MemoryRepo()",
            "+        # Empty list of commits",
            "+        self.assertEqual([], find_merge_base(r, []))",
            "+",
            "+    def test_find_merge_base_single(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit()",
            "+        r.object_store.add_objects([(base, None)])",
            "+        # Single commit returns itself",
            "+        self.assertEqual([base.id], find_merge_base(r, [base.id]))",
            "+",
            "+    def test_find_merge_base_identical(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit()",
            "+        r.object_store.add_objects([(base, None)])",
            "+        # When the same commit is in both positions",
            "+        self.assertEqual([base.id], find_merge_base(r, [base.id, base.id]))",
            "+",
            "+    def test_find_merge_base_linear(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit()",
            "+        c1 = make_commit(parents=[base.id])",
            "+        c2 = make_commit(parents=[c1.id])",
            "+        r.object_store.add_objects([(base, None), (c1, None), (c2, None)])",
            "+        # Base of c1 and c2 is c1",
            "+        self.assertEqual([c1.id], find_merge_base(r, [c1.id, c2.id]))",
            "+        # Base of c2 and c1 is c1",
            "+        self.assertEqual([c1.id], find_merge_base(r, [c2.id, c1.id]))",
            "+",
            "+    def test_find_merge_base_diverged(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit()",
            "+        c1 = make_commit(parents=[base.id])",
            "+        c2a = make_commit(parents=[c1.id], message=b\"2a\")",
            "+        c2b = make_commit(parents=[c1.id], message=b\"2b\")",
            "+        r.object_store.add_objects([(base, None), (c1, None), (c2a, None), (c2b, None)])",
            "+        # Merge base of two diverged commits is their common parent",
            "+        self.assertEqual([c1.id], find_merge_base(r, [c2a.id, c2b.id]))",
            "+",
            "+    def test_find_merge_base_with_min_stamp(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit(commit_time=100)",
            "+        c1 = make_commit(parents=[base.id], commit_time=200)",
            "+        c2 = make_commit(parents=[c1.id], commit_time=300)",
            "+        r.object_store.add_objects([(base, None), (c1, None), (c2, None)])",
            "+",
            "+        # Normal merge base finding works",
            "+        self.assertEqual([c1.id], find_merge_base(r, [c1.id, c2.id]))",
            "+",
            "+    def test_find_merge_base_multiple_common_ancestors(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit(commit_time=100)",
            "+        c1a = make_commit(parents=[base.id], commit_time=200, message=b\"c1a\")",
            "+        c1b = make_commit(parents=[base.id], commit_time=201, message=b\"c1b\")",
            "+        c2 = make_commit(parents=[c1a.id, c1b.id], commit_time=300)",
            "+        c3 = make_commit(parents=[c1a.id, c1b.id], commit_time=301)",
            "+        r.object_store.add_objects(",
            "+            [(base, None), (c1a, None), (c1b, None), (c2, None), (c3, None)]",
            "+        )",
            "+",
            "+        # Merge base should include both c1a and c1b since both are common ancestors",
            "+        bases = find_merge_base(r, [c2.id, c3.id])",
            "+        self.assertEqual(2, len(bases))",
            "+        self.assertIn(c1a.id, bases)",
            "+        self.assertIn(c1b.id, bases)",
            "+",
            "+",
            "+class FindOctopusBaseTests(TestCase):",
            "+    def test_find_octopus_base_empty(self) -> None:",
            "+        r = MemoryRepo()",
            "+        # Empty list of commits",
            "+        self.assertEqual([], find_octopus_base(r, []))",
            "+",
            "+    def test_find_octopus_base_single(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit()",
            "+        r.object_store.add_objects([(base, None)])",
            "+        # Single commit returns itself",
            "+        self.assertEqual([base.id], find_octopus_base(r, [base.id]))",
            "+",
            "+    def test_find_octopus_base_two_commits(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit()",
            "+        c1 = make_commit(parents=[base.id])",
            "+        c2 = make_commit(parents=[c1.id])",
            "+        r.object_store.add_objects([(base, None), (c1, None), (c2, None)])",
            "+        # With two commits it should call find_merge_base",
            "+        self.assertEqual([c1.id], find_octopus_base(r, [c1.id, c2.id]))",
            "+",
            "+    def test_find_octopus_base_multiple(self) -> None:",
            "+        r = MemoryRepo()",
            "+        base = make_commit()",
            "+        c1 = make_commit(parents=[base.id])",
            "+        c2a = make_commit(parents=[c1.id], message=b\"2a\")",
            "+        c2b = make_commit(parents=[c1.id], message=b\"2b\")",
            "+        c2c = make_commit(parents=[c1.id], message=b\"2c\")",
            "+        r.object_store.add_objects(",
            "+            [(base, None), (c1, None), (c2a, None), (c2b, None), (c2c, None)]",
            "+        )",
            "+        # Common ancestor of all three branches",
            "+        self.assertEqual([c1.id], find_octopus_base(r, [c2a.id, c2b.id, c2c.id]))",
            "+",
            "+",
            " class CanFastForwardTests(TestCase):",
            "     def test_ff(self) -> None:",
            "         r = MemoryRepo()",
            "         base = make_commit()",
            "         c1 = make_commit(parents=[base.id])",
            "         c2 = make_commit(parents=[c1.id])",
            "         r.object_store.add_objects([(base, None), (c1, None), (c2, None)])",
            "@@ -188,14 +299,197 @@",
            "         c2b = make_commit(parents=[c1.id], message=b\"2b\")",
            "         r.object_store.add_objects([(base, None), (c1, None), (c2a, None), (c2b, None)])",
            "         self.assertTrue(can_fast_forward(r, c1.id, c2a.id))",
            "         self.assertTrue(can_fast_forward(r, c1.id, c2b.id))",
            "         self.assertFalse(can_fast_forward(r, c2a.id, c2b.id))",
            "         self.assertFalse(can_fast_forward(r, c2b.id, c2a.id))",
            " ",
            "+    def test_shallow_repository(self) -> None:",
            "+        r = MemoryRepo()",
            "+        # Create a shallow repository structure:",
            "+        # base (missing) -> c1 -> c2",
            "+        # We only have c1 and c2, base is missing (shallow boundary at c1)",
            "+",
            "+        # Create a fake base commit ID (won't exist in repo)",
            "+        base_sha = b\"1\" * 20  # Valid SHA format but doesn't exist (20 bytes)",
            "+",
            "+        c1 = make_commit(parents=[base_sha], commit_time=2000)",
            "+        c2 = make_commit(parents=[c1.id], commit_time=3000)",
            "+",
            "+        # Only add c1 and c2 to the repo (base is missing)",
            "+        r.object_store.add_objects([(c1, None), (c2, None)])",
            "+",
            "+        # Mark c1 as shallow using the proper API",
            "+        r.update_shallow([c1.id], [])",
            "+",
            "+        # Should be able to fast-forward from c1 to c2",
            "+        self.assertTrue(can_fast_forward(r, c1.id, c2.id))",
            "+",
            "+        # Should return False when trying to check against missing parent",
            "+        # (not raise KeyError)",
            "+        self.assertFalse(can_fast_forward(r, base_sha, c1.id))",
            "+        self.assertFalse(can_fast_forward(r, base_sha, c2.id))",
            "+",
            "+",
            "+class FindLCAsTests(TestCase):",
            "+    \"\"\"Tests for _find_lcas function with shallow repository support.\"\"\"",
            "+",
            "+    def test_find_lcas_normal(self) -> None:",
            "+        \"\"\"Test _find_lcas works normally without shallow commits.\"\"\"",
            "+        # Set up a simple repository structure:",
            "+        #   base",
            "+        #   /  \\",
            "+        #  c1  c2",
            "+        commits = {",
            "+            b\"base\": (1000, []),",
            "+            b\"c1\": (2000, [b\"base\"]),",
            "+            b\"c2\": (3000, [b\"base\"]),",
            "+        }",
            "+",
            "+        def lookup_parents(sha):",
            "+            return commits[sha][1]",
            "+",
            "+        def lookup_stamp(sha):",
            "+            return commits[sha][0]",
            "+",
            "+        # Find LCA of c1 and c2, should be base",
            "+        lcas = _find_lcas(lookup_parents, b\"c1\", [b\"c2\"], lookup_stamp)",
            "+        self.assertEqual(lcas, [b\"base\"])",
            "+",
            "+    def test_find_lcas_with_shallow_missing_c1(self) -> None:",
            "+        \"\"\"Test _find_lcas when c1 doesn't exist in shallow clone.\"\"\"",
            "+        # Only have c2 and base, c1 is missing (shallow boundary)",
            "+        commits = {",
            "+            b\"base\": (1000, []),",
            "+            b\"c2\": (3000, [b\"base\"]),",
            "+        }",
            "+        shallows = {b\"c2\"}  # c2 is at shallow boundary",
            "+",
            "+        def lookup_parents(sha):",
            "+            return commits[sha][1]",
            "+",
            "+        def lookup_stamp(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][0]",
            "+",
            "+        # c1 doesn't exist, but we have shallow commits",
            "+        lcas = _find_lcas(",
            "+            lookup_parents, b\"c1\", [b\"c2\"], lookup_stamp, shallows=shallows",
            "+        )",
            "+        # Should handle gracefully",
            "+        self.assertEqual(lcas, [])",
            "+",
            "+    def test_find_lcas_with_shallow_missing_parent(self) -> None:",
            "+        \"\"\"Test _find_lcas when parent commits are missing in shallow clone.\"\"\"",
            "+        # Have c1 and c2, but base is missing",
            "+        commits = {",
            "+            b\"c1\": (2000, [b\"base\"]),  # base doesn't exist",
            "+            b\"c2\": (3000, [b\"base\"]),  # base doesn't exist",
            "+        }",
            "+        shallows = {b\"c1\", b\"c2\"}  # Both at shallow boundary",
            "+",
            "+        def lookup_parents(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][1]",
            "+",
            "+        def lookup_stamp(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][0]",
            "+",
            "+        # Should handle missing parent gracefully",
            "+        lcas = _find_lcas(",
            "+            lookup_parents, b\"c1\", [b\"c2\"], lookup_stamp, shallows=shallows",
            "+        )",
            "+        # Can't find LCA because parents are missing",
            "+        self.assertEqual(lcas, [])",
            "+",
            "+    def test_find_lcas_with_shallow_partial_history(self) -> None:",
            "+        \"\"\"Test _find_lcas with partial history in shallow clone.\"\"\"",
            "+        # Complex structure where some history is missing:",
            "+        #      base (missing)",
            "+        #      /  \\",
            "+        #    c1    c2",
            "+        #     |     |",
            "+        #    c3    c4",
            "+        commits = {",
            "+            b\"c1\": (2000, [b\"base\"]),  # base missing",
            "+            b\"c2\": (2500, [b\"base\"]),  # base missing",
            "+            b\"c3\": (3000, [b\"c1\"]),",
            "+            b\"c4\": (3500, [b\"c2\"]),",
            "+        }",
            "+        shallows = {b\"c1\", b\"c2\"}  # c1 and c2 are at shallow boundary",
            "+",
            "+        def lookup_parents(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][1]",
            "+",
            "+        def lookup_stamp(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][0]",
            "+",
            "+        # Find LCA of c3 and c4",
            "+        lcas = _find_lcas(",
            "+            lookup_parents, b\"c3\", [b\"c4\"], lookup_stamp, shallows=shallows",
            "+        )",
            "+        # Can't determine LCA because base is missing",
            "+        self.assertEqual(lcas, [])",
            "+",
            "+    def test_find_lcas_without_shallows_raises_keyerror(self) -> None:",
            "+        \"\"\"Test _find_lcas raises KeyError when commit missing without shallows.\"\"\"",
            "+        commits = {",
            "+            b\"c2\": (3000, [b\"base\"]),",
            "+        }",
            "+",
            "+        def lookup_parents(sha):",
            "+            return commits[sha][1]",
            "+",
            "+        def lookup_stamp(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][0]",
            "+",
            "+        # Without shallows parameter, should raise KeyError",
            "+        with self.assertRaises(KeyError):",
            "+            _find_lcas(lookup_parents, b\"c1\", [b\"c2\"], lookup_stamp)",
            "+",
            "+    def test_find_lcas_octopus_with_shallow(self) -> None:",
            "+        \"\"\"Test _find_lcas with multiple commits in shallow clone.\"\"\"",
            "+        # Structure:",
            "+        #    base (missing)",
            "+        #   / | \\",
            "+        #  c1 c2 c3",
            "+        commits = {",
            "+            b\"c1\": (2000, [b\"base\"]),",
            "+            b\"c2\": (2100, [b\"base\"]),",
            "+            b\"c3\": (2200, [b\"base\"]),",
            "+        }",
            "+        shallows = {b\"c1\", b\"c2\", b\"c3\"}",
            "+",
            "+        def lookup_parents(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][1]",
            "+",
            "+        def lookup_stamp(sha):",
            "+            if sha not in commits:",
            "+                raise KeyError(sha)",
            "+            return commits[sha][0]",
            "+",
            "+        # Find LCA of c1 with c2 and c3",
            "+        lcas = _find_lcas(",
            "+            lookup_parents, b\"c1\", [b\"c2\", b\"c3\"], lookup_stamp, shallows=shallows",
            "+        )",
            "+        # Can't find LCA because base is missing",
            "+        self.assertEqual(lcas, [])",
            "+",
            " ",
            " class WorkListTest(TestCase):",
            "     def test_WorkList(self) -> None:",
            "         # tuples of (timestamp, value) are stored in a Priority MaxQueue",
            "         # repeated use of get should return them in maxheap timestamp",
            "         # order: largest time value (most recent in time) first then earlier/older",
            "         wlst = WorkList()",
            "@@ -203,7 +497,52 @@",
            "         wlst.add((50, \"Test Value 2\"))",
            "         wlst.add((200, \"Test Value 3\"))",
            "         self.assertEqual(wlst.get(), (200, \"Test Value 3\"))",
            "         self.assertEqual(wlst.get(), (100, \"Test Value 1\"))",
            "         wlst.add((150, \"Test Value 4\"))",
            "         self.assertEqual(wlst.get(), (150, \"Test Value 4\"))",
            "         self.assertEqual(wlst.get(), (50, \"Test Value 2\"))",
            "+",
            "+    def test_WorkList_iter(self) -> None:",
            "+        # Test the iter method of WorkList",
            "+        wlst = WorkList()",
            "+        wlst.add((100, \"Value 1\"))",
            "+        wlst.add((200, \"Value 2\"))",
            "+        wlst.add((50, \"Value 3\"))",
            "+",
            "+        # Collect all items from iter",
            "+        items = list(wlst.iter())",
            "+",
            "+        # Items should be in their original order, not sorted",
            "+        self.assertEqual(len(items), 3)",
            "+",
            "+        # Check the values are present with correct timestamps",
            "+        timestamps = [dt for dt, _ in items]",
            "+        values = [val for _, val in items]",
            "+",
            "+        self.assertIn(100, timestamps)",
            "+        self.assertIn(200, timestamps)",
            "+        self.assertIn(50, timestamps)",
            "+        self.assertIn(\"Value 1\", values)",
            "+        self.assertIn(\"Value 2\", values)",
            "+        self.assertIn(\"Value 3\", values)",
            "+",
            "+    def test_WorkList_empty_get(self) -> None:",
            "+        # Test getting from an empty WorkList",
            "+        wlst = WorkList()",
            "+        with self.assertRaises(IndexError):",
            "+            wlst.get()",
            "+",
            "+    def test_WorkList_empty_iter(self) -> None:",
            "+        # Test iterating over an empty WorkList",
            "+        wlst = WorkList()",
            "+        items = list(wlst.iter())",
            "+        self.assertEqual([], items)",
            "+",
            "+    def test_WorkList_empty_heap(self) -> None:",
            "+        # The current implementation raises IndexError when the heap is empty",
            "+        wlst = WorkList()",
            "+        # Ensure pq is empty",
            "+        wlst.pq = []",
            "+        # get should raise IndexError when heap is empty",
            "+        with self.assertRaises(IndexError):",
            "+            wlst.get()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_ignore.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_ignore.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_ignore.py",
            "@@ -32,14 +32,15 @@",
            "     IgnoreFilterManager,",
            "     IgnoreFilterStack,",
            "     Pattern,",
            "     match_pattern,",
            "     read_ignore_patterns,",
            "     translate,",
            " )",
            "+from dulwich.porcelain import _quote_path",
            " from dulwich.repo import Repo",
            " ",
            " from . import TestCase",
            " ",
            " POSITIVE_MATCH_TESTS = [",
            "     (b\"foo.c\", b\"*.c\"),",
            "     (b\".c\", b\"*.c\"),",
            "@@ -74,18 +75,18 @@",
            "     (b\"*.c\", b\"(?ms)(.*/)?[^/]*\\\\.c/?\\\\Z\"),",
            "     (b\"foo.c\", b\"(?ms)(.*/)?foo\\\\.c/?\\\\Z\"),",
            "     (b\"/*.c\", b\"(?ms)[^/]*\\\\.c/?\\\\Z\"),",
            "     (b\"/foo.c\", b\"(?ms)foo\\\\.c/?\\\\Z\"),",
            "     (b\"foo.c\", b\"(?ms)(.*/)?foo\\\\.c/?\\\\Z\"),",
            "     (b\"foo.[ch]\", b\"(?ms)(.*/)?foo\\\\.[ch]/?\\\\Z\"),",
            "     (b\"bar/\", b\"(?ms)(.*/)?bar\\\\/\\\\Z\"),",
            "-    (b\"foo/**\", b\"(?ms)foo(/.*)?/?\\\\Z\"),",
            "-    (b\"foo/**/blie.c\", b\"(?ms)foo(/.*)?\\\\/blie\\\\.c/?\\\\Z\"),",
            "+    (b\"foo/**\", b\"(?ms)foo/.*/?\\\\Z\"),",
            "+    (b\"foo/**/blie.c\", b\"(?ms)foo/(?:[^/]+/)*blie\\\\.c/?\\\\Z\"),",
            "     (b\"**/bla.c\", b\"(?ms)(.*/)?bla\\\\.c/?\\\\Z\"),",
            "-    (b\"foo/**/bar\", b\"(?ms)foo(/.*)?\\\\/bar/?\\\\Z\"),",
            "+    (b\"foo/**/bar\", b\"(?ms)foo/(?:[^/]+/)*bar/?\\\\Z\"),",
            "     (b\"foo/bar/*\", b\"(?ms)foo\\\\/bar\\\\/[^/]+/?\\\\Z\"),",
            "     (b\"/foo\\\\[bar\\\\]\", b\"(?ms)foo\\\\[bar\\\\]/?\\\\Z\"),",
            "     (b\"/foo[bar]\", b\"(?ms)foo[bar]/?\\\\Z\"),",
            "     (b\"/foo[0-9]\", b\"(?ms)foo[0-9]/?\\\\Z\"),",
            " ]",
            " ",
            " ",
            "@@ -188,14 +189,36 @@",
            " ",
            "     def test_regex_special(self) -> None:",
            "         # See https://github.com/dulwich/dulwich/issues/930#issuecomment-1026166429",
            "         filter = IgnoreFilter([b\"/foo\\\\[bar\\\\]\", b\"/foo\"])",
            "         self.assertTrue(filter.is_ignored(\"foo\"))",
            "         self.assertTrue(filter.is_ignored(\"foo[bar]\"))",
            " ",
            "+    def test_from_path_pathlib(self) -> None:",
            "+        import tempfile",
            "+        from pathlib import Path",
            "+",
            "+        # Create a temporary .gitignore file",
            "+        with tempfile.NamedTemporaryFile(",
            "+            mode=\"w\", suffix=\".gitignore\", delete=False",
            "+        ) as f:",
            "+            f.write(\"*.pyc\\n__pycache__/\\n\")",
            "+            temp_path = f.name",
            "+",
            "+        self.addCleanup(os.unlink, temp_path)",
            "+",
            "+        # Test with pathlib.Path",
            "+        path_obj = Path(temp_path)",
            "+        ignore_filter = IgnoreFilter.from_path(path_obj)",
            "+",
            "+        # Test that it loaded the patterns correctly",
            "+        self.assertTrue(ignore_filter.is_ignored(\"test.pyc\"))",
            "+        self.assertTrue(ignore_filter.is_ignored(\"__pycache__/\"))",
            "+        self.assertFalse(ignore_filter.is_ignored(\"test.py\"))",
            "+",
            " ",
            " class IgnoreFilterStackTests(TestCase):",
            "     def test_stack_first(self) -> None:",
            "         filter1 = IgnoreFilter([b\"[a].c\", b\"[b].c\", b\"![d].c\"])",
            "         filter2 = IgnoreFilter([b\"[a].c\", b\"![b],c\", b\"[c].c\", b\"[d].c\"])",
            "         stack = IgnoreFilterStack([filter1, filter2])",
            "         self.assertIs(True, stack.is_ignored(b\"a.c\"))",
            "@@ -275,7 +298,168 @@",
            "             f.write(b\"!a/*.txt\\n\")",
            "         m = IgnoreFilterManager.from_repo(repo)",
            "         os.mkdir(os.path.join(repo.path, \"a\"))",
            "         self.assertIs(None, m.is_ignored(\"a\"))",
            "         self.assertIs(None, m.is_ignored(\"a/\"))",
            "         self.assertFalse(m.is_ignored(\"a/b.txt\"))",
            "         self.assertTrue(m.is_ignored(\"a/c.dat\"))",
            "+",
            "+    def test_issue_1203_directory_negation(self) -> None:",
            "+        \"\"\"Test for issue #1203: gitignore patterns with directory negation.\"\"\"",
            "+        tmp_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+        repo = Repo.init(tmp_dir)",
            "+",
            "+        # Create .gitignore with the patterns from the issue",
            "+        with open(os.path.join(repo.path, \".gitignore\"), \"wb\") as f:",
            "+            f.write(b\"data/**\\n\")",
            "+            f.write(b\"!data/*/\\n\")",
            "+",
            "+        # Create directory structure",
            "+        os.makedirs(os.path.join(repo.path, \"data\", \"subdir\"))",
            "+",
            "+        m = IgnoreFilterManager.from_repo(repo)",
            "+",
            "+        # Test the expected behavior",
            "+        self.assertTrue(",
            "+            m.is_ignored(\"data/test.dvc\")",
            "+        )  # File in data/ should be ignored",
            "+        self.assertFalse(m.is_ignored(\"data/\"))  # data/ directory should not be ignored",
            "+        self.assertTrue(",
            "+            m.is_ignored(\"data/subdir/\")",
            "+        )  # Subdirectory should be ignored (matches Git behavior)",
            "+",
            "+",
            "+class QuotePathTests(TestCase):",
            "+    \"\"\"Tests for _quote_path function.\"\"\"",
            "+",
            "+    def test_ascii_paths(self) -> None:",
            "+        \"\"\"Test that ASCII paths are not quoted.\"\"\"",
            "+        self.assertEqual(_quote_path(\"file.txt\"), \"file.txt\")",
            "+        self.assertEqual(_quote_path(\"dir/file.txt\"), \"dir/file.txt\")",
            "+        self.assertEqual(_quote_path(\"path with spaces.txt\"), \"path with spaces.txt\")",
            "+",
            "+    def test_unicode_paths(self) -> None:",
            "+        \"\"\"Test that unicode paths are quoted with C-style escapes.\"\"\"",
            "+        # Russian characters",
            "+        self.assertEqual(",
            "+            _quote_path(\"тест.txt\"), '\"\\\\321\\\\202\\\\320\\\\265\\\\321\\\\201\\\\321\\\\202.txt\"'",
            "+        )",
            "+        # Chinese characters",
            "+        self.assertEqual(",
            "+            _quote_path(\"файл.测试\"),",
            "+            '\"\\\\321\\\\204\\\\320\\\\260\\\\320\\\\271\\\\320\\\\273.\\\\346\\\\265\\\\213\\\\350\\\\257\\\\225\"',",
            "+        )",
            "+        # Mixed ASCII and unicode",
            "+        self.assertEqual(",
            "+            _quote_path(\"test-тест.txt\"),",
            "+            '\"test-\\\\321\\\\202\\\\320\\\\265\\\\321\\\\201\\\\321\\\\202.txt\"',",
            "+        )",
            "+",
            "+    def test_special_characters(self) -> None:",
            "+        \"\"\"Test that special characters are properly escaped.\"\"\"",
            "+        # Quotes in filename",
            "+        self.assertEqual(",
            "+            _quote_path('file\"with\"quotes.txt'), '\"file\\\\\"with\\\\\"quotes.txt\"'",
            "+        )",
            "+        # Backslashes in filename",
            "+        self.assertEqual(",
            "+            _quote_path(\"file\\\\with\\\\backslashes.txt\"),",
            "+            '\"file\\\\\\\\with\\\\\\\\backslashes.txt\"',",
            "+        )",
            "+        # Mixed special chars and unicode",
            "+        self.assertEqual(",
            "+            _quote_path('тест\"файл.txt'),",
            "+            '\"\\\\321\\\\202\\\\320\\\\265\\\\321\\\\201\\\\321\\\\202\\\\\"\\\\321\\\\204\\\\320\\\\260\\\\320\\\\271\\\\320\\\\273.txt\"',",
            "+        )",
            "+",
            "+    def test_empty_and_edge_cases(self) -> None:",
            "+        \"\"\"Test edge cases.\"\"\"",
            "+        self.assertEqual(_quote_path(\"\"), \"\")",
            "+        self.assertEqual(_quote_path(\"a\"), \"a\")  # Single ASCII char",
            "+        self.assertEqual(_quote_path(\"я\"), '\"\\\\321\\\\217\"')  # Single unicode char",
            "+",
            "+",
            "+class CheckIgnoreQuotePathTests(TestCase):",
            "+    \"\"\"Integration tests for check_ignore with quote_path parameter.\"\"\"",
            "+",
            "+    def setUp(self) -> None:",
            "+        self.test_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, self.test_dir)",
            "+",
            "+    def test_quote_path_true_unicode_filenames(self) -> None:",
            "+        \"\"\"Test that quote_path=True returns quoted unicode filenames.\"\"\"",
            "+        from dulwich import porcelain",
            "+",
            "+        # Create a repository",
            "+        repo = Repo.init(self.test_dir)",
            "+        self.addCleanup(repo.close)",
            "+",
            "+        # Create .gitignore with unicode patterns",
            "+        gitignore_path = os.path.join(self.test_dir, \".gitignore\")",
            "+        with open(gitignore_path, \"w\", encoding=\"utf-8\") as f:",
            "+            f.write(\"тест*\\n\")",
            "+            f.write(\"*.测试\\n\")",
            "+",
            "+        # Create unicode files",
            "+        test_files = [\"тест.txt\", \"файл.测试\", \"normal.txt\"]",
            "+        for filename in test_files:",
            "+            filepath = os.path.join(self.test_dir, filename)",
            "+            with open(filepath, \"w\", encoding=\"utf-8\") as f:",
            "+                f.write(\"test content\")",
            "+",
            "+        # Test with quote_path=True (default)",
            "+        abs_paths = [os.path.join(self.test_dir, f) for f in test_files]",
            "+        ignored_quoted = set(",
            "+            porcelain.check_ignore(self.test_dir, abs_paths, quote_path=True)",
            "+        )",
            "+",
            "+        # Test with quote_path=False",
            "+        ignored_unquoted = set(",
            "+            porcelain.check_ignore(self.test_dir, abs_paths, quote_path=False)",
            "+        )",
            "+",
            "+        # Verify quoted results",
            "+        expected_quoted = {",
            "+            '\"\\\\321\\\\202\\\\320\\\\265\\\\321\\\\201\\\\321\\\\202.txt\"',  # тест.txt",
            "+            '\"\\\\321\\\\204\\\\320\\\\260\\\\320\\\\271\\\\320\\\\273.\\\\346\\\\265\\\\213\\\\350\\\\257\\\\225\"',  # файл.测试",
            "+        }",
            "+        self.assertEqual(ignored_quoted, expected_quoted)",
            "+",
            "+        # Verify unquoted results",
            "+        expected_unquoted = {\"тест.txt\", \"файл.测试\"}",
            "+        self.assertEqual(ignored_unquoted, expected_unquoted)",
            "+",
            "+    def test_quote_path_ascii_filenames(self) -> None:",
            "+        \"\"\"Test that ASCII filenames are unaffected by quote_path setting.\"\"\"",
            "+        from dulwich import porcelain",
            "+",
            "+        # Create a repository",
            "+        repo = Repo.init(self.test_dir)",
            "+        self.addCleanup(repo.close)",
            "+",
            "+        # Create .gitignore",
            "+        gitignore_path = os.path.join(self.test_dir, \".gitignore\")",
            "+        with open(gitignore_path, \"w\") as f:",
            "+            f.write(\"*.tmp\\n\")",
            "+            f.write(\"test*\\n\")",
            "+",
            "+        # Create ASCII files",
            "+        test_files = [\"test.txt\", \"file.tmp\", \"normal.txt\"]",
            "+        for filename in test_files:",
            "+            filepath = os.path.join(self.test_dir, filename)",
            "+            with open(filepath, \"w\") as f:",
            "+                f.write(\"test content\")",
            "+",
            "+        # Test both settings",
            "+        abs_paths = [os.path.join(self.test_dir, f) for f in test_files]",
            "+        ignored_quoted = set(",
            "+            porcelain.check_ignore(self.test_dir, abs_paths, quote_path=True)",
            "+        )",
            "+        ignored_unquoted = set(",
            "+            porcelain.check_ignore(self.test_dir, abs_paths, quote_path=False)",
            "+        )",
            "+",
            "+        # Both should return the same results for ASCII filenames",
            "+        expected = {\"test.txt\", \"file.tmp\"}",
            "+        self.assertEqual(ignored_quoted, expected)",
            "+        self.assertEqual(ignored_unquoted, expected)",
            "├── encoding",
            "│ @@ -1 +1 @@",
            "│ -us-ascii",
            "│ +utf-8"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_index.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_index.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_index.py",
            "@@ -29,24 +29,33 @@",
            " import tempfile",
            " from io import BytesIO",
            " ",
            " from dulwich.index import (",
            "     Index,",
            "     IndexEntry,",
            "     SerializedIndexEntry,",
            "+    _compress_path,",
            "+    _decode_varint,",
            "+    _decompress_path,",
            "+    _encode_varint,",
            "     _fs_to_tree_path,",
            "     _tree_to_fs_path,",
            "     build_index_from_tree,",
            "     cleanup_mode,",
            "     commit_tree,",
            "     get_unstaged_changes,",
            "+    index_entry_from_directory,",
            "+    index_entry_from_path,",
            "     index_entry_from_stat,",
            "+    iter_fresh_entries,",
            "     read_index,",
            "     read_index_dict,",
            "+    update_working_tree,",
            "     validate_path_element_default,",
            "+    validate_path_element_hfs,",
            "     validate_path_element_ntfs,",
            "     write_cache_time,",
            "     write_index,",
            "     write_index_dict,",
            " )",
            " from dulwich.object_store import MemoryObjectStore",
            " from dulwich.objects import S_IFGITLINK, Blob, Commit, Tree",
            "@@ -80,14 +89,17 @@",
            " class SimpleIndexTestCase(IndexTestCase):",
            "     def test_len(self) -> None:",
            "         self.assertEqual(1, len(self.get_simple_index(\"index\")))",
            " ",
            "     def test_iter(self) -> None:",
            "         self.assertEqual([b\"bla\"], list(self.get_simple_index(\"index\")))",
            " ",
            "+    def test_iter_skip_hash(self) -> None:",
            "+        self.assertEqual([b\"bla\"], list(self.get_simple_index(\"index_skip_hash\")))",
            "+",
            "     def test_iterobjects(self) -> None:",
            "         self.assertEqual(",
            "             [(b\"bla\", b\"e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\", 33188)],",
            "             list(self.get_simple_index(\"index\").iterobjects()),",
            "         )",
            " ",
            "     def test_getitem(self) -> None:",
            "@@ -98,14 +110,16 @@",
            "                 2050,",
            "                 3761020,",
            "                 33188,",
            "                 1000,",
            "                 1000,",
            "                 0,",
            "                 b\"e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\",",
            "+                0,",
            "+                0,",
            "             ),",
            "             self.get_simple_index(\"index\")[b\"bla\"],",
            "         )",
            " ",
            "     def test_empty(self) -> None:",
            "         i = self.get_simple_index(\"notanindex\")",
            "         self.assertEqual(0, len(i))",
            "@@ -115,14 +129,47 @@",
            "         i = self.get_simple_index(\"index\")",
            "         changes = list(i.changes_from_tree(MemoryObjectStore(), None))",
            "         self.assertEqual(1, len(changes))",
            "         (oldname, newname), (oldmode, newmode), (oldsha, newsha) = changes[0]",
            "         self.assertEqual(b\"bla\", newname)",
            "         self.assertEqual(b\"e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\", newsha)",
            " ",
            "+    def test_index_pathlib(self) -> None:",
            "+        import tempfile",
            "+        from pathlib import Path",
            "+",
            "+        # Create a temporary index file",
            "+        with tempfile.NamedTemporaryFile(suffix=\".index\", delete=False) as f:",
            "+            temp_path = f.name",
            "+",
            "+        self.addCleanup(os.unlink, temp_path)",
            "+",
            "+        # Test creating Index with pathlib.Path",
            "+        path_obj = Path(temp_path)",
            "+        index = Index(path_obj, read=False)",
            "+        self.assertEqual(str(path_obj), index.path)",
            "+",
            "+        # Add an entry and write",
            "+        index[b\"test\"] = IndexEntry(",
            "+            ctime=(0, 0),",
            "+            mtime=(0, 0),",
            "+            dev=0,",
            "+            ino=0,",
            "+            mode=33188,",
            "+            uid=0,",
            "+            gid=0,",
            "+            size=0,",
            "+            sha=b\"e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\",",
            "+        )",
            "+        index.write()",
            "+",
            "+        # Read it back with pathlib.Path",
            "+        index2 = Index(path_obj)",
            "+        self.assertIn(b\"test\", index2)",
            "+",
            " ",
            " class SimpleIndexWriterTestCase(IndexTestCase):",
            "     def setUp(self) -> None:",
            "         IndexTestCase.setUp(self)",
            "         self.tempdir = tempfile.mkdtemp()",
            " ",
            "     def tearDown(self) -> None:",
            "@@ -173,14 +220,16 @@",
            "                 2050,",
            "                 3761020,",
            "                 33188,",
            "                 1000,",
            "                 1000,",
            "                 0,",
            "                 b\"e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\",",
            "+                0,",
            "+                0,",
            "             )",
            "         }",
            "         filename = os.path.join(self.tempdir, \"test-simple-write-index\")",
            "         with open(filename, \"wb+\") as x:",
            "             write_index_dict(x, entries)",
            " ",
            "         with open(filename, \"rb\") as x:",
            "@@ -283,14 +332,16 @@",
            "                 64769,",
            "                 131078,",
            "                 16384,",
            "                 1000,",
            "                 1000,",
            "                 12288,",
            "                 b\"2222222222222222222222222222222222222222\",",
            "+                0,",
            "+                0,",
            "             ),",
            "         )",
            " ",
            "     def test_override_mode(self) -> None:",
            "         st = os.stat_result(",
            "             (",
            "                 stat.S_IFREG + 0o644,",
            "@@ -314,14 +365,16 @@",
            "                 64769,",
            "                 131078,",
            "                 33261,",
            "                 1000,",
            "                 1000,",
            "                 12288,",
            "                 b\"2222222222222222222222222222222222222222\",",
            "+                0,",
            "+                0,",
            "             ),",
            "         )",
            " ",
            " ",
            " class BuildIndexTests(TestCase):",
            "     def assertReasonableIndexEntry(self, index_entry, mode, filesize, sha) -> None:",
            "         self.assertEqual(index_entry.mode, mode)  # mode",
            "@@ -638,14 +691,57 @@",
            " ",
            "             # dir c",
            "             cpath = os.path.join(repo.path, \"c\")",
            "             self.assertTrue(os.path.isdir(cpath))",
            "             self.assertEqual(index[b\"c\"].mode, S_IFGITLINK)  # mode",
            "             self.assertEqual(index[b\"c\"].sha, c.id)  # sha",
            " ",
            "+    def test_with_line_ending_normalization(self) -> None:",
            "+        \"\"\"Test that build_index_from_tree applies line-ending normalization.\"\"\"",
            "+        repo_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, repo_dir)",
            "+",
            "+        from dulwich.line_ending import BlobNormalizer",
            "+",
            "+        with Repo.init(repo_dir) as repo:",
            "+            # Set up autocrlf config",
            "+            config = repo.get_config()",
            "+            config.set((b\"core\",), b\"autocrlf\", b\"true\")",
            "+            config.write_to_path()",
            "+",
            "+            # Create blob with LF line endings",
            "+            content_lf = b\"line1\\nline2\\nline3\\n\"",
            "+            blob = Blob.from_string(content_lf)",
            "+",
            "+            tree = Tree()",
            "+            tree[b\"test.txt\"] = (stat.S_IFREG | 0o644, blob.id)",
            "+",
            "+            repo.object_store.add_objects([(blob, None), (tree, None)])",
            "+",
            "+            # Create blob normalizer",
            "+            blob_normalizer = BlobNormalizer(config, {})",
            "+",
            "+            # Build index with normalization",
            "+            build_index_from_tree(",
            "+                repo.path,",
            "+                repo.index_path(),",
            "+                repo.object_store,",
            "+                tree.id,",
            "+                blob_normalizer=blob_normalizer,",
            "+            )",
            "+",
            "+            # On Windows with autocrlf=true, file should have CRLF line endings",
            "+            test_file = os.path.join(repo.path, \"test.txt\")",
            "+            with open(test_file, \"rb\") as f:",
            "+                content = f.read()",
            "+",
            "+            # autocrlf=true means LF -> CRLF on checkout (on all platforms for testing)",
            "+            expected_content = b\"line1\\r\\nline2\\r\\nline3\\r\\n\"",
            "+            self.assertEqual(content, expected_content)",
            "+",
            " ",
            " class GetUnstagedChangesTests(TestCase):",
            "     def test_get_unstaged_changes(self) -> None:",
            "         \"\"\"Unit test for get_unstaged_changes.\"\"\"",
            "         repo_dir = tempfile.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, repo_dir)",
            "         with Repo.init(repo_dir) as repo:",
            "@@ -761,26 +857,1579 @@",
            "         self.assertTrue(validate_path_element_ntfs(b\"bla\"))",
            "         self.assertTrue(validate_path_element_ntfs(b\".bla\"))",
            "         self.assertFalse(validate_path_element_ntfs(b\".git\"))",
            "         self.assertFalse(validate_path_element_ntfs(b\".giT\"))",
            "         self.assertFalse(validate_path_element_ntfs(b\"..\"))",
            "         self.assertFalse(validate_path_element_ntfs(b\"git~1\"))",
            " ",
            "+    def test_hfs(self) -> None:",
            "+        # Normal paths should pass",
            "+        self.assertTrue(validate_path_element_hfs(b\"bla\"))",
            "+        self.assertTrue(validate_path_element_hfs(b\".bla\"))",
            "+",
            "+        # Basic .git variations should fail",
            "+        self.assertFalse(validate_path_element_hfs(b\".git\"))",
            "+        self.assertFalse(validate_path_element_hfs(b\".giT\"))",
            "+        self.assertFalse(validate_path_element_hfs(b\".GIT\"))",
            "+        self.assertFalse(validate_path_element_hfs(b\"..\"))",
            "+",
            "+        # git~1 should also fail on HFS+",
            "+        self.assertFalse(validate_path_element_hfs(b\"git~1\"))",
            "+",
            "+        # Test HFS+ Unicode normalization attacks",
            "+        # .g\\u200cit (zero-width non-joiner)",
            "+        self.assertFalse(validate_path_element_hfs(b\".g\\xe2\\x80\\x8cit\"))",
            "+",
            "+        # .gi\\u200dt (zero-width joiner)",
            "+        self.assertFalse(validate_path_element_hfs(b\".gi\\xe2\\x80\\x8dt\"))",
            "+",
            "+        # Test other ignorable characters",
            "+        # .g\\ufeffit (zero-width no-break space)",
            "+        self.assertFalse(validate_path_element_hfs(b\".g\\xef\\xbb\\xbfit\"))",
            "+",
            "+        # Valid Unicode that shouldn't be confused with .git",
            "+        self.assertTrue(validate_path_element_hfs(b\".g\\xc3\\xaft\"))  # .gït",
            "+        self.assertTrue(validate_path_element_hfs(b\"git\"))  # git without dot",
            "+",
            " ",
            " class TestTreeFSPathConversion(TestCase):",
            "     def test_tree_to_fs_path(self) -> None:",
            "         tree_path = \"délwíçh/foo\".encode()",
            "         fs_path = _tree_to_fs_path(b\"/prefix/path\", tree_path)",
            "         self.assertEqual(",
            "             fs_path,",
            "             os.fsencode(os.path.join(\"/prefix/path\", \"délwíçh\", \"foo\")),",
            "         )",
            " ",
            "+    def test_tree_to_fs_path_windows_separator(self) -> None:",
            "+        tree_path = b\"path/with/slash\"",
            "+        original_sep = os.sep.encode(\"ascii\")",
            "+        # Temporarily modify os_sep_bytes to test Windows path conversion",
            "+        # This simulates Windows behavior on all platforms for testing",
            "+        import dulwich.index",
            "+",
            "+        dulwich.index.os_sep_bytes = b\"\\\\\"",
            "+        self.addCleanup(setattr, dulwich.index, \"os_sep_bytes\", original_sep)",
            "+",
            "+        fs_path = _tree_to_fs_path(b\"/prefix/path\", tree_path)",
            "+",
            "+        # The function should join the prefix path with the converted tree path",
            "+        # The expected behavior is that the path separators in the tree_path are",
            "+        # converted to the platform-specific separator (which we've set to backslash)",
            "+        expected_path = os.path.join(b\"/prefix/path\", b\"path\\\\with\\\\slash\")",
            "+        self.assertEqual(fs_path, expected_path)",
            "+",
            "     def test_fs_to_tree_path_str(self) -> None:",
            "         fs_path = os.path.join(os.path.join(\"délwíçh\", \"foo\"))",
            "         tree_path = _fs_to_tree_path(fs_path)",
            "         self.assertEqual(tree_path, \"délwíçh/foo\".encode())",
            " ",
            "     def test_fs_to_tree_path_bytes(self) -> None:",
            "         fs_path = os.path.join(os.fsencode(os.path.join(\"délwíçh\", \"foo\")))",
            "         tree_path = _fs_to_tree_path(fs_path)",
            "         self.assertEqual(tree_path, \"délwíçh/foo\".encode())",
            "+",
            "+    def test_fs_to_tree_path_windows_separator(self) -> None:",
            "+        # Test conversion of Windows paths to tree paths",
            "+        fs_path = b\"path\\\\with\\\\backslash\"",
            "+        original_sep = os.sep.encode(\"ascii\")",
            "+        # Temporarily modify os_sep_bytes to test Windows path conversion",
            "+        import dulwich.index",
            "+",
            "+        dulwich.index.os_sep_bytes = b\"\\\\\"",
            "+        self.addCleanup(setattr, dulwich.index, \"os_sep_bytes\", original_sep)",
            "+",
            "+        tree_path = _fs_to_tree_path(fs_path)",
            "+        self.assertEqual(tree_path, b\"path/with/backslash\")",
            "+",
            "+",
            "+class TestIndexEntryFromPath(TestCase):",
            "+    def setUp(self):",
            "+        self.tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, self.tempdir)",
            "+",
            "+    def test_index_entry_from_path_file(self) -> None:",
            "+        \"\"\"Test creating index entry from a regular file.\"\"\"",
            "+        # Create a test file",
            "+        test_file = os.path.join(self.tempdir, \"testfile\")",
            "+        with open(test_file, \"wb\") as f:",
            "+            f.write(b\"test content\")",
            "+",
            "+        # Get the index entry",
            "+        entry = index_entry_from_path(os.fsencode(test_file))",
            "+",
            "+        # Verify the entry was created with the right mode",
            "+        self.assertIsNotNone(entry)",
            "+        self.assertEqual(cleanup_mode(os.stat(test_file).st_mode), entry.mode)",
            "+",
            "+    @skipIf(not can_symlink(), \"Requires symlink support\")",
            "+    def test_index_entry_from_path_symlink(self) -> None:",
            "+        \"\"\"Test creating index entry from a symlink.\"\"\"",
            "+        # Create a target file",
            "+        target_file = os.path.join(self.tempdir, \"target\")",
            "+        with open(target_file, \"wb\") as f:",
            "+            f.write(b\"target content\")",
            "+",
            "+        # Create a symlink",
            "+        link_file = os.path.join(self.tempdir, \"symlink\")",
            "+        os.symlink(target_file, link_file)",
            "+",
            "+        # Get the index entry",
            "+        entry = index_entry_from_path(os.fsencode(link_file))",
            "+",
            "+        # Verify the entry was created with the right mode",
            "+        self.assertIsNotNone(entry)",
            "+        self.assertEqual(cleanup_mode(os.lstat(link_file).st_mode), entry.mode)",
            "+",
            "+    def test_index_entry_from_path_directory(self) -> None:",
            "+        \"\"\"Test creating index entry from a directory (should return None).\"\"\"",
            "+        # Create a directory",
            "+        test_dir = os.path.join(self.tempdir, \"testdir\")",
            "+        os.mkdir(test_dir)",
            "+",
            "+        # Get the index entry for a directory",
            "+        entry = index_entry_from_path(os.fsencode(test_dir))",
            "+",
            "+        # Should return None for regular directories",
            "+        self.assertIsNone(entry)",
            "+",
            "+    def test_index_entry_from_directory_regular(self) -> None:",
            "+        \"\"\"Test index_entry_from_directory with a regular directory.\"\"\"",
            "+        # Create a directory",
            "+        test_dir = os.path.join(self.tempdir, \"testdir\")",
            "+        os.mkdir(test_dir)",
            "+",
            "+        # Get stat for the directory",
            "+        st = os.lstat(test_dir)",
            "+",
            "+        # Get the index entry for a regular directory",
            "+        entry = index_entry_from_directory(st, os.fsencode(test_dir))",
            "+",
            "+        # Should return None for regular directories",
            "+        self.assertIsNone(entry)",
            "+",
            "+    def test_index_entry_from_directory_git_submodule(self) -> None:",
            "+        \"\"\"Test index_entry_from_directory with a Git submodule.\"\"\"",
            "+        # Create a git repository that will be a submodule",
            "+        sub_repo_dir = os.path.join(self.tempdir, \"subrepo\")",
            "+        os.mkdir(sub_repo_dir)",
            "+",
            "+        # Create the .git directory to make it look like a git repo",
            "+        git_dir = os.path.join(sub_repo_dir, \".git\")",
            "+        os.mkdir(git_dir)",
            "+",
            "+        # Create HEAD file with a fake commit SHA",
            "+        head_sha = b\"1234567890\" * 4  # 40-char fake SHA",
            "+        with open(os.path.join(git_dir, \"HEAD\"), \"wb\") as f:",
            "+            f.write(head_sha)",
            "+",
            "+        # Get stat for the submodule directory",
            "+        st = os.lstat(sub_repo_dir)",
            "+",
            "+        # Get the index entry for a git submodule directory",
            "+        entry = index_entry_from_directory(st, os.fsencode(sub_repo_dir))",
            "+",
            "+        # Since we don't have a proper git setup, this might still return None",
            "+        # This test just ensures the code path is executed",
            "+        if entry is not None:",
            "+            # If an entry is returned, it should have the gitlink mode",
            "+            self.assertEqual(entry.mode, S_IFGITLINK)",
            "+",
            "+    def test_index_entry_from_path_with_object_store(self) -> None:",
            "+        \"\"\"Test creating index entry with object store.\"\"\"",
            "+        # Create a test file",
            "+        test_file = os.path.join(self.tempdir, \"testfile\")",
            "+        with open(test_file, \"wb\") as f:",
            "+            f.write(b\"test content\")",
            "+",
            "+        # Create a memory object store",
            "+        object_store = MemoryObjectStore()",
            "+",
            "+        # Get the index entry and add to object store",
            "+        entry = index_entry_from_path(os.fsencode(test_file), object_store)",
            "+",
            "+        # Verify we can access the blob from the object store",
            "+        self.assertIsNotNone(entry)",
            "+        blob = object_store[entry.sha]",
            "+        self.assertEqual(b\"test content\", blob.data)",
            "+",
            "+    def test_iter_fresh_entries(self) -> None:",
            "+        \"\"\"Test iterating over fresh entries.\"\"\"",
            "+        # Create some test files",
            "+        file1 = os.path.join(self.tempdir, \"file1\")",
            "+        with open(file1, \"wb\") as f:",
            "+            f.write(b\"file1 content\")",
            "+",
            "+        file2 = os.path.join(self.tempdir, \"file2\")",
            "+        with open(file2, \"wb\") as f:",
            "+            f.write(b\"file2 content\")",
            "+",
            "+        # Create a memory object store",
            "+        object_store = MemoryObjectStore()",
            "+",
            "+        # Get fresh entries",
            "+        paths = [b\"file1\", b\"file2\", b\"nonexistent\"]",
            "+        entries = dict(",
            "+            iter_fresh_entries(paths, os.fsencode(self.tempdir), object_store)",
            "+        )",
            "+",
            "+        # Verify both files got entries but nonexistent file is None",
            "+        self.assertIn(b\"file1\", entries)",
            "+        self.assertIn(b\"file2\", entries)",
            "+        self.assertIn(b\"nonexistent\", entries)",
            "+        self.assertIsNotNone(entries[b\"file1\"])",
            "+        self.assertIsNotNone(entries[b\"file2\"])",
            "+        self.assertIsNone(entries[b\"nonexistent\"])",
            "+",
            "+        # Check that blobs were added to object store",
            "+        blob1 = object_store[entries[b\"file1\"].sha]",
            "+        self.assertEqual(b\"file1 content\", blob1.data)",
            "+",
            "+        blob2 = object_store[entries[b\"file2\"].sha]",
            "+        self.assertEqual(b\"file2 content\", blob2.data)",
            "+",
            "+    def test_read_submodule_head(self) -> None:",
            "+        \"\"\"Test reading the HEAD of a submodule.\"\"\"",
            "+        from dulwich.index import read_submodule_head",
            "+",
            "+        # Create a test repo that will be our \"submodule\"",
            "+        sub_repo_dir = os.path.join(self.tempdir, \"subrepo\")",
            "+        os.mkdir(sub_repo_dir)",
            "+        submodule_repo = Repo.init(sub_repo_dir)",
            "+",
            "+        # Create a file and commit it to establish a HEAD",
            "+        test_file = os.path.join(sub_repo_dir, \"testfile\")",
            "+        with open(test_file, \"wb\") as f:",
            "+            f.write(b\"test content\")",
            "+",
            "+        submodule_repo.stage([\"testfile\"])",
            "+        commit_id = submodule_repo.do_commit(b\"Test commit for submodule\")",
            "+",
            "+        # Test reading the HEAD",
            "+        head_sha = read_submodule_head(sub_repo_dir)",
            "+        self.assertEqual(commit_id, head_sha)",
            "+",
            "+        # Test with bytes path",
            "+        head_sha_bytes = read_submodule_head(os.fsencode(sub_repo_dir))",
            "+        self.assertEqual(commit_id, head_sha_bytes)",
            "+",
            "+        # Test with non-existent path",
            "+        non_repo_dir = os.path.join(self.tempdir, \"nonrepo\")",
            "+        os.mkdir(non_repo_dir)",
            "+        self.assertIsNone(read_submodule_head(non_repo_dir))",
            "+",
            "+        # Test with path that doesn't have a .git directory",
            "+        not_git_dir = os.path.join(self.tempdir, \"notgit\")",
            "+        os.mkdir(not_git_dir)",
            "+        self.assertIsNone(read_submodule_head(not_git_dir))",
            "+",
            "+    def test_has_directory_changed(self) -> None:",
            "+        \"\"\"Test checking if a directory has changed.\"\"\"",
            "+        from dulwich.index import IndexEntry, _has_directory_changed",
            "+",
            "+        # Setup mock IndexEntry",
            "+        mock_entry = IndexEntry(",
            "+            (1230680220, 0),",
            "+            (1230680220, 0),",
            "+            2050,",
            "+            3761020,",
            "+            33188,",
            "+            1000,",
            "+            1000,",
            "+            0,",
            "+            b\"e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\",",
            "+            0,",
            "+            0,",
            "+        )",
            "+",
            "+        # Test with a regular directory (not a submodule)",
            "+        reg_dir = os.path.join(self.tempdir, \"regular_dir\")",
            "+        os.mkdir(reg_dir)",
            "+",
            "+        # Should return True for regular directory",
            "+        self.assertTrue(_has_directory_changed(os.fsencode(reg_dir), mock_entry))",
            "+",
            "+        # Create a git repository to test submodule scenarios",
            "+        sub_repo_dir = os.path.join(self.tempdir, \"subrepo\")",
            "+        os.mkdir(sub_repo_dir)",
            "+        submodule_repo = Repo.init(sub_repo_dir)",
            "+",
            "+        # Create a file and commit it to establish a HEAD",
            "+        test_file = os.path.join(sub_repo_dir, \"testfile\")",
            "+        with open(test_file, \"wb\") as f:",
            "+            f.write(b\"test content\")",
            "+",
            "+        submodule_repo.stage([\"testfile\"])",
            "+        commit_id = submodule_repo.do_commit(b\"Test commit for submodule\")",
            "+",
            "+        # Create an entry with the correct commit SHA",
            "+        correct_entry = IndexEntry(",
            "+            (1230680220, 0),",
            "+            (1230680220, 0),",
            "+            2050,",
            "+            3761020,",
            "+            33188,",
            "+            1000,",
            "+            1000,",
            "+            0,",
            "+            commit_id,",
            "+            0,",
            "+            0,",
            "+        )",
            "+",
            "+        # Create an entry with an incorrect commit SHA",
            "+        incorrect_entry = IndexEntry(",
            "+            (1230680220, 0),",
            "+            (1230680220, 0),",
            "+            2050,",
            "+            3761020,",
            "+            33188,",
            "+            1000,",
            "+            1000,",
            "+            0,",
            "+            b\"0000000000000000000000000000000000000000\",",
            "+            0,",
            "+            0,",
            "+        )",
            "+",
            "+        # Should return False for submodule with correct SHA",
            "+        self.assertFalse(",
            "+            _has_directory_changed(os.fsencode(sub_repo_dir), correct_entry)",
            "+        )",
            "+",
            "+        # Should return True for submodule with incorrect SHA",
            "+        self.assertTrue(",
            "+            _has_directory_changed(os.fsencode(sub_repo_dir), incorrect_entry)",
            "+        )",
            "+",
            "+    def test_get_unstaged_changes(self) -> None:",
            "+        \"\"\"Test detecting unstaged changes in a working tree.\"\"\"",
            "+        from dulwich.index import (",
            "+            ConflictedIndexEntry,",
            "+            Index,",
            "+            IndexEntry,",
            "+            get_unstaged_changes,",
            "+        )",
            "+",
            "+        # Create a test repo",
            "+        repo_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, repo_dir)",
            "+",
            "+        # Create test index",
            "+        index = Index(os.path.join(repo_dir, \"index\"))",
            "+",
            "+        # Create an actual hash of our test content",
            "+        from dulwich.objects import Blob",
            "+",
            "+        test_blob = Blob()",
            "+        test_blob.data = b\"initial content\"",
            "+",
            "+        # Create some test files with known contents",
            "+        file1_path = os.path.join(repo_dir, \"file1\")",
            "+        with open(file1_path, \"wb\") as f:",
            "+            f.write(b\"initial content\")",
            "+",
            "+        file2_path = os.path.join(repo_dir, \"file2\")",
            "+        with open(file2_path, \"wb\") as f:",
            "+            f.write(b\"initial content\")",
            "+",
            "+        # Add them to index",
            "+        entry1 = IndexEntry(",
            "+            (1230680220, 0),",
            "+            (1230680220, 0),",
            "+            2050,",
            "+            3761020,",
            "+            33188,",
            "+            1000,",
            "+            1000,",
            "+            0,",
            "+            b\"e69de29bb2d1d6434b8b29ae775ad8c2e48c5391\",  # Not matching actual content",
            "+            0,",
            "+            0,",
            "+        )",
            "+",
            "+        entry2 = IndexEntry(",
            "+            (1230680220, 0),",
            "+            (1230680220, 0),",
            "+            2050,",
            "+            3761020,",
            "+            33188,",
            "+            1000,",
            "+            1000,",
            "+            0,",
            "+            test_blob.id,  # Will be content's real hash",
            "+            0,",
            "+            0,",
            "+        )",
            "+",
            "+        # Add a file that has a conflict",
            "+        entry_conflict = ConflictedIndexEntry(b\"conflict\", {0: None, 1: None, 2: None})",
            "+",
            "+        index._byname = {",
            "+            b\"file1\": entry1,",
            "+            b\"file2\": entry2,",
            "+            b\"file3\": IndexEntry(",
            "+                (1230680220, 0),",
            "+                (1230680220, 0),",
            "+                2050,",
            "+                3761020,",
            "+                33188,",
            "+                1000,",
            "+                1000,",
            "+                0,",
            "+                b\"0000000000000000000000000000000000000000\",",
            "+                0,",
            "+                0,",
            "+            ),",
            "+            b\"conflict\": entry_conflict,",
            "+        }",
            "+",
            "+        # Get unstaged changes",
            "+        changes = list(get_unstaged_changes(index, repo_dir))",
            "+",
            "+        # File1 should be unstaged (content doesn't match hash)",
            "+        # File3 doesn't exist (deleted)",
            "+        # Conflict is always unstaged",
            "+        self.assertEqual(sorted(changes), [b\"conflict\", b\"file1\", b\"file3\"])",
            "+",
            "+        # Create directory where there should be a file",
            "+        os.mkdir(os.path.join(repo_dir, \"file4\"))",
            "+        index._byname[b\"file4\"] = entry1",
            "+",
            "+        # Get unstaged changes again",
            "+        changes = list(get_unstaged_changes(index, repo_dir))",
            "+",
            "+        # Now file4 should also be unstaged because it's a directory instead of a file",
            "+        self.assertEqual(sorted(changes), [b\"conflict\", b\"file1\", b\"file3\", b\"file4\"])",
            "+",
            "+        # Create a custom blob filter function",
            "+        def filter_blob_callback(blob, path):",
            "+            # Modify blob to make it look changed",
            "+            blob.data = b\"modified \" + blob.data",
            "+            return blob",
            "+",
            "+        # Get unstaged changes with blob filter",
            "+        changes = list(get_unstaged_changes(index, repo_dir, filter_blob_callback))",
            "+",
            "+        # Now both file1 and file2 should be unstaged due to the filter",
            "+        self.assertEqual(",
            "+            sorted(changes), [b\"conflict\", b\"file1\", b\"file2\", b\"file3\", b\"file4\"]",
            "+        )",
            "+",
            "+",
            "+class TestManyFilesFeature(TestCase):",
            "+    \"\"\"Tests for the manyFiles feature (index version 4 and skipHash).\"\"\"",
            "+",
            "+    def setUp(self):",
            "+        self.tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, self.tempdir)",
            "+",
            "+    def test_index_version_4_parsing(self):",
            "+        \"\"\"Test that index version 4 files can be parsed.\"\"\"",
            "+        index_path = os.path.join(self.tempdir, \"index\")",
            "+",
            "+        # Create an index with version 4",
            "+        index = Index(index_path, read=False, version=4)",
            "+",
            "+        # Add some entries",
            "+        entry = IndexEntry(",
            "+            ctime=(1234567890, 0),",
            "+            mtime=(1234567890, 0),",
            "+            dev=1,",
            "+            ino=1,",
            "+            mode=0o100644,",
            "+            uid=1000,",
            "+            gid=1000,",
            "+            size=5,",
            "+            sha=b\"0\" * 40,",
            "+        )",
            "+        index[b\"test.txt\"] = entry",
            "+",
            "+        # Write and read back",
            "+        index.write()",
            "+",
            "+        # Read the index back",
            "+        index2 = Index(index_path)",
            "+        self.assertEqual(index2._version, 4)",
            "+        self.assertIn(b\"test.txt\", index2)",
            "+",
            "+    def test_skip_hash_feature(self):",
            "+        \"\"\"Test that skipHash feature works correctly.\"\"\"",
            "+        index_path = os.path.join(self.tempdir, \"index\")",
            "+",
            "+        # Create an index with skipHash enabled",
            "+        index = Index(index_path, read=False, skip_hash=True)",
            "+",
            "+        # Add some entries",
            "+        entry = IndexEntry(",
            "+            ctime=(1234567890, 0),",
            "+            mtime=(1234567890, 0),",
            "+            dev=1,",
            "+            ino=1,",
            "+            mode=0o100644,",
            "+            uid=1000,",
            "+            gid=1000,",
            "+            size=5,",
            "+            sha=b\"0\" * 40,",
            "+        )",
            "+        index[b\"test.txt\"] = entry",
            "+",
            "+        # Write the index",
            "+        index.write()",
            "+",
            "+        # Verify the file was written with zero hash",
            "+        with open(index_path, \"rb\") as f:",
            "+            f.seek(-20, 2)  # Seek to last 20 bytes",
            "+            trailing_hash = f.read(20)",
            "+            self.assertEqual(trailing_hash, b\"\\x00\" * 20)",
            "+",
            "+        # Verify we can still read it back",
            "+        index2 = Index(index_path)",
            "+        self.assertIn(b\"test.txt\", index2)",
            "+",
            "+    def test_version_4_no_padding(self):",
            "+        \"\"\"Test that version 4 entries have no padding.\"\"\"",
            "+        # Create entries with names that would show compression benefits",
            "+        entries = [",
            "+            SerializedIndexEntry(",
            "+                name=b\"src/main/java/com/example/Service.java\",",
            "+                ctime=(1234567890, 0),",
            "+                mtime=(1234567890, 0),",
            "+                dev=1,",
            "+                ino=1,",
            "+                mode=0o100644,",
            "+                uid=1000,",
            "+                gid=1000,",
            "+                size=5,",
            "+                sha=b\"0\" * 40,",
            "+                flags=0,",
            "+                extended_flags=0,",
            "+            ),",
            "+            SerializedIndexEntry(",
            "+                name=b\"src/main/java/com/example/Controller.java\",",
            "+                ctime=(1234567890, 0),",
            "+                mtime=(1234567890, 0),",
            "+                dev=1,",
            "+                ino=2,",
            "+                mode=0o100644,",
            "+                uid=1000,",
            "+                gid=1000,",
            "+                size=5,",
            "+                sha=b\"1\" * 40,",
            "+                flags=0,",
            "+                extended_flags=0,",
            "+            ),",
            "+        ]",
            "+",
            "+        # Test version 2 (with padding, full paths)",
            "+        buf_v2 = BytesIO()",
            "+        from dulwich.index import write_cache_entry",
            "+",
            "+        previous_path = b\"\"",
            "+        for entry in entries:",
            "+            # Set proper flags for v2",
            "+            entry_v2 = SerializedIndexEntry(",
            "+                entry.name,",
            "+                entry.ctime,",
            "+                entry.mtime,",
            "+                entry.dev,",
            "+                entry.ino,",
            "+                entry.mode,",
            "+                entry.uid,",
            "+                entry.gid,",
            "+                entry.size,",
            "+                entry.sha,",
            "+                len(entry.name),",
            "+                entry.extended_flags,",
            "+            )",
            "+            write_cache_entry(buf_v2, entry_v2, version=2, previous_path=previous_path)",
            "+            previous_path = entry.name",
            "+        v2_data = buf_v2.getvalue()",
            "+",
            "+        # Test version 4 (path compression, no padding)",
            "+        buf_v4 = BytesIO()",
            "+        previous_path = b\"\"",
            "+        for entry in entries:",
            "+            write_cache_entry(buf_v4, entry, version=4, previous_path=previous_path)",
            "+            previous_path = entry.name",
            "+        v4_data = buf_v4.getvalue()",
            "+",
            "+        # Version 4 should be shorter due to compression and no padding",
            "+        self.assertLess(len(v4_data), len(v2_data))",
            "+",
            "+        # Both should parse correctly",
            "+        buf_v2.seek(0)",
            "+        from dulwich.index import read_cache_entry",
            "+",
            "+        previous_path = b\"\"",
            "+        parsed_v2_entries = []",
            "+        for _ in entries:",
            "+            parsed = read_cache_entry(buf_v2, version=2, previous_path=previous_path)",
            "+            parsed_v2_entries.append(parsed)",
            "+            previous_path = parsed.name",
            "+",
            "+        buf_v4.seek(0)",
            "+        previous_path = b\"\"",
            "+        parsed_v4_entries = []",
            "+        for _ in entries:",
            "+            parsed = read_cache_entry(buf_v4, version=4, previous_path=previous_path)",
            "+            parsed_v4_entries.append(parsed)",
            "+            previous_path = parsed.name",
            "+",
            "+        # Both should have the same paths",
            "+        for v2_entry, v4_entry in zip(parsed_v2_entries, parsed_v4_entries):",
            "+            self.assertEqual(v2_entry.name, v4_entry.name)",
            "+            self.assertEqual(v2_entry.sha, v4_entry.sha)",
            "+",
            "+",
            "+class TestManyFilesRepoIntegration(TestCase):",
            "+    \"\"\"Tests for manyFiles feature integration with Repo.\"\"\"",
            "+",
            "+    def setUp(self):",
            "+        self.tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, self.tempdir)",
            "+",
            "+    def test_repo_with_manyfiles_config(self):",
            "+        \"\"\"Test that a repository with feature.manyFiles=true uses the right settings.\"\"\"",
            "+        # Create a new repository",
            "+        repo = Repo.init(self.tempdir)",
            "+",
            "+        # Set feature.manyFiles=true in config",
            "+        config = repo.get_config()",
            "+        config.set(b\"feature\", b\"manyFiles\", b\"true\")",
            "+        config.write_to_path()",
            "+",
            "+        # Open the index - should have skipHash enabled and version 4",
            "+        index = repo.open_index()",
            "+        self.assertTrue(index._skip_hash)",
            "+        self.assertEqual(index._version, 4)",
            "+",
            "+    def test_repo_with_explicit_index_settings(self):",
            "+        \"\"\"Test that explicit index.version and index.skipHash work.\"\"\"",
            "+        # Create a new repository",
            "+        repo = Repo.init(self.tempdir)",
            "+",
            "+        # Set explicit index settings",
            "+        config = repo.get_config()",
            "+        config.set(b\"index\", b\"version\", b\"3\")",
            "+        config.set(b\"index\", b\"skipHash\", b\"false\")",
            "+        config.write_to_path()",
            "+",
            "+        # Open the index - should respect explicit settings",
            "+        index = repo.open_index()",
            "+        self.assertFalse(index._skip_hash)",
            "+        self.assertEqual(index._version, 3)",
            "+",
            "+",
            "+class TestPathPrefixCompression(TestCase):",
            "+    \"\"\"Tests for index version 4 path prefix compression.\"\"\"",
            "+",
            "+    def setUp(self):",
            "+        self.tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, self.tempdir)",
            "+",
            "+    def test_varint_encoding_decoding(self):",
            "+        \"\"\"Test variable-width integer encoding and decoding.\"\"\"",
            "+        test_values = [0, 1, 127, 128, 255, 256, 16383, 16384, 65535, 65536]",
            "+",
            "+        for value in test_values:",
            "+            encoded = _encode_varint(value)",
            "+            decoded, _ = _decode_varint(encoded, 0)",
            "+            self.assertEqual(value, decoded, f\"Failed for value {value}\")",
            "+",
            "+    def test_path_compression_simple(self):",
            "+        \"\"\"Test simple path compression cases.\"\"\"",
            "+        # Test case 1: No common prefix",
            "+        compressed = _compress_path(b\"file1.txt\", b\"\")",
            "+        decompressed, _ = _decompress_path(compressed, 0, b\"\")",
            "+        self.assertEqual(b\"file1.txt\", decompressed)",
            "+",
            "+        # Test case 2: Common prefix",
            "+        compressed = _compress_path(b\"src/file2.txt\", b\"src/file1.txt\")",
            "+        decompressed, _ = _decompress_path(compressed, 0, b\"src/file1.txt\")",
            "+        self.assertEqual(b\"src/file2.txt\", decompressed)",
            "+",
            "+        # Test case 3: Completely different paths",
            "+        compressed = _compress_path(b\"docs/readme.md\", b\"src/file1.txt\")",
            "+        decompressed, _ = _decompress_path(compressed, 0, b\"src/file1.txt\")",
            "+        self.assertEqual(b\"docs/readme.md\", decompressed)",
            "+",
            "+    def test_path_compression_deep_directories(self):",
            "+        \"\"\"Test compression with deep directory structures.\"\"\"",
            "+        path1 = b\"src/main/java/com/example/service/UserService.java\"",
            "+        path2 = b\"src/main/java/com/example/service/OrderService.java\"",
            "+        path3 = b\"src/main/java/com/example/model/User.java\"",
            "+",
            "+        # Compress path2 relative to path1",
            "+        compressed = _compress_path(path2, path1)",
            "+        decompressed, _ = _decompress_path(compressed, 0, path1)",
            "+        self.assertEqual(path2, decompressed)",
            "+",
            "+        # Compress path3 relative to path2",
            "+        compressed = _compress_path(path3, path2)",
            "+        decompressed, _ = _decompress_path(compressed, 0, path2)",
            "+        self.assertEqual(path3, decompressed)",
            "+",
            "+    def test_index_version_4_with_compression(self):",
            "+        \"\"\"Test full index version 4 write/read with path compression.\"\"\"",
            "+        index_path = os.path.join(self.tempdir, \"index\")",
            "+",
            "+        # Create an index with version 4",
            "+        index = Index(index_path, read=False, version=4)",
            "+",
            "+        # Add multiple entries with common prefixes",
            "+        paths = [",
            "+            b\"src/main/java/App.java\",",
            "+            b\"src/main/java/Utils.java\",",
            "+            b\"src/main/resources/config.properties\",",
            "+            b\"src/test/java/AppTest.java\",",
            "+            b\"docs/README.md\",",
            "+            b\"docs/INSTALL.md\",",
            "+        ]",
            "+",
            "+        for i, path in enumerate(paths):",
            "+            entry = IndexEntry(",
            "+                ctime=(1234567890, 0),",
            "+                mtime=(1234567890, 0),",
            "+                dev=1,",
            "+                ino=i + 1,",
            "+                mode=0o100644,",
            "+                uid=1000,",
            "+                gid=1000,",
            "+                size=10,",
            "+                sha=f\"{i:040d}\".encode(),",
            "+            )",
            "+            index[path] = entry",
            "+",
            "+        # Write and read back",
            "+        index.write()",
            "+",
            "+        # Read the index back",
            "+        index2 = Index(index_path)",
            "+        self.assertEqual(index2._version, 4)",
            "+",
            "+        # Verify all paths were preserved correctly",
            "+        for path in paths:",
            "+            self.assertIn(path, index2)",
            "+",
            "+        # Verify the index file is smaller than version 2 would be",
            "+        with open(index_path, \"rb\") as f:",
            "+            v4_size = len(f.read())",
            "+",
            "+        # Create equivalent version 2 index for comparison",
            "+        index_v2_path = os.path.join(self.tempdir, \"index_v2\")",
            "+        index_v2 = Index(index_v2_path, read=False, version=2)",
            "+        for path in paths:",
            "+            entry = IndexEntry(",
            "+                ctime=(1234567890, 0),",
            "+                mtime=(1234567890, 0),",
            "+                dev=1,",
            "+                ino=1,",
            "+                mode=0o100644,",
            "+                uid=1000,",
            "+                gid=1000,",
            "+                size=10,",
            "+                sha=b\"0\" * 40,",
            "+            )",
            "+            index_v2[path] = entry",
            "+        index_v2.write()",
            "+",
            "+        with open(index_v2_path, \"rb\") as f:",
            "+            v2_size = len(f.read())",
            "+",
            "+        # Version 4 should be smaller due to compression",
            "+        self.assertLess(",
            "+            v4_size, v2_size, \"Version 4 index should be smaller than version 2\"",
            "+        )",
            "+",
            "+    def test_path_compression_edge_cases(self):",
            "+        \"\"\"Test edge cases in path compression.\"\"\"",
            "+        # Empty paths",
            "+        compressed = _compress_path(b\"\", b\"\")",
            "+        decompressed, _ = _decompress_path(compressed, 0, b\"\")",
            "+        self.assertEqual(b\"\", decompressed)",
            "+",
            "+        # Path identical to previous",
            "+        compressed = _compress_path(b\"same.txt\", b\"same.txt\")",
            "+        decompressed, _ = _decompress_path(compressed, 0, b\"same.txt\")",
            "+        self.assertEqual(b\"same.txt\", decompressed)",
            "+",
            "+        # Path shorter than previous",
            "+        compressed = _compress_path(b\"short\", b\"very/long/path/file.txt\")",
            "+        decompressed, _ = _decompress_path(compressed, 0, b\"very/long/path/file.txt\")",
            "+        self.assertEqual(b\"short\", decompressed)",
            "+",
            "+",
            "+class TestUpdateWorkingTree(TestCase):",
            "+    def setUp(self):",
            "+        self.tempdir = tempfile.mkdtemp()",
            "+",
            "+        def cleanup_tempdir():",
            "+            \"\"\"Remove tempdir, handling read-only files on Windows.\"\"\"",
            "+",
            "+            def remove_readonly(func, path, excinfo):",
            "+                \"\"\"Error handler for Windows read-only files.\"\"\"",
            "+                import stat",
            "+",
            "+                if sys.platform == \"win32\" and excinfo[0] is PermissionError:",
            "+                    os.chmod(path, stat.S_IWRITE)",
            "+                    func(path)",
            "+                else:",
            "+                    raise",
            "+",
            "+            shutil.rmtree(self.tempdir, onerror=remove_readonly)",
            "+",
            "+        self.addCleanup(cleanup_tempdir)",
            "+",
            "+        self.repo = Repo.init(self.tempdir)",
            "+",
            "+    def test_update_working_tree_with_blob_normalizer(self):",
            "+        \"\"\"Test update_working_tree with a blob normalizer.\"\"\"",
            "+",
            "+        # Create a simple blob normalizer that converts CRLF to LF",
            "+        class TestBlobNormalizer:",
            "+            def checkout_normalize(self, blob, path):",
            "+                # Convert CRLF to LF during checkout",
            "+                new_blob = Blob()",
            "+                new_blob.data = blob.data.replace(b\"\\r\\n\", b\"\\n\")",
            "+                return new_blob",
            "+",
            "+        # Create a tree with a file containing CRLF",
            "+        blob = Blob()",
            "+        blob.data = b\"Hello\\r\\nWorld\\r\\n\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree = Tree()",
            "+        tree[b\"test.txt\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree)",
            "+",
            "+        # Update working tree with normalizer",
            "+        normalizer = TestBlobNormalizer()",
            "+        update_working_tree(",
            "+            self.repo,",
            "+            None,  # old_tree_id",
            "+            tree.id,  # new_tree_id",
            "+            blob_normalizer=normalizer,",
            "+        )",
            "+",
            "+        # Check that the file was written with LF line endings",
            "+        test_file = os.path.join(self.tempdir, \"test.txt\")",
            "+        with open(test_file, \"rb\") as f:",
            "+            content = f.read()",
            "+",
            "+        self.assertEqual(b\"Hello\\nWorld\\n\", content)",
            "+",
            "+        # Check that the index has the original blob SHA",
            "+        index = self.repo.open_index()",
            "+        self.assertEqual(blob.id, index[b\"test.txt\"].sha)",
            "+",
            "+    def test_update_working_tree_without_blob_normalizer(self):",
            "+        \"\"\"Test update_working_tree without a blob normalizer.\"\"\"",
            "+        # Create a tree with a file containing CRLF",
            "+        blob = Blob()",
            "+        blob.data = b\"Hello\\r\\nWorld\\r\\n\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree = Tree()",
            "+        tree[b\"test.txt\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree)",
            "+",
            "+        # Update working tree without normalizer",
            "+        update_working_tree(",
            "+            self.repo,",
            "+            None,  # old_tree_id",
            "+            tree.id,  # new_tree_id",
            "+            blob_normalizer=None,",
            "+        )",
            "+",
            "+        # Check that the file was written with original CRLF line endings",
            "+        test_file = os.path.join(self.tempdir, \"test.txt\")",
            "+        with open(test_file, \"rb\") as f:",
            "+            content = f.read()",
            "+",
            "+        self.assertEqual(b\"Hello\\r\\nWorld\\r\\n\", content)",
            "+",
            "+        # Check that the index has the blob SHA",
            "+        index = self.repo.open_index()",
            "+        self.assertEqual(blob.id, index[b\"test.txt\"].sha)",
            "+",
            "+    def test_update_working_tree_remove_directory(self):",
            "+        \"\"\"Test that update_working_tree properly removes directories.\"\"\"",
            "+        # Create initial tree with a directory containing files",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"content1\"",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"content2\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"dir/file1.txt\"] = (0o100644, blob1.id)",
            "+        tree1[b\"dir/file2.txt\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1 (create directory with files)",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Verify directory and files exist",
            "+        dir_path = os.path.join(self.tempdir, \"dir\")",
            "+        self.assertTrue(os.path.isdir(dir_path))",
            "+        self.assertTrue(os.path.exists(os.path.join(dir_path, \"file1.txt\")))",
            "+        self.assertTrue(os.path.exists(os.path.join(dir_path, \"file2.txt\")))",
            "+",
            "+        # Create empty tree (remove everything)",
            "+        tree2 = Tree()",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update to empty tree",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Verify directory was removed",
            "+        self.assertFalse(os.path.exists(dir_path))",
            "+",
            "+    def test_update_working_tree_submodule_to_file(self):",
            "+        \"\"\"Test replacing a submodule directory with a file.\"\"\"",
            "+        # Create tree with submodule",
            "+        submodule_sha = b\"a\" * 40",
            "+        tree1 = Tree()",
            "+        tree1[b\"submodule\"] = (S_IFGITLINK, submodule_sha)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree with submodule",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Verify submodule directory exists with .git file",
            "+        submodule_path = os.path.join(self.tempdir, \"submodule\")",
            "+        self.assertTrue(os.path.isdir(submodule_path))",
            "+        self.assertTrue(os.path.exists(os.path.join(submodule_path, \".git\")))",
            "+",
            "+        # Create tree with file at same path",
            "+        blob = Blob()",
            "+        blob.data = b\"file content\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"submodule\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update to tree with file (should remove submodule directory and create file)",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Verify it's now a file",
            "+        self.assertTrue(os.path.isfile(submodule_path))",
            "+        with open(submodule_path, \"rb\") as f:",
            "+            self.assertEqual(b\"file content\", f.read())",
            "+",
            "+    def test_update_working_tree_directory_with_nested_subdir(self):",
            "+        \"\"\"Test removing directory with nested subdirectories.\"\"\"",
            "+        # Create tree with nested directories",
            "+        blob = Blob()",
            "+        blob.data = b\"deep content\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"a/b/c/file.txt\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Verify nested structure exists",
            "+        path_a = os.path.join(self.tempdir, \"a\")",
            "+        path_b = os.path.join(path_a, \"b\")",
            "+        path_c = os.path.join(path_b, \"c\")",
            "+        file_path = os.path.join(path_c, \"file.txt\")",
            "+",
            "+        self.assertTrue(os.path.exists(file_path))",
            "+",
            "+        # Create empty tree",
            "+        tree2 = Tree()",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update to empty tree",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Verify all directories were removed",
            "+        self.assertFalse(os.path.exists(path_a))",
            "+",
            "+    def test_update_working_tree_file_replaced_by_dir_not_removed(self):",
            "+        \"\"\"Test that a directory replacing a git file is left alone if not empty.\"\"\"",
            "+        # Create tree with a file",
            "+        blob = Blob()",
            "+        blob.data = b\"file content\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"path\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Verify file exists",
            "+        file_path = os.path.join(self.tempdir, \"path\")",
            "+        self.assertTrue(os.path.isfile(file_path))",
            "+",
            "+        # Manually replace file with directory containing untracked file",
            "+        os.remove(file_path)",
            "+        os.mkdir(file_path)",
            "+        with open(os.path.join(file_path, \"untracked.txt\"), \"w\") as f:",
            "+            f.write(\"untracked content\")",
            "+",
            "+        # Create empty tree",
            "+        tree2 = Tree()",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should succeed but leave the directory alone",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Directory should still exist with its contents",
            "+        self.assertTrue(os.path.isdir(file_path))",
            "+        self.assertTrue(os.path.exists(os.path.join(file_path, \"untracked.txt\")))",
            "+",
            "+    def test_update_working_tree_file_replaced_by_empty_dir_removed(self):",
            "+        \"\"\"Test that an empty directory replacing a git file is removed.\"\"\"",
            "+        # Create tree with a file",
            "+        blob = Blob()",
            "+        blob.data = b\"file content\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"path\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Verify file exists",
            "+        file_path = os.path.join(self.tempdir, \"path\")",
            "+        self.assertTrue(os.path.isfile(file_path))",
            "+",
            "+        # Manually replace file with empty directory",
            "+        os.remove(file_path)",
            "+        os.mkdir(file_path)",
            "+",
            "+        # Create empty tree",
            "+        tree2 = Tree()",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should remove the empty directory",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Directory should be gone",
            "+        self.assertFalse(os.path.exists(file_path))",
            "+",
            "+    def test_update_working_tree_symlink_transitions(self):",
            "+        \"\"\"Test transitions involving symlinks.\"\"\"",
            "+        # Skip on Windows where symlinks might not be supported",
            "+        if sys.platform == \"win32\":",
            "+            self.skipTest(\"Symlinks not fully supported on Windows\")",
            "+",
            "+        # Create tree with symlink",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"target/path\"",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"link\"] = (0o120000, blob1.id)  # Symlink mode",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree with symlink",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        link_path = os.path.join(self.tempdir, \"link\")",
            "+        self.assertTrue(os.path.islink(link_path))",
            "+        self.assertEqual(b\"target/path\", os.readlink(link_path).encode())",
            "+",
            "+        # Test 1: Replace symlink with regular file",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"file content\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"link\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        self.assertFalse(os.path.islink(link_path))",
            "+        self.assertTrue(os.path.isfile(link_path))",
            "+        with open(link_path, \"rb\") as f:",
            "+            self.assertEqual(b\"file content\", f.read())",
            "+",
            "+        # Test 2: Replace file with symlink",
            "+        update_working_tree(self.repo, tree2.id, tree1.id)",
            "+",
            "+        self.assertTrue(os.path.islink(link_path))",
            "+        self.assertEqual(b\"target/path\", os.readlink(link_path).encode())",
            "+",
            "+        # Test 3: Replace symlink with directory (manually)",
            "+        os.unlink(link_path)",
            "+        os.mkdir(link_path)",
            "+",
            "+        # Create empty tree",
            "+        tree3 = Tree()",
            "+        self.repo.object_store.add_object(tree3)",
            "+",
            "+        # Should remove empty directory",
            "+        update_working_tree(self.repo, tree1.id, tree3.id)",
            "+        self.assertFalse(os.path.exists(link_path))",
            "+",
            "+    def test_update_working_tree_modified_file_to_dir_transition(self):",
            "+        \"\"\"Test that modified files are not removed when they should be directories.\"\"\"",
            "+        # Create tree with file",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"original content\"",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"path\"] = (0o100644, blob1.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        file_path = os.path.join(self.tempdir, \"path\")",
            "+",
            "+        # Modify the file locally",
            "+        with open(file_path, \"w\") as f:",
            "+            f.write(\"modified content\")",
            "+",
            "+        # Create tree where path is a directory with file",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"subfile content\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"path/subfile\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should fail because can't create directory where modified file exists",
            "+        with self.assertRaises(IOError):",
            "+            update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # File should still exist with modifications",
            "+        self.assertTrue(os.path.isfile(file_path))",
            "+        with open(file_path) as f:",
            "+            self.assertEqual(\"modified content\", f.read())",
            "+",
            "+    def test_update_working_tree_executable_transitions(self):",
            "+        \"\"\"Test transitions involving executable bit changes.\"\"\"",
            "+        # Skip on Windows where executable bit is not supported",
            "+        if sys.platform == \"win32\":",
            "+            self.skipTest(\"Executable bit not supported on Windows\")",
            "+",
            "+        # Create tree with non-executable file",
            "+        blob = Blob()",
            "+        blob.data = b\"#!/bin/sh\\necho hello\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"script.sh\"] = (0o100644, blob.id)  # Non-executable",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        script_path = os.path.join(self.tempdir, \"script.sh\")",
            "+        self.assertTrue(os.path.isfile(script_path))",
            "+",
            "+        # Check it's not executable",
            "+        mode = os.stat(script_path).st_mode",
            "+        self.assertFalse(mode & stat.S_IXUSR)",
            "+",
            "+        # Create tree with executable file (same content)",
            "+        tree2 = Tree()",
            "+        tree2[b\"script.sh\"] = (0o100755, blob.id)  # Executable",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update to tree2",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Check it's now executable",
            "+        mode = os.stat(script_path).st_mode",
            "+        self.assertTrue(mode & stat.S_IXUSR)",
            "+",
            "+    def test_update_working_tree_submodule_with_untracked_files(self):",
            "+        \"\"\"Test that submodules with untracked files are not removed.\"\"\"",
            "+        from dulwich.objects import S_IFGITLINK, Tree",
            "+",
            "+        # Create tree with submodule",
            "+        submodule_sha = b\"a\" * 40",
            "+        tree1 = Tree()",
            "+        tree1[b\"submodule\"] = (S_IFGITLINK, submodule_sha)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree with submodule",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Add untracked file to submodule directory",
            "+        submodule_path = os.path.join(self.tempdir, \"submodule\")",
            "+        untracked_path = os.path.join(submodule_path, \"untracked.txt\")",
            "+        with open(untracked_path, \"w\") as f:",
            "+            f.write(\"untracked content\")",
            "+",
            "+        # Create empty tree",
            "+        tree2 = Tree()",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should not remove submodule directory with untracked files",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Directory should still exist with untracked file",
            "+        self.assertTrue(os.path.isdir(submodule_path))",
            "+        self.assertTrue(os.path.exists(untracked_path))",
            "+",
            "+    def test_update_working_tree_dir_to_file_with_subdir(self):",
            "+        \"\"\"Test replacing directory structure with a file.\"\"\"",
            "+        # Create tree with nested directory structure",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"content1\"",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"content2\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"dir/subdir/file1\"] = (0o100644, blob1.id)",
            "+        tree1[b\"dir/subdir/file2\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Verify structure exists",
            "+        dir_path = os.path.join(self.tempdir, \"dir\")",
            "+        self.assertTrue(os.path.isdir(dir_path))",
            "+",
            "+        # Add an untracked file to make directory truly non-empty",
            "+        untracked_path = os.path.join(dir_path, \"untracked.txt\")",
            "+        with open(untracked_path, \"w\") as f:",
            "+            f.write(\"untracked content\")",
            "+",
            "+        # Create tree with file at \"dir\" path",
            "+        blob3 = Blob()",
            "+        blob3.data = b\"replacement file\"",
            "+        self.repo.object_store.add_object(blob3)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"dir\"] = (0o100644, blob3.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should fail because directory is not empty",
            "+        with self.assertRaises(IsADirectoryError):",
            "+            update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Directory should still exist",
            "+        self.assertTrue(os.path.isdir(dir_path))",
            "+",
            "+    def test_update_working_tree_case_sensitivity(self):",
            "+        \"\"\"Test handling of case-sensitive filename changes.\"\"\"",
            "+        # Create tree with lowercase file",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"lowercase content\"",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"readme.txt\"] = (0o100644, blob1.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Create tree with uppercase file (different content)",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"uppercase content\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"README.txt\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update to tree2",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Check what exists (behavior depends on filesystem)",
            "+        lowercase_path = os.path.join(self.tempdir, \"readme.txt\")",
            "+        uppercase_path = os.path.join(self.tempdir, \"README.txt\")",
            "+",
            "+        # On case-insensitive filesystems, one will overwrite the other",
            "+        # On case-sensitive filesystems, both may exist",
            "+        self.assertTrue(",
            "+            os.path.exists(lowercase_path) or os.path.exists(uppercase_path)",
            "+        )",
            "+",
            "+    def test_update_working_tree_deeply_nested_removal(self):",
            "+        \"\"\"Test removal of deeply nested directory structures.\"\"\"",
            "+        # Create deeply nested structure",
            "+        blob = Blob()",
            "+        blob.data = b\"deep content\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree1 = Tree()",
            "+        # Create a very deep path",
            "+        deep_path = b\"/\".join([b\"level%d\" % i for i in range(10)])",
            "+        tree1[deep_path + b\"/file.txt\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Verify deep structure exists",
            "+        current_path = self.tempdir",
            "+        for i in range(10):",
            "+            current_path = os.path.join(current_path, f\"level{i}\")",
            "+            self.assertTrue(os.path.isdir(current_path))",
            "+",
            "+        # Create empty tree",
            "+        tree2 = Tree()",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should remove all empty directories",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Verify top level directory is gone",
            "+        top_level = os.path.join(self.tempdir, \"level0\")",
            "+        self.assertFalse(os.path.exists(top_level))",
            "+",
            "+    def test_update_working_tree_read_only_files(self):",
            "+        \"\"\"Test handling of read-only files during updates.\"\"\"",
            "+        # Create tree with file",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"original content\"",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"readonly.txt\"] = (0o100644, blob1.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Make file read-only",
            "+        file_path = os.path.join(self.tempdir, \"readonly.txt\")",
            "+        os.chmod(file_path, 0o444)  # Read-only",
            "+",
            "+        # Create tree with modified file",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"new content\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"readonly.txt\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should handle read-only file",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        # Verify content was updated",
            "+        with open(file_path, \"rb\") as f:",
            "+            self.assertEqual(b\"new content\", f.read())",
            "+",
            "+    def test_update_working_tree_invalid_filenames(self):",
            "+        \"\"\"Test handling of invalid filenames for the platform.\"\"\"",
            "+        # Create tree with potentially problematic filenames",
            "+        blob = Blob()",
            "+        blob.data = b\"content\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree = Tree()",
            "+        # Add files with names that might be invalid on some platforms",
            "+        tree[b\"valid.txt\"] = (0o100644, blob.id)",
            "+        if sys.platform != \"win32\":",
            "+            # These are invalid on Windows but valid on Unix",
            "+            tree[b\"file:with:colons.txt\"] = (0o100644, blob.id)",
            "+            tree[b\"file<with>brackets.txt\"] = (0o100644, blob.id)",
            "+        self.repo.object_store.add_object(tree)",
            "+",
            "+        # Update should skip invalid files based on validation",
            "+        update_working_tree(self.repo, None, tree.id)",
            "+",
            "+        # Valid file should exist",
            "+        self.assertTrue(os.path.exists(os.path.join(self.tempdir, \"valid.txt\")))",
            "+",
            "+    def test_update_working_tree_symlink_to_directory(self):",
            "+        \"\"\"Test replacing a symlink pointing to a directory with a real directory.\"\"\"",
            "+        if sys.platform == \"win32\":",
            "+            self.skipTest(\"Symlinks not fully supported on Windows\")",
            "+",
            "+        # Create a target directory",
            "+        target_dir = os.path.join(self.tempdir, \"target\")",
            "+        os.mkdir(target_dir)",
            "+        with open(os.path.join(target_dir, \"file.txt\"), \"w\") as f:",
            "+            f.write(\"target file\")",
            "+",
            "+        # Create tree with symlink pointing to directory",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"target\"  # Relative path to target directory",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"link\"] = (0o120000, blob1.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        link_path = os.path.join(self.tempdir, \"link\")",
            "+        self.assertTrue(os.path.islink(link_path))",
            "+",
            "+        # Create tree with actual directory at same path",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"new file content\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"link/newfile.txt\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should replace symlink with actual directory",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+",
            "+        self.assertFalse(os.path.islink(link_path))",
            "+        self.assertTrue(os.path.isdir(link_path))",
            "+        self.assertTrue(os.path.exists(os.path.join(link_path, \"newfile.txt\")))",
            "+",
            "+    def test_update_working_tree_comprehensive_transitions(self):",
            "+        \"\"\"Test all possible file type transitions comprehensively.\"\"\"",
            "+        # Skip on Windows where symlinks might not be supported",
            "+        if sys.platform == \"win32\":",
            "+            self.skipTest(\"Symlinks not fully supported on Windows\")",
            "+",
            "+        # Create blobs for different file types",
            "+        file_blob = Blob()",
            "+        file_blob.data = b\"regular file content\"",
            "+        self.repo.object_store.add_object(file_blob)",
            "+",
            "+        exec_blob = Blob()",
            "+        exec_blob.data = b\"#!/bin/sh\\necho executable\"",
            "+        self.repo.object_store.add_object(exec_blob)",
            "+",
            "+        link_blob = Blob()",
            "+        link_blob.data = b\"target/path\"",
            "+        self.repo.object_store.add_object(link_blob)",
            "+",
            "+        submodule_sha = b\"a\" * 40",
            "+",
            "+        # Test 1: Regular file → Submodule",
            "+        tree1 = Tree()",
            "+        tree1[b\"item\"] = (0o100644, file_blob.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"item\"] = (S_IFGITLINK, submodule_sha)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+        self.assertTrue(os.path.isfile(os.path.join(self.tempdir, \"item\")))",
            "+",
            "+        update_working_tree(self.repo, tree1.id, tree2.id)",
            "+        self.assertTrue(os.path.isdir(os.path.join(self.tempdir, \"item\")))",
            "+",
            "+        # Test 2: Submodule → Executable file",
            "+        tree3 = Tree()",
            "+        tree3[b\"item\"] = (0o100755, exec_blob.id)",
            "+        self.repo.object_store.add_object(tree3)",
            "+",
            "+        update_working_tree(self.repo, tree2.id, tree3.id)",
            "+        item_path = os.path.join(self.tempdir, \"item\")",
            "+        self.assertTrue(os.path.isfile(item_path))",
            "+        if sys.platform != \"win32\":",
            "+            self.assertTrue(os.access(item_path, os.X_OK))",
            "+",
            "+        # Test 3: Executable file → Symlink",
            "+        tree4 = Tree()",
            "+        tree4[b\"item\"] = (0o120000, link_blob.id)",
            "+        self.repo.object_store.add_object(tree4)",
            "+",
            "+        update_working_tree(self.repo, tree3.id, tree4.id)",
            "+        self.assertTrue(os.path.islink(item_path))",
            "+",
            "+        # Test 4: Symlink → Submodule",
            "+        tree5 = Tree()",
            "+        tree5[b\"item\"] = (S_IFGITLINK, submodule_sha)",
            "+        self.repo.object_store.add_object(tree5)",
            "+",
            "+        update_working_tree(self.repo, tree4.id, tree5.id)",
            "+        self.assertTrue(os.path.isdir(item_path))",
            "+",
            "+        # Test 5: Clean up - Submodule → absent",
            "+        tree6 = Tree()",
            "+        self.repo.object_store.add_object(tree6)",
            "+",
            "+        update_working_tree(self.repo, tree5.id, tree6.id)",
            "+        self.assertFalse(os.path.exists(item_path))",
            "+",
            "+        # Test 6: Symlink → Executable file",
            "+        tree7 = Tree()",
            "+        tree7[b\"item2\"] = (0o120000, link_blob.id)",
            "+        self.repo.object_store.add_object(tree7)",
            "+",
            "+        update_working_tree(self.repo, tree6.id, tree7.id)",
            "+        item2_path = os.path.join(self.tempdir, \"item2\")",
            "+        self.assertTrue(os.path.islink(item2_path))",
            "+",
            "+        tree8 = Tree()",
            "+        tree8[b\"item2\"] = (0o100755, exec_blob.id)",
            "+        self.repo.object_store.add_object(tree8)",
            "+",
            "+        update_working_tree(self.repo, tree7.id, tree8.id)",
            "+        self.assertTrue(os.path.isfile(item2_path))",
            "+        if sys.platform != \"win32\":",
            "+            self.assertTrue(os.access(item2_path, os.X_OK))",
            "+",
            "+    def test_update_working_tree_partial_update_failure(self):",
            "+        \"\"\"Test handling when update fails partway through.\"\"\"",
            "+        # Create initial tree",
            "+        blob1 = Blob()",
            "+        blob1.data = b\"file1 content\"",
            "+        self.repo.object_store.add_object(blob1)",
            "+",
            "+        blob2 = Blob()",
            "+        blob2.data = b\"file2 content\"",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        tree1 = Tree()",
            "+        tree1[b\"file1.txt\"] = (0o100644, blob1.id)",
            "+        tree1[b\"file2.txt\"] = (0o100644, blob2.id)",
            "+        self.repo.object_store.add_object(tree1)",
            "+",
            "+        # Update to tree1",
            "+        update_working_tree(self.repo, None, tree1.id)",
            "+",
            "+        # Create a directory where file2.txt is, to cause a conflict",
            "+        file2_path = os.path.join(self.tempdir, \"file2.txt\")",
            "+        os.remove(file2_path)",
            "+        os.mkdir(file2_path)",
            "+        # Add untracked file to prevent removal",
            "+        with open(os.path.join(file2_path, \"blocker.txt\"), \"w\") as f:",
            "+            f.write(\"blocking content\")",
            "+",
            "+        # Create tree with updates to both files",
            "+        blob3 = Blob()",
            "+        blob3.data = b\"file1 updated\"",
            "+        self.repo.object_store.add_object(blob3)",
            "+",
            "+        blob4 = Blob()",
            "+        blob4.data = b\"file2 updated\"",
            "+        self.repo.object_store.add_object(blob4)",
            "+",
            "+        tree2 = Tree()",
            "+        tree2[b\"file1.txt\"] = (0o100644, blob3.id)",
            "+        tree2[b\"file2.txt\"] = (0o100644, blob4.id)",
            "+        self.repo.object_store.add_object(tree2)",
            "+",
            "+        # Update should partially succeed - file1 updated, file2 blocked",
            "+        try:",
            "+            update_working_tree(self.repo, tree1.id, tree2.id)",
            "+        except IsADirectoryError:",
            "+            # Expected to fail on file2 because it's a directory",
            "+            pass",
            "+",
            "+        # file1 should be updated",
            "+        with open(os.path.join(self.tempdir, \"file1.txt\"), \"rb\") as f:",
            "+            self.assertEqual(b\"file1 updated\", f.read())",
            "+",
            "+        # file2 should still be a directory",
            "+        self.assertTrue(os.path.isdir(file2_path))"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_lfs.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_lfs.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_lfs.py",
            "@@ -20,15 +20,15 @@",
            " #",
            " ",
            " \"\"\"Tests for LFS support.\"\"\"",
            " ",
            " import shutil",
            " import tempfile",
            " ",
            "-from dulwich.lfs import LFSStore",
            "+from dulwich.lfs import LFSFilterDriver, LFSPointer, LFSStore",
            " ",
            " from . import TestCase",
            " ",
            " ",
            " class LFSTests(TestCase):",
            "     def setUp(self) -> None:",
            "         super().setUp()",
            "@@ -39,7 +39,248 @@",
            "     def test_create(self) -> None:",
            "         sha = self.lfs.write_object([b\"a\", b\"b\"])",
            "         with self.lfs.open_object(sha) as f:",
            "             self.assertEqual(b\"ab\", f.read())",
            " ",
            "     def test_missing(self) -> None:",
            "         self.assertRaises(KeyError, self.lfs.open_object, \"abcdeabcdeabcdeabcde\")",
            "+",
            "+    def test_write_object_empty(self) -> None:",
            "+        \"\"\"Test writing an empty object.\"\"\"",
            "+        sha = self.lfs.write_object([])",
            "+        with self.lfs.open_object(sha) as f:",
            "+            self.assertEqual(b\"\", f.read())",
            "+",
            "+    def test_write_object_multiple_chunks(self) -> None:",
            "+        \"\"\"Test writing an object with multiple chunks.\"\"\"",
            "+        chunks = [b\"chunk1\", b\"chunk2\", b\"chunk3\"]",
            "+        sha = self.lfs.write_object(chunks)",
            "+        with self.lfs.open_object(sha) as f:",
            "+            self.assertEqual(b\"\".join(chunks), f.read())",
            "+",
            "+    def test_sha_path_calculation(self) -> None:",
            "+        \"\"\"Test the internal sha path calculation.\"\"\"",
            "+        # The implementation splits the sha into parts for directory structure",
            "+        # Write and verify we can read it back",
            "+        sha = self.lfs.write_object([b\"test data\"])",
            "+        self.assertEqual(len(sha), 64)  # SHA-256 is 64 hex chars",
            "+",
            "+        # Open should succeed, which verifies the path calculation works",
            "+        with self.lfs.open_object(sha) as f:",
            "+            self.assertEqual(b\"test data\", f.read())",
            "+",
            "+    def test_create_lfs_dir(self) -> None:",
            "+        \"\"\"Test creating an LFS directory when it doesn't exist.\"\"\"",
            "+        import os",
            "+",
            "+        # Create a temporary directory for the test",
            "+        lfs_parent_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, lfs_parent_dir)",
            "+",
            "+        # Create a path for the LFS directory",
            "+        lfs_dir = os.path.join(lfs_parent_dir, \"lfs\")",
            "+",
            "+        # Create the LFS store",
            "+        LFSStore.create(lfs_dir)",
            "+",
            "+        # Verify the directories were created",
            "+        self.assertTrue(os.path.isdir(lfs_dir))",
            "+        self.assertTrue(os.path.isdir(os.path.join(lfs_dir, \"tmp\")))",
            "+        self.assertTrue(os.path.isdir(os.path.join(lfs_dir, \"objects\")))",
            "+",
            "+",
            "+class LFSPointerTests(TestCase):",
            "+    def test_from_bytes_valid(self) -> None:",
            "+        \"\"\"Test parsing a valid LFS pointer.\"\"\"",
            "+        pointer_data = (",
            "+            b\"version https://git-lfs.github.com/spec/v1\\n\"",
            "+            b\"oid sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\n\"",
            "+            b\"size 0\\n\"",
            "+        )",
            "+        pointer = LFSPointer.from_bytes(pointer_data)",
            "+        self.assertIsNotNone(pointer)",
            "+        self.assertEqual(",
            "+            pointer.oid,",
            "+            \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\",",
            "+        )",
            "+        self.assertEqual(pointer.size, 0)",
            "+",
            "+    def test_from_bytes_with_extra_fields(self) -> None:",
            "+        \"\"\"Test parsing LFS pointer with extra fields (should still work).\"\"\"",
            "+        pointer_data = (",
            "+            b\"version https://git-lfs.github.com/spec/v1\\n\"",
            "+            b\"oid sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\n\"",
            "+            b\"size 1234\\n\"",
            "+            b\"x-custom-field value\\n\"",
            "+        )",
            "+        pointer = LFSPointer.from_bytes(pointer_data)",
            "+        self.assertIsNotNone(pointer)",
            "+        self.assertEqual(pointer.size, 1234)",
            "+",
            "+    def test_from_bytes_invalid_version(self) -> None:",
            "+        \"\"\"Test parsing with invalid version line.\"\"\"",
            "+        pointer_data = (",
            "+            b\"version https://invalid.com/spec/v1\\n\"",
            "+            b\"oid sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\n\"",
            "+            b\"size 0\\n\"",
            "+        )",
            "+        pointer = LFSPointer.from_bytes(pointer_data)",
            "+        self.assertIsNone(pointer)",
            "+",
            "+    def test_from_bytes_missing_oid(self) -> None:",
            "+        \"\"\"Test parsing with missing OID.\"\"\"",
            "+        pointer_data = b\"version https://git-lfs.github.com/spec/v1\\nsize 0\\n\"",
            "+        pointer = LFSPointer.from_bytes(pointer_data)",
            "+        self.assertIsNone(pointer)",
            "+",
            "+    def test_from_bytes_missing_size(self) -> None:",
            "+        \"\"\"Test parsing with missing size.\"\"\"",
            "+        pointer_data = (",
            "+            b\"version https://git-lfs.github.com/spec/v1\\n\"",
            "+            b\"oid sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\n\"",
            "+        )",
            "+        pointer = LFSPointer.from_bytes(pointer_data)",
            "+        self.assertIsNone(pointer)",
            "+",
            "+    def test_from_bytes_invalid_size(self) -> None:",
            "+        \"\"\"Test parsing with invalid size.\"\"\"",
            "+        pointer_data = (",
            "+            b\"version https://git-lfs.github.com/spec/v1\\n\"",
            "+            b\"oid sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\n\"",
            "+            b\"size not_a_number\\n\"",
            "+        )",
            "+        pointer = LFSPointer.from_bytes(pointer_data)",
            "+        self.assertIsNone(pointer)",
            "+",
            "+    def test_from_bytes_binary_data(self) -> None:",
            "+        \"\"\"Test parsing binary data (not an LFS pointer).\"\"\"",
            "+        binary_data = b\"\\x00\\x01\\x02\\x03\\x04\"",
            "+        pointer = LFSPointer.from_bytes(binary_data)",
            "+        self.assertIsNone(pointer)",
            "+",
            "+    def test_to_bytes(self) -> None:",
            "+        \"\"\"Test converting LFS pointer to bytes.\"\"\"",
            "+        pointer = LFSPointer(",
            "+            \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", 1234",
            "+        )",
            "+        data = pointer.to_bytes()",
            "+        expected = (",
            "+            b\"version https://git-lfs.github.com/spec/v1\\n\"",
            "+            b\"oid sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\\n\"",
            "+            b\"size 1234\\n\"",
            "+        )",
            "+        self.assertEqual(data, expected)",
            "+",
            "+    def test_round_trip(self) -> None:",
            "+        \"\"\"Test converting to bytes and back.\"\"\"",
            "+        original = LFSPointer(",
            "+            \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", 9876",
            "+        )",
            "+        data = original.to_bytes()",
            "+        parsed = LFSPointer.from_bytes(data)",
            "+        self.assertIsNotNone(parsed)",
            "+        self.assertEqual(parsed.oid, original.oid)",
            "+        self.assertEqual(parsed.size, original.size)",
            "+",
            "+    def test_is_valid_oid(self) -> None:",
            "+        \"\"\"Test OID validation.\"\"\"",
            "+        # Valid SHA256",
            "+        valid_pointer = LFSPointer(",
            "+            \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", 0",
            "+        )",
            "+        self.assertTrue(valid_pointer.is_valid_oid())",
            "+",
            "+        # Too short",
            "+        short_pointer = LFSPointer(\"e3b0c44298fc1c14\", 0)",
            "+        self.assertFalse(short_pointer.is_valid_oid())",
            "+",
            "+        # Invalid hex characters",
            "+        invalid_pointer = LFSPointer(",
            "+            \"g3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", 0",
            "+        )",
            "+        self.assertFalse(invalid_pointer.is_valid_oid())",
            "+",
            "+",
            "+class LFSFilterDriverTests(TestCase):",
            "+    def setUp(self) -> None:",
            "+        super().setUp()",
            "+        self.test_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, self.test_dir)",
            "+        self.lfs_store = LFSStore.create(self.test_dir)",
            "+        self.filter_driver = LFSFilterDriver(self.lfs_store)",
            "+",
            "+    def test_clean_new_file(self) -> None:",
            "+        \"\"\"Test clean filter on new file content.\"\"\"",
            "+        content = b\"This is a test file content\"",
            "+        result = self.filter_driver.clean(content)",
            "+",
            "+        # Result should be an LFS pointer",
            "+        pointer = LFSPointer.from_bytes(result)",
            "+        self.assertIsNotNone(pointer)",
            "+        self.assertEqual(pointer.size, len(content))",
            "+",
            "+        # Content should be stored in LFS",
            "+        with self.lfs_store.open_object(pointer.oid) as f:",
            "+            self.assertEqual(f.read(), content)",
            "+",
            "+    def test_clean_existing_pointer(self) -> None:",
            "+        \"\"\"Test clean filter on already-pointer content.\"\"\"",
            "+        # Create a pointer",
            "+        pointer = LFSPointer(",
            "+            \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\", 1234",
            "+        )",
            "+        pointer_data = pointer.to_bytes()",
            "+",
            "+        # Clean should return the pointer unchanged",
            "+        result = self.filter_driver.clean(pointer_data)",
            "+        self.assertEqual(result, pointer_data)",
            "+",
            "+    def test_smudge_valid_pointer(self) -> None:",
            "+        \"\"\"Test smudge filter with valid pointer.\"\"\"",
            "+        # Store some content",
            "+        content = b\"This is the actual file content\"",
            "+        sha = self.lfs_store.write_object([content])",
            "+",
            "+        # Create pointer",
            "+        pointer = LFSPointer(sha, len(content))",
            "+        pointer_data = pointer.to_bytes()",
            "+",
            "+        # Smudge should return the actual content",
            "+        result = self.filter_driver.smudge(pointer_data)",
            "+        self.assertEqual(result, content)",
            "+",
            "+    def test_smudge_missing_object(self) -> None:",
            "+        \"\"\"Test smudge filter with missing LFS object.\"\"\"",
            "+        # Create pointer to non-existent object",
            "+        pointer = LFSPointer(",
            "+            \"0000000000000000000000000000000000000000000000000000000000000000\", 1234",
            "+        )",
            "+        pointer_data = pointer.to_bytes()",
            "+",
            "+        # Smudge should return the pointer as-is when object is missing",
            "+        result = self.filter_driver.smudge(pointer_data)",
            "+        self.assertEqual(result, pointer_data)",
            "+",
            "+    def test_smudge_non_pointer(self) -> None:",
            "+        \"\"\"Test smudge filter on non-pointer content.\"\"\"",
            "+        content = b\"This is not an LFS pointer\"",
            "+",
            "+        # Smudge should return content unchanged",
            "+        result = self.filter_driver.smudge(content)",
            "+        self.assertEqual(result, content)",
            "+",
            "+    def test_round_trip(self) -> None:",
            "+        \"\"\"Test clean followed by smudge.\"\"\"",
            "+        original_content = b\"Round trip test content\"",
            "+",
            "+        # Clean (working tree -> repo)",
            "+        pointer_data = self.filter_driver.clean(original_content)",
            "+",
            "+        # Verify it's a pointer",
            "+        pointer = LFSPointer.from_bytes(pointer_data)",
            "+        self.assertIsNotNone(pointer)",
            "+",
            "+        # Smudge (repo -> working tree)",
            "+        restored_content = self.filter_driver.smudge(pointer_data)",
            "+",
            "+        # Should get back the original content",
            "+        self.assertEqual(restored_content, original_content)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_mailmap.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_mailmap.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_mailmap.py",
            "@@ -95,7 +95,74 @@",
            "             b\"Jane Doe <jane@desktop.(none)>\",",
            "             m.lookup(b\"Jane D. <jane@desktop.(none)>\"),",
            "         )",
            "         self.assertEqual(",
            "             b\"Some Dude <some@dude.xx>\", m.lookup(b\"nick1 <bugs@company.xx>\")",
            "         )",
            "         self.assertEqual(b\"CTO <cto@company.xx>\", m.lookup(b\"CTO <cto@coompany.xx>\"))",
            "+",
            "+    def test_lookup_with_identity_tuple(self) -> None:",
            "+        \"\"\"Test lookup using an identity tuple instead of a string.\"\"\"",
            "+        m = Mailmap()",
            "+        m.add_entry(",
            "+            (b\"Real Name\", b\"real@example.com\"), (b\"Alias\", b\"alias@example.com\")",
            "+        )",
            "+",
            "+        # Test lookup with a tuple",
            "+        self.assertEqual(",
            "+            (b\"Real Name\", b\"real@example.com\"),",
            "+            m.lookup((b\"Alias\", b\"alias@example.com\")),",
            "+        )",
            "+",
            "+        # Test lookup with another tuple that doesn't match anything",
            "+        self.assertEqual(",
            "+            (b\"Unknown\", b\"unknown@example.com\"),",
            "+            m.lookup((b\"Unknown\", b\"unknown@example.com\")),",
            "+        )",
            "+",
            "+    def test_lookup_with_no_match(self) -> None:",
            "+        \"\"\"Test lookup when no match is found.\"\"\"",
            "+        m = Mailmap()",
            "+        m.add_entry(",
            "+            (b\"Real Name\", b\"real@example.com\"), (b\"Alias\", b\"alias@example.com\")",
            "+        )",
            "+",
            "+        # No match should return the original identity",
            "+        original = b\"Unknown <unknown@example.com>\"",
            "+        self.assertEqual(original, m.lookup(original))",
            "+",
            "+    def test_lookup_partial_matches(self) -> None:",
            "+        \"\"\"Test lookup with partial matches (name or email only).\"\"\"",
            "+        m = Mailmap()",
            "+        # Add entry with only name",
            "+        m.add_entry((b\"Real Name\", None), (b\"Any Name\", None))",
            "+        # Add entry with only email",
            "+        m.add_entry((None, b\"real@example.com\"), (None, b\"other@example.com\"))",
            "+",
            "+        # Match by name",
            "+        self.assertEqual(",
            "+            b\"Real Name <any@example.com>\", m.lookup(b\"Any Name <any@example.com>\")",
            "+        )",
            "+",
            "+        # Match by email",
            "+        self.assertEqual(",
            "+            b\"Any Name <real@example.com>\", m.lookup(b\"Any Name <other@example.com>\")",
            "+        )",
            "+",
            "+    def test_add_entry_name_or_email_only(self) -> None:",
            "+        \"\"\"Test adding entries with only name or only email.\"\"\"",
            "+        m = Mailmap()",
            "+",
            "+        # Entry with only canonical name",
            "+        m.add_entry((b\"Real Name\", None), (b\"Alias\", b\"alias@example.com\"))",
            "+",
            "+        # Entry with only canonical email",
            "+        m.add_entry((None, b\"real@example.com\"), (b\"Other\", b\"other@example.com\"))",
            "+",
            "+        # Lookup should properly combine the identity parts",
            "+        self.assertEqual(",
            "+            b\"Real Name <alias@example.com>\", m.lookup(b\"Alias <alias@example.com>\")",
            "+        )",
            "+",
            "+        self.assertEqual(",
            "+            b\"Other <real@example.com>\", m.lookup(b\"Other <other@example.com>\")",
            "+        )"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_object_store.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_object_store.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_object_store.py",
            "@@ -110,14 +110,50 @@",
            "         o = MemoryObjectStore()",
            " ",
            "         f = BytesIO()",
            "         entries = build_pack(f, [], store=o)",
            "         self.assertEqual([], entries)",
            "         o.add_thin_pack(f.read, None)",
            " ",
            "+    def test_add_pack_data_with_deltas(self) -> None:",
            "+        \"\"\"Test that add_pack_data properly handles delta objects.",
            "+",
            "+        This test verifies that MemoryObjectStore.add_pack_data can handle",
            "+        pack data containing delta objects. Before the fix for issue #1179,",
            "+        this would fail with AssertionError when trying to call sha_file()",
            "+        on unresolved delta objects.",
            "+",
            "+        The fix routes through add_pack() which properly resolves deltas.",
            "+        \"\"\"",
            "+        o1 = MemoryObjectStore()",
            "+        o2 = MemoryObjectStore()",
            "+        base_blob = make_object(Blob, data=b\"base data\")",
            "+        o1.add_object(base_blob)",
            "+",
            "+        # Create a pack with a delta object",
            "+        f = BytesIO()",
            "+        entries = build_pack(",
            "+            f,",
            "+            [",
            "+                (REF_DELTA, (base_blob.id, b\"more data\")),",
            "+            ],",
            "+            store=o1,",
            "+        )",
            "+",
            "+        # Use add_thin_pack which internally calls add_pack_data",
            "+        # This demonstrates the scenario where delta resolution is needed",
            "+        f.seek(0)",
            "+        o2.add_object(base_blob)  # Need base object for thin pack",
            "+        o2.add_thin_pack(f.read, None)",
            "+",
            "+        # Verify the delta object was properly resolved and added",
            "+        packed_blob_sha = sha_to_hex(entries[0][3])",
            "+        self.assertIn(packed_blob_sha, o2)",
            "+        self.assertEqual((Blob.type_num, b\"more data\"), o2.get_raw(packed_blob_sha))",
            "+",
            " ",
            " class DiskObjectStoreTests(PackBasedObjectStoreTests, TestCase):",
            "     def setUp(self) -> None:",
            "         TestCase.setUp(self)",
            "         self.store_dir = tempfile.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, self.store_dir)",
            "         self.store = DiskObjectStore.init(self.store_dir)",
            "@@ -268,47 +304,211 @@",
            "             abort()",
            "             raise",
            "         else:",
            "             commit()",
            " ",
            "     def test_add_thin_pack(self) -> None:",
            "         o = DiskObjectStore(self.store_dir)",
            "-        try:",
            "-            blob = make_object(Blob, data=b\"yummy data\")",
            "-            o.add_object(blob)",
            "+        self.addCleanup(o.close)",
            " ",
            "-            f = BytesIO()",
            "-            entries = build_pack(",
            "-                f,",
            "-                [",
            "-                    (REF_DELTA, (blob.id, b\"more yummy data\")),",
            "-                ],",
            "-                store=o,",
            "-            )",
            "+        blob = make_object(Blob, data=b\"yummy data\")",
            "+        o.add_object(blob)",
            " ",
            "-            with o.add_thin_pack(f.read, None) as pack:",
            "-                packed_blob_sha = sha_to_hex(entries[0][3])",
            "-                pack.check_length_and_checksum()",
            "-                self.assertEqual(sorted([blob.id, packed_blob_sha]), list(pack))",
            "-                self.assertTrue(o.contains_packed(packed_blob_sha))",
            "-                self.assertTrue(o.contains_packed(blob.id))",
            "-                self.assertEqual(",
            "-                    (Blob.type_num, b\"more yummy data\"),",
            "-                    o.get_raw(packed_blob_sha),",
            "-                )",
            "-        finally:",
            "-            o.close()",
            "+        f = BytesIO()",
            "+        entries = build_pack(",
            "+            f,",
            "+            [",
            "+                (REF_DELTA, (blob.id, b\"more yummy data\")),",
            "+            ],",
            "+            store=o,",
            "+        )",
            "+",
            "+        with o.add_thin_pack(f.read, None) as pack:",
            "+            packed_blob_sha = sha_to_hex(entries[0][3])",
            "+            pack.check_length_and_checksum()",
            "+            self.assertEqual(sorted([blob.id, packed_blob_sha]), list(pack))",
            "+            self.assertTrue(o.contains_packed(packed_blob_sha))",
            "+            self.assertTrue(o.contains_packed(blob.id))",
            "+            self.assertEqual(",
            "+                (Blob.type_num, b\"more yummy data\"),",
            "+                o.get_raw(packed_blob_sha),",
            "+            )",
            " ",
            "     def test_add_thin_pack_empty(self) -> None:",
            "         with closing(DiskObjectStore(self.store_dir)) as o:",
            "             f = BytesIO()",
            "             entries = build_pack(f, [], store=o)",
            "             self.assertEqual([], entries)",
            "             o.add_thin_pack(f.read, None)",
            " ",
            "+    def test_pack_index_version_config(self) -> None:",
            "+        # Test that pack.indexVersion configuration is respected",
            "+        from dulwich.config import ConfigDict",
            "+        from dulwich.pack import load_pack_index",
            "+",
            "+        # Create config with pack.indexVersion = 1",
            "+        config = ConfigDict()",
            "+        config[(b\"pack\",)] = {b\"indexVersion\": b\"1\"}",
            "+",
            "+        # Create object store with config",
            "+        store_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, store_dir)",
            "+        os.makedirs(os.path.join(store_dir, \"pack\"))",
            "+        store = DiskObjectStore.from_config(store_dir, config)",
            "+        self.addCleanup(store.close)",
            "+",
            "+        # Create some objects to pack",
            "+        b1 = make_object(Blob, data=b\"blob1\")",
            "+        b2 = make_object(Blob, data=b\"blob2\")",
            "+        store.add_objects([(b1, None), (b2, None)])",
            "+",
            "+        # Add a pack",
            "+        f, commit, abort = store.add_pack()",
            "+        try:",
            "+            # build_pack expects (type_num, data) tuples",
            "+            objects_spec = [",
            "+                (b1.type_num, b1.as_raw_string()),",
            "+                (b2.type_num, b2.as_raw_string()),",
            "+            ]",
            "+            build_pack(f, objects_spec, store=store)",
            "+            commit()",
            "+        except:",
            "+            abort()",
            "+            raise",
            "+",
            "+        # Find the created pack index",
            "+        pack_dir = os.path.join(store_dir, \"pack\")",
            "+        idx_files = [f for f in os.listdir(pack_dir) if f.endswith(\".idx\")]",
            "+        self.assertEqual(1, len(idx_files))",
            "+",
            "+        # Load and verify it's version 1",
            "+        idx_path = os.path.join(pack_dir, idx_files[0])",
            "+        idx = load_pack_index(idx_path)",
            "+        self.assertEqual(1, idx.version)",
            "+",
            "+        # Test version 3",
            "+        config[(b\"pack\",)] = {b\"indexVersion\": b\"3\"}",
            "+        store_dir2 = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, store_dir2)",
            "+        os.makedirs(os.path.join(store_dir2, \"pack\"))",
            "+        store2 = DiskObjectStore.from_config(store_dir2, config)",
            "+        self.addCleanup(store2.close)",
            "+",
            "+        b3 = make_object(Blob, data=b\"blob3\")",
            "+        store2.add_objects([(b3, None)])",
            "+",
            "+        f2, commit2, abort2 = store2.add_pack()",
            "+        try:",
            "+            objects_spec2 = [(b3.type_num, b3.as_raw_string())]",
            "+            build_pack(f2, objects_spec2, store=store2)",
            "+            commit2()",
            "+        except:",
            "+            abort2()",
            "+            raise",
            "+",
            "+        # Find and verify version 3 index",
            "+        pack_dir2 = os.path.join(store_dir2, \"pack\")",
            "+        idx_files2 = [f for f in os.listdir(pack_dir2) if f.endswith(\".idx\")]",
            "+        self.assertEqual(1, len(idx_files2))",
            "+",
            "+        idx_path2 = os.path.join(pack_dir2, idx_files2[0])",
            "+        idx2 = load_pack_index(idx_path2)",
            "+        self.assertEqual(3, idx2.version)",
            "+",
            "+    def test_prune_orphaned_tempfiles(self) -> None:",
            "+        import time",
            "+",
            "+        # Create an orphaned temporary pack file in the repository directory",
            "+        tmp_pack_path = os.path.join(self.store_dir, \"tmp_pack_test123\")",
            "+        with open(tmp_pack_path, \"wb\") as f:",
            "+            f.write(b\"temporary pack data\")",
            "+",
            "+        # Create an orphaned .pack file without .idx in pack directory",
            "+        pack_dir = os.path.join(self.store_dir, \"pack\")",
            "+        orphaned_pack_path = os.path.join(pack_dir, \"pack-orphaned.pack\")",
            "+        with open(orphaned_pack_path, \"wb\") as f:",
            "+            f.write(b\"orphaned pack data\")",
            "+",
            "+        # Make files appear old by modifying mtime (older than grace period)",
            "+        from dulwich.object_store import DEFAULT_TEMPFILE_GRACE_PERIOD",
            "+",
            "+        old_time = time.time() - (",
            "+            DEFAULT_TEMPFILE_GRACE_PERIOD + 3600",
            "+        )  # grace period + 1 hour",
            "+        os.utime(tmp_pack_path, (old_time, old_time))",
            "+        os.utime(orphaned_pack_path, (old_time, old_time))",
            "+",
            "+        # Create a recent temporary file that should NOT be cleaned",
            "+        recent_tmp_path = os.path.join(self.store_dir, \"tmp_pack_recent\")",
            "+        with open(recent_tmp_path, \"wb\") as f:",
            "+            f.write(b\"recent temp data\")",
            "+",
            "+        # Run prune",
            "+        self.store.prune()",
            "+",
            "+        # Check that old orphaned files were removed",
            "+        self.assertFalse(os.path.exists(tmp_pack_path))",
            "+        self.assertFalse(os.path.exists(orphaned_pack_path))",
            "+",
            "+        # Check that recent file was NOT removed",
            "+        self.assertTrue(os.path.exists(recent_tmp_path))",
            "+",
            "+        # Cleanup the recent file",
            "+        os.remove(recent_tmp_path)",
            "+",
            "+    def test_prune_with_custom_grace_period(self) -> None:",
            "+        \"\"\"Test that prune respects custom grace period.\"\"\"",
            "+        import time",
            "+",
            "+        # Create a temporary file that's 1 hour old",
            "+        tmp_pack_path = os.path.join(self.store_dir, \"tmp_pack_1hour\")",
            "+        with open(tmp_pack_path, \"wb\") as f:",
            "+            f.write(b\"1 hour old data\")",
            "+",
            "+        # Make it 1 hour old",
            "+        old_time = time.time() - 3600  # 1 hour ago",
            "+        os.utime(tmp_pack_path, (old_time, old_time))",
            "+",
            "+        # Prune with default grace period (2 weeks) - should NOT remove",
            "+        self.store.prune()",
            "+        self.assertTrue(os.path.exists(tmp_pack_path))",
            "+",
            "+        # Prune with 30 minute grace period - should remove",
            "+        self.store.prune(grace_period=1800)  # 30 minutes",
            "+        self.assertFalse(os.path.exists(tmp_pack_path))",
            "+",
            "+    def test_gc_prunes_tempfiles(self) -> None:",
            "+        \"\"\"Test that garbage collection prunes temporary files.\"\"\"",
            "+        import time",
            "+",
            "+        from dulwich.gc import garbage_collect",
            "+        from dulwich.repo import Repo",
            "+",
            "+        # Create a repository with the store",
            "+        repo = Repo.init(self.store_dir)",
            "+",
            "+        # Create an old orphaned temporary file in the objects directory",
            "+        tmp_pack_path = os.path.join(repo.object_store.path, \"tmp_pack_old\")",
            "+        with open(tmp_pack_path, \"wb\") as f:",
            "+            f.write(b\"old temporary data\")",
            "+",
            "+        # Make it old (older than grace period)",
            "+        from dulwich.object_store import DEFAULT_TEMPFILE_GRACE_PERIOD",
            "+",
            "+        old_time = time.time() - (",
            "+            DEFAULT_TEMPFILE_GRACE_PERIOD + 3600",
            "+        )  # grace period + 1 hour",
            "+        os.utime(tmp_pack_path, (old_time, old_time))",
            "+",
            "+        # Run garbage collection",
            "+        garbage_collect(repo)",
            "+",
            "+        # Verify the orphaned file was cleaned up",
            "+        self.assertFalse(os.path.exists(tmp_pack_path))",
            "+",
            " ",
            " class TreeLookupPathTests(TestCase):",
            "     def setUp(self) -> None:",
            "         TestCase.setUp(self)",
            "         self.store = MemoryObjectStore()",
            "         blob_a = make_object(Blob, data=b\"a\")",
            "         blob_b = make_object(Blob, data=b\"b\")"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_objects.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_objects.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_objects.py",
            "@@ -613,19 +613,25 @@",
            "                 b\"ab64bbdcc51b170d21588e5c5d391ee5c0c96dfd\",",
            "                 b\"4cffe90e0a41ad3f5190079d7c8f036bde29cbe6\",",
            "             ],",
            "             c.parents,",
            "         )",
            "         expected_time = datetime.datetime(2007, 3, 24, 22, 1, 59)",
            "         self.assertEqual(",
            "-            expected_time, datetime.datetime.utcfromtimestamp(c.commit_time)",
            "+            expected_time,",
            "+            datetime.datetime.fromtimestamp(",
            "+                c.commit_time, datetime.timezone.utc",
            "+            ).replace(tzinfo=None),",
            "         )",
            "         self.assertEqual(0, c.commit_timezone)",
            "         self.assertEqual(",
            "-            expected_time, datetime.datetime.utcfromtimestamp(c.author_time)",
            "+            expected_time,",
            "+            datetime.datetime.fromtimestamp(",
            "+                c.author_time, datetime.timezone.utc",
            "+            ).replace(tzinfo=None),",
            "         )",
            "         self.assertEqual(0, c.author_timezone)",
            "         self.assertEqual(None, c.encoding)",
            " ",
            "     def test_custom(self) -> None:",
            "         c = Commit.from_string(self.make_commit_text(extra={b\"extra-field\": b\"data\"}))",
            "         self.assertEqual([(b\"extra-field\", b\"data\")], c._extra)",
            "@@ -998,14 +1004,90 @@",
            "         _do_test_sorted_tree_items_name_order, _sorted_tree_items_py",
            "     )",
            "     if _sorted_tree_items_rs is not None:",
            "         test_sorted_tree_items_name_order_extension = ext_functest_builder(",
            "             _do_test_sorted_tree_items_name_order, _sorted_tree_items_rs",
            "         )",
            " ",
            "+    def _do_test_sorted_tree_items_issue_1325(self, sorted_tree_items) -> None:",
            "+        \"\"\"Test case to reproduce issue #1325: submodules incorrectly sorted as directories.",
            "+",
            "+        The bug: Rust uses (mode & 0o40000 != 0) which incorrectly matches",
            "+        submodules (0o160000) since 0o160000 & 0o40000 = 0o40000",
            "+        \"\"\"",
            "+        # Test case 1: Minimal test - submodule vs file",
            "+        entries = {",
            "+            b\"sub\": (",
            "+                0o160000,",
            "+                b\"a03f423a81b39df39dc87fd269736ca86d80c186\",",
            "+            ),  # submodule",
            "+            b\"sub.txt\": (0o100644, b\"81b39df39dc87fd269736ca86d80c186a03f423a\"),  # file",
            "+        }",
            "+",
            "+        result = list(sorted_tree_items(entries, False))",
            "+        paths = [entry.path for entry in result]",
            "+",
            "+        # Submodules should sort as regular files, not directories",
            "+        # Expected order: sub, sub.txt",
            "+        # Bug causes: sub.txt, sub (because sub is treated as sub/)",
            "+        self.assertEqual([b\"sub\", b\"sub.txt\"], paths)",
            "+",
            "+        # Test case 2: Scenario from issue - file rename + submodule",
            "+        # This simulates the \"gamma\" scenario mentioned in the issue",
            "+        entries2 = {",
            "+            b\"alpha\": (0o100644, b\"a03f423a81b39df39dc87fd269736ca86d80c186\"),",
            "+            b\"beta\": (0o100644, b\"81b39df39dc87fd269736ca86d80c186a03f423a\"),",
            "+            b\"gamma\": (",
            "+                0o160000,",
            "+                b\"d80c186a03f423a81b39df39dc87fd269736ca86\",",
            "+            ),  # submodule (was file)",
            "+            b\"delta\": (0o100644, b\"cf7a729ca69bfabd0995fc9b083e86a18215bd91\"),",
            "+        }",
            "+",
            "+        result2 = list(sorted_tree_items(entries2, False))",
            "+        paths2 = [entry.path for entry in result2]",
            "+",
            "+        # All entries should sort in alphabetical order since none are directories",
            "+        self.assertEqual([b\"alpha\", b\"beta\", b\"delta\", b\"gamma\"], paths2)",
            "+",
            "+    test_sorted_tree_items_issue_1325 = functest_builder(",
            "+        _do_test_sorted_tree_items_issue_1325, _sorted_tree_items_py",
            "+    )",
            "+    if _sorted_tree_items_rs is not None:",
            "+        test_sorted_tree_items_issue_1325_extension = ext_functest_builder(",
            "+            _do_test_sorted_tree_items_issue_1325, _sorted_tree_items_rs",
            "+        )",
            "+",
            "+    def test_sorted_tree_items_issue_1325_comparison(self) -> None:",
            "+        \"\"\"Direct comparison test to show the difference between Python and Rust implementations.\"\"\"",
            "+        if _sorted_tree_items_rs is None:",
            "+            self.skipTest(\"Rust extension not available\")",
            "+",
            "+        # Minimal test case: submodule vs file",
            "+        entries = {",
            "+            b\"sub\": (",
            "+                0o160000,",
            "+                b\"a03f423a81b39df39dc87fd269736ca86d80c186\",",
            "+            ),  # submodule",
            "+            b\"sub.txt\": (0o100644, b\"81b39df39dc87fd269736ca86d80c186a03f423a\"),  # file",
            "+        }",
            "+",
            "+        # Get results from both implementations",
            "+        py_result = list(_sorted_tree_items_py(entries, False))",
            "+        rs_result = list(_sorted_tree_items_rs(entries, False))",
            "+",
            "+        # Show the actual ordering from each",
            "+        py_paths = [entry.path for entry in py_result]",
            "+        rs_paths = [entry.path for entry in rs_result]",
            "+",
            "+        # This test shows the bug: Rust treats submodules as directories",
            "+        self.assertEqual(",
            "+            py_paths, rs_paths, \"Bug: Rust treats submodules (0o160000) as directories\"",
            "+        )",
            "+",
            "     def test_check(self) -> None:",
            "         t = Tree",
            "         sha = hex_to_sha(a_sha)",
            " ",
            "         # filenames",
            "         self.assertCheckSucceeds(t, b\"100644 .a\\0\" + sha)",
            "         self.assertCheckFails(t, b\"100644 \\0\" + sha)",
            "@@ -1085,15 +1167,15 @@",
            "                 b\"423423423 +0000\\n\\n\"",
            "             ),",
            "             x.as_raw_string(),",
            "         )",
            " ",
            " ",
            " default_tagger = (",
            "-    b\"Linus Torvalds <torvalds@woody.linux-foundation.org> \" b\"1183319674 -0700\"",
            "+    b\"Linus Torvalds <torvalds@woody.linux-foundation.org> 1183319674 -0700\"",
            " )",
            " default_message = b\"\"\"Linux 2.6.22-rc7",
            " -----BEGIN PGP SIGNATURE-----",
            " Version: GnuPG v1.4.7 (GNU/Linux)",
            " ",
            " iD8DBQBGiAaAF3YsRnbiHLsRAitMAKCiLboJkQECM/jpYsY3WPfvUgLXkACgg3ql",
            " OK2XeQOiEeXtT76rV4t2WR4=",
            "@@ -1135,15 +1217,17 @@",
            "             b\"Linus Torvalds <torvalds@woody.linux-foundation.org>\", x.tagger",
            "         )",
            "         self.assertEqual(b\"v2.6.22-rc7\", x.name)",
            "         object_type, object_sha = x.object",
            "         self.assertEqual(b\"a38d6181ff27824c79fc7df825164a212eff6a3f\", object_sha)",
            "         self.assertEqual(Commit, object_type)",
            "         self.assertEqual(",
            "-            datetime.datetime.utcfromtimestamp(x.tag_time),",
            "+            datetime.datetime.fromtimestamp(x.tag_time, datetime.timezone.utc).replace(",
            "+                tzinfo=None",
            "+            ),",
            "             datetime.datetime(2007, 7, 1, 19, 54, 34),",
            "         )",
            "         self.assertEqual(-25200, x.tag_timezone)",
            " ",
            "     def test_parse_no_tagger(self) -> None:",
            "         x = Tag()",
            "         x.set_raw_string(self.make_tag_text(tagger=None))",
            "@@ -1155,15 +1239,17 @@",
            "         x = Tag()",
            "         x.set_raw_string(self.make_tag_text(message=None))",
            "         self.assertEqual(None, x.message)",
            "         self.assertEqual(",
            "             b\"Linus Torvalds <torvalds@woody.linux-foundation.org>\", x.tagger",
            "         )",
            "         self.assertEqual(",
            "-            datetime.datetime.utcfromtimestamp(x.tag_time),",
            "+            datetime.datetime.fromtimestamp(x.tag_time, datetime.timezone.utc).replace(",
            "+                tzinfo=None",
            "+            ),",
            "             datetime.datetime(2007, 7, 1, 19, 54, 34),",
            "         )",
            "         self.assertEqual(-25200, x.tag_timezone)",
            "         self.assertEqual(b\"v2.6.22-rc7\", x.name)",
            " ",
            "     def test_check(self) -> None:",
            "         self.assertCheckSucceeds(Tag, self.make_tag_text())",
            "@@ -1190,16 +1276,15 @@",
            "         self.assertCheckFails(Tag, self.make_tag_text(object_sha=b\"xxx\"))",
            " ",
            "     def test_check_tag_with_unparseable_field(self) -> None:",
            "         self.assertCheckFails(",
            "             Tag,",
            "             self.make_tag_text(",
            "                 tagger=(",
            "-                    b\"Linus Torvalds <torvalds@woody.linux-foundation.org> \"",
            "-                    b\"423423+0000\"",
            "+                    b\"Linus Torvalds <torvalds@woody.linux-foundation.org> 423423+0000\"",
            "                 )",
            "             ),",
            "         )",
            " ",
            "     def test_check_tag_with_overflow_time(self) -> None:",
            "         \"\"\"Date with overflow should raise an ObjectFormatException when checked.\"\"\"",
            "         author = f\"Some Dude <some@dude.org> {MAX_TIME + 1} +0000\"",
            "@@ -1373,25 +1458,25 @@",
            "     def test_format_timezone_pdt(self) -> None:",
            "         self.assertEqual(b\"-0400\", format_timezone(-4 * 60 * 60))",
            " ",
            "     def test_parse_timezone_pdt(self) -> None:",
            "         self.assertEqual((-4 * 60 * 60, False), parse_timezone(b\"-0400\"))",
            " ",
            "     def test_format_timezone_pdt_half(self) -> None:",
            "-        self.assertEqual(b\"-0440\", format_timezone(int(((-4 * 60) - 40) * 60)))",
            "+        self.assertEqual(b\"-0440\", format_timezone(((-4 * 60) - 40) * 60))",
            " ",
            "     def test_format_timezone_double_negative(self) -> None:",
            "-        self.assertEqual(b\"--700\", format_timezone(int((7 * 60) * 60), True))",
            "+        self.assertEqual(b\"--700\", format_timezone(((7 * 60) * 60), True))",
            " ",
            "     def test_parse_timezone_pdt_half(self) -> None:",
            "         self.assertEqual((((-4 * 60) - 40) * 60, False), parse_timezone(b\"-0440\"))",
            " ",
            "     def test_parse_timezone_double_negative(self) -> None:",
            "-        self.assertEqual((int((7 * 60) * 60), False), parse_timezone(b\"+700\"))",
            "-        self.assertEqual((int((7 * 60) * 60), True), parse_timezone(b\"--700\"))",
            "+        self.assertEqual((((7 * 60) * 60), False), parse_timezone(b\"+700\"))",
            "+        self.assertEqual((((7 * 60) * 60), True), parse_timezone(b\"--700\"))",
            " ",
            " ",
            " class ShaFileCopyTests(TestCase):",
            "     def assert_copy(self, orig) -> None:",
            "         oclass = object_class(orig.type_num)",
            " ",
            "         copy = orig.copy()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_objectspec.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_objectspec.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_objectspec.py",
            "@@ -19,15 +19,15 @@",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Tests for revision spec parsing.\"\"\"",
            " ",
            " # TODO: Round-trip parse-serialize-parse and serialize-parse-serialize tests.",
            " ",
            "-from dulwich.objects import Blob",
            "+from dulwich.objects import Blob, Commit, Tag",
            " from dulwich.objectspec import (",
            "     parse_commit,",
            "     parse_commit_range,",
            "     parse_object,",
            "     parse_ref,",
            "     parse_refs,",
            "     parse_reftuple,",
            "@@ -80,14 +80,90 @@",
            "         self.assertEqual(c1, parse_commit(r, c1.id))",
            " ",
            "     def test_commit_by_short_sha(self) -> None:",
            "         r = MemoryRepo()",
            "         [c1] = build_commit_graph(r.object_store, [[1]])",
            "         self.assertEqual(c1, parse_commit(r, c1.id[:10]))",
            " ",
            "+    def test_annotated_tag(self) -> None:",
            "+        r = MemoryRepo()",
            "+        [c1] = build_commit_graph(r.object_store, [[1]])",
            "+        # Create an annotated tag pointing to the commit",
            "+        tag = Tag()",
            "+        tag.name = b\"v1.0\"",
            "+        tag.message = b\"Test tag\"",
            "+        tag.tag_time = 1234567890",
            "+        tag.tag_timezone = 0",
            "+        tag.object = (Commit, c1.id)",
            "+        tag.tagger = b\"Test Tagger <test@example.com>\"",
            "+        r.object_store.add_object(tag)",
            "+        # parse_commit should follow the tag to the commit",
            "+        self.assertEqual(c1, parse_commit(r, tag.id))",
            "+",
            "+    def test_nested_tags(self) -> None:",
            "+        r = MemoryRepo()",
            "+        [c1] = build_commit_graph(r.object_store, [[1]])",
            "+        # Create an annotated tag pointing to the commit",
            "+        tag1 = Tag()",
            "+        tag1.name = b\"v1.0\"",
            "+        tag1.message = b\"Test tag\"",
            "+        tag1.tag_time = 1234567890",
            "+        tag1.tag_timezone = 0",
            "+        tag1.object = (Commit, c1.id)",
            "+        tag1.tagger = b\"Test Tagger <test@example.com>\"",
            "+        r.object_store.add_object(tag1)",
            "+",
            "+        # Create another tag pointing to the first tag",
            "+        tag2 = Tag()",
            "+        tag2.name = b\"v1.0-release\"",
            "+        tag2.message = b\"Release tag\"",
            "+        tag2.tag_time = 1234567900",
            "+        tag2.tag_timezone = 0",
            "+        tag2.object = (Tag, tag1.id)",
            "+        tag2.tagger = b\"Test Tagger <test@example.com>\"",
            "+        r.object_store.add_object(tag2)",
            "+",
            "+        # parse_commit should follow both tags to the commit",
            "+        self.assertEqual(c1, parse_commit(r, tag2.id))",
            "+",
            "+    def test_tag_to_missing_commit(self) -> None:",
            "+        r = MemoryRepo()",
            "+        # Create a tag pointing to a non-existent commit",
            "+        missing_sha = b\"1234567890123456789012345678901234567890\"",
            "+        tag = Tag()",
            "+        tag.name = b\"v1.0\"",
            "+        tag.message = b\"Test tag\"",
            "+        tag.tag_time = 1234567890",
            "+        tag.tag_timezone = 0",
            "+        tag.object = (Commit, missing_sha)",
            "+        tag.tagger = b\"Test Tagger <test@example.com>\"",
            "+        r.object_store.add_object(tag)",
            "+",
            "+        # Should raise KeyError for missing commit",
            "+        self.assertRaises(KeyError, parse_commit, r, tag.id)",
            "+",
            "+    def test_tag_to_blob(self) -> None:",
            "+        r = MemoryRepo()",
            "+        # Create a blob",
            "+        blob = Blob.from_string(b\"Test content\")",
            "+        r.object_store.add_object(blob)",
            "+",
            "+        # Create a tag pointing to the blob",
            "+        tag = Tag()",
            "+        tag.name = b\"blob-tag\"",
            "+        tag.message = b\"Tag pointing to blob\"",
            "+        tag.tag_time = 1234567890",
            "+        tag.tag_timezone = 0",
            "+        tag.object = (Blob, blob.id)",
            "+        tag.tagger = b\"Test Tagger <test@example.com>\"",
            "+        r.object_store.add_object(tag)",
            "+",
            "+        # Should raise ValueError as it's not a commit",
            "+        self.assertRaises(ValueError, parse_commit, r, tag.id)",
            "+",
            " ",
            " class ParseRefTests(TestCase):",
            "     def test_nonexistent(self) -> None:",
            "         r = {}",
            "         self.assertRaises(KeyError, parse_ref, r, b\"thisdoesnotexist\")",
            " ",
            "     def test_ambiguous_ref(self) -> None:"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_pack.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_pack.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_pack.py",
            "@@ -38,14 +38,15 @@",
            " from dulwich.pack import (",
            "     OFS_DELTA,",
            "     REF_DELTA,",
            "     DeltaChainIterator,",
            "     MemoryPackIndex,",
            "     Pack,",
            "     PackData,",
            "+    PackIndex3,",
            "     PackStreamReader,",
            "     UnpackedObject,",
            "     UnresolvedDeltas,",
            "     _delta_encode_size,",
            "     _encode_copy_operation,",
            "     apply_delta,",
            "     compute_file_sha,",
            "@@ -54,14 +55,15 @@",
            "     load_pack_index,",
            "     read_zlib_chunks,",
            "     unpack_object,",
            "     write_pack,",
            "     write_pack_header,",
            "     write_pack_index_v1,",
            "     write_pack_index_v2,",
            "+    write_pack_index_v3,",
            "     write_pack_object,",
            " )",
            " from dulwich.tests.utils import build_pack, make_object",
            " ",
            " from . import TestCase",
            " ",
            " pack1_sha = b\"bc63ddad95e7321ee734ea11a7a62d314e0d7481\"",
            "@@ -216,14 +218,43 @@",
            "             b\"a\" * 0x10000,",
            "             b\"\\x80\\x80\\x04\\x80\\x80\\x04\\x80\" + b\"a\" * 0x10000,",
            "         )",
            "         self.assertRaises(",
            "             ApplyDeltaError, apply_delta, b\"\", b\"\\x00\\x80\\x02\\xb0\\x11\\x11\"",
            "         )",
            " ",
            "+    def test_apply_delta_invalid_opcode(self) -> None:",
            "+        \"\"\"Test apply_delta with an invalid opcode.\"\"\"",
            "+        # Create a delta with an invalid opcode (0xff is not valid)",
            "+        invalid_delta = [b\"\\xff\\x01\\x02\"]",
            "+        base = b\"test base\"",
            "+",
            "+        # Should raise ApplyDeltaError",
            "+        self.assertRaises(ApplyDeltaError, apply_delta, base, invalid_delta)",
            "+",
            "+    def test_create_delta_insert_only(self) -> None:",
            "+        \"\"\"Test create_delta when only insertions are required.\"\"\"",
            "+        base = b\"\"",
            "+        target = b\"brand new content\"",
            "+        delta = list(create_delta(base, target))",
            "+",
            "+        # Apply the delta to verify it works correctly",
            "+        result = apply_delta(base, delta)",
            "+        self.assertEqual(target, b\"\".join(result))",
            "+",
            "+    def test_create_delta_copy_only(self) -> None:",
            "+        \"\"\"Test create_delta when only copy operations are required.\"\"\"",
            "+        base = b\"content to be copied\"",
            "+        target = b\"content to be copied\"  # Identical to base",
            "+        delta = list(create_delta(base, target))",
            "+",
            "+        # Apply the delta to verify",
            "+        result = apply_delta(base, delta)",
            "+        self.assertEqual(target, b\"\".join(result))",
            "+",
            "     def test_pypy_issue(self) -> None:",
            "         # Test for https://github.com/jelmer/dulwich/issues/509 /",
            "         # https://bitbucket.org/pypy/pypy/issues/2499/cpyext-pystring_asstring-doesnt-work",
            "         chunks = [",
            "             b\"tree 03207ccf58880a748188836155ceed72f03d65d6\\n\"",
            "             b\"parent 408fbab530fd4abe49249a636a10f10f44d07a21\\n\"",
            "             b\"author Victor Stinner <victor.stinner@gmail.com> \"",
            "@@ -245,15 +276,15 @@",
            "         ]",
            "         res = apply_delta(chunks, delta)",
            "         expected = [",
            "             b\"tree ff3c181a393d5a7270cddc01ea863818a8621ca8\\n\"",
            "             b\"parent 20a103cc90135494162e819f98d0edfc1f1fba6b\",",
            "             b\"\\nauthor Victor Stinner <victor.stinner@gmail.com> 14213\",",
            "             b\"10738\",",
            "-            b\" +0100\\ncommitter Victor Stinner <victor.stinner@gmail.com> \" b\"14213\",",
            "+            b\" +0100\\ncommitter Victor Stinner <victor.stinner@gmail.com> 14213\",",
            "             b\"10738 +0100\",",
            "             b\"\\n\\nStreamWriter: close() now clears the reference to the \"",
            "             b\"transport\\n\\n\"",
            "             b\"StreamWriter now raises an exception if it is closed: \"",
            "             b\"write(), writelines(),\\n\"",
            "             b\"write_eof(), can_write_eof(), get_extra_info(), drain().\\n\",",
            "         ]",
            "@@ -277,14 +308,31 @@",
            "         with self.get_pack_data(pack1_sha) as p:",
            "             self.assertEqual(3, len(p))",
            " ",
            "     def test_index_check(self) -> None:",
            "         with self.get_pack_data(pack1_sha) as p:",
            "             self.assertSucceeds(p.check)",
            " ",
            "+    def test_get_stored_checksum(self) -> None:",
            "+        \"\"\"Test getting the stored checksum of the pack data.\"\"\"",
            "+        with self.get_pack_data(pack1_sha) as p:",
            "+            checksum = p.get_stored_checksum()",
            "+            self.assertEqual(20, len(checksum))",
            "+            # Verify it's a valid SHA1 hash (20 bytes)",
            "+            self.assertIsInstance(checksum, bytes)",
            "+",
            "+    # Removed test_check_pack_data_size as it was accessing private attributes",
            "+",
            "+    def test_close_twice(self) -> None:",
            "+        \"\"\"Test that calling close multiple times is safe.\"\"\"",
            "+        p = self.get_pack_data(pack1_sha)",
            "+        p.close()",
            "+        # Second close should not raise an exception",
            "+        p.close()",
            "+",
            "     def test_iter_unpacked(self) -> None:",
            "         with self.get_pack_data(pack1_sha) as p:",
            "             commit_data = (",
            "                 b\"tree b2a2766a2879c209ab1176e7e778b81ae422eeaa\\n\"",
            "                 b\"author James Westby <jw+debian@jameswestby.net> \"",
            "                 b\"1174945067 +0100\\n\"",
            "                 b\"committer James Westby <jw+debian@jameswestby.net> \"",
            "@@ -357,14 +405,33 @@",
            "             filename = os.path.join(self.tempdir, \"v2test.idx\")",
            "             p.create_index_v2(filename)",
            "             idx1 = load_pack_index(filename)",
            "             idx2 = self.get_pack_index(pack1_sha)",
            "             self.assertEqual(oct(os.stat(filename).st_mode), indexmode)",
            "             self.assertEqual(idx1, idx2)",
            " ",
            "+    def test_create_index_v3(self) -> None:",
            "+        with self.get_pack_data(pack1_sha) as p:",
            "+            filename = os.path.join(self.tempdir, \"v3test.idx\")",
            "+            p.create_index_v3(filename)",
            "+            idx1 = load_pack_index(filename)",
            "+            idx2 = self.get_pack_index(pack1_sha)",
            "+            self.assertEqual(oct(os.stat(filename).st_mode), indexmode)",
            "+            self.assertEqual(idx1, idx2)",
            "+            self.assertIsInstance(idx1, PackIndex3)",
            "+            self.assertEqual(idx1.version, 3)",
            "+",
            "+    def test_create_index_version3(self) -> None:",
            "+        with self.get_pack_data(pack1_sha) as p:",
            "+            filename = os.path.join(self.tempdir, \"version3test.idx\")",
            "+            p.create_index(filename, version=3)",
            "+            idx = load_pack_index(filename)",
            "+            self.assertIsInstance(idx, PackIndex3)",
            "+            self.assertEqual(idx.version, 3)",
            "+",
            "     def test_compute_file_sha(self) -> None:",
            "         f = BytesIO(b\"abcd1234wxyz\")",
            "         self.assertEqual(",
            "             sha1(b\"abcd1234wxyz\").hexdigest(), compute_file_sha(f).hexdigest()",
            "         )",
            "         self.assertEqual(",
            "             sha1(b\"abcd1234wxyz\").hexdigest(),",
            "@@ -418,14 +485,16 @@",
            "         with self.get_pack(pack1_sha) as p:",
            "             tuples = p.pack_tuples()",
            "             expected = {(p[s], None) for s in [commit_sha, tree_sha, a_sha]}",
            "             self.assertEqual(expected, set(list(tuples)))",
            "             self.assertEqual(expected, set(list(tuples)))",
            "             self.assertEqual(3, len(tuples))",
            " ",
            "+    # Removed test_pack_tuples_with_progress as it was using parameters not supported by the API",
            "+",
            "     def test_get_object_at(self) -> None:",
            "         \"\"\"Tests random access for non-delta objects.\"\"\"",
            "         with self.get_pack(pack1_sha) as p:",
            "             obj = p[a_sha]",
            "             self.assertEqual(obj.type_name, b\"blob\")",
            "             self.assertEqual(obj.sha().hexdigest().encode(\"ascii\"), a_sha)",
            "             obj = p[tree_sha]",
            "@@ -538,14 +607,40 @@",
            " ",
            "     def test_iterobjects_subset(self) -> None:",
            "         with self.get_pack(pack1_sha) as p:",
            "             objs = {o.id: o for o in p.iterobjects_subset([commit_sha])}",
            "             self.assertEqual(1, len(objs))",
            "             self.assertIsInstance(objs[commit_sha], Commit)",
            " ",
            "+    def test_iterobjects_subset_empty(self) -> None:",
            "+        \"\"\"Test iterobjects_subset with an empty subset.\"\"\"",
            "+        with self.get_pack(pack1_sha) as p:",
            "+            objs = list(p.iterobjects_subset([]))",
            "+            self.assertEqual(0, len(objs))",
            "+",
            "+    def test_iterobjects_subset_nonexistent(self) -> None:",
            "+        \"\"\"Test iterobjects_subset with non-existent object IDs.\"\"\"",
            "+        with self.get_pack(pack1_sha) as p:",
            "+            # Create a fake SHA that doesn't exist in the pack",
            "+            fake_sha = b\"1\" * 40",
            "+",
            "+            # KeyError is expected when trying to access a non-existent object",
            "+            # We'll use a try-except block to test the behavior",
            "+            try:",
            "+                list(p.iterobjects_subset([fake_sha]))",
            "+                self.fail(\"Expected KeyError when accessing non-existent object\")",
            "+            except KeyError:",
            "+                pass  # This is the expected behavior",
            "+",
            "+    def test_check_length_and_checksum(self) -> None:",
            "+        \"\"\"Test that check_length_and_checksum works correctly.\"\"\"",
            "+        with self.get_pack(pack1_sha) as p:",
            "+            # This should not raise an exception",
            "+            p.check_length_and_checksum()",
            "+",
            " ",
            " class TestThinPack(PackTests):",
            "     def setUp(self) -> None:",
            "         super().setUp()",
            "         self.store = MemoryObjectStore()",
            "         self.blobs = {}",
            "         for blob in (b\"foo\", b\"bar\", b\"foo1234\", b\"bar2468\"):",
            "@@ -800,14 +895,189 @@",
            "         self._write_fn = write_pack_index_v2",
            " ",
            "     def tearDown(self) -> None:",
            "         TestCase.tearDown(self)",
            "         BaseTestFilePackIndexWriting.tearDown(self)",
            " ",
            " ",
            "+class TestPackIndexWritingv3(TestCase, BaseTestFilePackIndexWriting):",
            "+    def setUp(self) -> None:",
            "+        TestCase.setUp(self)",
            "+        BaseTestFilePackIndexWriting.setUp(self)",
            "+        self._has_crc32_checksum = True",
            "+        self._supports_large = True",
            "+        self._expected_version = 3",
            "+        self._write_fn = write_pack_index_v3",
            "+",
            "+    def tearDown(self) -> None:",
            "+        TestCase.tearDown(self)",
            "+        BaseTestFilePackIndexWriting.tearDown(self)",
            "+",
            "+    def test_load_v3_index_returns_packindex3(self) -> None:",
            "+        \"\"\"Test that loading a v3 index file returns a PackIndex3 instance.\"\"\"",
            "+        entries = [(b\"abcd\" * 5, 0, zlib.crc32(b\"\"))]",
            "+        filename = os.path.join(self.tempdir, \"test.idx\")",
            "+        self.writeIndex(filename, entries, b\"1234567890\" * 2)",
            "+        idx = load_pack_index(filename)",
            "+        self.assertIsInstance(idx, PackIndex3)",
            "+        self.assertEqual(idx.version, 3)",
            "+        self.assertEqual(idx.hash_algorithm, 1)  # SHA-1",
            "+        self.assertEqual(idx.hash_size, 20)",
            "+        self.assertEqual(idx.shortened_oid_len, 20)",
            "+",
            "+    def test_v3_hash_algorithm(self) -> None:",
            "+        \"\"\"Test v3 index correctly handles hash algorithm field.\"\"\"",
            "+        entries = [(b\"a\" * 20, 42, zlib.crc32(b\"data\"))]",
            "+        filename = os.path.join(self.tempdir, \"test_hash.idx\")",
            "+        # Write v3 index with SHA-1 (algorithm=1)",
            "+        with GitFile(filename, \"wb\") as f:",
            "+            write_pack_index_v3(f, entries, b\"1\" * 20, hash_algorithm=1)",
            "+        idx = load_pack_index(filename)",
            "+        self.assertEqual(idx.hash_algorithm, 1)",
            "+        self.assertEqual(idx.hash_size, 20)",
            "+",
            "+    def test_v3_sha256_length(self) -> None:",
            "+        \"\"\"Test v3 index with SHA-256 hash length.\"\"\"",
            "+        # For now, test that SHA-256 is not yet implemented",
            "+        entries = [(b\"a\" * 32, 42, zlib.crc32(b\"data\"))]",
            "+        filename = os.path.join(self.tempdir, \"test_sha256.idx\")",
            "+        # SHA-256 should raise NotImplementedError",
            "+        with self.assertRaises(NotImplementedError) as cm:",
            "+            with GitFile(filename, \"wb\") as f:",
            "+                write_pack_index_v3(f, entries, b\"1\" * 32, hash_algorithm=2)",
            "+        self.assertIn(\"SHA-256\", str(cm.exception))",
            "+",
            "+    def test_v3_invalid_hash_algorithm(self) -> None:",
            "+        \"\"\"Test v3 index with invalid hash algorithm.\"\"\"",
            "+        entries = [(b\"a\" * 20, 42, zlib.crc32(b\"data\"))]",
            "+        filename = os.path.join(self.tempdir, \"test_invalid.idx\")",
            "+        # Invalid hash algorithm should raise ValueError",
            "+        with self.assertRaises(ValueError) as cm:",
            "+            with GitFile(filename, \"wb\") as f:",
            "+                write_pack_index_v3(f, entries, b\"1\" * 20, hash_algorithm=99)",
            "+        self.assertIn(\"Unknown hash algorithm\", str(cm.exception))",
            "+",
            "+    def test_v3_wrong_hash_length(self) -> None:",
            "+        \"\"\"Test v3 index with mismatched hash length.\"\"\"",
            "+        # Entry with wrong hash length for SHA-1",
            "+        entries = [(b\"a\" * 15, 42, zlib.crc32(b\"data\"))]  # Too short",
            "+        filename = os.path.join(self.tempdir, \"test_wrong_len.idx\")",
            "+        with self.assertRaises(ValueError) as cm:",
            "+            with GitFile(filename, \"wb\") as f:",
            "+                write_pack_index_v3(f, entries, b\"1\" * 20, hash_algorithm=1)",
            "+        self.assertIn(\"wrong length\", str(cm.exception))",
            "+",
            "+",
            "+class WritePackIndexTests(TestCase):",
            "+    \"\"\"Tests for the configurable write_pack_index function.\"\"\"",
            "+",
            "+    def test_default_pack_index_version_constant(self) -> None:",
            "+        from dulwich.pack import DEFAULT_PACK_INDEX_VERSION",
            "+",
            "+        # Ensure the constant is set to version 2 (current Git default)",
            "+        self.assertEqual(2, DEFAULT_PACK_INDEX_VERSION)",
            "+",
            "+    def test_write_pack_index_defaults_to_v2(self) -> None:",
            "+        import tempfile",
            "+",
            "+        from dulwich.pack import (",
            "+            DEFAULT_PACK_INDEX_VERSION,",
            "+            load_pack_index,",
            "+            write_pack_index,",
            "+        )",
            "+",
            "+        tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tempdir)",
            "+",
            "+        entries = [(b\"1\" * 20, 42, zlib.crc32(b\"data\"))]",
            "+        filename = os.path.join(tempdir, \"test_default.idx\")",
            "+",
            "+        with GitFile(filename, \"wb\") as f:",
            "+            write_pack_index(f, entries, b\"P\" * 20)",
            "+",
            "+        idx = load_pack_index(filename)",
            "+        self.assertEqual(DEFAULT_PACK_INDEX_VERSION, idx.version)",
            "+",
            "+    def test_write_pack_index_version_1(self) -> None:",
            "+        import tempfile",
            "+",
            "+        from dulwich.pack import load_pack_index, write_pack_index",
            "+",
            "+        tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tempdir)",
            "+",
            "+        entries = [(b\"1\" * 20, 42, zlib.crc32(b\"data\"))]",
            "+        filename = os.path.join(tempdir, \"test_v1.idx\")",
            "+",
            "+        with GitFile(filename, \"wb\") as f:",
            "+            write_pack_index(f, entries, b\"P\" * 20, version=1)",
            "+",
            "+        idx = load_pack_index(filename)",
            "+        self.assertEqual(1, idx.version)",
            "+",
            "+    def test_write_pack_index_version_3(self) -> None:",
            "+        import tempfile",
            "+",
            "+        from dulwich.pack import load_pack_index, write_pack_index",
            "+",
            "+        tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tempdir)",
            "+",
            "+        entries = [(b\"1\" * 20, 42, zlib.crc32(b\"data\"))]",
            "+        filename = os.path.join(tempdir, \"test_v3.idx\")",
            "+",
            "+        with GitFile(filename, \"wb\") as f:",
            "+            write_pack_index(f, entries, b\"P\" * 20, version=3)",
            "+",
            "+        idx = load_pack_index(filename)",
            "+        self.assertEqual(3, idx.version)",
            "+",
            "+    def test_write_pack_index_invalid_version(self) -> None:",
            "+        import tempfile",
            "+",
            "+        from dulwich.pack import write_pack_index",
            "+",
            "+        tempdir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tempdir)",
            "+",
            "+        entries = [(b\"1\" * 20, 42, zlib.crc32(b\"data\"))]",
            "+        filename = os.path.join(tempdir, \"test_invalid.idx\")",
            "+",
            "+        with self.assertRaises(ValueError) as cm:",
            "+            with GitFile(filename, \"wb\") as f:",
            "+                write_pack_index(f, entries, b\"P\" * 20, version=99)",
            "+        self.assertIn(\"Unsupported pack index version: 99\", str(cm.exception))",
            "+",
            "+",
            "+class MockFileWithoutFileno:",
            "+    \"\"\"Mock file-like object without fileno method.\"\"\"",
            "+",
            "+    def __init__(self, content):",
            "+        self.content = content",
            "+        self.position = 0",
            "+",
            "+    def read(self, size=None):",
            "+        if size is None:",
            "+            result = self.content[self.position :]",
            "+            self.position = len(self.content)",
            "+        else:",
            "+            result = self.content[self.position : self.position + size]",
            "+            self.position += size",
            "+        return result",
            "+",
            "+    def seek(self, position):",
            "+        self.position = position",
            "+",
            "+    def tell(self):",
            "+        return self.position",
            "+",
            "+",
            "+# Removed the PackWithoutMmapTests class since it was using private methods",
            "+",
            "+",
            " class ReadZlibTests(TestCase):",
            "     decomp = (",
            "         b\"tree 4ada885c9196b6b6fa08744b5862bf92896fc002\\n\"",
            "         b\"parent None\\n\"",
            "         b\"author Jelmer Vernooij <jelmer@samba.org> 1228980214 +0000\\n\"",
            "         b\"committer Jelmer Vernooij <jelmer@samba.org> 1228980214 +0000\\n\"",
            "         b\"\\n\"",
            "@@ -997,17 +1267,17 @@",
            "             unpacked.obj_type_num,",
            "             b\"\".join(unpacked.obj_chunks),",
            "             unpacked.sha(),",
            "             unpacked.crc32,",
            "         )",
            " ",
            "     def _resolve_object(self, offset, pack_type_num, base_chunks):",
            "-        assert (",
            "-            offset not in self._unpacked_offsets",
            "-        ), f\"Attempted to re-inflate offset {offset}\"",
            "+        assert offset not in self._unpacked_offsets, (",
            "+            f\"Attempted to re-inflate offset {offset}\"",
            "+        )",
            "         self._unpacked_offsets.add(offset)",
            "         return super()._resolve_object(offset, pack_type_num, base_chunks)",
            " ",
            " ",
            " class DeltaChainIteratorTests(TestCase):",
            "     def setUp(self) -> None:",
            "         super().setUp()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_patch.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_patch.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_patch.py",
            "@@ -87,15 +87,15 @@",
            " -- ",
            " 1.7.0.4",
            " \"\"\"",
            "         c, diff, version = git_am_patch_split(StringIO(text.decode(\"utf-8\")), \"utf-8\")",
            "         self.assertEqual(b\"Jelmer Vernooij <jelmer@samba.org>\", c.committer)",
            "         self.assertEqual(b\"Jelmer Vernooij <jelmer@samba.org>\", c.author)",
            "         self.assertEqual(",
            "-            b\"Remove executable bit from prey.ico \" b\"(triggers a warning).\\n\",",
            "+            b\"Remove executable bit from prey.ico (triggers a warning).\\n\",",
            "             c.message,",
            "         )",
            "         self.assertEqual(",
            "             b\"\"\" pixmaps/prey.ico |  Bin 9662 -> 9662 bytes",
            "  1 files changed, 0 insertions(+), 0 deletions(-)",
            "  mode change 100755 => 100644 pixmaps/prey.ico",
            " ",
            "@@ -119,15 +119,15 @@",
            " -- ",
            " 1.7.0.4",
            " \"\"\"",
            "         c, diff, version = git_am_patch_split(BytesIO(text))",
            "         self.assertEqual(b\"Jelmer Vernooij <jelmer@samba.org>\", c.committer)",
            "         self.assertEqual(b\"Jelmer Vernooij <jelmer@samba.org>\", c.author)",
            "         self.assertEqual(",
            "-            b\"Remove executable bit from prey.ico \" b\"(triggers a warning).\\n\",",
            "+            b\"Remove executable bit from prey.ico (triggers a warning).\\n\",",
            "             c.message,",
            "         )",
            "         self.assertEqual(",
            "             b\"\"\" pixmaps/prey.ico |  Bin 9662 -> 9662 bytes",
            "  1 files changed, 0 insertions(+), 0 deletions(-)",
            "  mode change 100755 => 100644 pixmaps/prey.ico",
            " ",
            "@@ -216,15 +216,15 @@",
            " ",
            " \"\"\"",
            "         c, diff, version = git_am_patch_split(BytesIO(text), \"utf-8\")",
            "         self.assertEqual(None, version)",
            " ",
            "     def test_extract_mercurial(self) -> NoReturn:",
            "         raise SkipTest(",
            "-            \"git_am_patch_split doesn't handle Mercurial patches \" \"properly yet\"",
            "+            \"git_am_patch_split doesn't handle Mercurial patches properly yet\"",
            "         )",
            "         expected_diff = \"\"\"\\",
            " diff --git a/dulwich/tests/test_patch.py b/dulwich/tests/test_patch.py",
            " --- a/dulwich/tests/test_patch.py",
            " +++ b/dulwich/tests/test_patch.py",
            " @@ -158,7 +158,7 @@"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_porcelain.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_porcelain.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_porcelain.py",
            "@@ -33,18 +33,25 @@",
            " import tempfile",
            " import threading",
            " import time",
            " from io import BytesIO, StringIO",
            " from unittest import skipIf",
            " ",
            " from dulwich import porcelain",
            "+from dulwich.client import SendPackResult",
            " from dulwich.diff_tree import tree_changes",
            " from dulwich.errors import CommitError",
            "-from dulwich.objects import ZERO_SHA, Blob, Tag, Tree",
            "-from dulwich.porcelain import CheckoutError",
            "+from dulwich.object_store import DEFAULT_TEMPFILE_GRACE_PERIOD",
            "+from dulwich.objects import ZERO_SHA, Blob, Commit, Tag, Tree",
            "+from dulwich.porcelain import (",
            "+    CheckoutError,  # Hypothetical or real error class",
            "+    CountObjectsResult,",
            "+    add,",
            "+    commit,",
            "+)",
            " from dulwich.repo import NoIndexPresent, Repo",
            " from dulwich.server import DictBackend",
            " from dulwich.tests.utils import build_commit_graph, make_commit, make_object",
            " from dulwich.web import make_server, make_wsgi_chain",
            " ",
            " from . import TestCase",
            " ",
            "@@ -411,14 +418,15 @@",
            "             committer=\"Bob <bob@example.com>\",",
            "             commit_timezone=18000,",
            "         )",
            "         self.assertIsInstance(sha, bytes)",
            "         self.assertEqual(len(sha), 40)",
            " ",
            "         commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "         self.assertEqual(commit._author_timezone, 18000)",
            "         self.assertEqual(commit._commit_timezone, 18000)",
            " ",
            "         self.overrideEnv(\"GIT_AUTHOR_DATE\", \"1995-11-20T19:12:08-0501\")",
            "         self.overrideEnv(\"GIT_COMMITTER_DATE\", \"1995-11-20T19:12:08-0501\")",
            " ",
            "         sha = porcelain.commit(",
            "@@ -427,14 +435,15 @@",
            "             author=\"Joe <joe@example.com>\",",
            "             committer=\"Bob <bob@example.com>\",",
            "         )",
            "         self.assertIsInstance(sha, bytes)",
            "         self.assertEqual(len(sha), 40)",
            " ",
            "         commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "         self.assertEqual(commit._author_timezone, -18060)",
            "         self.assertEqual(commit._commit_timezone, -18060)",
            " ",
            "         self.overrideEnv(\"GIT_AUTHOR_DATE\", None)",
            "         self.overrideEnv(\"GIT_COMMITTER_DATE\", None)",
            " ",
            "         local_timezone = time.localtime().tm_gmtoff",
            "@@ -445,14 +454,15 @@",
            "             author=\"Joe <joe@example.com>\",",
            "             committer=\"Bob <bob@example.com>\",",
            "         )",
            "         self.assertIsInstance(sha, bytes)",
            "         self.assertEqual(len(sha), 40)",
            " ",
            "         commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "         self.assertEqual(commit._author_timezone, local_timezone)",
            "         self.assertEqual(commit._commit_timezone, local_timezone)",
            " ",
            " ",
            " @skipIf(",
            "     platform.python_implementation() == \"PyPy\" or sys.platform == \"win32\",",
            "     \"gpgme not easily available or supported on Windows and PyPy\",",
            "@@ -474,25 +484,27 @@",
            "             committer=\"Bob <bob@example.com>\",",
            "             signoff=True,",
            "         )",
            "         self.assertIsInstance(sha, bytes)",
            "         self.assertEqual(len(sha), 40)",
            " ",
            "         commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "         # GPG Signatures aren't deterministic, so we can't do a static assertion.",
            "         commit.verify()",
            "         commit.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            " ",
            "         self.import_non_default_key()",
            "         self.assertRaises(",
            "             gpg.errors.MissingSignatures,",
            "             commit.verify,",
            "             keyids=[PorcelainGpgTestCase.NON_DEFAULT_KEY_ID],",
            "         )",
            " ",
            "+        assert isinstance(commit, Commit)",
            "         commit.committer = b\"Alice <alice@example.com>\"",
            "         self.assertRaises(",
            "             gpg.errors.BadSignatures,",
            "             commit.verify,",
            "         )",
            " ",
            "     def test_non_default_key(self) -> None:",
            "@@ -511,17 +523,211 @@",
            "             committer=\"Bob <bob@example.com>\",",
            "             signoff=PorcelainGpgTestCase.NON_DEFAULT_KEY_ID,",
            "         )",
            "         self.assertIsInstance(sha, bytes)",
            "         self.assertEqual(len(sha), 40)",
            " ",
            "         commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "         # GPG Signatures aren't deterministic, so we can't do a static assertion.",
            "         commit.verify()",
            " ",
            "+    def test_sign_uses_config_signingkey(self) -> None:",
            "+        \"\"\"Test that sign=True uses user.signingKey from config.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up user.signingKey in config",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create commit with sign=True (should use signingKey from config)",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=\"Signed with configured key\",",
            "+            author=\"Joe <joe@example.com>\",",
            "+            committer=\"Bob <bob@example.com>\",",
            "+            signoff=True,  # This should read user.signingKey from config",
            "+        )",
            "+",
            "+        self.assertIsInstance(sha, bytes)",
            "+        self.assertEqual(len(sha), 40)",
            "+",
            "+        commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "+        # Verify the commit is signed with the configured key",
            "+        commit.verify()",
            "+        commit.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            "+",
            "+    def test_commit_gpg_sign_config_enabled(self) -> None:",
            "+        \"\"\"Test that commit.gpgSign=true automatically signs commits.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up user.signingKey and commit.gpgSign in config",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"commit\",), \"gpgSign\", True)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create commit without explicit signoff parameter (should auto-sign due to config)",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=\"Auto-signed commit\",",
            "+            author=\"Joe <joe@example.com>\",",
            "+            committer=\"Bob <bob@example.com>\",",
            "+            # No signoff parameter - should use commit.gpgSign config",
            "+        )",
            "+",
            "+        self.assertIsInstance(sha, bytes)",
            "+        self.assertEqual(len(sha), 40)",
            "+",
            "+        commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "+        # Verify the commit is signed due to config",
            "+        commit.verify()",
            "+        commit.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            "+",
            "+    def test_commit_gpg_sign_config_disabled(self) -> None:",
            "+        \"\"\"Test that commit.gpgSign=false does not sign commits.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up user.signingKey and commit.gpgSign=false in config",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"commit\",), \"gpgSign\", False)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create commit without explicit signoff parameter (should not sign)",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=\"Unsigned commit\",",
            "+            author=\"Joe <joe@example.com>\",",
            "+            committer=\"Bob <bob@example.com>\",",
            "+            # No signoff parameter - should use commit.gpgSign=false config",
            "+        )",
            "+",
            "+        self.assertIsInstance(sha, bytes)",
            "+        self.assertEqual(len(sha), 40)",
            "+",
            "+        commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "+        # Verify the commit is not signed",
            "+        self.assertIsNone(commit._gpgsig)",
            "+",
            "+    def test_commit_gpg_sign_config_no_signing_key(self) -> None:",
            "+        \"\"\"Test that commit.gpgSign=true works without user.signingKey (uses default).\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up commit.gpgSign but no user.signingKey",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"commit\",), \"gpgSign\", True)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create commit without explicit signoff parameter (should auto-sign with default key)",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=\"Default signed commit\",",
            "+            author=\"Joe <joe@example.com>\",",
            "+            committer=\"Bob <bob@example.com>\",",
            "+            # No signoff parameter - should use commit.gpgSign config with default key",
            "+        )",
            "+",
            "+        self.assertIsInstance(sha, bytes)",
            "+        self.assertEqual(len(sha), 40)",
            "+",
            "+        commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "+        # Verify the commit is signed with default key",
            "+        commit.verify()",
            "+",
            "+    def test_explicit_signoff_overrides_config(self) -> None:",
            "+        \"\"\"Test that explicit signoff parameter overrides commit.gpgSign config.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up commit.gpgSign=false but explicitly pass signoff=True",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"commit\",), \"gpgSign\", False)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create commit with explicit signoff=True (should override config)",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=\"Explicitly signed commit\",",
            "+            author=\"Joe <joe@example.com>\",",
            "+            committer=\"Bob <bob@example.com>\",",
            "+            signoff=True,  # This should override commit.gpgSign=false",
            "+        )",
            "+",
            "+        self.assertIsInstance(sha, bytes)",
            "+        self.assertEqual(len(sha), 40)",
            "+",
            "+        commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "+        # Verify the commit is signed despite config=false",
            "+        commit.verify()",
            "+        commit.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            "+",
            "+    def test_explicit_false_disables_signing(self) -> None:",
            "+        \"\"\"Test that explicit signoff=False disables signing even with config=true.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up commit.gpgSign=true but explicitly pass signoff=False",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"commit\",), \"gpgSign\", True)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create commit with explicit signoff=False (should disable signing)",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=\"Explicitly unsigned commit\",",
            "+            author=\"Joe <joe@example.com>\",",
            "+            committer=\"Bob <bob@example.com>\",",
            "+            signoff=False,  # This should override commit.gpgSign=true",
            "+        )",
            "+",
            "+        self.assertIsInstance(sha, bytes)",
            "+        self.assertEqual(len(sha), 40)",
            "+",
            "+        commit = self.repo.get_object(sha)",
            "+        assert isinstance(commit, Commit)",
            "+        # Verify the commit is NOT signed despite config=true",
            "+        self.assertIsNone(commit._gpgsig)",
            "+",
            " ",
            " class TimezoneTests(PorcelainTestCase):",
            "     def put_envs(self, value) -> None:",
            "         self.overrideEnv(\"GIT_AUTHOR_DATE\", value)",
            "         self.overrideEnv(\"GIT_COMMITTER_DATE\", value)",
            " ",
            "     def fallback(self, value) -> None:",
            "@@ -715,16 +921,18 @@",
            "         self.assertEqual(0, len(target_repo.open_index()))",
            "         self.assertEqual(c3.id, target_repo.refs[b\"refs/tags/foo\"])",
            "         self.assertNotIn(b\"f1\", os.listdir(target_path))",
            "         self.assertNotIn(b\"f2\", os.listdir(target_path))",
            "         c = r.get_config()",
            "         encoded_path = self.repo.path",
            "         if not isinstance(encoded_path, bytes):",
            "-            encoded_path = encoded_path.encode(\"utf-8\")",
            "-        self.assertEqual(encoded_path, c.get((b\"remote\", b\"origin\"), b\"url\"))",
            "+            encoded_path_bytes = encoded_path.encode(\"utf-8\")",
            "+        else:",
            "+            encoded_path_bytes = encoded_path",
            "+        self.assertEqual(encoded_path_bytes, c.get((b\"remote\", b\"origin\"), b\"url\"))",
            "         self.assertEqual(",
            "             b\"+refs/heads/*:refs/remotes/origin/*\",",
            "             c.get((b\"remote\", b\"origin\"), b\"fetch\"),",
            "         )",
            " ",
            "     def test_simple_local_with_checkout(self) -> None:",
            "         f1_1 = make_object(Blob, data=b\"f1\")",
            "@@ -875,26 +1083,151 @@",
            "         self.addCleanup(shutil.rmtree, target_path)",
            "         errstream = porcelain.NoneStream()",
            "         with porcelain.clone(",
            "             self.repo.path, target_path, checkout=True, errstream=errstream",
            "         ) as r:",
            "             self.assertEqual(c3.id, r.refs[b\"HEAD\"])",
            " ",
            "+    def test_clone_pathlib(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        f1_1 = make_object(Blob, data=b\"f1\")",
            "+        commit_spec = [[1]]",
            "+        trees = {1: [(b\"f1\", f1_1)]}",
            "+",
            "+        c1 = build_commit_graph(self.repo.object_store, commit_spec, trees)[0]",
            "+        self.repo.refs[b\"refs/heads/master\"] = c1.id",
            "+",
            "+        target_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, target_dir)",
            "+        target_path = Path(target_dir) / \"clone_repo\"",
            "+",
            "+        errstream = BytesIO()",
            "+        r = porcelain.clone(",
            "+            self.repo.path, target_path, checkout=False, errstream=errstream",
            "+        )",
            "+        self.addCleanup(r.close)",
            "+        self.assertEqual(r.path, str(target_path))",
            "+        self.assertTrue(os.path.exists(str(target_path)))",
            "+",
            "+    def test_clone_with_recurse_submodules(self) -> None:",
            "+        # Create a submodule repository",
            "+        sub_repo_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, sub_repo_path)",
            "+        sub_repo = Repo.init(sub_repo_path)",
            "+        self.addCleanup(sub_repo.close)",
            "+",
            "+        # Add a file to the submodule repo",
            "+        sub_file = os.path.join(sub_repo_path, \"subfile.txt\")",
            "+        with open(sub_file, \"w\") as f:",
            "+            f.write(\"submodule content\")",
            "+",
            "+        porcelain.add(sub_repo, paths=[sub_file])",
            "+        sub_commit = porcelain.commit(",
            "+            sub_repo,",
            "+            message=b\"Initial submodule commit\",",
            "+            author=b\"Test Author <test@example.com>\",",
            "+            committer=b\"Test Committer <test@example.com>\",",
            "+        )",
            "+",
            "+        # Create main repository with submodule",
            "+        main_file = os.path.join(self.repo.path, \"main.txt\")",
            "+        with open(main_file, \"w\") as f:",
            "+            f.write(\"main content\")",
            "+",
            "+        porcelain.add(self.repo, paths=[main_file])",
            "+        porcelain.submodule_add(self.repo, sub_repo_path, \"sub\")",
            "+",
            "+        # Manually add the submodule to the index since submodule_add doesn't do it",
            "+        # when the repository is local (to maintain backward compatibility)",
            "+        from dulwich.index import IndexEntry",
            "+        from dulwich.objects import S_IFGITLINK",
            "+",
            "+        index = self.repo.open_index()",
            "+        index[b\"sub\"] = IndexEntry(",
            "+            ctime=0,",
            "+            mtime=0,",
            "+            dev=0,",
            "+            ino=0,",
            "+            mode=S_IFGITLINK,",
            "+            uid=0,",
            "+            gid=0,",
            "+            size=0,",
            "+            sha=sub_commit,",
            "+            flags=0,",
            "+        )",
            "+        index.write()",
            "+",
            "+        porcelain.add(self.repo, paths=[\".gitmodules\"])",
            "+        porcelain.commit(",
            "+            self.repo,",
            "+            message=b\"Add submodule\",",
            "+            author=b\"Test Author <test@example.com>\",",
            "+            committer=b\"Test Committer <test@example.com>\",",
            "+        )",
            "+",
            "+        # Clone with recurse_submodules",
            "+        target_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, target_path)",
            "+",
            "+        cloned = porcelain.clone(",
            "+            self.repo.path,",
            "+            target_path,",
            "+            recurse_submodules=True,",
            "+        )",
            "+        self.addCleanup(cloned.close)",
            "+",
            "+        # Check main file exists",
            "+        cloned_main = os.path.join(target_path, \"main.txt\")",
            "+        self.assertTrue(os.path.exists(cloned_main))",
            "+        with open(cloned_main) as f:",
            "+            self.assertEqual(f.read(), \"main content\")",
            "+",
            "+        # Check submodule file exists",
            "+        cloned_sub_file = os.path.join(target_path, \"sub\", \"subfile.txt\")",
            "+        self.assertTrue(os.path.exists(cloned_sub_file))",
            "+        with open(cloned_sub_file) as f:",
            "+            self.assertEqual(f.read(), \"submodule content\")",
            "+",
            " ",
            " class InitTests(TestCase):",
            "     def test_non_bare(self) -> None:",
            "         repo_dir = tempfile.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, repo_dir)",
            "         porcelain.init(repo_dir)",
            " ",
            "     def test_bare(self) -> None:",
            "         repo_dir = tempfile.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, repo_dir)",
            "         porcelain.init(repo_dir, bare=True)",
            " ",
            "+    def test_init_pathlib(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        repo_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, repo_dir)",
            "+        repo_path = Path(repo_dir)",
            "+",
            "+        # Test non-bare repo with pathlib",
            "+        repo = porcelain.init(repo_path)",
            "+        self.assertTrue(os.path.exists(os.path.join(repo_dir, \".git\")))",
            "+        repo.close()",
            "+",
            "+    def test_init_bare_pathlib(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        repo_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, repo_dir)",
            "+        repo_path = Path(repo_dir)",
            "+",
            "+        # Test bare repo with pathlib",
            "+        repo = porcelain.init(repo_path, bare=True)",
            "+        self.assertTrue(os.path.exists(os.path.join(repo_dir, \"refs\")))",
            "+        repo.close()",
            "+",
            " ",
            " class AddTests(PorcelainTestCase):",
            "     def test_add_default_paths(self) -> None:",
            "         # create a file for initial commit",
            "         fullpath = os.path.join(self.repo.path, \"blah\")",
            "         with open(fullpath, \"w\") as f:",
            "             f.write(\"\\n\")",
            "@@ -909,50 +1242,50 @@",
            "         # Add a second test file and a file in a directory",
            "         with open(os.path.join(self.repo.path, \"foo\"), \"w\") as f:",
            "             f.write(\"\\n\")",
            "         os.mkdir(os.path.join(self.repo.path, \"adir\"))",
            "         with open(os.path.join(self.repo.path, \"adir\", \"afile\"), \"w\") as f:",
            "             f.write(\"\\n\")",
            "         cwd = os.getcwd()",
            "-        try:",
            "-            os.chdir(self.repo.path)",
            "-            self.assertEqual({\"foo\", \"blah\", \"adir\", \".git\"}, set(os.listdir(\".\")))",
            "-            self.assertEqual(",
            "-                ([\"foo\", os.path.join(\"adir\", \"afile\")], set()),",
            "-                porcelain.add(self.repo.path),",
            "-            )",
            "-        finally:",
            "-            os.chdir(cwd)",
            "+        self.addCleanup(os.chdir, cwd)",
            "+        os.chdir(self.repo.path)",
            "+        self.assertEqual({\"foo\", \"blah\", \"adir\", \".git\"}, set(os.listdir(\".\")))",
            "+        added, ignored = porcelain.add(self.repo.path)",
            "+        # Normalize paths to use forward slashes for comparison",
            "+        added_normalized = [path.replace(os.sep, \"/\") for path in added]",
            "+        self.assertEqual(",
            "+            (added_normalized, ignored),",
            "+            ([\"foo\", \"adir/afile\"], set()),",
            "+        )",
            " ",
            "         # Check that foo was added and nothing in .git was modified",
            "         index = self.repo.open_index()",
            "         self.assertEqual(sorted(index), [b\"adir/afile\", b\"blah\", b\"foo\"])",
            " ",
            "     def test_add_default_paths_subdir(self) -> None:",
            "         os.mkdir(os.path.join(self.repo.path, \"foo\"))",
            "         with open(os.path.join(self.repo.path, \"blah\"), \"w\") as f:",
            "             f.write(\"\\n\")",
            "         with open(os.path.join(self.repo.path, \"foo\", \"blie\"), \"w\") as f:",
            "             f.write(\"\\n\")",
            " ",
            "         cwd = os.getcwd()",
            "-        try:",
            "-            os.chdir(os.path.join(self.repo.path, \"foo\"))",
            "-            porcelain.add(repo=self.repo.path)",
            "-            porcelain.commit(",
            "-                repo=self.repo.path,",
            "-                message=b\"test\",",
            "-                author=b\"test <email>\",",
            "-                committer=b\"test <email>\",",
            "-            )",
            "-        finally:",
            "-            os.chdir(cwd)",
            "+        self.addCleanup(os.chdir, cwd)",
            "+        os.chdir(os.path.join(self.repo.path, \"foo\"))",
            "+        porcelain.add(repo=self.repo.path)",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            " ",
            "         index = self.repo.open_index()",
            "-        self.assertEqual(sorted(index), [b\"foo/blie\"])",
            "+        # After fix: add() with no paths should behave like git add -A (add everything)",
            "+        self.assertEqual(sorted(index), [b\"blah\", b\"foo/blie\"])",
            " ",
            "     def test_add_file(self) -> None:",
            "         fullpath = os.path.join(self.repo.path, \"foo\")",
            "         with open(fullpath, \"w\") as f:",
            "             f.write(\"BAR\")",
            "         porcelain.add(self.repo.path, paths=[fullpath])",
            "         self.assertIn(b\"foo\", self.repo.open_index())",
            "@@ -973,15 +1306,41 @@",
            "                 os.path.join(self.repo.path, \"foo\"),",
            "                 os.path.join(self.repo.path, \"bar\"),",
            "                 os.path.join(self.repo.path, \"subdir\"),",
            "             ],",
            "         )",
            "         self.assertIn(b\"bar\", self.repo.open_index())",
            "         self.assertEqual({\"bar\"}, set(added))",
            "-        self.assertEqual({\"foo\", os.path.join(\"subdir\", \"\")}, ignored)",
            "+        self.assertEqual({\"foo\", \"subdir/\"}, ignored)",
            "+",
            "+    def test_add_from_ignored_directory(self) -> None:",
            "+        # Test for issue #550 - adding files when cwd is in ignored directory",
            "+        # Create .gitignore that ignores build/",
            "+        with open(os.path.join(self.repo.path, \".gitignore\"), \"w\") as f:",
            "+            f.write(\"build/\\n\")",
            "+",
            "+        # Create ignored directory",
            "+        build_dir = os.path.join(self.repo.path, \"build\")",
            "+        os.mkdir(build_dir)",
            "+",
            "+        # Create a file in the repo (not in ignored directory)",
            "+        src_file = os.path.join(self.repo.path, \"source.py\")",
            "+        with open(src_file, \"w\") as f:",
            "+            f.write(\"print('hello')\\n\")",
            "+",
            "+        # Save current directory and change to ignored directory",
            "+        original_cwd = os.getcwd()",
            "+        try:",
            "+            os.chdir(build_dir)",
            "+            # Add file using absolute path from within ignored directory",
            "+            (added, ignored) = porcelain.add(self.repo.path, paths=[src_file])",
            "+            self.assertIn(b\"source.py\", self.repo.open_index())",
            "+            self.assertEqual({\"source.py\"}, set(added))",
            "+        finally:",
            "+            os.chdir(original_cwd)",
            " ",
            "     def test_add_file_absolute_path(self) -> None:",
            "         # Absolute paths are (not yet) supported",
            "         with open(os.path.join(self.repo.path, \"foo\"), \"w\") as f:",
            "             f.write(\"BAR\")",
            "         porcelain.add(self.repo, paths=[os.path.join(self.repo.path, \"foo\")])",
            "         self.assertIn(b\"foo\", self.repo.open_index())",
            "@@ -1000,14 +1359,16 @@",
            "             porcelain.add,",
            "             self.repo,",
            "             paths=[\"../foo\"],",
            "         )",
            "         self.assertEqual([], list(self.repo.open_index()))",
            " ",
            "     def test_add_file_clrf_conversion(self) -> None:",
            "+        from dulwich.index import IndexEntry",
            "+",
            "         # Set the right configuration to the repo",
            "         c = self.repo.get_config()",
            "         c.set(\"core\", \"autocrlf\", \"input\")",
            "         c.write_to_path()",
            " ",
            "         # Add a file with CRLF line-ending",
            "         fullpath = os.path.join(self.repo.path, \"foo\")",
            "@@ -1016,17 +1377,440 @@",
            "         porcelain.add(self.repo.path, paths=[fullpath])",
            " ",
            "         # The line-endings should have been converted to LF",
            "         index = self.repo.open_index()",
            "         self.assertIn(b\"foo\", index)",
            " ",
            "         entry = index[b\"foo\"]",
            "+        assert isinstance(entry, IndexEntry)",
            "         blob = self.repo[entry.sha]",
            "         self.assertEqual(blob.data, b\"line1\\nline2\")",
            " ",
            "+    def test_add_symlink_outside_repo(self) -> None:",
            "+        \"\"\"Test adding a symlink that points outside the repository.\"\"\"",
            "+        # Create a symlink pointing outside the repository",
            "+        symlink_path = os.path.join(self.repo.path, \"symlink_to_nowhere\")",
            "+        os.symlink(\"/nonexistent/path\", symlink_path)",
            "+",
            "+        # Adding the symlink should succeed (matching Git's behavior)",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[symlink_path])",
            "+",
            "+        # Should successfully add the symlink",
            "+        self.assertIn(\"symlink_to_nowhere\", added)",
            "+        self.assertEqual(len(ignored), 0)",
            "+",
            "+        # Verify symlink is actually staged",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"symlink_to_nowhere\", index)",
            "+",
            "+    def test_add_symlink_to_file_inside_repo(self) -> None:",
            "+        \"\"\"Test adding a symlink that points to a file inside the repository.\"\"\"",
            "+        # Create a regular file",
            "+        target_file = os.path.join(self.repo.path, \"target.txt\")",
            "+        with open(target_file, \"w\") as f:",
            "+            f.write(\"target content\")",
            "+",
            "+        # Create a symlink to the file",
            "+        symlink_path = os.path.join(self.repo.path, \"link_to_target\")",
            "+        os.symlink(\"target.txt\", symlink_path)",
            "+",
            "+        # Add both the target and the symlink",
            "+        added, ignored = porcelain.add(",
            "+            self.repo.path, paths=[target_file, symlink_path]",
            "+        )",
            "+",
            "+        # Both should be added successfully",
            "+        self.assertIn(\"target.txt\", added)",
            "+        self.assertIn(\"link_to_target\", added)",
            "+        self.assertEqual(len(ignored), 0)",
            "+",
            "+        # Verify both are in the index",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"target.txt\", index)",
            "+        self.assertIn(b\"link_to_target\", index)",
            "+",
            "+    def test_add_symlink_to_directory_inside_repo(self) -> None:",
            "+        \"\"\"Test adding a symlink that points to a directory inside the repository.\"\"\"",
            "+        # Create a directory with some files",
            "+        target_dir = os.path.join(self.repo.path, \"target_dir\")",
            "+        os.mkdir(target_dir)",
            "+        with open(os.path.join(target_dir, \"file1.txt\"), \"w\") as f:",
            "+            f.write(\"content1\")",
            "+        with open(os.path.join(target_dir, \"file2.txt\"), \"w\") as f:",
            "+            f.write(\"content2\")",
            "+",
            "+        # Create a symlink to the directory",
            "+        symlink_path = os.path.join(self.repo.path, \"link_to_dir\")",
            "+        os.symlink(\"target_dir\", symlink_path)",
            "+",
            "+        # Add the symlink",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[symlink_path])",
            "+",
            "+        # When adding a symlink to a directory, it follows the symlink and adds contents",
            "+        self.assertEqual(len(added), 2)",
            "+        self.assertIn(\"link_to_dir/file1.txt\", added)",
            "+        self.assertIn(\"link_to_dir/file2.txt\", added)",
            "+        self.assertEqual(len(ignored), 0)",
            "+",
            "+        # Verify files are added through the symlink path",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"link_to_dir/file1.txt\", index)",
            "+        self.assertIn(b\"link_to_dir/file2.txt\", index)",
            "+        # The original target directory files are not added",
            "+        self.assertNotIn(b\"target_dir/file1.txt\", index)",
            "+        self.assertNotIn(b\"target_dir/file2.txt\", index)",
            "+",
            "+    def test_add_symlink_chain(self) -> None:",
            "+        \"\"\"Test adding a chain of symlinks (symlink to symlink).\"\"\"",
            "+        # Create a regular file",
            "+        target_file = os.path.join(self.repo.path, \"original.txt\")",
            "+        with open(target_file, \"w\") as f:",
            "+            f.write(\"original content\")",
            "+",
            "+        # Create first symlink",
            "+        first_link = os.path.join(self.repo.path, \"link1\")",
            "+        os.symlink(\"original.txt\", first_link)",
            "+",
            "+        # Create second symlink pointing to first",
            "+        second_link = os.path.join(self.repo.path, \"link2\")",
            "+        os.symlink(\"link1\", second_link)",
            "+",
            "+        # Add all files",
            "+        added, ignored = porcelain.add(",
            "+            self.repo.path, paths=[target_file, first_link, second_link]",
            "+        )",
            "+",
            "+        # All should be added",
            "+        self.assertEqual(len(added), 3)",
            "+        self.assertIn(\"original.txt\", added)",
            "+        self.assertIn(\"link1\", added)",
            "+        self.assertIn(\"link2\", added)",
            "+",
            "+        # Verify all are in the index",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"original.txt\", index)",
            "+        self.assertIn(b\"link1\", index)",
            "+        self.assertIn(b\"link2\", index)",
            "+",
            "+    def test_add_broken_symlink(self) -> None:",
            "+        \"\"\"Test adding a broken symlink (points to non-existent target).\"\"\"",
            "+        # Create a symlink to a non-existent file",
            "+        broken_link = os.path.join(self.repo.path, \"broken_link\")",
            "+        os.symlink(\"does_not_exist.txt\", broken_link)",
            "+",
            "+        # Add the broken symlink",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[broken_link])",
            "+",
            "+        # Should be added successfully (Git tracks the symlink, not its target)",
            "+        self.assertIn(\"broken_link\", added)",
            "+        self.assertEqual(len(ignored), 0)",
            "+",
            "+        # Verify it's in the index",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"broken_link\", index)",
            "+",
            "+    def test_add_symlink_relative_outside_repo(self) -> None:",
            "+        \"\"\"Test adding a symlink that uses '..' to point outside the repository.\"\"\"",
            "+        # Create a file outside the repo",
            "+        outside_file = os.path.join(self.test_dir, \"outside.txt\")",
            "+        with open(outside_file, \"w\") as f:",
            "+            f.write(\"outside content\")",
            "+",
            "+        # Create a symlink using relative path to go outside",
            "+        symlink_path = os.path.join(self.repo.path, \"link_outside\")",
            "+        os.symlink(\"../outside.txt\", symlink_path)",
            "+",
            "+        # Add the symlink",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[symlink_path])",
            "+",
            "+        # Should be added successfully",
            "+        self.assertIn(\"link_outside\", added)",
            "+        self.assertEqual(len(ignored), 0)",
            "+",
            "+        # Verify it's in the index",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"link_outside\", index)",
            "+",
            "+    def test_add_symlink_absolute_to_system(self) -> None:",
            "+        \"\"\"Test adding a symlink with absolute path to system directory.\"\"\"",
            "+        # Create a symlink to a system directory",
            "+        symlink_path = os.path.join(self.repo.path, \"link_to_tmp\")",
            "+        if os.name == \"nt\":",
            "+            # On Windows, use a system directory like TEMP",
            "+            symlink_target = os.environ[\"TEMP\"]",
            "+        else:",
            "+            # On Unix-like systems, use /tmp",
            "+            symlink_target = \"/tmp\"",
            "+        os.symlink(symlink_target, symlink_path)",
            "+",
            "+        # Adding a symlink to a directory outside the repo should raise ValueError",
            "+        with self.assertRaises(ValueError) as cm:",
            "+            porcelain.add(self.repo.path, paths=[symlink_path])",
            "+",
            "+        # Check that the error indicates the path is outside the repository",
            "+        self.assertIn(\"is not in the subpath of\", str(cm.exception))",
            "+",
            "+    def test_add_file_through_symlink(self) -> None:",
            "+        \"\"\"Test adding a file through a symlinked directory.\"\"\"",
            "+        # Create a directory with a file",
            "+        real_dir = os.path.join(self.repo.path, \"real_dir\")",
            "+        os.mkdir(real_dir)",
            "+        real_file = os.path.join(real_dir, \"file.txt\")",
            "+        with open(real_file, \"w\") as f:",
            "+            f.write(\"content\")",
            "+",
            "+        # Create a symlink to the directory",
            "+        link_dir = os.path.join(self.repo.path, \"link_dir\")",
            "+        os.symlink(\"real_dir\", link_dir)",
            "+",
            "+        # Try to add the file through the symlink path",
            "+        symlink_file_path = os.path.join(link_dir, \"file.txt\")",
            "+",
            "+        # This should add the real file, not create a new entry",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[symlink_file_path])",
            "+",
            "+        # The real file should be added",
            "+        self.assertIn(\"real_dir/file.txt\", added)",
            "+        self.assertEqual(len(added), 1)",
            "+",
            "+        # Verify correct path in index",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"real_dir/file.txt\", index)",
            "+        # Should not create a separate entry for the symlink path",
            "+        self.assertNotIn(b\"link_dir/file.txt\", index)",
            "+",
            "+    def test_add_repo_path(self) -> None:",
            "+        \"\"\"Test adding the repository path itself should add all untracked files.\"\"\"",
            "+        # Create some untracked files",
            "+        with open(os.path.join(self.repo.path, \"file1.txt\"), \"w\") as f:",
            "+            f.write(\"content1\")",
            "+        with open(os.path.join(self.repo.path, \"file2.txt\"), \"w\") as f:",
            "+            f.write(\"content2\")",
            "+",
            "+        # Add the repository path itself",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[self.repo.path])",
            "+",
            "+        # Should add all untracked files, not stage './'",
            "+        self.assertIn(\"file1.txt\", added)",
            "+        self.assertIn(\"file2.txt\", added)",
            "+        self.assertNotIn(\"./\", added)",
            "+",
            "+        # Verify files are actually staged",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"file1.txt\", index)",
            "+        self.assertIn(b\"file2.txt\", index)",
            "+",
            "+    def test_add_directory_contents(self) -> None:",
            "+        \"\"\"Test adding a directory adds all files within it.\"\"\"",
            "+        # Create a subdirectory with multiple files",
            "+        subdir = os.path.join(self.repo.path, \"subdir\")",
            "+        os.mkdir(subdir)",
            "+        with open(os.path.join(subdir, \"file1.txt\"), \"w\") as f:",
            "+            f.write(\"content1\")",
            "+        with open(os.path.join(subdir, \"file2.txt\"), \"w\") as f:",
            "+            f.write(\"content2\")",
            "+        with open(os.path.join(subdir, \"file3.txt\"), \"w\") as f:",
            "+            f.write(\"content3\")",
            "+",
            "+        # Add the directory",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[\"subdir\"])",
            "+",
            "+        # Should add all files in the directory",
            "+        self.assertEqual(len(added), 3)",
            "+        # Normalize paths to use forward slashes for comparison",
            "+        added_normalized = [path.replace(os.sep, \"/\") for path in added]",
            "+        self.assertIn(\"subdir/file1.txt\", added_normalized)",
            "+        self.assertIn(\"subdir/file2.txt\", added_normalized)",
            "+        self.assertIn(\"subdir/file3.txt\", added_normalized)",
            "+",
            "+        # Verify files are actually staged",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"subdir/file1.txt\", index)",
            "+        self.assertIn(b\"subdir/file2.txt\", index)",
            "+        self.assertIn(b\"subdir/file3.txt\", index)",
            "+",
            "+    def test_add_nested_directories(self) -> None:",
            "+        \"\"\"Test adding a directory with nested subdirectories.\"\"\"",
            "+        # Create nested directory structure",
            "+        dir1 = os.path.join(self.repo.path, \"dir1\")",
            "+        dir2 = os.path.join(dir1, \"dir2\")",
            "+        dir3 = os.path.join(dir2, \"dir3\")",
            "+        os.makedirs(dir3)",
            "+",
            "+        # Add files at each level",
            "+        with open(os.path.join(dir1, \"file1.txt\"), \"w\") as f:",
            "+            f.write(\"level1\")",
            "+        with open(os.path.join(dir2, \"file2.txt\"), \"w\") as f:",
            "+            f.write(\"level2\")",
            "+        with open(os.path.join(dir3, \"file3.txt\"), \"w\") as f:",
            "+            f.write(\"level3\")",
            "+",
            "+        # Add the top-level directory",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[\"dir1\"])",
            "+",
            "+        # Should add all files recursively",
            "+        self.assertEqual(len(added), 3)",
            "+        # Normalize paths to use forward slashes for comparison",
            "+        added_normalized = [path.replace(os.sep, \"/\") for path in added]",
            "+        self.assertIn(\"dir1/file1.txt\", added_normalized)",
            "+        self.assertIn(\"dir1/dir2/file2.txt\", added_normalized)",
            "+        self.assertIn(\"dir1/dir2/dir3/file3.txt\", added_normalized)",
            "+",
            "+        # Verify files are actually staged",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"dir1/file1.txt\", index)",
            "+        self.assertIn(b\"dir1/dir2/file2.txt\", index)",
            "+        self.assertIn(b\"dir1/dir2/dir3/file3.txt\", index)",
            "+",
            "+    def test_add_directory_with_tracked_files(self) -> None:",
            "+        \"\"\"Test adding a directory with some files already tracked.\"\"\"",
            "+        # Create a subdirectory with files",
            "+        subdir = os.path.join(self.repo.path, \"mixed\")",
            "+        os.mkdir(subdir)",
            "+",
            "+        # Create and commit one file",
            "+        tracked_file = os.path.join(subdir, \"tracked.txt\")",
            "+        with open(tracked_file, \"w\") as f:",
            "+            f.write(\"already tracked\")",
            "+        porcelain.add(self.repo.path, paths=[tracked_file])",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"Add tracked file\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Add more untracked files",
            "+        with open(os.path.join(subdir, \"untracked1.txt\"), \"w\") as f:",
            "+            f.write(\"new file 1\")",
            "+        with open(os.path.join(subdir, \"untracked2.txt\"), \"w\") as f:",
            "+            f.write(\"new file 2\")",
            "+",
            "+        # Add the directory",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[\"mixed\"])",
            "+",
            "+        # Should only add the untracked files",
            "+        self.assertEqual(len(added), 2)",
            "+        # Normalize paths to use forward slashes for comparison",
            "+        added_normalized = [path.replace(os.sep, \"/\") for path in added]",
            "+        self.assertIn(\"mixed/untracked1.txt\", added_normalized)",
            "+        self.assertIn(\"mixed/untracked2.txt\", added_normalized)",
            "+        self.assertNotIn(\"mixed/tracked.txt\", added)",
            "+",
            "+        # Verify the index contains all files",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"mixed/tracked.txt\", index)",
            "+        self.assertIn(b\"mixed/untracked1.txt\", index)",
            "+        self.assertIn(b\"mixed/untracked2.txt\", index)",
            "+",
            "+    def test_add_directory_with_gitignore(self) -> None:",
            "+        \"\"\"Test adding a directory respects .gitignore patterns.\"\"\"",
            "+        # Create .gitignore",
            "+        with open(os.path.join(self.repo.path, \".gitignore\"), \"w\") as f:",
            "+            f.write(\"*.log\\n*.tmp\\nbuild/\\n\")",
            "+",
            "+        # Create directory with mixed files",
            "+        testdir = os.path.join(self.repo.path, \"testdir\")",
            "+        os.mkdir(testdir)",
            "+",
            "+        # Create various files",
            "+        with open(os.path.join(testdir, \"important.txt\"), \"w\") as f:",
            "+            f.write(\"keep this\")",
            "+        with open(os.path.join(testdir, \"debug.log\"), \"w\") as f:",
            "+            f.write(\"ignore this\")",
            "+        with open(os.path.join(testdir, \"temp.tmp\"), \"w\") as f:",
            "+            f.write(\"ignore this too\")",
            "+        with open(os.path.join(testdir, \"readme.md\"), \"w\") as f:",
            "+            f.write(\"keep this too\")",
            "+",
            "+        # Create a build directory that should be ignored",
            "+        builddir = os.path.join(testdir, \"build\")",
            "+        os.mkdir(builddir)",
            "+        with open(os.path.join(builddir, \"output.txt\"), \"w\") as f:",
            "+            f.write(\"ignore entire directory\")",
            "+",
            "+        # Add the directory",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[\"testdir\"])",
            "+",
            "+        # Should only add non-ignored files",
            "+        # Normalize paths to use forward slashes for comparison",
            "+        added_normalized = {path.replace(os.sep, \"/\") for path in added}",
            "+        self.assertEqual(",
            "+            added_normalized, {\"testdir/important.txt\", \"testdir/readme.md\"}",
            "+        )",
            "+",
            "+        # Check ignored files",
            "+        # Normalize paths to use forward slashes for comparison",
            "+        ignored_normalized = {path.replace(os.sep, \"/\") for path in ignored}",
            "+        self.assertIn(\"testdir/debug.log\", ignored_normalized)",
            "+        self.assertIn(\"testdir/temp.tmp\", ignored_normalized)",
            "+        self.assertIn(\"testdir/build/\", ignored_normalized)",
            "+",
            "+    def test_add_multiple_directories(self) -> None:",
            "+        \"\"\"Test adding multiple directories in one call.\"\"\"",
            "+        # Create multiple directories",
            "+        for dirname in [\"dir1\", \"dir2\", \"dir3\"]:",
            "+            dirpath = os.path.join(self.repo.path, dirname)",
            "+            os.mkdir(dirpath)",
            "+            # Add files to each directory",
            "+            for i in range(2):",
            "+                with open(os.path.join(dirpath, f\"file{i}.txt\"), \"w\") as f:",
            "+                    f.write(f\"content {dirname} {i}\")",
            "+",
            "+        # Add all directories at once",
            "+        added, ignored = porcelain.add(self.repo.path, paths=[\"dir1\", \"dir2\", \"dir3\"])",
            "+",
            "+        # Should add all files from all directories",
            "+        self.assertEqual(len(added), 6)",
            "+        # Normalize paths to use forward slashes for comparison",
            "+        added_normalized = [path.replace(os.sep, \"/\") for path in added]",
            "+        for dirname in [\"dir1\", \"dir2\", \"dir3\"]:",
            "+            for i in range(2):",
            "+                self.assertIn(f\"{dirname}/file{i}.txt\", added_normalized)",
            "+",
            "+        # Verify all files are staged",
            "+        index = self.repo.open_index()",
            "+        self.assertEqual(len(index), 6)",
            "+",
            "+    def test_add_default_paths_includes_modified_files(self) -> None:",
            "+        \"\"\"Test that add() with no paths includes both untracked and modified files.\"\"\"",
            "+        # Create and commit initial file",
            "+        initial_file = os.path.join(self.repo.path, \"existing.txt\")",
            "+        with open(initial_file, \"w\") as f:",
            "+            f.write(\"initial content\\n\")",
            "+        porcelain.add(repo=self.repo.path, paths=[initial_file])",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"initial commit\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Modify the existing file (this creates an unstaged change)",
            "+        with open(initial_file, \"w\") as f:",
            "+            f.write(\"modified content\\n\")",
            "+",
            "+        # Create a new untracked file",
            "+        new_file = os.path.join(self.repo.path, \"new.txt\")",
            "+        with open(new_file, \"w\") as f:",
            "+            f.write(\"new file content\\n\")",
            "+",
            "+        # Call add() with no paths - should stage both modified and untracked files",
            "+        added_files, ignored_files = porcelain.add(repo=self.repo.path)",
            "+",
            "+        # Verify both files were added",
            "+        self.assertIn(\"existing.txt\", added_files)",
            "+        self.assertIn(\"new.txt\", added_files)",
            "+        self.assertEqual(len(ignored_files), 0)",
            "+",
            "+        # Verify both files are now staged",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"existing.txt\", index)",
            "+        self.assertIn(b\"new.txt\", index)",
            "+",
            " ",
            " class RemoveTests(PorcelainTestCase):",
            "     def test_remove_file(self) -> None:",
            "         fullpath = os.path.join(self.repo.path, \"foo\")",
            "         with open(fullpath, \"w\") as f:",
            "             f.write(\"BAR\")",
            "         porcelain.add(self.repo.path, paths=[fullpath])",
            "@@ -1034,67 +1818,377 @@",
            "             repo=self.repo,",
            "             message=b\"test\",",
            "             author=b\"test <email>\",",
            "             committer=b\"test <email>\",",
            "         )",
            "         self.assertTrue(os.path.exists(os.path.join(self.repo.path, \"foo\")))",
            "         cwd = os.getcwd()",
            "-        try:",
            "-            os.chdir(self.repo.path)",
            "-            porcelain.remove(self.repo.path, paths=[\"foo\"])",
            "-        finally:",
            "-            os.chdir(cwd)",
            "+        self.addCleanup(os.chdir, cwd)",
            "+        os.chdir(self.repo.path)",
            "+        porcelain.remove(self.repo.path, paths=[\"foo\"])",
            "         self.assertFalse(os.path.exists(os.path.join(self.repo.path, \"foo\")))",
            " ",
            "     def test_remove_file_staged(self) -> None:",
            "         fullpath = os.path.join(self.repo.path, \"foo\")",
            "         with open(fullpath, \"w\") as f:",
            "             f.write(\"BAR\")",
            "         cwd = os.getcwd()",
            "-        try:",
            "-            os.chdir(self.repo.path)",
            "-            porcelain.add(self.repo.path, paths=[fullpath])",
            "-            self.assertRaises(Exception, porcelain.rm, self.repo.path, paths=[\"foo\"])",
            "-        finally:",
            "-            os.chdir(cwd)",
            "+        self.addCleanup(os.chdir, cwd)",
            "+        os.chdir(self.repo.path)",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        self.assertRaises(Exception, porcelain.rm, self.repo.path, paths=[\"foo\"])",
            " ",
            "     def test_remove_file_removed_on_disk(self) -> None:",
            "         fullpath = os.path.join(self.repo.path, \"foo\")",
            "         with open(fullpath, \"w\") as f:",
            "             f.write(\"BAR\")",
            "         porcelain.add(self.repo.path, paths=[fullpath])",
            "         cwd = os.getcwd()",
            "-        try:",
            "-            os.chdir(self.repo.path)",
            "-            os.remove(fullpath)",
            "-            porcelain.remove(self.repo.path, paths=[\"foo\"])",
            "-        finally:",
            "+        self.addCleanup(os.chdir, cwd)",
            "+        os.chdir(self.repo.path)",
            "+        os.remove(fullpath)",
            "+        porcelain.remove(self.repo.path, paths=[\"foo\"])",
            "+        self.assertFalse(os.path.exists(os.path.join(self.repo.path, \"foo\")))",
            "+",
            "+    def test_remove_from_different_directory(self) -> None:",
            "+        # Create a subdirectory with a file",
            "+        subdir = os.path.join(self.repo.path, \"mydir\")",
            "+        os.makedirs(subdir)",
            "+        fullpath = os.path.join(subdir, \"myfile\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+",
            "+        # Add and commit the file",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Change to a different directory",
            "+        cwd = os.getcwd()",
            "+        tempdir = tempfile.mkdtemp()",
            "+",
            "+        def cleanup():",
            "             os.chdir(cwd)",
            "+            shutil.rmtree(tempdir)",
            "+",
            "+        self.addCleanup(cleanup)",
            "+        os.chdir(tempdir)",
            "+        # Remove the file using relative path from repository root",
            "+        porcelain.remove(self.repo.path, paths=[\"mydir/myfile\"])",
            "+",
            "+        # Verify file was removed",
            "+        self.assertFalse(os.path.exists(fullpath))",
            "+",
            "+    def test_remove_with_absolute_path(self) -> None:",
            "+        # Create a file",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+",
            "+        # Add and commit the file",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Change to a different directory",
            "+        cwd = os.getcwd()",
            "+        tempdir = tempfile.mkdtemp()",
            "+",
            "+        def cleanup():",
            "+            os.chdir(cwd)",
            "+            shutil.rmtree(tempdir)",
            "+",
            "+        self.addCleanup(cleanup)",
            "+        os.chdir(tempdir)",
            "+        # Remove the file using absolute path",
            "+        porcelain.remove(self.repo.path, paths=[fullpath])",
            "+",
            "+        # Verify file was removed",
            "+        self.assertFalse(os.path.exists(fullpath))",
            "+",
            "+    def test_remove_with_filter_normalization(self) -> None:",
            "+        # Enable autocrlf to normalize line endings",
            "+        config = self.repo.get_config()",
            "+        config.set((\"core\",), \"autocrlf\", b\"true\")",
            "+        config.write_to_path()",
            "+",
            "+        # Create a file with LF line endings (will be stored with LF in index)",
            "+        fullpath = os.path.join(self.repo.path, \"foo.txt\")",
            "+        with open(fullpath, \"wb\") as f:",
            "+            f.write(b\"line1\\nline2\\nline3\")",
            "+",
            "+        # Add and commit the file (stored with LF in index)",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"Add file with LF\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Simulate checkout with CRLF conversion (as would happen on Windows)",
            "+        with open(fullpath, \"wb\") as f:",
            "+            f.write(b\"line1\\r\\nline2\\r\\nline3\")",
            "+",
            "+        # Verify file exists",
            "+        self.assertTrue(os.path.exists(fullpath))",
            "+",
            "+        # Remove the file - this should not fail even though working tree has CRLF",
            "+        # and index has LF (thanks to the normalization in the commit)",
            "+        cwd = os.getcwd()",
            "+        os.chdir(self.repo.path)",
            "+        self.addCleanup(os.chdir, cwd)",
            "+        porcelain.remove(self.repo.path, paths=[\"foo.txt\"])",
            "+",
            "+        # Verify file was removed",
            "+        self.assertFalse(os.path.exists(fullpath))",
            "+",
            "+",
            "+class MvTests(PorcelainTestCase):",
            "+    def test_mv_file(self) -> None:",
            "+        # Create a file",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+",
            "+        # Add and commit the file",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Move the file",
            "+        porcelain.mv(self.repo.path, \"foo\", \"bar\")",
            "+",
            "+        # Verify old path doesn't exist and new path does",
            "+        self.assertFalse(os.path.exists(os.path.join(self.repo.path, \"foo\")))",
            "+        self.assertTrue(os.path.exists(os.path.join(self.repo.path, \"bar\")))",
            "+",
            "+        # Verify index was updated",
            "+        index = self.repo.open_index()",
            "+        self.assertNotIn(b\"foo\", index)",
            "+        self.assertIn(b\"bar\", index)",
            "+",
            "+    def test_mv_file_to_existing_directory(self) -> None:",
            "+        # Create a file and a directory",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+",
            "+        dirpath = os.path.join(self.repo.path, \"mydir\")",
            "+        os.makedirs(dirpath)",
            "+",
            "+        # Add and commit the file",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Move the file into the directory",
            "+        porcelain.mv(self.repo.path, \"foo\", \"mydir\")",
            "+",
            "+        # Verify file moved into directory",
            "         self.assertFalse(os.path.exists(os.path.join(self.repo.path, \"foo\")))",
            "+        self.assertTrue(os.path.exists(os.path.join(self.repo.path, \"mydir\", \"foo\")))",
            "+",
            "+        # Verify index was updated",
            "+        index = self.repo.open_index()",
            "+        self.assertNotIn(b\"foo\", index)",
            "+        self.assertIn(b\"mydir/foo\", index)",
            "+",
            "+    def test_mv_file_force_overwrite(self) -> None:",
            "+        # Create two files",
            "+        fullpath1 = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath1, \"w\") as f:",
            "+            f.write(\"FOO\")",
            "+",
            "+        fullpath2 = os.path.join(self.repo.path, \"bar\")",
            "+        with open(fullpath2, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+",
            "+        # Add and commit both files",
            "+        porcelain.add(self.repo.path, paths=[fullpath1, fullpath2])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Try to move without force (should fail)",
            "+        self.assertRaises(porcelain.Error, porcelain.mv, self.repo.path, \"foo\", \"bar\")",
            "+",
            "+        # Move with force",
            "+        porcelain.mv(self.repo.path, \"foo\", \"bar\", force=True)",
            "+",
            "+        # Verify foo doesn't exist and bar has foo's content",
            "+        self.assertFalse(os.path.exists(os.path.join(self.repo.path, \"foo\")))",
            "+        with open(os.path.join(self.repo.path, \"bar\")) as f:",
            "+            self.assertEqual(f.read(), \"FOO\")",
            "+",
            "+    def test_mv_file_not_tracked(self) -> None:",
            "+        # Create an untracked file",
            "+        fullpath = os.path.join(self.repo.path, \"untracked\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"UNTRACKED\")",
            "+",
            "+        # Try to move it (should fail)",
            "+        self.assertRaises(",
            "+            porcelain.Error, porcelain.mv, self.repo.path, \"untracked\", \"tracked\"",
            "+        )",
            "+",
            "+    def test_mv_file_not_exists(self) -> None:",
            "+        # Try to move a non-existent file",
            "+        self.assertRaises(",
            "+            porcelain.Error, porcelain.mv, self.repo.path, \"nonexistent\", \"destination\"",
            "+        )",
            "+",
            "+    def test_mv_absolute_paths(self) -> None:",
            "+        # Create a file",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+",
            "+        # Add and commit the file",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Move using absolute paths",
            "+        dest_path = os.path.join(self.repo.path, \"bar\")",
            "+        porcelain.mv(self.repo.path, fullpath, dest_path)",
            "+",
            "+        # Verify file moved",
            "+        self.assertFalse(os.path.exists(fullpath))",
            "+        self.assertTrue(os.path.exists(dest_path))",
            "+",
            "+    def test_mv_from_different_directory(self) -> None:",
            "+        # Create a subdirectory with a file",
            "+        subdir = os.path.join(self.repo.path, \"mydir\")",
            "+        os.makedirs(subdir)",
            "+        fullpath = os.path.join(subdir, \"myfile\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+",
            "+        # Add and commit the file",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=self.repo,",
            "+            message=b\"test\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Change to a different directory and move the file",
            "+        cwd = os.getcwd()",
            "+        tempdir = tempfile.mkdtemp()",
            "+",
            "+        def cleanup():",
            "+            os.chdir(cwd)",
            "+            shutil.rmtree(tempdir)",
            "+",
            "+        self.addCleanup(cleanup)",
            "+        os.chdir(tempdir)",
            "+        # Move the file using relative path from repository root",
            "+        porcelain.mv(self.repo.path, \"mydir/myfile\", \"renamed\")",
            "+",
            "+        # Verify file was moved",
            "+        self.assertFalse(os.path.exists(fullpath))",
            "+        self.assertTrue(os.path.exists(os.path.join(self.repo.path, \"renamed\")))",
            " ",
            " ",
            " class LogTests(PorcelainTestCase):",
            "     def test_simple(self) -> None:",
            "         c1, c2, c3 = build_commit_graph(",
            "             self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "         )",
            "         self.repo.refs[b\"HEAD\"] = c3.id",
            "+        self.maxDiff = None",
            "         outstream = StringIO()",
            "         porcelain.log(self.repo.path, outstream=outstream)",
            "-        self.assertEqual(3, outstream.getvalue().count(\"-\" * 50))",
            "+        self.assertEqual(",
            "+            outstream.getvalue(),",
            "+            \"\"\"\\",
            "+--------------------------------------------------",
            "+commit: 4a3b887baa9ecb2d054d2469b628aef84e2d74f0",
            "+merge: 7508036b1cfec5aa9cef0d5a7f04abcecfe09112",
            "+Author: Test Author <test@nodomain.com>",
            "+Committer: Test Committer <test@nodomain.com>",
            "+Date:   Fri Jan 01 2010 00:00:00 +0000",
            "+",
            "+Commit 3",
            "+",
            "+--------------------------------------------------",
            "+commit: 7508036b1cfec5aa9cef0d5a7f04abcecfe09112",
            "+Author: Test Author <test@nodomain.com>",
            "+Committer: Test Committer <test@nodomain.com>",
            "+Date:   Fri Jan 01 2010 00:00:00 +0000",
            "+",
            "+Commit 2",
            "+",
            "+--------------------------------------------------",
            "+commit: 11d3cf672a19366435c1983c7340b008ec6b8bf3",
            "+Author: Test Author <test@nodomain.com>",
            "+Committer: Test Committer <test@nodomain.com>",
            "+Date:   Fri Jan 01 2010 00:00:00 +0000",
            "+",
            "+Commit 1",
            "+",
            "+\"\"\",",
            "+        )",
            " ",
            "     def test_max_entries(self) -> None:",
            "         c1, c2, c3 = build_commit_graph(",
            "             self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "         )",
            "         self.repo.refs[b\"HEAD\"] = c3.id",
            "         outstream = StringIO()",
            "         porcelain.log(self.repo.path, outstream=outstream, max_entries=1)",
            "         self.assertEqual(1, outstream.getvalue().count(\"-\" * 50))",
            " ",
            "+    def test_no_revisions(self) -> None:",
            "+        outstream = StringIO()",
            "+        porcelain.log(self.repo.path, outstream=outstream)",
            "+        self.assertEqual(\"\", outstream.getvalue())",
            "+",
            "+    def test_empty_message(self) -> None:",
            "+        c1 = make_commit(message=\"\")",
            "+        self.repo.object_store.add_object(c1)",
            "+        self.repo.refs[b\"HEAD\"] = c1.id",
            "+        outstream = StringIO()",
            "+        porcelain.log(self.repo.path, outstream=outstream)",
            "+        self.assertEqual(",
            "+            outstream.getvalue(),",
            "+            \"\"\"\\",
            "+--------------------------------------------------",
            "+commit: 4a7ad5552fad70647a81fb9a4a923ccefcca4b76",
            "+Author: Test Author <test@nodomain.com>",
            "+Committer: Test Committer <test@nodomain.com>",
            "+Date:   Fri Jan 01 2010 00:00:00 +0000",
            "+\"\"\",",
            "+        )",
            "+",
            " ",
            " class ShowTests(PorcelainTestCase):",
            "     def test_nolist(self) -> None:",
            "         c1, c2, c3 = build_commit_graph(",
            "             self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "         )",
            "         self.repo.refs[b\"HEAD\"] = c3.id",
            "@@ -1188,14 +2282,59 @@",
            " --- /dev/null",
            " +++ b/somename",
            " @@ -0,0 +1 @@",
            " +The Foo",
            " \"\"\",",
            "         )",
            " ",
            "+    def test_tag_unicode(self) -> None:",
            "+        a = Blob.from_string(b\"The Foo\\n\")",
            "+        ta = Tree()",
            "+        ta.add(b\"somename\", 0o100644, a.id)",
            "+        ca = make_commit(tree=ta.id)",
            "+        self.repo.object_store.add_objects([(a, None), (ta, None), (ca, None)])",
            "+        porcelain.tag_create(",
            "+            self.repo.path,",
            "+            \"tryme\",",
            "+            \"foo <foo@bar.com>\",",
            "+            \"bar\",",
            "+            annotated=True,",
            "+            objectish=ca.id,",
            "+            tag_time=1552854211,",
            "+            tag_timezone=0,",
            "+        )",
            "+        outstream = StringIO()",
            "+        porcelain.show(self.repo, objects=[b\"refs/tags/tryme\"], outstream=outstream)",
            "+        self.maxDiff = None",
            "+        self.assertMultiLineEqual(",
            "+            outstream.getvalue(),",
            "+            \"\"\"\\",
            "+Tagger: foo <foo@bar.com>",
            "+Date:   Sun Mar 17 2019 20:23:31 +0000",
            "+",
            "+bar",
            "+",
            "+--------------------------------------------------",
            "+commit: 344da06c1bb85901270b3e8875c988a027ec087d",
            "+Author: Test Author <test@nodomain.com>",
            "+Committer: Test Committer <test@nodomain.com>",
            "+Date:   Fri Jan 01 2010 00:00:00 +0000",
            "+",
            "+Test message.",
            "+",
            "+diff --git a/somename b/somename",
            "+new file mode 100644",
            "+index 0000000..ea5c7bf",
            "+--- /dev/null",
            "++++ b/somename",
            "+@@ -0,0 +1 @@",
            "++The Foo",
            "+\"\"\",",
            "+        )",
            "+",
            "     def test_commit_with_change(self) -> None:",
            "         a = Blob.from_string(b\"The Foo\\n\")",
            "         ta = Tree()",
            "         ta.add(b\"somename\", 0o100644, a.id)",
            "         ca = make_commit(tree=ta.id)",
            "         b = Blob.from_string(b\"The Bar\\n\")",
            "         tb = Tree()",
            "@@ -1356,26 +2495,28 @@",
            "         self.assertEqual(list(tags.keys()), [b\"tryme\"])",
            "         tag = self.repo[b\"refs/tags/tryme\"]",
            "         self.assertIsInstance(tag, Tag)",
            "         self.assertEqual(b\"foo <foo@bar.com>\", tag.tagger)",
            "         self.assertEqual(b\"bar\\n\", tag.message)",
            "         self.assertRecentTimestamp(tag.tag_time)",
            "         tag = self.repo[b\"refs/tags/tryme\"]",
            "+        assert isinstance(tag, Tag)",
            "         # GPG Signatures aren't deterministic, so we can't do a static assertion.",
            "         tag.verify()",
            "         tag.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            " ",
            "         self.import_non_default_key()",
            "         self.assertRaises(",
            "             gpg.errors.MissingSignatures,",
            "             tag.verify,",
            "             keyids=[PorcelainGpgTestCase.NON_DEFAULT_KEY_ID],",
            "         )",
            " ",
            "-        tag._chunked_text = [b\"bad data\", tag._signature]",
            "+        assert tag.signature is not None",
            "+        tag._chunked_text = [b\"bad data\", tag.signature]",
            "         self.assertRaises(",
            "             gpg.errors.BadSignatures,",
            "             tag.verify,",
            "         )",
            " ",
            "     def test_non_default_key(self) -> None:",
            "         c1, c2, c3 = build_commit_graph(",
            "@@ -1388,28 +2529,228 @@",
            " ",
            "         porcelain.tag_create(",
            "             self.repo.path,",
            "             b\"tryme\",",
            "             b\"foo <foo@bar.com>\",",
            "             b\"bar\",",
            "             annotated=True,",
            "-            sign=PorcelainGpgTestCase.NON_DEFAULT_KEY_ID,",
            "+            sign=True,",
            "         )",
            " ",
            "         tags = self.repo.refs.as_dict(b\"refs/tags\")",
            "         self.assertEqual(list(tags.keys()), [b\"tryme\"])",
            "         tag = self.repo[b\"refs/tags/tryme\"]",
            "         self.assertIsInstance(tag, Tag)",
            "         self.assertEqual(b\"foo <foo@bar.com>\", tag.tagger)",
            "         self.assertEqual(b\"bar\\n\", tag.message)",
            "         self.assertRecentTimestamp(tag.tag_time)",
            "         tag = self.repo[b\"refs/tags/tryme\"]",
            "+        assert isinstance(tag, Tag)",
            "         # GPG Signatures aren't deterministic, so we can't do a static assertion.",
            "         tag.verify()",
            " ",
            "+    def test_sign_uses_config_signingkey(self) -> None:",
            "+        \"\"\"Test that sign=True uses user.signingKey from config.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up user.signingKey in config",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create tag with sign=True (should use signingKey from config)",
            "+        porcelain.tag_create(",
            "+            self.repo.path,",
            "+            b\"signed-tag\",",
            "+            b\"foo <foo@bar.com>\",",
            "+            b\"Tag with configured key\",",
            "+            annotated=True,",
            "+            sign=True,  # This should read user.signingKey from config",
            "+        )",
            "+",
            "+        tags = self.repo.refs.as_dict(b\"refs/tags\")",
            "+        self.assertEqual(list(tags.keys()), [b\"signed-tag\"])",
            "+        tag = self.repo[b\"refs/tags/signed-tag\"]",
            "+        self.assertIsInstance(tag, Tag)",
            "+",
            "+        # Verify the tag is signed with the configured key",
            "+        tag.verify()",
            "+        tag.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            "+",
            "+    def test_tag_gpg_sign_config_enabled(self) -> None:",
            "+        \"\"\"Test that tag.gpgSign=true automatically signs tags.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up user.signingKey and tag.gpgSign in config",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"tag\",), \"gpgSign\", True)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create tag without explicit sign parameter (should auto-sign due to config)",
            "+        porcelain.tag_create(",
            "+            self.repo.path,",
            "+            b\"auto-signed-tag\",",
            "+            b\"foo <foo@bar.com>\",",
            "+            b\"Auto-signed tag\",",
            "+            annotated=True,",
            "+            # No sign parameter - should use tag.gpgSign config",
            "+        )",
            "+",
            "+        tags = self.repo.refs.as_dict(b\"refs/tags\")",
            "+        self.assertEqual(list(tags.keys()), [b\"auto-signed-tag\"])",
            "+        tag = self.repo[b\"refs/tags/auto-signed-tag\"]",
            "+        self.assertIsInstance(tag, Tag)",
            "+",
            "+        # Verify the tag is signed due to config",
            "+        tag.verify()",
            "+        tag.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            "+",
            "+    def test_tag_gpg_sign_config_disabled(self) -> None:",
            "+        \"\"\"Test that tag.gpgSign=false does not sign tags.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up user.signingKey and tag.gpgSign=false in config",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"tag\",), \"gpgSign\", False)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create tag without explicit sign parameter (should not sign)",
            "+        porcelain.tag_create(",
            "+            self.repo.path,",
            "+            b\"unsigned-tag\",",
            "+            b\"foo <foo@bar.com>\",",
            "+            b\"Unsigned tag\",",
            "+            annotated=True,",
            "+            # No sign parameter - should use tag.gpgSign=false config",
            "+        )",
            "+",
            "+        tags = self.repo.refs.as_dict(b\"refs/tags\")",
            "+        self.assertEqual(list(tags.keys()), [b\"unsigned-tag\"])",
            "+        tag = self.repo[b\"refs/tags/unsigned-tag\"]",
            "+        self.assertIsInstance(tag, Tag)",
            "+",
            "+        # Verify the tag is not signed",
            "+        self.assertIsNone(tag._signature)",
            "+",
            "+    def test_tag_gpg_sign_config_no_signing_key(self) -> None:",
            "+        \"\"\"Test that tag.gpgSign=true works without user.signingKey (uses default).\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up tag.gpgSign but no user.signingKey",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"tag\",), \"gpgSign\", True)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create tag without explicit sign parameter (should auto-sign with default key)",
            "+        porcelain.tag_create(",
            "+            self.repo.path,",
            "+            b\"default-signed-tag\",",
            "+            b\"foo <foo@bar.com>\",",
            "+            b\"Default signed tag\",",
            "+            annotated=True,",
            "+            # No sign parameter - should use tag.gpgSign config with default key",
            "+        )",
            "+",
            "+        tags = self.repo.refs.as_dict(b\"refs/tags\")",
            "+        self.assertEqual(list(tags.keys()), [b\"default-signed-tag\"])",
            "+        tag = self.repo[b\"refs/tags/default-signed-tag\"]",
            "+        self.assertIsInstance(tag, Tag)",
            "+",
            "+        # Verify the tag is signed with default key",
            "+        tag.verify()",
            "+",
            "+    def test_explicit_sign_overrides_config(self) -> None:",
            "+        \"\"\"Test that explicit sign parameter overrides tag.gpgSign config.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up tag.gpgSign=false but explicitly pass sign=True",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"tag\",), \"gpgSign\", False)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create tag with explicit sign=True (should override config)",
            "+        porcelain.tag_create(",
            "+            self.repo.path,",
            "+            b\"explicit-signed-tag\",",
            "+            b\"foo <foo@bar.com>\",",
            "+            b\"Explicitly signed tag\",",
            "+            annotated=True,",
            "+            sign=True,  # This should override tag.gpgSign=false",
            "+        )",
            "+",
            "+        tags = self.repo.refs.as_dict(b\"refs/tags\")",
            "+        self.assertEqual(list(tags.keys()), [b\"explicit-signed-tag\"])",
            "+        tag = self.repo[b\"refs/tags/explicit-signed-tag\"]",
            "+        self.assertIsInstance(tag, Tag)",
            "+",
            "+        # Verify the tag is signed despite config=false",
            "+        tag.verify()",
            "+        tag.verify(keyids=[PorcelainGpgTestCase.DEFAULT_KEY_ID])",
            "+",
            "+    def test_explicit_false_disables_tag_signing(self) -> None:",
            "+        \"\"\"Test that explicit sign=False disables signing even with config=true.\"\"\"",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "+        )",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Set up tag.gpgSign=true but explicitly pass sign=False",
            "+        cfg = self.repo.get_config()",
            "+        cfg.set((\"user\",), \"signingKey\", PorcelainGpgTestCase.DEFAULT_KEY_ID)",
            "+        cfg.set((\"tag\",), \"gpgSign\", True)",
            "+        cfg.write_to_path()",
            "+",
            "+        self.import_default_key()",
            "+",
            "+        # Create tag with explicit sign=False (should disable signing)",
            "+        porcelain.tag_create(",
            "+            self.repo.path,",
            "+            b\"explicit-unsigned-tag\",",
            "+            b\"foo <foo@bar.com>\",",
            "+            b\"Explicitly unsigned tag\",",
            "+            annotated=True,",
            "+            sign=False,  # This should override tag.gpgSign=true",
            "+        )",
            "+",
            "+        tags = self.repo.refs.as_dict(b\"refs/tags\")",
            "+        self.assertEqual(list(tags.keys()), [b\"explicit-unsigned-tag\"])",
            "+        tag = self.repo[b\"refs/tags/explicit-unsigned-tag\"]",
            "+        self.assertIsInstance(tag, Tag)",
            "+",
            "+        # Verify the tag is NOT signed despite config=true",
            "+        self.assertIsNone(tag._signature)",
            "+",
            " ",
            " class TagCreateTests(PorcelainTestCase):",
            "     def test_annotated(self) -> None:",
            "         c1, c2, c3 = build_commit_graph(",
            "             self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "         )",
            "         self.repo.refs[b\"HEAD\"] = c3.id",
            "@@ -1497,15 +2838,15 @@",
            "             f.write(b\"OOH\")",
            " ",
            "         porcelain.reset(self.repo, \"hard\", b\"HEAD\")",
            " ",
            "         index = self.repo.open_index()",
            "         changes = list(",
            "             tree_changes(",
            "-                self.repo,",
            "+                self.repo.object_store,",
            "                 index.commit(self.repo.object_store),",
            "                 self.repo[b\"HEAD\"].tree,",
            "             )",
            "         )",
            " ",
            "         self.assertEqual([], changes)",
            " ",
            "@@ -1532,22 +2873,270 @@",
            "         )",
            " ",
            "         porcelain.reset(self.repo, \"hard\", sha)",
            " ",
            "         index = self.repo.open_index()",
            "         changes = list(",
            "             tree_changes(",
            "-                self.repo,",
            "+                self.repo.object_store,",
            "                 index.commit(self.repo.object_store),",
            "                 self.repo[sha].tree,",
            "             )",
            "         )",
            " ",
            "         self.assertEqual([], changes)",
            " ",
            "+    def test_hard_commit_short_hash(self) -> None:",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Some message\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        with open(fullpath, \"wb\") as f:",
            "+            f.write(b\"BAZ\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Some other message\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Test with short hash (7 characters)",
            "+        short_sha = sha[:7].decode(\"ascii\")",
            "+        porcelain.reset(self.repo, \"hard\", short_sha)",
            "+",
            "+        index = self.repo.open_index()",
            "+        changes = list(",
            "+            tree_changes(",
            "+                self.repo.object_store,",
            "+                index.commit(self.repo.object_store),",
            "+                self.repo[sha].tree,",
            "+            )",
            "+        )",
            "+",
            "+        self.assertEqual([], changes)",
            "+",
            "+    def test_hard_deletes_untracked_files(self) -> None:",
            "+        \"\"\"Test that reset --hard deletes files that don't exist in target tree.\"\"\"",
            "+        # Create and commit a file",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        sha1 = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"First commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Create another file and commit",
            "+        fullpath2 = os.path.join(self.repo.path, \"bar\")",
            "+        with open(fullpath2, \"w\") as f:",
            "+            f.write(\"BAZ\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath2])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Second commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Reset hard to first commit - this should delete 'bar'",
            "+        porcelain.reset(self.repo, \"hard\", sha1)",
            "+",
            "+        # Check that 'foo' still exists and 'bar' is deleted",
            "+        self.assertTrue(os.path.exists(fullpath))",
            "+        self.assertFalse(os.path.exists(fullpath2))",
            "+",
            "+        # Check index matches first commit",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"foo\", index)",
            "+        self.assertNotIn(b\"bar\", index)",
            "+",
            "+    def test_hard_deletes_files_in_subdirs(self) -> None:",
            "+        \"\"\"Test that reset --hard deletes files in subdirectories.\"\"\"",
            "+        # Create and commit files in subdirectory",
            "+        subdir = os.path.join(self.repo.path, \"subdir\")",
            "+        os.makedirs(subdir)",
            "+        file1 = os.path.join(subdir, \"file1\")",
            "+        file2 = os.path.join(subdir, \"file2\")",
            "+",
            "+        with open(file1, \"w\") as f:",
            "+            f.write(\"content1\")",
            "+        with open(file2, \"w\") as f:",
            "+            f.write(\"content2\")",
            "+",
            "+        porcelain.add(self.repo.path, paths=[file1, file2])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"First commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Remove one file from subdirectory and commit",
            "+        porcelain.rm(self.repo.path, paths=[file2])",
            "+        sha2 = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Remove file2\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Create file2 again (untracked)",
            "+        with open(file2, \"w\") as f:",
            "+            f.write(\"new content\")",
            "+",
            "+        # Reset to commit that has file2 removed - should delete untracked file2",
            "+        porcelain.reset(self.repo, \"hard\", sha2)",
            "+",
            "+        self.assertTrue(os.path.exists(file1))",
            "+        self.assertFalse(os.path.exists(file2))",
            "+",
            "+    def test_hard_reset_to_remote_branch(self) -> None:",
            "+        \"\"\"Test reset --hard to remote branch deletes local files not in remote.\"\"\"",
            "+        # Create a file and commit",
            "+        file1 = os.path.join(self.repo.path, \"file1\")",
            "+        with open(file1, \"w\") as f:",
            "+            f.write(\"content1\")",
            "+        porcelain.add(self.repo.path, paths=[file1])",
            "+        sha1 = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Initial commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Create a \"remote\" ref that doesn't have additional files",
            "+        self.repo.refs[b\"refs/remotes/origin/master\"] = sha1",
            "+",
            "+        # Add another file locally and commit",
            "+        file2 = os.path.join(self.repo.path, \"file2\")",
            "+        with open(file2, \"w\") as f:",
            "+            f.write(\"content2\")",
            "+        porcelain.add(self.repo.path, paths=[file2])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Add file2\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Both files should exist",
            "+        self.assertTrue(os.path.exists(file1))",
            "+        self.assertTrue(os.path.exists(file2))",
            "+",
            "+        # Reset to remote branch - should delete file2",
            "+        porcelain.reset(self.repo, \"hard\", b\"refs/remotes/origin/master\")",
            "+",
            "+        # file1 should exist, file2 should be deleted",
            "+        self.assertTrue(os.path.exists(file1))",
            "+        self.assertFalse(os.path.exists(file2))",
            "+",
            "+    def test_mixed_reset(self) -> None:",
            "+        # Create initial commit",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        first_sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"First commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Make second commit with modified content",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAZ\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Second commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Modify working tree without staging",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"MODIFIED\")",
            "+",
            "+        # Mixed reset to first commit",
            "+        porcelain.reset(self.repo, \"mixed\", first_sha)",
            "+",
            "+        # Check that HEAD points to first commit",
            "+        self.assertEqual(self.repo.head(), first_sha)",
            "+",
            "+        # Check that index matches first commit",
            "+        index = self.repo.open_index()",
            "+        changes = list(",
            "+            tree_changes(",
            "+                self.repo.object_store,",
            "+                index.commit(self.repo.object_store),",
            "+                self.repo[first_sha].tree,",
            "+            )",
            "+        )",
            "+        self.assertEqual([], changes)",
            "+",
            "+        # Check that working tree is unchanged (still has \"MODIFIED\")",
            "+        with open(fullpath) as f:",
            "+            self.assertEqual(f.read(), \"MODIFIED\")",
            "+",
            "+    def test_soft_reset(self) -> None:",
            "+        # Create initial commit",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAR\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        first_sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"First commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Make second commit with modified content",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"BAZ\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Second commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Stage a new change",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"STAGED\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+",
            "+        # Soft reset to first commit",
            "+        porcelain.reset(self.repo, \"soft\", first_sha)",
            "+",
            "+        # Check that HEAD points to first commit",
            "+        self.assertEqual(self.repo.head(), first_sha)",
            "+",
            "+        # Check that index still has the staged change (not reset)",
            "+        index = self.repo.open_index()",
            "+        # The index should still contain the staged content, not the first commit's content",
            "+        self.assertIn(b\"foo\", index)",
            "+",
            "+        # Check that working tree is unchanged",
            "+        with open(fullpath) as f:",
            "+            self.assertEqual(f.read(), \"STAGED\")",
            "+",
            " ",
            " class ResetFileTests(PorcelainTestCase):",
            "     def test_reset_modify_file_to_commit(self) -> None:",
            "         file = \"foo\"",
            "         full_path = os.path.join(self.repo.path, file)",
            " ",
            "         with open(full_path, \"w\") as f:",
            "@@ -1624,83 +3213,255 @@",
            "         committer=b\"Jane <jane@example.com>\",",
            "         author=b\"John <john@example.com>\",",
            "     )",
            " ",
            "     return sha, file_path",
            " ",
            " ",
            "+class RevertTests(PorcelainTestCase):",
            "+    def test_revert_simple(self) -> None:",
            "+        # Create initial commit",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"initial content\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Initial commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Make a change",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"modified content\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        change_sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Change content\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Revert the change",
            "+        revert_sha = porcelain.revert(self.repo.path, commits=[change_sha])",
            "+",
            "+        # Check the file content is back to initial",
            "+        with open(fullpath) as f:",
            "+            self.assertEqual(\"initial content\\n\", f.read())",
            "+",
            "+        # Check the revert commit message",
            "+        revert_commit = self.repo[revert_sha]",
            "+        self.assertIn(b'Revert \"Change content\"', revert_commit.message)",
            "+        self.assertIn(change_sha[:7], revert_commit.message)",
            "+",
            "+    def test_revert_multiple(self) -> None:",
            "+        # Create initial commit",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"line1\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Initial commit\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Add line2",
            "+        with open(fullpath, \"a\") as f:",
            "+            f.write(\"line2\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        commit1 = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Add line2\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Add line3",
            "+        with open(fullpath, \"a\") as f:",
            "+            f.write(\"line3\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        commit2 = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Add line3\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Revert both commits (in reverse order)",
            "+        porcelain.revert(self.repo.path, commits=[commit2, commit1])",
            "+",
            "+        # Check file is back to initial state",
            "+        with open(fullpath) as f:",
            "+            self.assertEqual(\"line1\\n\", f.read())",
            "+",
            "+    def test_revert_no_commit(self) -> None:",
            "+        # Create initial commit",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"initial\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Initial\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Make a change",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"changed\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        change_sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Change\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Revert with no_commit",
            "+        result = porcelain.revert(self.repo.path, commits=[change_sha], no_commit=True)",
            "+",
            "+        # Should return None",
            "+        self.assertIsNone(result)",
            "+",
            "+        # File should be reverted",
            "+        with open(fullpath) as f:",
            "+            self.assertEqual(\"initial\\n\", f.read())",
            "+",
            "+        # HEAD should still point to the change commit",
            "+        self.assertEqual(self.repo.refs[b\"HEAD\"], change_sha)",
            "+",
            "+    def test_revert_custom_message(self) -> None:",
            "+        # Create commits",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"initial\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Initial\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"changed\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        change_sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Change\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Revert with custom message",
            "+        custom_msg = \"Custom revert message\"",
            "+        revert_sha = porcelain.revert(",
            "+            self.repo.path, commits=[change_sha], message=custom_msg",
            "+        )",
            "+",
            "+        # Check the message",
            "+        revert_commit = self.repo[revert_sha]",
            "+        self.assertEqual(custom_msg.encode(\"utf-8\"), revert_commit.message)",
            "+",
            "+    def test_revert_no_parent(self) -> None:",
            "+        # Try to revert the initial commit (no parent)",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"content\\n\")",
            "+        porcelain.add(self.repo.path, paths=[fullpath])",
            "+        initial_sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"Initial\",",
            "+            committer=b\"Jane <jane@example.com>\",",
            "+            author=b\"John <john@example.com>\",",
            "+        )",
            "+",
            "+        # Should raise an error",
            "+        with self.assertRaises(porcelain.Error) as cm:",
            "+            porcelain.revert(self.repo.path, commits=[initial_sha])",
            "+        self.assertIn(\"no parents\", str(cm.exception))",
            "+",
            "+",
            " class CheckoutTests(PorcelainTestCase):",
            "     def setUp(self) -> None:",
            "         super().setUp()",
            "         self._sha, self._foo_path = _commit_file_with_content(",
            "             self.repo, \"foo\", \"hello\\n\"",
            "         )",
            "         porcelain.branch_create(self.repo, \"uni\")",
            " ",
            "     def test_checkout_to_existing_branch(self) -> None:",
            "         self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            "         self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            " ",
            "     def test_checkout_to_non_existing_branch(self) -> None:",
            "         self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            " ",
            "         with self.assertRaises(KeyError):",
            "-            porcelain.checkout_branch(self.repo, b\"bob\")",
            "+            porcelain.checkout(self.repo, b\"bob\")",
            " ",
            "         self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            " ",
            "     def test_checkout_to_branch_with_modified_files(self) -> None:",
            "         with open(self._foo_path, \"a\") as f:",
            "             f.write(\"new message\\n\")",
            "         porcelain.add(self.repo, paths=[self._foo_path])",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual(",
            "             [{\"add\": [], \"delete\": [], \"modify\": [b\"foo\"]}, [], []], status",
            "         )",
            " ",
            "-        # Both branches have file 'foo' checkout should be fine.",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "-        self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            "+        # The new checkout behavior prevents switching with staged changes",
            "+        with self.assertRaises(porcelain.CheckoutError):",
            "+            porcelain.checkout(self.repo, b\"uni\")",
            " ",
            "-        status = list(porcelain.status(self.repo))",
            "-        self.assertEqual(",
            "-            [{\"add\": [], \"delete\": [], \"modify\": [b\"foo\"]}, [], []], status",
            "-        )",
            "+        # Should still be on master",
            "+        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+",
            "+        # Force checkout should work",
            "+        porcelain.checkout(self.repo, b\"uni\", force=True)",
            "+        self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            " ",
            "     def test_checkout_with_deleted_files(self) -> None:",
            "         porcelain.remove(self.repo.path, [os.path.join(self.repo.path, \"foo\")])",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual(",
            "             [{\"add\": [], \"delete\": [b\"foo\"], \"modify\": []}, [], []], status",
            "         )",
            " ",
            "-        # Both branches have file 'foo' checkout should be fine.",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "-        self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            "+        # The new checkout behavior prevents switching with staged deletions",
            "+        with self.assertRaises(porcelain.CheckoutError):",
            "+            porcelain.checkout(self.repo, b\"uni\")",
            " ",
            "-        status = list(porcelain.status(self.repo))",
            "-        self.assertEqual(",
            "-            [{\"add\": [], \"delete\": [b\"foo\"], \"modify\": []}, [], []], status",
            "-        )",
            "+        # Should still be on master",
            "+        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+",
            "+        # Force checkout should work",
            "+        porcelain.checkout(self.repo, b\"uni\", force=True)",
            "+        self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            " ",
            "     def test_checkout_to_branch_with_added_files(self) -> None:",
            "         file_path = os.path.join(self.repo.path, \"bar\")",
            " ",
            "         with open(file_path, \"w\") as f:",
            "             f.write(\"bar content\\n\")",
            "         porcelain.add(self.repo, paths=[file_path])",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual(",
            "             [{\"add\": [b\"bar\"], \"delete\": [], \"modify\": []}, [], []], status",
            "         )",
            " ",
            "         # Both branches have file 'foo' checkout should be fine.",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            "         self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual(",
            "             [{\"add\": [b\"bar\"], \"delete\": [], \"modify\": []}, [], []], status",
            "         )",
            " ",
            "@@ -1713,24 +3474,25 @@",
            "             f.write(\"bar content\\n\")",
            "         porcelain.add(self.repo, paths=[nee_path])",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual(",
            "             [{\"add\": [], \"delete\": [], \"modify\": [b\"nee\"]}, [], []], status",
            "         )",
            " ",
            "-        # 'uni' branch doesn't have 'nee' and it has been modified, should result in the checkout being aborted.",
            "-        with self.assertRaises(CheckoutError):",
            "-            porcelain.checkout_branch(self.repo, b\"uni\")",
            "-",
            "-        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+        # The new checkout behavior allows switching if the file doesn't exist in target branch",
            "+        # (changes can be preserved)",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            "+        self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            " ",
            "+        # The staged changes are lost and the file is removed from working tree",
            "+        # because it doesn't exist in the target branch",
            "         status = list(porcelain.status(self.repo))",
            "-        self.assertEqual(",
            "-            [{\"add\": [], \"delete\": [], \"modify\": [b\"nee\"]}, [], []], status",
            "-        )",
            "+        # File 'nee' is gone completely",
            "+        self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            "+        self.assertFalse(os.path.exists(nee_path))",
            " ",
            "     def test_checkout_to_branch_with_modified_file_not_present_forced(self) -> None:",
            "         # Commit a new file that the other branch doesn't have.",
            "         _, nee_path = _commit_file_with_content(self.repo, \"nee\", \"Good content\\n\")",
            " ",
            "         # Modify the file the other branch doesn't have.",
            "         with open(nee_path, \"a\") as f:",
            "@@ -1738,15 +3500,15 @@",
            "         porcelain.add(self.repo, paths=[nee_path])",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual(",
            "             [{\"add\": [], \"delete\": [], \"modify\": [b\"nee\"]}, [], []], status",
            "         )",
            " ",
            "         # 'uni' branch doesn't have 'nee' and it has been modified, but we force to reset the entire index.",
            "-        porcelain.checkout_branch(self.repo, b\"uni\", force=True)",
            "+        porcelain.checkout(self.repo, b\"uni\", force=True)",
            " ",
            "         self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "     def test_checkout_to_branch_with_unstaged_files(self) -> None:",
            "@@ -1755,35 +3517,39 @@",
            "             f.write(\"new message\")",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual(",
            "             [{\"add\": [], \"delete\": [], \"modify\": []}, [b\"foo\"], []], status",
            "         )",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        # The new checkout behavior prevents switching with unstaged changes",
            "+        with self.assertRaises(porcelain.CheckoutError):",
            "+            porcelain.checkout(self.repo, b\"uni\")",
            " ",
            "-        status = list(porcelain.status(self.repo))",
            "-        self.assertEqual(",
            "-            [{\"add\": [], \"delete\": [], \"modify\": []}, [b\"foo\"], []], status",
            "-        )",
            "+        # Should still be on master",
            "+        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+",
            "+        # Force checkout should work",
            "+        porcelain.checkout(self.repo, b\"uni\", force=True)",
            "+        self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            " ",
            "     def test_checkout_to_branch_with_untracked_files(self) -> None:",
            "         with open(os.path.join(self.repo.path, \"neu\"), \"a\") as f:",
            "             f.write(\"new message\\n\")",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], [\"neu\"]], status)",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], [\"neu\"]], status)",
            " ",
            "     def test_checkout_to_branch_with_new_files(self) -> None:",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            "         sub_directory = os.path.join(self.repo.path, \"sub1\")",
            "         os.mkdir(sub_directory)",
            "         for index in range(5):",
            "             _commit_file_with_content(",
            "                 self.repo, \"new_file_\" + str(index + 1), \"Some content\\n\"",
            "             )",
            "             _commit_file_with_content(",
            "@@ -1791,20 +3557,20 @@",
            "                 os.path.join(\"sub1\", \"new_file_\" + str(index + 10)),",
            "                 \"Good content\\n\",",
            "             )",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"master\")",
            "+        porcelain.checkout(self.repo, b\"master\")",
            "         self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            "         self.assertEqual(b\"uni\", porcelain.active_branch(self.repo))",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "     def test_checkout_to_branch_with_file_in_sub_directory(self) -> None:",
            "         sub_directory = os.path.join(self.repo.path, \"sub1\", \"sub2\")",
            "         os.makedirs(sub_directory)",
            "@@ -1822,23 +3588,23 @@",
            "         )",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "         self.assertTrue(os.path.isdir(sub_directory))",
            "         self.assertTrue(os.path.isdir(os.path.dirname(sub_directory)))",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "         self.assertFalse(os.path.isdir(sub_directory))",
            "         self.assertFalse(os.path.isdir(os.path.dirname(sub_directory)))",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"master\")",
            "+        porcelain.checkout(self.repo, b\"master\")",
            " ",
            "         self.assertTrue(os.path.isdir(sub_directory))",
            "         self.assertTrue(os.path.isdir(os.path.dirname(sub_directory)))",
            " ",
            "     def test_checkout_to_branch_with_multiple_files_in_sub_directory(self) -> None:",
            "         sub_directory = os.path.join(self.repo.path, \"sub1\", \"sub2\")",
            "         os.makedirs(sub_directory)",
            "@@ -1860,15 +3626,15 @@",
            "         )",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "         self.assertTrue(os.path.isdir(sub_directory))",
            "         self.assertTrue(os.path.isdir(os.path.dirname(sub_directory)))",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"uni\")",
            "+        porcelain.checkout(self.repo, b\"uni\")",
            " ",
            "         status = list(porcelain.status(self.repo))",
            "         self.assertEqual([{\"add\": [], \"delete\": [], \"modify\": []}, [], []], status)",
            " ",
            "         self.assertFalse(os.path.isdir(sub_directory))",
            "         self.assertFalse(os.path.isdir(os.path.dirname(sub_directory)))",
            " ",
            "@@ -1883,21 +3649,21 @@",
            "             committer=b\"Jane <jane@example.com>\",",
            "             author=b\"John <john@example.com>\",",
            "         )",
            " ",
            "     def test_checkout_to_commit_sha(self) -> None:",
            "         self._commit_something_wrong()",
            " ",
            "-        porcelain.checkout_branch(self.repo, self._sha)",
            "+        porcelain.checkout(self.repo, self._sha)",
            "         self.assertEqual(self._sha, self.repo.head())",
            " ",
            "     def test_checkout_to_head(self) -> None:",
            "         new_sha = self._commit_something_wrong()",
            " ",
            "-        porcelain.checkout_branch(self.repo, b\"HEAD\")",
            "+        porcelain.checkout(self.repo, b\"HEAD\")",
            "         self.assertEqual(new_sha, self.repo.head())",
            " ",
            "     def _checkout_remote_branch(self):",
            "         errstream = BytesIO()",
            "         outstream = BytesIO()",
            " ",
            "         porcelain.commit(",
            "@@ -1909,18 +3675,16 @@",
            " ",
            "         # Setup target repo cloned from temp test repo",
            "         clone_path = tempfile.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, clone_path)",
            "         target_repo = porcelain.clone(",
            "             self.repo.path, target=clone_path, errstream=errstream",
            "         )",
            "-        try:",
            "-            self.assertEqual(target_repo[b\"HEAD\"], self.repo[b\"HEAD\"])",
            "-        finally:",
            "-            target_repo.close()",
            "+        self.addCleanup(target_repo.close)",
            "+        self.assertEqual(target_repo[b\"HEAD\"], self.repo[b\"HEAD\"])",
            " ",
            "         # create a second file to be pushed back to origin",
            "         handle, fullpath = tempfile.mkstemp(dir=clone_path)",
            "         os.close(handle)",
            "         porcelain.add(repo=clone_path, paths=[fullpath])",
            "         porcelain.commit(",
            "             repo=clone_path,",
            "@@ -1945,22 +3709,27 @@",
            "         )",
            " ",
            "         self.assertEqual(",
            "             target_repo.refs[b\"refs/remotes/origin/foo\"],",
            "             target_repo.refs[b\"HEAD\"],",
            "         )",
            " ",
            "-        porcelain.checkout_branch(target_repo, b\"origin/foo\")",
            "+        # The new checkout behavior treats origin/foo as a ref and creates detached HEAD",
            "+        porcelain.checkout(target_repo, b\"origin/foo\")",
            "         original_id = target_repo[b\"HEAD\"].id",
            "         uni_id = target_repo[b\"refs/remotes/origin/uni\"].id",
            " ",
            "+        # Should be in detached HEAD state",
            "+        with self.assertRaises((ValueError, IndexError)):",
            "+            porcelain.active_branch(target_repo)",
            "+",
            "         expected_refs = {",
            "             b\"HEAD\": original_id,",
            "             b\"refs/heads/master\": original_id,",
            "-            b\"refs/heads/foo\": original_id,",
            "+            # No local foo branch is created anymore",
            "             b\"refs/remotes/origin/foo\": original_id,",
            "             b\"refs/remotes/origin/uni\": uni_id,",
            "             b\"refs/remotes/origin/HEAD\": new_id,",
            "             b\"refs/remotes/origin/master\": new_id,",
            "         }",
            "         self.assertEqual(expected_refs, target_repo.get_refs())",
            " ",
            "@@ -1968,30 +3737,318 @@",
            " ",
            "     def test_checkout_remote_branch(self) -> None:",
            "         repo = self._checkout_remote_branch()",
            "         repo.close()",
            " ",
            "     def test_checkout_remote_branch_then_master_then_remote_branch_again(self) -> None:",
            "         target_repo = self._checkout_remote_branch()",
            "-        self.assertEqual(b\"foo\", porcelain.active_branch(target_repo))",
            "-        _commit_file_with_content(target_repo, \"bar\", \"something\\n\")",
            "+        # Should be in detached HEAD state",
            "+        with self.assertRaises((ValueError, IndexError)):",
            "+            porcelain.active_branch(target_repo)",
            "+",
            "+        # Save the commit SHA before adding bar",
            "+        detached_commit_sha, _ = _commit_file_with_content(",
            "+            target_repo, \"bar\", \"something\\n\"",
            "+        )",
            "         self.assertTrue(os.path.isfile(os.path.join(target_repo.path, \"bar\")))",
            " ",
            "-        porcelain.checkout_branch(target_repo, b\"master\")",
            "+        porcelain.checkout(target_repo, b\"master\")",
            " ",
            "         self.assertEqual(b\"master\", porcelain.active_branch(target_repo))",
            "         self.assertFalse(os.path.isfile(os.path.join(target_repo.path, \"bar\")))",
            " ",
            "-        porcelain.checkout_branch(target_repo, b\"origin/foo\")",
            "+        # Going back to origin/foo won't have bar because the commit was made in detached state",
            "+        porcelain.checkout(target_repo, b\"origin/foo\")",
            " ",
            "-        self.assertEqual(b\"foo\", porcelain.active_branch(target_repo))",
            "+        # Should be in detached HEAD state again",
            "+        with self.assertRaises((ValueError, IndexError)):",
            "+            porcelain.active_branch(target_repo)",
            "+        # bar is NOT there because we're back at the original origin/foo commit",
            "+        self.assertFalse(os.path.isfile(os.path.join(target_repo.path, \"bar\")))",
            "+",
            "+        # But we can checkout the specific commit to get bar back",
            "+        porcelain.checkout(target_repo, detached_commit_sha.decode())",
            "         self.assertTrue(os.path.isfile(os.path.join(target_repo.path, \"bar\")))",
            " ",
            "         target_repo.close()",
            " ",
            "+    def test_checkout_new_branch_from_remote_sets_tracking(self) -> None:",
            "+        # Create a \"remote\" repository",
            "+        remote_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, remote_path)",
            "+        remote_repo = porcelain.init(remote_path)",
            "+",
            "+        # Add a commit to the remote",
            "+        remote_sha, _ = _commit_file_with_content(",
            "+            remote_repo, \"bar\", \"remote content\\n\"",
            "+        )",
            "+",
            "+        # Clone the remote repository",
            "+        target_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, target_path)",
            "+        target_repo = porcelain.clone(remote_path, target_path)",
            "+",
            "+        # Create a remote tracking branch reference",
            "+        remote_branch_ref = b\"refs/remotes/origin/feature\"",
            "+        target_repo.refs[remote_branch_ref] = remote_sha",
            "+",
            "+        # Checkout a new branch from the remote branch",
            "+        porcelain.checkout(target_repo, remote_branch_ref, new_branch=b\"local-feature\")",
            "+",
            "+        # Verify the branch was created and is active",
            "+        self.assertEqual(b\"local-feature\", porcelain.active_branch(target_repo))",
            "+",
            "+        # Verify tracking configuration was set",
            "+        config = target_repo.get_config()",
            "+        self.assertEqual(",
            "+            b\"origin\", config.get((b\"branch\", b\"local-feature\"), b\"remote\")",
            "+        )",
            "+        self.assertEqual(",
            "+            b\"refs/heads/feature\", config.get((b\"branch\", b\"local-feature\"), b\"merge\")",
            "+        )",
            "+",
            "+        target_repo.close()",
            "+        remote_repo.close()",
            "+",
            "+",
            "+class GeneralCheckoutTests(PorcelainTestCase):",
            "+    \"\"\"Tests for the general checkout function that handles branches, tags, and commits.\"\"\"",
            "+",
            "+    def setUp(self) -> None:",
            "+        super().setUp()",
            "+        # Create initial commit",
            "+        self._sha1, self._foo_path = _commit_file_with_content(",
            "+            self.repo, \"foo\", \"initial content\\n\"",
            "+        )",
            "+        # Create a branch",
            "+        porcelain.branch_create(self.repo, \"feature\")",
            "+        # Create another commit on master",
            "+        self._sha2, self._bar_path = _commit_file_with_content(",
            "+            self.repo, \"bar\", \"bar content\\n\"",
            "+        )",
            "+        # Create a tag",
            "+        porcelain.tag_create(self.repo, \"v1.0\", objectish=self._sha1)",
            "+",
            "+    def test_checkout_branch(self) -> None:",
            "+        \"\"\"Test checking out a branch.\"\"\"",
            "+        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+",
            "+        # Checkout feature branch",
            "+        porcelain.checkout(self.repo, \"feature\")",
            "+        self.assertEqual(b\"feature\", porcelain.active_branch(self.repo))",
            "+",
            "+        # File 'bar' should not exist in feature branch",
            "+        self.assertFalse(os.path.exists(self._bar_path))",
            "+",
            "+        # Go back to master",
            "+        porcelain.checkout(self.repo, \"master\")",
            "+        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+",
            "+        # File 'bar' should exist again",
            "+        self.assertTrue(os.path.exists(self._bar_path))",
            "+",
            "+    def test_checkout_commit(self) -> None:",
            "+        \"\"\"Test checking out a specific commit (detached HEAD).\"\"\"",
            "+        # Checkout first commit by SHA",
            "+        porcelain.checkout(self.repo, self._sha1.decode(\"ascii\"))",
            "+",
            "+        # Should be in detached HEAD state - active_branch raises IndexError",
            "+        with self.assertRaises((ValueError, IndexError)):",
            "+            porcelain.active_branch(self.repo)",
            "+",
            "+        # File 'bar' should not exist",
            "+        self.assertFalse(os.path.exists(self._bar_path))",
            "+",
            "+        # HEAD should point to the commit",
            "+        self.assertEqual(self._sha1, self.repo.refs[b\"HEAD\"])",
            "+",
            "+    def test_checkout_tag(self) -> None:",
            "+        \"\"\"Test checking out a tag (detached HEAD).\"\"\"",
            "+        # Checkout tag",
            "+        porcelain.checkout(self.repo, \"v1.0\")",
            "+",
            "+        # Should be in detached HEAD state - active_branch raises IndexError",
            "+        with self.assertRaises((ValueError, IndexError)):",
            "+            porcelain.active_branch(self.repo)",
            "+",
            "+        # File 'bar' should not exist (tag points to first commit)",
            "+        self.assertFalse(os.path.exists(self._bar_path))",
            "+",
            "+        # HEAD should point to the tagged commit",
            "+        self.assertEqual(self._sha1, self.repo.refs[b\"HEAD\"])",
            "+",
            "+    def test_checkout_new_branch(self) -> None:",
            "+        \"\"\"Test creating a new branch during checkout (like git checkout -b).\"\"\"",
            "+        # Create and checkout new branch from current HEAD",
            "+        porcelain.checkout(self.repo, \"master\", new_branch=\"new-feature\")",
            "+",
            "+        self.assertEqual(b\"new-feature\", porcelain.active_branch(self.repo))",
            "+        self.assertTrue(os.path.exists(self._bar_path))",
            "+",
            "+        # Create and checkout new branch from specific commit",
            "+        porcelain.checkout(self.repo, self._sha1.decode(\"ascii\"), new_branch=\"from-old\")",
            "+",
            "+        self.assertEqual(b\"from-old\", porcelain.active_branch(self.repo))",
            "+        self.assertFalse(os.path.exists(self._bar_path))",
            "+",
            "+    def test_checkout_with_uncommitted_changes(self) -> None:",
            "+        \"\"\"Test checkout behavior with uncommitted changes.\"\"\"",
            "+        # Modify a file",
            "+        with open(self._foo_path, \"w\") as f:",
            "+            f.write(\"modified content\\n\")",
            "+",
            "+        # Should raise error when trying to checkout",
            "+        with self.assertRaises(porcelain.CheckoutError) as cm:",
            "+            porcelain.checkout(self.repo, \"feature\")",
            "+",
            "+        self.assertIn(\"local changes\", str(cm.exception))",
            "+        self.assertIn(\"foo\", str(cm.exception))",
            "+",
            "+        # Should still be on master",
            "+        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+",
            "+    def test_checkout_force(self) -> None:",
            "+        \"\"\"Test forced checkout discards local changes.\"\"\"",
            "+        # Modify a file",
            "+        with open(self._foo_path, \"w\") as f:",
            "+            f.write(\"modified content\\n\")",
            "+",
            "+        # Force checkout should succeed",
            "+        porcelain.checkout(self.repo, \"feature\", force=True)",
            "+",
            "+        self.assertEqual(b\"feature\", porcelain.active_branch(self.repo))",
            "+",
            "+        # Local changes should be discarded",
            "+        with open(self._foo_path) as f:",
            "+            content = f.read()",
            "+        self.assertEqual(\"initial content\\n\", content)",
            "+",
            "+    def test_checkout_nonexistent_ref(self) -> None:",
            "+        \"\"\"Test checkout of non-existent branch/commit.\"\"\"",
            "+        with self.assertRaises(KeyError):",
            "+            porcelain.checkout(self.repo, \"nonexistent\")",
            "+",
            "+    def test_checkout_partial_sha(self) -> None:",
            "+        \"\"\"Test checkout with partial SHA.\"\"\"",
            "+        # Git typically allows checkout with partial SHA",
            "+        partial_sha = self._sha1.decode(\"ascii\")[:7]",
            "+        porcelain.checkout(self.repo, partial_sha)",
            "+",
            "+        # Should be in detached HEAD state at the right commit",
            "+        self.assertEqual(self._sha1, self.repo.refs[b\"HEAD\"])",
            "+",
            "+    def test_checkout_preserves_untracked_files(self) -> None:",
            "+        \"\"\"Test that checkout preserves untracked files.\"\"\"",
            "+        # Create an untracked file",
            "+        untracked_path = os.path.join(self.repo.path, \"untracked.txt\")",
            "+        with open(untracked_path, \"w\") as f:",
            "+            f.write(\"untracked content\\n\")",
            "+",
            "+        # Checkout another branch",
            "+        porcelain.checkout(self.repo, \"feature\")",
            "+",
            "+        # Untracked file should still exist",
            "+        self.assertTrue(os.path.exists(untracked_path))",
            "+        with open(untracked_path) as f:",
            "+            content = f.read()",
            "+        self.assertEqual(\"untracked content\\n\", content)",
            "+",
            "+    def test_checkout_full_ref_paths(self) -> None:",
            "+        \"\"\"Test checkout with full ref paths.\"\"\"",
            "+        # Test checkout with full branch ref path",
            "+        porcelain.checkout(self.repo, \"refs/heads/feature\")",
            "+        self.assertEqual(b\"feature\", porcelain.active_branch(self.repo))",
            "+",
            "+        # Test checkout with full tag ref path",
            "+        porcelain.checkout(self.repo, \"refs/tags/v1.0\")",
            "+        # Should be in detached HEAD state",
            "+        with self.assertRaises((ValueError, IndexError)):",
            "+            porcelain.active_branch(self.repo)",
            "+        self.assertEqual(self._sha1, self.repo.refs[b\"HEAD\"])",
            "+",
            "+    def test_checkout_bytes_vs_string_target(self) -> None:",
            "+        \"\"\"Test that checkout works with both bytes and string targets.\"\"\"",
            "+        # Test with string target",
            "+        porcelain.checkout(self.repo, \"feature\")",
            "+        self.assertEqual(b\"feature\", porcelain.active_branch(self.repo))",
            "+",
            "+        # Test with bytes target",
            "+        porcelain.checkout(self.repo, b\"master\")",
            "+        self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            "+",
            "+    def test_checkout_new_branch_from_commit(self) -> None:",
            "+        \"\"\"Test creating a new branch from a specific commit.\"\"\"",
            "+        # Create new branch from first commit",
            "+        porcelain.checkout(self.repo, self._sha1.decode(), new_branch=\"from-commit\")",
            "+",
            "+        self.assertEqual(b\"from-commit\", porcelain.active_branch(self.repo))",
            "+        # Should be at the first commit (no bar file)",
            "+        self.assertFalse(os.path.exists(self._bar_path))",
            "+",
            "+    def test_checkout_with_staged_addition(self) -> None:",
            "+        \"\"\"Test checkout behavior with staged file additions.\"\"\"",
            "+        # Create and stage a new file that doesn't exist in target branch",
            "+        new_file_path = os.path.join(self.repo.path, \"new.txt\")",
            "+        with open(new_file_path, \"w\") as f:",
            "+            f.write(\"new file content\\n\")",
            "+        porcelain.add(self.repo, [new_file_path])",
            "+",
            "+        # This should succeed because the file doesn't exist in target branch",
            "+        porcelain.checkout(self.repo, \"feature\")",
            "+",
            "+        # Should be on feature branch",
            "+        self.assertEqual(b\"feature\", porcelain.active_branch(self.repo))",
            "+",
            "+        # The new file should still exist and be staged",
            "+        self.assertTrue(os.path.exists(new_file_path))",
            "+        status = porcelain.status(self.repo)",
            "+        self.assertIn(b\"new.txt\", status.staged[\"add\"])",
            "+",
            "+    def test_checkout_with_staged_modification_conflict(self) -> None:",
            "+        \"\"\"Test checkout behavior with staged modifications that would conflict.\"\"\"",
            "+        # Stage changes to a file that exists in both branches",
            "+        with open(self._foo_path, \"w\") as f:",
            "+            f.write(\"modified content\\n\")",
            "+        porcelain.add(self.repo, [self._foo_path])",
            "+",
            "+        # Should prevent checkout due to staged changes to existing file",
            "+        with self.assertRaises(porcelain.CheckoutError) as cm:",
            "+            porcelain.checkout(self.repo, \"feature\")",
            "+",
            "+        self.assertIn(\"local changes\", str(cm.exception))",
            "+        self.assertIn(\"foo\", str(cm.exception))",
            "+",
            "+    def test_checkout_head_reference(self) -> None:",
            "+        \"\"\"Test checkout of HEAD reference.\"\"\"",
            "+        # Move to feature branch first",
            "+        porcelain.checkout(self.repo, \"feature\")",
            "+",
            "+        # Checkout HEAD creates detached HEAD state",
            "+        porcelain.checkout(self.repo, \"HEAD\")",
            "+",
            "+        # Should be in detached HEAD state",
            "+        with self.assertRaises((ValueError, IndexError)):",
            "+            porcelain.active_branch(self.repo)",
            "+",
            "+    def test_checkout_error_messages(self) -> None:",
            "+        \"\"\"Test that checkout error messages are helpful.\"\"\"",
            "+        # Create uncommitted changes",
            "+        with open(self._foo_path, \"w\") as f:",
            "+            f.write(\"uncommitted changes\\n\")",
            "+",
            "+        # Try to checkout",
            "+        with self.assertRaises(porcelain.CheckoutError) as cm:",
            "+            porcelain.checkout(self.repo, \"feature\")",
            "+",
            "+        error_msg = str(cm.exception)",
            "+        self.assertIn(\"local changes\", error_msg)",
            "+        self.assertIn(\"foo\", error_msg)",
            "+        self.assertIn(\"overwritten\", error_msg)",
            "+        self.assertIn(\"commit or stash\", error_msg)",
            "+",
            " ",
            " class SubmoduleTests(PorcelainTestCase):",
            "     def test_empty(self) -> None:",
            "         porcelain.commit(",
            "             repo=self.repo.path,",
            "             message=b\"init\",",
            "             author=b\"author <email>\",",
            "@@ -2012,14 +4069,78 @@",
            "                 f.read(),",
            "             )",
            " ",
            "     def test_init(self) -> None:",
            "         porcelain.submodule_add(self.repo, \"../bar.git\", \"bar\")",
            "         porcelain.submodule_init(self.repo)",
            " ",
            "+    def test_update(self) -> None:",
            "+        # Create a submodule repository",
            "+        sub_repo_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, sub_repo_path)",
            "+        sub_repo = Repo.init(sub_repo_path)",
            "+        self.addCleanup(sub_repo.close)",
            "+",
            "+        # Add a file to the submodule repo",
            "+        sub_file = os.path.join(sub_repo_path, \"test.txt\")",
            "+        with open(sub_file, \"w\") as f:",
            "+            f.write(\"submodule content\")",
            "+",
            "+        porcelain.add(sub_repo, paths=[sub_file])",
            "+        sub_commit = porcelain.commit(",
            "+            sub_repo,",
            "+            message=b\"Initial submodule commit\",",
            "+            author=b\"Test Author <test@example.com>\",",
            "+            committer=b\"Test Committer <test@example.com>\",",
            "+        )",
            "+",
            "+        # Add the submodule to the main repository",
            "+        porcelain.submodule_add(self.repo, sub_repo_path, \"test_submodule\")",
            "+",
            "+        # Manually add the submodule to the index",
            "+        from dulwich.index import IndexEntry",
            "+        from dulwich.objects import S_IFGITLINK",
            "+",
            "+        index = self.repo.open_index()",
            "+        index[b\"test_submodule\"] = IndexEntry(",
            "+            ctime=0,",
            "+            mtime=0,",
            "+            dev=0,",
            "+            ino=0,",
            "+            mode=S_IFGITLINK,",
            "+            uid=0,",
            "+            gid=0,",
            "+            size=0,",
            "+            sha=sub_commit,",
            "+            flags=0,",
            "+        )",
            "+        index.write()",
            "+",
            "+        porcelain.add(self.repo, paths=[\".gitmodules\"])",
            "+        porcelain.commit(",
            "+            self.repo,",
            "+            message=b\"Add submodule\",",
            "+            author=b\"Test Author <test@example.com>\",",
            "+            committer=b\"Test Committer <test@example.com>\",",
            "+        )",
            "+",
            "+        # Initialize and update the submodule",
            "+        porcelain.submodule_init(self.repo)",
            "+        porcelain.submodule_update(self.repo)",
            "+",
            "+        # Check that the submodule directory exists",
            "+        submodule_path = os.path.join(self.repo.path, \"test_submodule\")",
            "+        self.assertTrue(os.path.exists(submodule_path))",
            "+",
            "+        # Check that the submodule file exists",
            "+        submodule_file = os.path.join(submodule_path, \"test.txt\")",
            "+        self.assertTrue(os.path.exists(submodule_file))",
            "+        with open(submodule_file) as f:",
            "+            self.assertEqual(f.read(), \"submodule content\")",
            "+",
            " ",
            " class PushTests(PorcelainTestCase):",
            "     def test_simple(self) -> None:",
            "         \"\"\"Basic test of porcelain push where self.repo is the remote.  First",
            "         clone the remote, commit a file to the clone, then push the changes",
            "         back to the remote.",
            "         \"\"\"",
            "@@ -2035,18 +4156,16 @@",
            " ",
            "         # Setup target repo cloned from temp test repo",
            "         clone_path = tempfile.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, clone_path)",
            "         target_repo = porcelain.clone(",
            "             self.repo.path, target=clone_path, errstream=errstream",
            "         )",
            "-        try:",
            "-            self.assertEqual(target_repo[b\"HEAD\"], self.repo[b\"HEAD\"])",
            "-        finally:",
            "-            target_repo.close()",
            "+        self.addCleanup(target_repo.close)",
            "+        self.assertEqual(target_repo[b\"HEAD\"], self.repo[b\"HEAD\"])",
            " ",
            "         # create a second file to be pushed back to origin",
            "         handle, fullpath = tempfile.mkstemp(dir=clone_path)",
            "         os.close(handle)",
            "         porcelain.add(repo=clone_path, paths=[fullpath])",
            "         porcelain.commit(",
            "             repo=clone_path,",
            "@@ -2088,15 +4207,15 @@",
            "             self.assertEqual(r_clone[b\"HEAD\"].id, self.repo[refs_path].id)",
            " ",
            "             # Get the change in the target repo corresponding to the add",
            "             # this will be in the foo branch.",
            "             change = next(",
            "                 iter(",
            "                     tree_changes(",
            "-                        self.repo,",
            "+                        self.repo.object_store,",
            "                         self.repo[b\"HEAD\"].tree,",
            "                         self.repo[b\"refs/heads/foo\"].tree,",
            "                     )",
            "                 )",
            "             )",
            "             self.assertEqual(",
            "                 os.path.basename(fullpath), change.new.path.decode(\"ascii\")",
            "@@ -2284,14 +4403,174 @@",
            "             },",
            "             self.repo.get_refs(),",
            "         )",
            " ",
            "         self.assertEqual(b\"\", outstream.getvalue())",
            "         self.assertTrue(re.match(b\"Push to .* successful.\\n\", errstream.getvalue()))",
            " ",
            "+    def test_push_returns_sendpackresult(self) -> None:",
            "+        \"\"\"Test that push returns a SendPackResult with per-ref information.\"\"\"",
            "+        outstream = BytesIO()",
            "+        errstream = BytesIO()",
            "+",
            "+        # Create initial commit",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"init\",",
            "+            author=b\"author <email>\",",
            "+            committer=b\"committer <email>\",",
            "+        )",
            "+",
            "+        # Setup target repo cloned from temp test repo",
            "+        clone_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, clone_path)",
            "+        target_repo = porcelain.clone(",
            "+            self.repo.path, target=clone_path, errstream=errstream",
            "+        )",
            "+        target_repo.close()",
            "+",
            "+        # Create a commit in the clone",
            "+        handle, fullpath = tempfile.mkstemp(dir=clone_path)",
            "+        os.close(handle)",
            "+        porcelain.add(repo=clone_path, paths=[fullpath])",
            "+        porcelain.commit(",
            "+            repo=clone_path,",
            "+            message=b\"push\",",
            "+            author=b\"author <email>\",",
            "+            committer=b\"committer <email>\",",
            "+        )",
            "+",
            "+        # Push and check the return value",
            "+        result = porcelain.push(",
            "+            clone_path,",
            "+            \"origin\",",
            "+            b\"HEAD:refs/heads/new-branch\",",
            "+            outstream=outstream,",
            "+            errstream=errstream,",
            "+        )",
            "+",
            "+        # Verify that we get a SendPackResult",
            "+        self.assertIsInstance(result, SendPackResult)",
            "+",
            "+        # Verify that it contains refs",
            "+        self.assertIsNotNone(result.refs)",
            "+        self.assertIn(b\"refs/heads/new-branch\", result.refs)",
            "+",
            "+        # Verify ref_status - should be None for successful updates",
            "+        if result.ref_status:",
            "+            self.assertIsNone(result.ref_status.get(b\"refs/heads/new-branch\"))",
            "+",
            "+    def test_mirror_mode(self) -> None:",
            "+        \"\"\"Test push with remote.<name>.mirror configuration.\"\"\"",
            "+        outstream = BytesIO()",
            "+        errstream = BytesIO()",
            "+",
            "+        # Create initial commit",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"init\",",
            "+            author=b\"author <email>\",",
            "+            committer=b\"committer <email>\",",
            "+        )",
            "+",
            "+        # Setup target repo cloned from temp test repo",
            "+        clone_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, clone_path)",
            "+        target_repo = porcelain.clone(",
            "+            self.repo.path, target=clone_path, errstream=errstream",
            "+        )",
            "+        target_repo.close()",
            "+",
            "+        # Create multiple refs in the clone",
            "+        with Repo(clone_path) as r_clone:",
            "+            # Create a new branch",
            "+            r_clone.refs[b\"refs/heads/feature\"] = r_clone[b\"HEAD\"].id",
            "+            # Create a tag",
            "+            r_clone.refs[b\"refs/tags/v1.0\"] = r_clone[b\"HEAD\"].id",
            "+            # Create a remote tracking branch",
            "+            r_clone.refs[b\"refs/remotes/upstream/main\"] = r_clone[b\"HEAD\"].id",
            "+",
            "+        # Create a branch in the remote that doesn't exist in clone",
            "+        self.repo.refs[b\"refs/heads/to-be-deleted\"] = self.repo[b\"HEAD\"].id",
            "+",
            "+        # Configure mirror mode",
            "+        with Repo(clone_path) as r_clone:",
            "+            config = r_clone.get_config()",
            "+            config.set((b\"remote\", b\"origin\"), b\"mirror\", True)",
            "+            config.write_to_path()",
            "+",
            "+        # Push with mirror mode",
            "+        porcelain.push(",
            "+            clone_path,",
            "+            \"origin\",",
            "+            outstream=outstream,",
            "+            errstream=errstream,",
            "+        )",
            "+",
            "+        # Verify refs were properly mirrored",
            "+        with Repo(clone_path) as r_clone:",
            "+            # All local branches should be pushed",
            "+            self.assertEqual(",
            "+                r_clone.refs[b\"refs/heads/feature\"],",
            "+                self.repo.refs[b\"refs/heads/feature\"],",
            "+            )",
            "+            # All tags should be pushed",
            "+            self.assertEqual(",
            "+                r_clone.refs[b\"refs/tags/v1.0\"], self.repo.refs[b\"refs/tags/v1.0\"]",
            "+            )",
            "+            # Remote tracking branches should be pushed",
            "+            self.assertEqual(",
            "+                r_clone.refs[b\"refs/remotes/upstream/main\"],",
            "+                self.repo.refs[b\"refs/remotes/upstream/main\"],",
            "+            )",
            "+",
            "+        # Verify the extra branch was deleted",
            "+        self.assertNotIn(b\"refs/heads/to-be-deleted\", self.repo.refs)",
            "+",
            "+    def test_mirror_mode_disabled(self) -> None:",
            "+        \"\"\"Test that mirror mode is properly disabled when set to false.\"\"\"",
            "+        outstream = BytesIO()",
            "+        errstream = BytesIO()",
            "+",
            "+        # Create initial commit",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"init\",",
            "+            author=b\"author <email>\",",
            "+            committer=b\"committer <email>\",",
            "+        )",
            "+",
            "+        # Setup target repo cloned from temp test repo",
            "+        clone_path = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, clone_path)",
            "+        target_repo = porcelain.clone(",
            "+            self.repo.path, target=clone_path, errstream=errstream",
            "+        )",
            "+        target_repo.close()",
            "+",
            "+        # Create a branch in the remote that doesn't exist in clone",
            "+        self.repo.refs[b\"refs/heads/should-not-be-deleted\"] = self.repo[b\"HEAD\"].id",
            "+",
            "+        # Explicitly set mirror mode to false",
            "+        with Repo(clone_path) as r_clone:",
            "+            config = r_clone.get_config()",
            "+            config.set((b\"remote\", b\"origin\"), b\"mirror\", False)",
            "+            config.write_to_path()",
            "+",
            "+        # Push normally (not mirror mode)",
            "+        porcelain.push(",
            "+            clone_path,",
            "+            \"origin\",",
            "+            outstream=outstream,",
            "+            errstream=errstream,",
            "+        )",
            "+",
            "+        # Verify the extra branch was NOT deleted",
            "+        self.assertIn(b\"refs/heads/should-not-be-deleted\", self.repo.refs)",
            "+",
            " ",
            " class PullTests(PorcelainTestCase):",
            "     def setUp(self) -> None:",
            "         super().setUp()",
            "         # create a file for initial commit",
            "         handle, fullpath = tempfile.mkstemp(dir=self.repo.path)",
            "         os.close(handle)",
            "@@ -2371,28 +4650,34 @@",
            "             errstream=errstream,",
            "         )",
            " ",
            "         # Check the target repo for pushed changes",
            "         with Repo(self.target_path) as r:",
            "             self.assertEqual(r[b\"refs/heads/master\"].id, c3a)",
            " ",
            "-        self.assertRaises(",
            "-            NotImplementedError,",
            "-            porcelain.pull,",
            "+        # Pull with merge should now work",
            "+        porcelain.pull(",
            "             self.target_path,",
            "             self.repo.path,",
            "             b\"refs/heads/master\",",
            "             outstream=outstream,",
            "             errstream=errstream,",
            "             fast_forward=False,",
            "         )",
            " ",
            "-        # Check the target repo for pushed changes",
            "+        # Check the target repo for merged changes",
            "         with Repo(self.target_path) as r:",
            "-            self.assertEqual(r[b\"refs/heads/master\"].id, c3a)",
            "+            # HEAD should now be a merge commit",
            "+            head = r[b\"HEAD\"]",
            "+            # It should have two parents",
            "+            self.assertEqual(len(head.parents), 2)",
            "+            # One parent should be the previous HEAD (c3a)",
            "+            self.assertIn(c3a, head.parents)",
            "+            # The other parent should be from the source repo",
            "+            self.assertIn(self.repo[b\"HEAD\"].id, head.parents)",
            " ",
            "     def test_no_refspec(self) -> None:",
            "         outstream = BytesIO()",
            "         errstream = BytesIO()",
            " ",
            "         # Pull changes into the cloned repo",
            "         porcelain.pull(",
            "@@ -2418,14 +4703,54 @@",
            "             errstream=errstream,",
            "         )",
            " ",
            "         # Check the target repo for pushed changes",
            "         with Repo(self.target_path) as r:",
            "             self.assertEqual(r[b\"HEAD\"].id, self.repo[b\"HEAD\"].id)",
            " ",
            "+    def test_pull_updates_working_tree(self) -> None:",
            "+        \"\"\"Test that pull updates the working tree with new files.\"\"\"",
            "+        outstream = BytesIO()",
            "+        errstream = BytesIO()",
            "+",
            "+        # Create a new file with content in the source repo",
            "+        new_file = os.path.join(self.repo.path, \"newfile.txt\")",
            "+        with open(new_file, \"w\") as f:",
            "+            f.write(\"This is new content\")",
            "+",
            "+        porcelain.add(repo=self.repo.path, paths=[new_file])",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"Add new file\",",
            "+            author=b\"test <email>\",",
            "+            committer=b\"test <email>\",",
            "+        )",
            "+",
            "+        # Before pull, the file should not exist in target",
            "+        target_file = os.path.join(self.target_path, \"newfile.txt\")",
            "+        self.assertFalse(os.path.exists(target_file))",
            "+",
            "+        # Pull changes into the cloned repo",
            "+        porcelain.pull(",
            "+            self.target_path,",
            "+            self.repo.path,",
            "+            b\"refs/heads/master\",",
            "+            outstream=outstream,",
            "+            errstream=errstream,",
            "+        )",
            "+",
            "+        # After pull, the file should exist with correct content",
            "+        self.assertTrue(os.path.exists(target_file))",
            "+        with open(target_file) as f:",
            "+            self.assertEqual(f.read(), \"This is new content\")",
            "+",
            "+        # Check the HEAD is updated too",
            "+        with Repo(self.target_path) as r:",
            "+            self.assertEqual(r[b\"HEAD\"].id, self.repo[b\"HEAD\"].id)",
            "+",
            " ",
            " class StatusTests(PorcelainTestCase):",
            "     def test_empty(self) -> None:",
            "         results = porcelain.status(self.repo)",
            "         self.assertEqual({\"add\": [], \"delete\": [], \"modify\": []}, results.staged)",
            "         self.assertEqual([], results.unstaged)",
            " ",
            "@@ -2512,14 +4837,77 @@",
            "         untracked_file = os.path.join(untracked_dir, \"untracked_file\")",
            "         with open(untracked_file, \"w\") as fh:",
            "             fh.write(\"untracked\")",
            " ",
            "         _, _, untracked = porcelain.status(self.repo.path, untracked_files=\"all\")",
            "         self.assertEqual(untracked, [\"untracked_dir/untracked_file\"])",
            " ",
            "+    def test_status_untracked_path_normal(self) -> None:",
            "+        # Create an untracked directory with multiple files",
            "+        untracked_dir = os.path.join(self.repo_path, \"untracked_dir\")",
            "+        os.mkdir(untracked_dir)",
            "+        untracked_file1 = os.path.join(untracked_dir, \"file1\")",
            "+        untracked_file2 = os.path.join(untracked_dir, \"file2\")",
            "+        with open(untracked_file1, \"w\") as fh:",
            "+            fh.write(\"untracked1\")",
            "+        with open(untracked_file2, \"w\") as fh:",
            "+            fh.write(\"untracked2\")",
            "+",
            "+        # Create a nested untracked directory",
            "+        nested_dir = os.path.join(untracked_dir, \"nested\")",
            "+        os.mkdir(nested_dir)",
            "+        nested_file = os.path.join(nested_dir, \"file3\")",
            "+        with open(nested_file, \"w\") as fh:",
            "+            fh.write(\"untracked3\")",
            "+",
            "+        # Test \"normal\" mode - should only show the directory, not individual files",
            "+        _, _, untracked = porcelain.status(self.repo.path, untracked_files=\"normal\")",
            "+        self.assertEqual(untracked, [\"untracked_dir/\"])",
            "+",
            "+        # Test \"all\" mode - should show all files",
            "+        _, _, untracked_all = porcelain.status(self.repo.path, untracked_files=\"all\")",
            "+        self.assertEqual(",
            "+            sorted(untracked_all),",
            "+            [",
            "+                \"untracked_dir/file1\",",
            "+                \"untracked_dir/file2\",",
            "+                \"untracked_dir/nested/file3\",",
            "+            ],",
            "+        )",
            "+",
            "+    def test_status_mixed_tracked_untracked(self) -> None:",
            "+        # Create a directory with both tracked and untracked files",
            "+        mixed_dir = os.path.join(self.repo_path, \"mixed_dir\")",
            "+        os.mkdir(mixed_dir)",
            "+",
            "+        # Add a tracked file",
            "+        tracked_file = os.path.join(mixed_dir, \"tracked.txt\")",
            "+        with open(tracked_file, \"w\") as fh:",
            "+            fh.write(\"tracked content\")",
            "+        porcelain.add(self.repo.path, paths=[tracked_file])",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"add tracked file\",",
            "+            author=b\"author <email>\",",
            "+            committer=b\"committer <email>\",",
            "+        )",
            "+",
            "+        # Add untracked files to the same directory",
            "+        untracked_file = os.path.join(mixed_dir, \"untracked.txt\")",
            "+        with open(untracked_file, \"w\") as fh:",
            "+            fh.write(\"untracked content\")",
            "+",
            "+        # In \"normal\" mode, should show individual untracked files in mixed dirs",
            "+        _, _, untracked = porcelain.status(self.repo.path, untracked_files=\"normal\")",
            "+        self.assertEqual(untracked, [\"mixed_dir/untracked.txt\"])",
            "+",
            "+        # In \"all\" mode, should be the same for mixed directories",
            "+        _, _, untracked_all = porcelain.status(self.repo.path, untracked_files=\"all\")",
            "+        self.assertEqual(untracked_all, [\"mixed_dir/untracked.txt\"])",
            "+",
            "     def test_status_crlf_mismatch(self) -> None:",
            "         # First make a commit as if the file has been added on a Linux system",
            "         # or with core.autocrlf=True",
            "         file_path = os.path.join(self.repo.path, \"crlf\")",
            "         with open(file_path, \"wb\") as f:",
            "             f.write(b\"line1\\nline2\")",
            "         porcelain.add(repo=self.repo.path, paths=[file_path])",
            "@@ -2661,19 +5049,17 @@",
            "         porcelain.commit(",
            "             repo=self.repo.path,",
            "             message=b\"test status\",",
            "             author=b\"author <email>\",",
            "             committer=b\"committer <email>\",",
            "         )",
            "         cwd = os.getcwd()",
            "-        try:",
            "-            os.chdir(self.repo.path)",
            "-            porcelain.remove(repo=self.repo.path, paths=[filename])",
            "-        finally:",
            "-            os.chdir(cwd)",
            "+        self.addCleanup(os.chdir, cwd)",
            "+        os.chdir(self.repo.path)",
            "+        porcelain.remove(repo=self.repo.path, paths=[filename])",
            "         changes = porcelain.get_tree_changes(self.repo.path)",
            " ",
            "         self.assertEqual(changes[\"delete\"][0], filename.encode(\"ascii\"))",
            "         self.assertEqual(len(changes[\"add\"]), 0)",
            "         self.assertEqual(len(changes[\"modify\"]), 0)",
            "         self.assertEqual(len(changes[\"delete\"]), 1)",
            " ",
            "@@ -2817,16 +5203,53 @@",
            "                     self.repo.path,",
            "                     self.repo.open_index(),",
            "                     untracked_files=\"invalid_value\",",
            "                 )",
            "             )",
            " ",
            "     def test_get_untracked_paths_normal(self) -> None:",
            "-        with self.assertRaises(NotImplementedError):",
            "-            _, _, _ = porcelain.status(repo=self.repo.path, untracked_files=\"normal\")",
            "+        # Create an untracked directory with files",
            "+        untracked_dir = os.path.join(self.repo.path, \"untracked_dir\")",
            "+        os.mkdir(untracked_dir)",
            "+        with open(os.path.join(untracked_dir, \"file1.txt\"), \"w\") as f:",
            "+            f.write(\"untracked content\")",
            "+        with open(os.path.join(untracked_dir, \"file2.txt\"), \"w\") as f:",
            "+            f.write(\"more untracked content\")",
            "+",
            "+        # Test that \"normal\" mode works and returns only the directory",
            "+        _, _, untracked = porcelain.status(",
            "+            repo=self.repo.path, untracked_files=\"normal\"",
            "+        )",
            "+        self.assertEqual(untracked, [\"untracked_dir/\"])",
            "+",
            "+    def test_get_untracked_paths_top_level_issue_1247(self) -> None:",
            "+        \"\"\"Test for issue #1247: ensure top-level untracked files are detected.\"\"\"",
            "+        # Create a single top-level untracked file",
            "+        with open(os.path.join(self.repo.path, \"sample.txt\"), \"w\") as f:",
            "+            f.write(\"test content\")",
            "+",
            "+        # Test get_untracked_paths directly",
            "+        untracked = list(",
            "+            porcelain.get_untracked_paths(",
            "+                self.repo.path, self.repo.path, self.repo.open_index()",
            "+            )",
            "+        )",
            "+        self.assertIn(",
            "+            \"sample.txt\",",
            "+            untracked,",
            "+            \"Top-level file 'sample.txt' should be in untracked list\",",
            "+        )",
            "+",
            "+        # Test via status",
            "+        status = porcelain.status(self.repo)",
            "+        self.assertIn(",
            "+            \"sample.txt\",",
            "+            status.untracked,",
            "+            \"Top-level file 'sample.txt' should be in status.untracked\",",
            "+        )",
            " ",
            " ",
            " # TODO(jelmer): Add test for dulwich.porcelain.daemon",
            " ",
            " ",
            " class UploadPackTests(PorcelainTestCase):",
            "     \"\"\"Tests for upload_pack.\"\"\"",
            "@@ -2879,14 +5302,126 @@",
            " ",
            "     def test_new_branch(self) -> None:",
            "         [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "         self.repo[b\"HEAD\"] = c1.id",
            "         porcelain.branch_create(self.repo, b\"foo\")",
            "         self.assertEqual({b\"master\", b\"foo\"}, set(porcelain.branch_list(self.repo)))",
            " ",
            "+    def test_sort_by_refname(self) -> None:",
            "+        \"\"\"Test branch.sort=refname (default alphabetical).\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+",
            "+        # Create branches in non-alphabetical order",
            "+        porcelain.branch_create(self.repo, b\"zebra\")",
            "+        porcelain.branch_create(self.repo, b\"alpha\")",
            "+        porcelain.branch_create(self.repo, b\"beta\")",
            "+",
            "+        # Set branch.sort to refname (though it's the default)",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"sort\", b\"refname\")",
            "+        config.write_to_path()",
            "+",
            "+        # Should be sorted alphabetically",
            "+        branches = porcelain.branch_list(self.repo)",
            "+        self.assertEqual([b\"alpha\", b\"beta\", b\"master\", b\"zebra\"], branches)",
            "+",
            "+    def test_sort_by_refname_reverse(self) -> None:",
            "+        \"\"\"Test branch.sort=-refname (reverse alphabetical).\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+",
            "+        # Create branches",
            "+        porcelain.branch_create(self.repo, b\"zebra\")",
            "+        porcelain.branch_create(self.repo, b\"alpha\")",
            "+        porcelain.branch_create(self.repo, b\"beta\")",
            "+",
            "+        # Set branch.sort to -refname",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"sort\", b\"-refname\")",
            "+        config.write_to_path()",
            "+",
            "+        # Should be sorted reverse alphabetically",
            "+        branches = porcelain.branch_list(self.repo)",
            "+        self.assertEqual([b\"zebra\", b\"master\", b\"beta\", b\"alpha\"], branches)",
            "+",
            "+    def test_sort_by_committerdate(self) -> None:",
            "+        \"\"\"Test branch.sort=committerdate.\"\"\"",
            "+        # Use build_commit_graph to create proper commits with specific times",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store,",
            "+            [[1], [2], [3]],",
            "+            attrs={",
            "+                1: {\"commit_time\": 1000},  # oldest",
            "+                2: {\"commit_time\": 2000},  # newest",
            "+                3: {\"commit_time\": 1500},  # middle",
            "+            },",
            "+        )",
            "+",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+",
            "+        # Create branches pointing to different commits",
            "+        self.repo.refs[b\"refs/heads/master\"] = c1.id  # master points to oldest",
            "+        self.repo.refs[b\"refs/heads/oldest\"] = c1.id",
            "+        self.repo.refs[b\"refs/heads/newest\"] = c2.id",
            "+        self.repo.refs[b\"refs/heads/middle\"] = c3.id",
            "+",
            "+        # Set branch.sort to committerdate",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"sort\", b\"committerdate\")",
            "+        config.write_to_path()",
            "+",
            "+        # Should be sorted by commit time (oldest first)",
            "+        branches = porcelain.branch_list(self.repo)",
            "+        self.assertEqual([b\"master\", b\"oldest\", b\"middle\", b\"newest\"], branches)",
            "+",
            "+    def test_sort_by_committerdate_reverse(self) -> None:",
            "+        \"\"\"Test branch.sort=-committerdate.\"\"\"",
            "+        # Use build_commit_graph to create proper commits with specific times",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store,",
            "+            [[1], [2], [3]],",
            "+            attrs={",
            "+                1: {\"commit_time\": 1000},  # oldest",
            "+                2: {\"commit_time\": 2000},  # newest",
            "+                3: {\"commit_time\": 1500},  # middle",
            "+            },",
            "+        )",
            "+",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+",
            "+        # Create branches pointing to different commits",
            "+        self.repo.refs[b\"refs/heads/master\"] = c1.id  # master points to oldest",
            "+        self.repo.refs[b\"refs/heads/oldest\"] = c1.id",
            "+        self.repo.refs[b\"refs/heads/newest\"] = c2.id",
            "+        self.repo.refs[b\"refs/heads/middle\"] = c3.id",
            "+",
            "+        # Set branch.sort to -committerdate",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"sort\", b\"-committerdate\")",
            "+        config.write_to_path()",
            "+",
            "+        # Should be sorted by commit time (newest first)",
            "+        branches = porcelain.branch_list(self.repo)",
            "+        self.assertEqual([b\"newest\", b\"middle\", b\"master\", b\"oldest\"], branches)",
            "+",
            "+    def test_sort_default(self) -> None:",
            "+        \"\"\"Test default sorting (no config).\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+",
            "+        # Create branches in non-alphabetical order",
            "+        porcelain.branch_create(self.repo, b\"zebra\")",
            "+        porcelain.branch_create(self.repo, b\"alpha\")",
            "+        porcelain.branch_create(self.repo, b\"beta\")",
            "+",
            "+        # No config set - should default to alphabetical",
            "+        branches = porcelain.branch_list(self.repo)",
            "+        self.assertEqual([b\"alpha\", b\"beta\", b\"master\", b\"zebra\"], branches)",
            "+",
            " ",
            " class BranchCreateTests(PorcelainTestCase):",
            "     def test_branch_exists(self) -> None:",
            "         [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "         self.repo[b\"HEAD\"] = c1.id",
            "         porcelain.branch_create(self.repo, b\"foo\")",
            "         self.assertRaises(porcelain.Error, porcelain.branch_create, self.repo, b\"foo\")",
            "@@ -2894,14 +5429,117 @@",
            " ",
            "     def test_new_branch(self) -> None:",
            "         [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "         self.repo[b\"HEAD\"] = c1.id",
            "         porcelain.branch_create(self.repo, b\"foo\")",
            "         self.assertEqual({b\"master\", b\"foo\"}, set(porcelain.branch_list(self.repo)))",
            " ",
            "+    def test_auto_setup_merge_true_from_remote_tracking(self) -> None:",
            "+        \"\"\"Test branch.autoSetupMerge=true sets up tracking from remote-tracking branch.\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+        # Create a remote-tracking branch",
            "+        self.repo.refs[b\"refs/remotes/origin/feature\"] = c1.id",
            "+",
            "+        # Set branch.autoSetupMerge to true (default)",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"autoSetupMerge\", b\"true\")",
            "+        config.write_to_path()",
            "+",
            "+        # Create branch from remote-tracking branch",
            "+        porcelain.branch_create(self.repo, \"myfeature\", \"origin/feature\")",
            "+",
            "+        # Verify tracking was set up",
            "+        config = self.repo.get_config()",
            "+        self.assertEqual(config.get((b\"branch\", b\"myfeature\"), b\"remote\"), b\"origin\")",
            "+        self.assertEqual(",
            "+            config.get((b\"branch\", b\"myfeature\"), b\"merge\"), b\"refs/heads/feature\"",
            "+        )",
            "+",
            "+    def test_auto_setup_merge_false(self) -> None:",
            "+        \"\"\"Test branch.autoSetupMerge=false disables tracking setup.\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+        # Create a remote-tracking branch",
            "+        self.repo.refs[b\"refs/remotes/origin/feature\"] = c1.id",
            "+",
            "+        # Set branch.autoSetupMerge to false",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"autoSetupMerge\", b\"false\")",
            "+        config.write_to_path()",
            "+",
            "+        # Create branch from remote-tracking branch",
            "+        porcelain.branch_create(self.repo, \"myfeature\", \"origin/feature\")",
            "+",
            "+        # Verify tracking was NOT set up",
            "+        config = self.repo.get_config()",
            "+        self.assertRaises(KeyError, config.get, (b\"branch\", b\"myfeature\"), b\"remote\")",
            "+        self.assertRaises(KeyError, config.get, (b\"branch\", b\"myfeature\"), b\"merge\")",
            "+",
            "+    def test_auto_setup_merge_always(self) -> None:",
            "+        \"\"\"Test branch.autoSetupMerge=always sets up tracking even from local branches.\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+        self.repo.refs[b\"refs/heads/main\"] = c1.id",
            "+",
            "+        # Set branch.autoSetupMerge to always",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"autoSetupMerge\", b\"always\")",
            "+        config.write_to_path()",
            "+",
            "+        # Create branch from local branch - normally wouldn't set up tracking",
            "+        porcelain.branch_create(self.repo, \"feature\", \"main\")",
            "+",
            "+        # With always, tracking should NOT be set up from local branches",
            "+        # (Git only sets up tracking from remote-tracking branches even with always)",
            "+        config = self.repo.get_config()",
            "+        self.assertRaises(KeyError, config.get, (b\"branch\", b\"feature\"), b\"remote\")",
            "+        self.assertRaises(KeyError, config.get, (b\"branch\", b\"feature\"), b\"merge\")",
            "+",
            "+    def test_auto_setup_merge_always_from_remote(self) -> None:",
            "+        \"\"\"Test branch.autoSetupMerge=always still sets up tracking from remote branches.\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+        # Create a remote-tracking branch",
            "+        self.repo.refs[b\"refs/remotes/origin/feature\"] = c1.id",
            "+",
            "+        # Set branch.autoSetupMerge to always",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\",), b\"autoSetupMerge\", b\"always\")",
            "+        config.write_to_path()",
            "+",
            "+        # Create branch from remote-tracking branch",
            "+        porcelain.branch_create(self.repo, \"myfeature\", \"origin/feature\")",
            "+",
            "+        # Verify tracking was set up",
            "+        config = self.repo.get_config()",
            "+        self.assertEqual(config.get((b\"branch\", b\"myfeature\"), b\"remote\"), b\"origin\")",
            "+        self.assertEqual(",
            "+            config.get((b\"branch\", b\"myfeature\"), b\"merge\"), b\"refs/heads/feature\"",
            "+        )",
            "+",
            "+    def test_auto_setup_merge_default(self) -> None:",
            "+        \"\"\"Test default behavior (no config) is same as true.\"\"\"",
            "+        [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "+        self.repo[b\"HEAD\"] = c1.id",
            "+        # Create a remote-tracking branch",
            "+        self.repo.refs[b\"refs/remotes/origin/feature\"] = c1.id",
            "+",
            "+        # Don't set any config - should default to true",
            "+",
            "+        # Create branch from remote-tracking branch",
            "+        porcelain.branch_create(self.repo, \"myfeature\", \"origin/feature\")",
            "+",
            "+        # Verify tracking was set up",
            "+        config = self.repo.get_config()",
            "+        self.assertEqual(config.get((b\"branch\", b\"myfeature\"), b\"remote\"), b\"origin\")",
            "+        self.assertEqual(",
            "+            config.get((b\"branch\", b\"myfeature\"), b\"merge\"), b\"refs/heads/feature\"",
            "+        )",
            "+",
            " ",
            " class BranchDeleteTests(PorcelainTestCase):",
            "     def test_simple(self) -> None:",
            "         [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "         self.repo[b\"HEAD\"] = c1.id",
            "         porcelain.branch_create(self.repo, b\"foo\")",
            "         self.assertIn(b\"foo\", porcelain.branch_list(self.repo))",
            "@@ -3081,18 +5719,18 @@",
            "         porcelain.commit(",
            "             repo=self.repo.path,",
            "             message=b\"test status\",",
            "             author=b\"author <email>\",",
            "             committer=b\"committer <email>\",",
            "         )",
            " ",
            "-        f = StringIO()",
            "-        porcelain.ls_tree(self.repo, b\"HEAD\", outstream=f)",
            "+        output = StringIO()",
            "+        porcelain.ls_tree(self.repo, b\"HEAD\", outstream=output)",
            "         self.assertEqual(",
            "-            f.getvalue(),",
            "+            output.getvalue(),",
            "             \"100644 blob 8b82634d7eae019850bb883f06abf428c58bc9aa\\tfoo\\n\",",
            "         )",
            " ",
            "     def test_recursive(self) -> None:",
            "         # Create a directory then write a dummy file in it",
            "         dirpath = os.path.join(self.repo.path, \"adir\")",
            "         filepath = os.path.join(dirpath, \"afile\")",
            "@@ -3102,46 +5740,51 @@",
            "         porcelain.add(repo=self.repo.path, paths=[filepath])",
            "         porcelain.commit(",
            "             repo=self.repo.path,",
            "             message=b\"test status\",",
            "             author=b\"author <email>\",",
            "             committer=b\"committer <email>\",",
            "         )",
            "-        f = StringIO()",
            "-        porcelain.ls_tree(self.repo, b\"HEAD\", outstream=f)",
            "+        output = StringIO()",
            "+        porcelain.ls_tree(self.repo, b\"HEAD\", outstream=output)",
            "         self.assertEqual(",
            "-            f.getvalue(),",
            "+            output.getvalue(),",
            "             \"40000 tree b145cc69a5e17693e24d8a7be0016ed8075de66d\\tadir\\n\",",
            "         )",
            "-        f = StringIO()",
            "-        porcelain.ls_tree(self.repo, b\"HEAD\", outstream=f, recursive=True)",
            "+        output2 = StringIO()",
            "+        porcelain.ls_tree(self.repo, b\"HEAD\", outstream=output2, recursive=True)",
            "         self.assertEqual(",
            "-            f.getvalue(),",
            "+            output2.getvalue(),",
            "             \"40000 tree b145cc69a5e17693e24d8a7be0016ed8075de66d\\tadir\\n\"",
            "             \"100644 blob 8b82634d7eae019850bb883f06abf428c58bc9aa\\tadir\"",
            "             \"/afile\\n\",",
            "         )",
            " ",
            " ",
            " class LsRemoteTests(PorcelainTestCase):",
            "     def test_empty(self) -> None:",
            "-        self.assertEqual({}, porcelain.ls_remote(self.repo.path))",
            "+        result = porcelain.ls_remote(self.repo.path)",
            "+        self.assertEqual({}, result.refs)",
            "+        self.assertEqual({}, result.symrefs)",
            " ",
            "     def test_some(self) -> None:",
            "         cid = porcelain.commit(",
            "             repo=self.repo.path,",
            "             message=b\"test status\",",
            "             author=b\"author <email>\",",
            "             committer=b\"committer <email>\",",
            "         )",
            " ",
            "+        result = porcelain.ls_remote(self.repo.path)",
            "         self.assertEqual(",
            "             {b\"refs/heads/master\": cid, b\"HEAD\": cid},",
            "-            porcelain.ls_remote(self.repo.path),",
            "+            result.refs,",
            "         )",
            "+        # HEAD should be a symref to refs/heads/master",
            "+        self.assertEqual({b\"HEAD\": b\"refs/heads/master\"}, result.symrefs)",
            " ",
            " ",
            " class LsFilesTests(PorcelainTestCase):",
            "     def test_empty(self) -> None:",
            "         self.assertEqual([], list(porcelain.ls_files(self.repo)))",
            " ",
            "     def test_simple(self) -> None:",
            "@@ -3217,24 +5860,22 @@",
            "     def test_check_added_rel(self) -> None:",
            "         with open(os.path.join(self.repo.path, \"foo\"), \"w\") as f:",
            "             f.write(\"BAR\")",
            "         self.repo.stage([\"foo\"])",
            "         with open(os.path.join(self.repo.path, \".gitignore\"), \"w\") as f:",
            "             f.write(\"foo\\n\")",
            "         cwd = os.getcwd()",
            "+        self.addCleanup(os.chdir, cwd)",
            "         os.mkdir(os.path.join(self.repo.path, \"bar\"))",
            "         os.chdir(os.path.join(self.repo.path, \"bar\"))",
            "-        try:",
            "-            self.assertEqual(list(porcelain.check_ignore(self.repo, [\"../foo\"])), [])",
            "-            self.assertEqual(",
            "-                [\"../foo\"],",
            "-                list(porcelain.check_ignore(self.repo, [\"../foo\"], no_index=True)),",
            "-            )",
            "-        finally:",
            "-            os.chdir(cwd)",
            "+        self.assertEqual(list(porcelain.check_ignore(self.repo, [\"../foo\"])), [])",
            "+        self.assertEqual(",
            "+            [\"../foo\"],",
            "+            list(porcelain.check_ignore(self.repo, [\"../foo\"], no_index=True)),",
            "+        )",
            " ",
            " ",
            " class UpdateHeadTests(PorcelainTestCase):",
            "     def test_set_to_branch(self) -> None:",
            "         [c1] = build_commit_graph(self.repo.object_store, [[1]])",
            "         self.repo.refs[b\"refs/heads/blah\"] = c1.id",
            "         porcelain.update_head(self.repo, \"blah\")",
            "@@ -3399,14 +6040,48 @@",
            "             committer=b\"Bob <bob@example.com>\",",
            "         )",
            "         self.assertEqual(",
            "             \"tryme-1-g{}\".format(sha.decode(\"ascii\")),",
            "             porcelain.describe(self.repo.path, abbrev=40),",
            "         )",
            " ",
            "+    def test_untagged_commit_abbreviation(self) -> None:",
            "+        _, _, c3 = build_commit_graph(self.repo.object_store, [[1], [2, 1], [3, 1, 2]])",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+        brief_description, complete_description = (",
            "+            porcelain.describe(self.repo),",
            "+            porcelain.describe(self.repo, abbrev=40),",
            "+        )",
            "+        self.assertTrue(complete_description.startswith(brief_description))",
            "+        self.assertEqual(",
            "+            \"g{}\".format(c3.id.decode(\"ascii\")),",
            "+            complete_description,",
            "+        )",
            "+",
            "+    def test_hash_length_dynamic(self) -> None:",
            "+        \"\"\"Test that hash length adjusts based on uniqueness.\"\"\"",
            "+        fullpath = os.path.join(self.repo.path, \"foo\")",
            "+        with open(fullpath, \"w\") as f:",
            "+            f.write(\"content\")",
            "+        porcelain.add(repo=self.repo.path, paths=[fullpath])",
            "+        sha = porcelain.commit(",
            "+            self.repo.path,",
            "+            message=b\"commit\",",
            "+            author=b\"Joe <joe@example.com>\",",
            "+            committer=b\"Bob <bob@example.com>\",",
            "+        )",
            "+",
            "+        # When abbrev is None, it should use find_unique_abbrev",
            "+        result = porcelain.describe(self.repo.path)",
            "+        # Should start with 'g' and have at least 7 characters",
            "+        self.assertTrue(result.startswith(\"g\"))",
            "+        self.assertGreaterEqual(len(result[1:]), 7)",
            "+        # Should be a prefix of the full SHA",
            "+        self.assertTrue(sha.decode(\"ascii\").startswith(result[1:]))",
            "+",
            " ",
            " class PathToTreeTests(PorcelainTestCase):",
            "     def setUp(self) -> None:",
            "         super().setUp()",
            "         self.fp = os.path.join(self.test_dir, \"bar\")",
            "         with open(self.fp, \"w\") as f:",
            "             f.write(\"something\")",
            "@@ -3430,38 +6105,36 @@",
            "     def test_path_to_tree_path_error(self) -> None:",
            "         with self.assertRaises(ValueError):",
            "             with tempfile.TemporaryDirectory() as od:",
            "                 porcelain.path_to_tree_path(od, self.fp)",
            " ",
            "     def test_path_to_tree_path_rel(self) -> None:",
            "         cwd = os.getcwd()",
            "+        self.addCleanup(os.chdir, cwd)",
            "         os.mkdir(os.path.join(self.repo.path, \"foo\"))",
            "         os.mkdir(os.path.join(self.repo.path, \"foo/bar\"))",
            "-        try:",
            "-            os.chdir(os.path.join(self.repo.path, \"foo/bar\"))",
            "-            with open(\"baz\", \"w\") as f:",
            "-                f.write(\"contents\")",
            "-            self.assertEqual(b\"bar/baz\", porcelain.path_to_tree_path(\"..\", \"baz\"))",
            "-            self.assertEqual(",
            "-                b\"bar/baz\",",
            "-                porcelain.path_to_tree_path(",
            "-                    os.path.join(os.getcwd(), \"..\"),",
            "-                    os.path.join(os.getcwd(), \"baz\"),",
            "-                ),",
            "-            )",
            "-            self.assertEqual(",
            "-                b\"bar/baz\",",
            "-                porcelain.path_to_tree_path(\"..\", os.path.join(os.getcwd(), \"baz\")),",
            "-            )",
            "-            self.assertEqual(",
            "-                b\"bar/baz\",",
            "-                porcelain.path_to_tree_path(os.path.join(os.getcwd(), \"..\"), \"baz\"),",
            "-            )",
            "-        finally:",
            "-            os.chdir(cwd)",
            "+        os.chdir(os.path.join(self.repo.path, \"foo/bar\"))",
            "+        with open(\"baz\", \"w\") as f:",
            "+            f.write(\"contents\")",
            "+        self.assertEqual(b\"bar/baz\", porcelain.path_to_tree_path(\"..\", \"baz\"))",
            "+        self.assertEqual(",
            "+            b\"bar/baz\",",
            "+            porcelain.path_to_tree_path(",
            "+                os.path.join(os.getcwd(), \"..\"),",
            "+                os.path.join(os.getcwd(), \"baz\"),",
            "+            ),",
            "+        )",
            "+        self.assertEqual(",
            "+            b\"bar/baz\",",
            "+            porcelain.path_to_tree_path(\"..\", os.path.join(os.getcwd(), \"baz\")),",
            "+        )",
            "+        self.assertEqual(",
            "+            b\"bar/baz\",",
            "+            porcelain.path_to_tree_path(os.path.join(os.getcwd(), \"..\"), \"baz\"),",
            "+        )",
            " ",
            " ",
            " class GetObjectByPathTests(PorcelainTestCase):",
            "     def test_simple(self) -> None:",
            "         fullpath = os.path.join(self.repo.path, \"foo\")",
            "         with open(fullpath, \"w\") as f:",
            "             f.write(\"BAR\")",
            "@@ -3507,14 +6180,52 @@",
            " ",
            " ",
            " class ActiveBranchTests(PorcelainTestCase):",
            "     def test_simple(self) -> None:",
            "         self.assertEqual(b\"master\", porcelain.active_branch(self.repo))",
            " ",
            " ",
            "+class BranchTrackingTests(PorcelainTestCase):",
            "+    def test_get_branch_merge(self) -> None:",
            "+        # Set up branch tracking configuration",
            "+        config = self.repo.get_config()",
            "+        config.set((b\"branch\", b\"master\"), b\"remote\", b\"origin\")",
            "+        config.set((b\"branch\", b\"master\"), b\"merge\", b\"refs/heads/main\")",
            "+        config.write_to_path()",
            "+",
            "+        # Test getting merge ref for current branch",
            "+        merge_ref = porcelain.get_branch_merge(self.repo)",
            "+        self.assertEqual(b\"refs/heads/main\", merge_ref)",
            "+",
            "+        # Test getting merge ref for specific branch",
            "+        merge_ref = porcelain.get_branch_merge(self.repo, b\"master\")",
            "+        self.assertEqual(b\"refs/heads/main\", merge_ref)",
            "+",
            "+        # Test branch without merge config",
            "+        with self.assertRaises(KeyError):",
            "+            porcelain.get_branch_merge(self.repo, b\"nonexistent\")",
            "+",
            "+    def test_set_branch_tracking(self) -> None:",
            "+        # Create a new branch",
            "+        sha, _ = _commit_file_with_content(self.repo, \"foo\", \"content\\n\")",
            "+        porcelain.branch_create(self.repo, \"feature\")",
            "+",
            "+        # Set up tracking",
            "+        porcelain.set_branch_tracking(",
            "+            self.repo, b\"feature\", b\"upstream\", b\"refs/heads/main\"",
            "+        )",
            "+",
            "+        # Verify configuration was written",
            "+        config = self.repo.get_config()",
            "+        self.assertEqual(b\"upstream\", config.get((b\"branch\", b\"feature\"), b\"remote\"))",
            "+        self.assertEqual(",
            "+            b\"refs/heads/main\", config.get((b\"branch\", b\"feature\"), b\"merge\")",
            "+        )",
            "+",
            "+",
            " class FindUniqueAbbrevTests(PorcelainTestCase):",
            "     def test_simple(self) -> None:",
            "         c1, c2, c3 = build_commit_graph(",
            "             self.repo.object_store, [[1], [2, 1], [3, 1, 2]]",
            "         )",
            "         self.repo.refs[b\"HEAD\"] = c3.id",
            "         self.assertEqual(",
            "@@ -3662,7 +6373,1050 @@",
            "         self.assertEqual(",
            "             [(object_type, tag) for _, object_type, tag in versions],",
            "             [",
            "                 (b\"tag\", b\"refs/tags/v1.0\"),",
            "                 (b\"tag\", b\"refs/tags/v1.1\"),",
            "             ],",
            "         )",
            "+",
            "+",
            "+class SparseCheckoutTests(PorcelainTestCase):",
            "+    \"\"\"Integration tests for Dulwich's sparse checkout feature.\"\"\"",
            "+",
            "+    # NOTE: We do NOT override `setUp()` here because the parent class",
            "+    #       (PorcelainTestCase) already:",
            "+    #         1) Creates self.test_dir = a unique temp dir",
            "+    #         2) Creates a subdir named \"repo\"",
            "+    #         3) Calls Repo.init() on that path",
            "+    #       Re-initializing again caused FileExistsError.",
            "+",
            "+    #",
            "+    # Utility/Placeholder",
            "+    #",
            "+    def sparse_checkout(self, repo, patterns, force=False):",
            "+        \"\"\"Wrapper around the actual porcelain.sparse_checkout function",
            "+        to handle any test-specific setup or logging.",
            "+        \"\"\"",
            "+        return porcelain.sparse_checkout(repo, patterns, force=force)",
            "+",
            "+    def _write_file(self, rel_path, content):",
            "+        \"\"\"Helper to write a file in the repository working tree.\"\"\"",
            "+        abs_path = os.path.join(self.repo_path, rel_path)",
            "+        os.makedirs(os.path.dirname(abs_path), exist_ok=True)",
            "+        with open(abs_path, \"w\") as f:",
            "+            f.write(content)",
            "+        return abs_path",
            "+",
            "+    def _commit_file(self, rel_path, content):",
            "+        \"\"\"Helper to write, add, and commit a file.\"\"\"",
            "+        abs_path = self._write_file(rel_path, content)",
            "+        add(self.repo_path, paths=[abs_path])",
            "+        commit(self.repo_path, message=b\"Added \" + rel_path.encode(\"utf-8\"))",
            "+",
            "+    def _list_wtree_files(self):",
            "+        \"\"\"Return a set of all files (not dirs) present",
            "+        in the working tree, ignoring .git/.",
            "+        \"\"\"",
            "+        found_files = set()",
            "+        for root, dirs, files in os.walk(self.repo_path):",
            "+            # Skip .git in the walk",
            "+            if \".git\" in dirs:",
            "+                dirs.remove(\".git\")",
            "+",
            "+            for filename in files:",
            "+                file_rel = os.path.relpath(os.path.join(root, filename), self.repo_path)",
            "+                found_files.add(file_rel)",
            "+        return found_files",
            "+",
            "+    def test_only_included_paths_appear_in_wtree(self):",
            "+        \"\"\"Only included paths remain in the working tree, excluded paths are removed.",
            "+",
            "+        Commits two files, \"keep_me.txt\" and \"exclude_me.txt\". Then applies a",
            "+        sparse-checkout pattern containing only \"keep_me.txt\". Ensures that",
            "+        the latter remains in the working tree, while \"exclude_me.txt\" is",
            "+        removed. This verifies correct application of sparse-checkout patterns",
            "+        to remove files not listed.",
            "+        \"\"\"",
            "+        self._commit_file(\"keep_me.txt\", \"I'll stay\\n\")",
            "+        self._commit_file(\"exclude_me.txt\", \"I'll be excluded\\n\")",
            "+",
            "+        patterns = [\"keep_me.txt\"]",
            "+        self.sparse_checkout(self.repo, patterns)",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        expected_files = {\"keep_me.txt\"}",
            "+        self.assertEqual(",
            "+            expected_files,",
            "+            actual_files,",
            "+            f\"Expected only {expected_files}, but found {actual_files}\",",
            "+        )",
            "+",
            "+    def test_previously_included_paths_become_excluded(self):",
            "+        \"\"\"Previously included files become excluded after pattern changes.",
            "+",
            "+        Verifies that files initially brought into the working tree (e.g.,",
            "+        by including `data/`) can later be excluded by narrowing the",
            "+        sparse-checkout pattern to just `data/included_1.txt`. Confirms that",
            "+        the file `data/included_2.txt` remains in the index with",
            "+        skip-worktree set (rather than being removed entirely), ensuring",
            "+        data is not lost and Dulwich correctly updates the index flags.",
            "+        \"\"\"",
            "+        self._commit_file(\"data/included_1.txt\", \"some content\\n\")",
            "+        self._commit_file(\"data/included_2.txt\", \"other content\\n\")",
            "+",
            "+        initial_patterns = [\"data/\"]",
            "+        self.sparse_checkout(self.repo, initial_patterns)",
            "+",
            "+        updated_patterns = [\"data/included_1.txt\"]",
            "+        self.sparse_checkout(self.repo, updated_patterns)",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        expected_files = {os.path.join(\"data\", \"included_1.txt\")}",
            "+        self.assertEqual(expected_files, actual_files)",
            "+",
            "+        idx = self.repo.open_index()",
            "+        self.assertIn(b\"data/included_2.txt\", idx)",
            "+        entry = idx[b\"data/included_2.txt\"]",
            "+        self.assertTrue(entry.skip_worktree)",
            "+",
            "+    def test_force_removes_local_changes_for_excluded_paths(self):",
            "+        \"\"\"Forced sparse checkout removes local modifications for newly excluded paths.",
            "+",
            "+        Verifies that specifying force=True allows destructive operations",
            "+        which discard uncommitted changes. First, we commit \"file1.txt\" and",
            "+        then modify it. Next, we apply a pattern that excludes the file,",
            "+        using force=True. The local modifications (and the file) should",
            "+        be removed, leaving the working tree empty.",
            "+        \"\"\"",
            "+        self._commit_file(\"file1.txt\", \"original content\\n\")",
            "+",
            "+        file1_path = os.path.join(self.repo_path, \"file1.txt\")",
            "+        with open(file1_path, \"a\") as f:",
            "+            f.write(\"local changes!\\n\")",
            "+",
            "+        new_patterns = [\"some_other_file.txt\"]",
            "+        self.sparse_checkout(self.repo, new_patterns, force=True)",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        self.assertEqual(",
            "+            set(),",
            "+            actual_files,",
            "+            \"Force-sparse-checkout did not remove file with local changes.\",",
            "+        )",
            "+",
            "+    def test_destructive_refuse_uncommitted_changes_without_force(self):",
            "+        \"\"\"Fail on uncommitted changes for newly excluded paths without force.",
            "+",
            "+        Ensures that a sparse checkout is blocked if it would remove local",
            "+        modifications from the working tree. We commit 'config.yaml', then",
            "+        modify it, and finally attempt to exclude it via new patterns without",
            "+        using force=True. This should raise a CheckoutError rather than",
            "+        discarding the local changes.",
            "+        \"\"\"",
            "+        self._commit_file(\"config.yaml\", \"initial\\n\")",
            "+        cfg_path = os.path.join(self.repo_path, \"config.yaml\")",
            "+        with open(cfg_path, \"a\") as f:",
            "+            f.write(\"local modifications\\n\")",
            "+",
            "+        exclude_patterns = [\"docs/\"]",
            "+        with self.assertRaises(CheckoutError):",
            "+            self.sparse_checkout(self.repo, exclude_patterns, force=False)",
            "+",
            "+    def test_fnmatch_gitignore_pattern_expansion(self):",
            "+        \"\"\"Reading/writing patterns align with gitignore/fnmatch expansions.",
            "+",
            "+        Ensures that `sparse_checkout` interprets wildcard patterns (like `*.py`)",
            "+        in the same way Git's sparse-checkout would. Multiple files are committed",
            "+        to `src/` (e.g. `foo.py`, `foo_test.py`, `foo_helper.py`) and to `docs/`.",
            "+        Then the pattern `src/foo*.py` is applied, confirming that only the",
            "+        matching Python files remain in the working tree while the Markdown file",
            "+        under `docs/` is excluded.",
            "+",
            "+        Finally, verifies that the `.git/info/sparse-checkout` file contains the",
            "+        specified wildcard pattern (`src/foo*.py`), ensuring correct round-trip",
            "+        of user-supplied patterns.",
            "+        \"\"\"",
            "+        self._commit_file(\"src/foo.py\", \"print('hello')\\n\")",
            "+        self._commit_file(\"src/foo_test.py\", \"print('test')\\n\")",
            "+        self._commit_file(\"docs/readme.md\", \"# docs\\n\")",
            "+        self._commit_file(\"src/foo_helper.py\", \"print('helper')\\n\")",
            "+",
            "+        patterns = [\"src/foo*.py\"]",
            "+        self.sparse_checkout(self.repo, patterns)",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        expected_files = {",
            "+            os.path.join(\"src\", \"foo.py\"),",
            "+            os.path.join(\"src\", \"foo_test.py\"),",
            "+            os.path.join(\"src\", \"foo_helper.py\"),",
            "+        }",
            "+        self.assertEqual(",
            "+            expected_files,",
            "+            actual_files,",
            "+            \"Wildcard pattern not matched as expected. Either too strict or too broad.\",",
            "+        )",
            "+",
            "+        sc_file = os.path.join(self.repo_path, \".git\", \"info\", \"sparse-checkout\")",
            "+        self.assertTrue(os.path.isfile(sc_file))",
            "+        with open(sc_file) as f:",
            "+            lines = f.read().strip().split()",
            "+            self.assertIn(\"src/foo*.py\", lines)",
            "+",
            "+",
            "+class ConeModeTests(PorcelainTestCase):",
            "+    \"\"\"Provide integration tests for Dulwich's cone mode sparse checkout.",
            "+",
            "+    This test suite verifies the expected behavior for:",
            "+      * cone_mode_init",
            "+      * cone_mode_set",
            "+      * cone_mode_add",
            "+    Although Dulwich does not yet implement cone mode, these tests are",
            "+    prepared in advance to guide future development.",
            "+    \"\"\"",
            "+",
            "+    def setUp(self):",
            "+        \"\"\"Set up a fresh repository for each test.",
            "+",
            "+        This method creates a new empty repo_path and Repo object",
            "+        as provided by the PorcelainTestCase base class.",
            "+        \"\"\"",
            "+        super().setUp()",
            "+",
            "+    def _commit_file(self, rel_path, content=b\"contents\"):",
            "+        \"\"\"Add a file at the given relative path and commit it.",
            "+",
            "+        Creates necessary directories, writes the file content,",
            "+        stages, and commits. The commit message and author/committer",
            "+        are also provided.",
            "+        \"\"\"",
            "+        full_path = os.path.join(self.repo_path, rel_path)",
            "+        os.makedirs(os.path.dirname(full_path), exist_ok=True)",
            "+        with open(full_path, \"wb\") as f:",
            "+            f.write(content)",
            "+        porcelain.add(self.repo_path, paths=[full_path])",
            "+        porcelain.commit(",
            "+            self.repo_path,",
            "+            message=b\"Adding \" + rel_path.encode(\"utf-8\"),",
            "+            author=b\"Test Author <author@example.com>\",",
            "+            committer=b\"Test Committer <committer@example.com>\",",
            "+        )",
            "+",
            "+    def _list_wtree_files(self):",
            "+        \"\"\"Return a set of all file paths relative to the repository root.",
            "+",
            "+        Walks the working tree, skipping the .git directory.",
            "+        \"\"\"",
            "+        found_files = set()",
            "+        for root, dirs, files in os.walk(self.repo_path):",
            "+            if \".git\" in dirs:",
            "+                dirs.remove(\".git\")",
            "+            for fn in files:",
            "+                relp = os.path.relpath(os.path.join(root, fn), self.repo_path)",
            "+                found_files.add(relp)",
            "+        return found_files",
            "+",
            "+    def test_init_excludes_everything(self):",
            "+        \"\"\"Verify that cone_mode_init writes minimal patterns and empties the working tree.",
            "+",
            "+        Make some dummy files, commit them, then call cone_mode_init. Confirm",
            "+        that the working tree is empty, the sparse-checkout file has the",
            "+        minimal patterns (/*, !/*/), and the relevant config values are set.",
            "+        \"\"\"",
            "+        self._commit_file(\"docs/readme.md\", b\"# doc\\n\")",
            "+        self._commit_file(\"src/main.py\", b\"print('hello')\\n\")",
            "+",
            "+        porcelain.cone_mode_init(self.repo)",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        self.assertEqual(",
            "+            set(),",
            "+            actual_files,",
            "+            \"cone_mode_init did not exclude all files from the working tree.\",",
            "+        )",
            "+",
            "+        sp_path = os.path.join(self.repo_path, \".git\", \"info\", \"sparse-checkout\")",
            "+        with open(sp_path) as f:",
            "+            lines = [ln.strip() for ln in f if ln.strip()]",
            "+",
            "+        self.assertIn(\"/*\", lines)",
            "+        self.assertIn(\"!/*/\", lines)",
            "+",
            "+        config = self.repo.get_config()",
            "+        self.assertEqual(config.get((b\"core\",), b\"sparseCheckout\"), b\"true\")",
            "+        self.assertEqual(config.get((b\"core\",), b\"sparseCheckoutCone\"), b\"true\")",
            "+",
            "+    def test_set_specific_dirs(self):",
            "+        \"\"\"Verify that cone_mode_set overwrites the included directories to only the specified ones.",
            "+",
            "+        Initializes cone mode, commits some files, then calls cone_mode_set with",
            "+        a list of directories. Expects that only those directories remain in the",
            "+        working tree.",
            "+        \"\"\"",
            "+        porcelain.cone_mode_init(self.repo)",
            "+        self._commit_file(\"docs/readme.md\", b\"# doc\\n\")",
            "+        self._commit_file(\"src/main.py\", b\"print('hello')\\n\")",
            "+        self._commit_file(\"tests/test_foo.py\", b\"# tests\\n\")",
            "+",
            "+        # Everything is still excluded initially by init.",
            "+",
            "+        porcelain.cone_mode_set(self.repo, dirs=[\"docs\", \"src\"])",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        expected_files = {",
            "+            os.path.join(\"docs\", \"readme.md\"),",
            "+            os.path.join(\"src\", \"main.py\"),",
            "+        }",
            "+        self.assertEqual(",
            "+            expected_files,",
            "+            actual_files,",
            "+            \"Did not see only the 'docs/' and 'src/' dirs in the working tree.\",",
            "+        )",
            "+",
            "+        sp_path = os.path.join(self.repo_path, \".git\", \"info\", \"sparse-checkout\")",
            "+        with open(sp_path) as f:",
            "+            lines = [ln.strip() for ln in f if ln.strip()]",
            "+",
            "+        # For standard cone mode, we'd expect lines like:",
            "+        #    /*           (include top-level files)",
            "+        #    !/*/         (exclude subdirectories)",
            "+        #    !/docs/      (re-include docs)",
            "+        #    !/src/       (re-include src)",
            "+        # Instead of the wildcard-based lines the old test used.",
            "+        self.assertIn(\"/*\", lines)",
            "+        self.assertIn(\"!/*/\", lines)",
            "+        self.assertIn(\"/docs/\", lines)",
            "+        self.assertIn(\"/src/\", lines)",
            "+        self.assertNotIn(\"/tests/\", lines)",
            "+",
            "+    def test_set_overwrites_old_dirs(self):",
            "+        \"\"\"Ensure that calling cone_mode_set again overwrites old includes.",
            "+",
            "+        Initializes cone mode, includes two directories, then calls",
            "+        cone_mode_set again with a different directory to confirm the",
            "+        new set of includes replaces the old.",
            "+        \"\"\"",
            "+        porcelain.cone_mode_init(self.repo)",
            "+        self._commit_file(\"docs/readme.md\")",
            "+        self._commit_file(\"src/main.py\")",
            "+        self._commit_file(\"tests/test_bar.py\")",
            "+",
            "+        porcelain.cone_mode_set(self.repo, dirs=[\"docs\", \"src\"])",
            "+        self.assertEqual(",
            "+            {os.path.join(\"docs\", \"readme.md\"), os.path.join(\"src\", \"main.py\")},",
            "+            self._list_wtree_files(),",
            "+        )",
            "+",
            "+        # Overwrite includes, now only 'tests'",
            "+        porcelain.cone_mode_set(self.repo, dirs=[\"tests\"], force=True)",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        expected_files = {os.path.join(\"tests\", \"test_bar.py\")}",
            "+        self.assertEqual(expected_files, actual_files)",
            "+",
            "+    def test_force_removal_of_local_mods(self):",
            "+        \"\"\"Confirm that force=True removes local changes in excluded paths.",
            "+",
            "+        cone_mode_init and cone_mode_set are called, a file is locally modified,",
            "+        and then cone_mode_set is called again with force=True to exclude that path.",
            "+        The excluded file should be removed with no CheckoutError.",
            "+        \"\"\"",
            "+        porcelain.cone_mode_init(self.repo)",
            "+        porcelain.cone_mode_set(self.repo, dirs=[\"docs\"])",
            "+",
            "+        self._commit_file(\"docs/readme.md\", b\"Docs stuff\\n\")",
            "+        self._commit_file(\"src/main.py\", b\"print('hello')\\n\")",
            "+",
            "+        # Modify src/main.py",
            "+        with open(os.path.join(self.repo_path, \"src/main.py\"), \"ab\") as f:",
            "+            f.write(b\"extra line\\n\")",
            "+",
            "+        # Exclude src/ with force=True",
            "+        porcelain.cone_mode_set(self.repo, dirs=[\"docs\"], force=True)",
            "+",
            "+        actual_files = self._list_wtree_files()",
            "+        expected_files = {os.path.join(\"docs\", \"readme.md\")}",
            "+        self.assertEqual(expected_files, actual_files)",
            "+",
            "+    def test_add_and_merge_dirs(self):",
            "+        \"\"\"Verify that cone_mode_add merges new directories instead of overwriting them.",
            "+",
            "+        After initializing cone mode and including a single directory, call",
            "+        cone_mode_add with a new directory. Confirm that both directories",
            "+        remain included. Repeat for an additional directory to ensure it",
            "+        is merged, not overwritten.",
            "+        \"\"\"",
            "+        porcelain.cone_mode_init(self.repo)",
            "+        self._commit_file(\"docs/readme.md\", b\"# doc\\n\")",
            "+        self._commit_file(\"src/main.py\", b\"print('hello')\\n\")",
            "+        self._commit_file(\"tests/test_bar.py\", b\"# tests\\n\")",
            "+",
            "+        # Include \"docs\" only",
            "+        porcelain.cone_mode_set(self.repo, dirs=[\"docs\"])",
            "+        self.assertEqual({os.path.join(\"docs\", \"readme.md\")}, self._list_wtree_files())",
            "+",
            "+        # Add \"src\"",
            "+        porcelain.cone_mode_add(self.repo, dirs=[\"src\"])",
            "+        actual_files = self._list_wtree_files()",
            "+        self.assertEqual(",
            "+            {os.path.join(\"docs\", \"readme.md\"), os.path.join(\"src\", \"main.py\")},",
            "+            actual_files,",
            "+        )",
            "+",
            "+        # Add \"tests\" as well",
            "+        porcelain.cone_mode_add(self.repo, dirs=[\"tests\"])",
            "+        actual_files = self._list_wtree_files()",
            "+        expected_files = {",
            "+            os.path.join(\"docs\", \"readme.md\"),",
            "+            os.path.join(\"src\", \"main.py\"),",
            "+            os.path.join(\"tests\", \"test_bar.py\"),",
            "+        }",
            "+        self.assertEqual(expected_files, actual_files)",
            "+",
            "+        # Check .git/info/sparse-checkout",
            "+        sp_path = os.path.join(self.repo_path, \".git\", \"info\", \"sparse-checkout\")",
            "+        with open(sp_path) as f:",
            "+            lines = [ln.strip() for ln in f if ln.strip()]",
            "+",
            "+        # Standard cone mode lines:",
            "+        # \"/*\"    -> include top-level",
            "+        # \"!/*/\"  -> exclude subdirectories",
            "+        # \"!/docs/\", \"!/src/\", \"!/tests/\" -> re-include the directories we added",
            "+        self.assertIn(\"/*\", lines)",
            "+        self.assertIn(\"!/*/\", lines)",
            "+        self.assertIn(\"/docs/\", lines)",
            "+        self.assertIn(\"/src/\", lines)",
            "+        self.assertIn(\"/tests/\", lines)",
            "+",
            "+",
            "+class UnpackObjectsTest(PorcelainTestCase):",
            "+    def test_unpack_objects(self):",
            "+        \"\"\"Test unpacking objects from a pack file.\"\"\"",
            "+        # Create a test repository with some objects",
            "+        b1 = Blob()",
            "+        b1.data = b\"test content 1\"",
            "+        b2 = Blob()",
            "+        b2.data = b\"test content 2\"",
            "+",
            "+        # Add objects to the repo",
            "+        self.repo.object_store.add_object(b1)",
            "+        self.repo.object_store.add_object(b2)",
            "+",
            "+        # Create a pack file with these objects",
            "+        pack_path = os.path.join(self.test_dir, \"test_pack\")",
            "+        with (",
            "+            open(pack_path + \".pack\", \"wb\") as pack_f,",
            "+            open(pack_path + \".idx\", \"wb\") as idx_f,",
            "+        ):",
            "+            porcelain.pack_objects(",
            "+                self.repo,",
            "+                [b1.id, b2.id],",
            "+                pack_f,",
            "+                idx_f,",
            "+            )",
            "+",
            "+        # Create a new repository to unpack into",
            "+        target_repo_path = os.path.join(self.test_dir, \"target_repo\")",
            "+        target_repo = Repo.init(target_repo_path, mkdir=True)",
            "+        self.addCleanup(target_repo.close)",
            "+",
            "+        # Unpack the objects",
            "+        count = porcelain.unpack_objects(pack_path + \".pack\", target_repo_path)",
            "+",
            "+        # Verify the objects were unpacked",
            "+        self.assertEqual(2, count)",
            "+        self.assertIn(b1.id, target_repo.object_store)",
            "+        self.assertIn(b2.id, target_repo.object_store)",
            "+",
            "+        # Verify the content is correct",
            "+        unpacked_b1 = target_repo.object_store[b1.id]",
            "+        unpacked_b2 = target_repo.object_store[b2.id]",
            "+        self.assertEqual(b1.data, unpacked_b1.data)",
            "+        self.assertEqual(b2.data, unpacked_b2.data)",
            "+",
            "+",
            "+class CountObjectsTests(PorcelainTestCase):",
            "+    def test_count_objects_empty_repo(self):",
            "+        \"\"\"Test counting objects in an empty repository.\"\"\"",
            "+        stats = porcelain.count_objects(self.repo)",
            "+        self.assertEqual(0, stats.count)",
            "+        self.assertEqual(0, stats.size)",
            "+",
            "+    def test_count_objects_verbose_empty_repo(self):",
            "+        \"\"\"Test verbose counting in an empty repository.\"\"\"",
            "+        stats = porcelain.count_objects(self.repo, verbose=True)",
            "+        self.assertEqual(0, stats.count)",
            "+        self.assertEqual(0, stats.size)",
            "+        self.assertEqual(0, stats.in_pack)",
            "+        self.assertEqual(0, stats.packs)",
            "+        self.assertEqual(0, stats.size_pack)",
            "+",
            "+    def test_count_objects_with_loose_objects(self):",
            "+        \"\"\"Test counting loose objects.\"\"\"",
            "+        # Create some loose objects",
            "+        blob1 = make_object(Blob, data=b\"data1\")",
            "+        blob2 = make_object(Blob, data=b\"data2\")",
            "+        self.repo.object_store.add_object(blob1)",
            "+        self.repo.object_store.add_object(blob2)",
            "+",
            "+        stats = porcelain.count_objects(self.repo)",
            "+        self.assertEqual(2, stats.count)",
            "+        self.assertGreater(stats.size, 0)",
            "+",
            "+    def test_count_objects_verbose_with_objects(self):",
            "+        \"\"\"Test verbose counting with both loose and packed objects.\"\"\"",
            "+        # Add some loose objects",
            "+        for i in range(3):",
            "+            blob = make_object(Blob, data=f\"data{i}\".encode())",
            "+            self.repo.object_store.add_object(blob)",
            "+",
            "+        # Create a simple commit to have some objects in a pack",
            "+        tree = Tree()",
            "+        c1 = make_commit(tree=tree.id, message=b\"Test commit\")",
            "+        self.repo.object_store.add_objects([(tree, None), (c1, None)])",
            "+        self.repo.refs[b\"HEAD\"] = c1.id",
            "+",
            "+        # Repack to create a pack file",
            "+        porcelain.repack(self.repo)",
            "+",
            "+        stats = porcelain.count_objects(self.repo, verbose=True)",
            "+",
            "+        # After repacking, loose objects might be cleaned up",
            "+        self.assertIsInstance(stats.count, int)",
            "+        self.assertIsInstance(stats.size, int)",
            "+        self.assertGreater(stats.in_pack, 0)  # Should have packed objects",
            "+        self.assertGreater(stats.packs, 0)  # Should have at least one pack",
            "+        self.assertGreater(stats.size_pack, 0)  # Pack should have size",
            "+",
            "+        # Verify it's the correct dataclass type",
            "+        self.assertIsInstance(stats, CountObjectsResult)",
            "+",
            "+",
            "+class PruneTests(PorcelainTestCase):",
            "+    def test_prune_removes_old_tempfiles(self):",
            "+        \"\"\"Test that prune removes old temporary files.\"\"\"",
            "+        # Create an old temporary file in the objects directory",
            "+        objects_dir = os.path.join(self.repo.path, \".git\", \"objects\")",
            "+        tmp_pack_path = os.path.join(objects_dir, \"tmp_pack_test\")",
            "+        with open(tmp_pack_path, \"wb\") as f:",
            "+            f.write(b\"old temporary data\")",
            "+",
            "+        # Make it old",
            "+        old_time = time.time() - (DEFAULT_TEMPFILE_GRACE_PERIOD + 3600)",
            "+        os.utime(tmp_pack_path, (old_time, old_time))",
            "+",
            "+        # Run prune",
            "+        porcelain.prune(self.repo.path)",
            "+",
            "+        # Verify the file was removed",
            "+        self.assertFalse(os.path.exists(tmp_pack_path))",
            "+",
            "+    def test_prune_keeps_recent_tempfiles(self):",
            "+        \"\"\"Test that prune keeps recent temporary files.\"\"\"",
            "+        # Create a recent temporary file",
            "+        objects_dir = os.path.join(self.repo.path, \".git\", \"objects\")",
            "+        tmp_pack_path = os.path.join(objects_dir, \"tmp_pack_recent\")",
            "+        with open(tmp_pack_path, \"wb\") as f:",
            "+            f.write(b\"recent temporary data\")",
            "+        self.addCleanup(os.remove, tmp_pack_path)",
            "+",
            "+        # Run prune",
            "+        porcelain.prune(self.repo.path)",
            "+",
            "+        # Verify the file was NOT removed",
            "+        self.assertTrue(os.path.exists(tmp_pack_path))",
            "+",
            "+    def test_prune_with_custom_grace_period(self):",
            "+        \"\"\"Test prune with custom grace period.\"\"\"",
            "+        # Create a 1-hour-old temporary file",
            "+        objects_dir = os.path.join(self.repo.path, \".git\", \"objects\")",
            "+        tmp_pack_path = os.path.join(objects_dir, \"tmp_pack_1hour\")",
            "+        with open(tmp_pack_path, \"wb\") as f:",
            "+            f.write(b\"1 hour old data\")",
            "+",
            "+        # Make it 1 hour old",
            "+        old_time = time.time() - 3600",
            "+        os.utime(tmp_pack_path, (old_time, old_time))",
            "+",
            "+        # Prune with 30-minute grace period should remove it",
            "+        porcelain.prune(self.repo.path, grace_period=1800)",
            "+",
            "+        # Verify the file was removed",
            "+        self.assertFalse(os.path.exists(tmp_pack_path))",
            "+",
            "+    def test_prune_dry_run(self):",
            "+        \"\"\"Test prune in dry-run mode.\"\"\"",
            "+        # Create an old temporary file",
            "+        objects_dir = os.path.join(self.repo.path, \".git\", \"objects\")",
            "+        tmp_pack_path = os.path.join(objects_dir, \"tmp_pack_dryrun\")",
            "+        with open(tmp_pack_path, \"wb\") as f:",
            "+            f.write(b\"old temporary data\")",
            "+        self.addCleanup(os.remove, tmp_pack_path)",
            "+",
            "+        # Make it old",
            "+        old_time = time.time() - (DEFAULT_TEMPFILE_GRACE_PERIOD + 3600)",
            "+        os.utime(tmp_pack_path, (old_time, old_time))",
            "+",
            "+        # Run prune in dry-run mode",
            "+        porcelain.prune(self.repo.path, dry_run=True)",
            "+",
            "+        # Verify the file was NOT removed (dry run)",
            "+        self.assertTrue(os.path.exists(tmp_pack_path))",
            "+",
            "+",
            "+class FilterBranchTests(PorcelainTestCase):",
            "+    def setUp(self):",
            "+        super().setUp()",
            "+        # Create initial commits with different authors",
            "+        from dulwich.objects import Commit, Tree",
            "+",
            "+        # Create actual tree and blob objects",
            "+        tree = Tree()",
            "+        self.repo.object_store.add_object(tree)",
            "+",
            "+        c1 = Commit()",
            "+        c1.tree = tree.id",
            "+        c1.parents = []",
            "+        c1.author = b\"Old Author <old@example.com>\"",
            "+        c1.author_time = 1000",
            "+        c1.author_timezone = 0",
            "+        c1.committer = b\"Old Committer <old@example.com>\"",
            "+        c1.commit_time = 1000",
            "+        c1.commit_timezone = 0",
            "+        c1.message = b\"Initial commit\"",
            "+        self.repo.object_store.add_object(c1)",
            "+",
            "+        c2 = Commit()",
            "+        c2.tree = tree.id",
            "+        c2.parents = [c1.id]",
            "+        c2.author = b\"Another Author <another@example.com>\"",
            "+        c2.author_time = 2000",
            "+        c2.author_timezone = 0",
            "+        c2.committer = b\"Another Committer <another@example.com>\"",
            "+        c2.commit_time = 2000",
            "+        c2.commit_timezone = 0",
            "+        c2.message = b\"Second commit\\n\\nWith body\"",
            "+        self.repo.object_store.add_object(c2)",
            "+",
            "+        c3 = Commit()",
            "+        c3.tree = tree.id",
            "+        c3.parents = [c2.id]",
            "+        c3.author = b\"Third Author <third@example.com>\"",
            "+        c3.author_time = 3000",
            "+        c3.author_timezone = 0",
            "+        c3.committer = b\"Third Committer <third@example.com>\"",
            "+        c3.commit_time = 3000",
            "+        c3.commit_timezone = 0",
            "+        c3.message = b\"Third commit\"",
            "+        self.repo.object_store.add_object(c3)",
            "+",
            "+        self.repo.refs[b\"refs/heads/master\"] = c3.id",
            "+        self.repo.refs.set_symbolic_ref(b\"HEAD\", b\"refs/heads/master\")",
            "+",
            "+        # Store IDs for test assertions",
            "+        self.c1_id = c1.id",
            "+        self.c2_id = c2.id",
            "+        self.c3_id = c3.id",
            "+",
            "+    def test_filter_branch_author(self):",
            "+        \"\"\"Test filtering branch with author changes.\"\"\"",
            "+",
            "+        def filter_author(author):",
            "+            # Change all authors to \"New Author\"",
            "+            return b\"New Author <new@example.com>\"",
            "+",
            "+        result = porcelain.filter_branch(",
            "+            self.repo_path, \"master\", filter_author=filter_author",
            "+        )",
            "+",
            "+        # Check that we have mappings for all commits",
            "+        self.assertEqual(len(result), 3)",
            "+",
            "+        # Verify the branch ref was updated",
            "+        new_head = self.repo.refs[b\"refs/heads/master\"]",
            "+        self.assertNotEqual(new_head, self.c3_id)",
            "+",
            "+        # Verify the original ref was saved",
            "+        original_ref = self.repo.refs[b\"refs/original/refs/heads/master\"]",
            "+        self.assertEqual(original_ref, self.c3_id)",
            "+",
            "+        # Check that authors were updated",
            "+        new_commit = self.repo[new_head]",
            "+        self.assertEqual(new_commit.author, b\"New Author <new@example.com>\")",
            "+",
            "+        # Check parent chain",
            "+        parent = self.repo[new_commit.parents[0]]",
            "+        self.assertEqual(parent.author, b\"New Author <new@example.com>\")",
            "+",
            "+    def test_filter_branch_message(self):",
            "+        \"\"\"Test filtering branch with message changes.\"\"\"",
            "+",
            "+        def filter_message(message):",
            "+            # Add prefix to all messages",
            "+            return b\"[FILTERED] \" + message",
            "+",
            "+        porcelain.filter_branch(self.repo_path, \"master\", filter_message=filter_message)",
            "+",
            "+        # Verify messages were updated",
            "+        new_head = self.repo.refs[b\"refs/heads/master\"]",
            "+        new_commit = self.repo[new_head]",
            "+        self.assertTrue(new_commit.message.startswith(b\"[FILTERED] \"))",
            "+",
            "+    def test_filter_branch_custom_filter(self):",
            "+        \"\"\"Test filtering branch with custom filter function.\"\"\"",
            "+",
            "+        def custom_filter(commit):",
            "+            # Change both author and message",
            "+            return {",
            "+                \"author\": b\"Custom Author <custom@example.com>\",",
            "+                \"message\": b\"Custom: \" + commit.message,",
            "+            }",
            "+",
            "+        porcelain.filter_branch(self.repo_path, \"master\", filter_fn=custom_filter)",
            "+",
            "+        # Verify custom filter was applied",
            "+        new_head = self.repo.refs[b\"refs/heads/master\"]",
            "+        new_commit = self.repo[new_head]",
            "+        self.assertEqual(new_commit.author, b\"Custom Author <custom@example.com>\")",
            "+        self.assertTrue(new_commit.message.startswith(b\"Custom: \"))",
            "+",
            "+    def test_filter_branch_no_changes(self):",
            "+        \"\"\"Test filtering branch with no changes.\"\"\"",
            "+        result = porcelain.filter_branch(self.repo_path, \"master\")",
            "+",
            "+        # All commits should map to themselves",
            "+        for old_sha, new_sha in result.items():",
            "+            self.assertEqual(old_sha, new_sha)",
            "+",
            "+        # HEAD should be unchanged",
            "+        self.assertEqual(self.repo.refs[b\"refs/heads/master\"], self.c3_id)",
            "+",
            "+    def test_filter_branch_force(self):",
            "+        \"\"\"Test force filtering a previously filtered branch.\"\"\"",
            "+        # First filter",
            "+        porcelain.filter_branch(",
            "+            self.repo_path, \"master\", filter_message=lambda m: b\"First: \" + m",
            "+        )",
            "+",
            "+        # Try again without force - should fail",
            "+        with self.assertRaises(porcelain.Error):",
            "+            porcelain.filter_branch(",
            "+                self.repo_path, \"master\", filter_message=lambda m: b\"Second: \" + m",
            "+            )",
            "+",
            "+        # Try again with force - should succeed",
            "+        porcelain.filter_branch(",
            "+            self.repo_path,",
            "+            \"master\",",
            "+            filter_message=lambda m: b\"Second: \" + m,",
            "+            force=True,",
            "+        )",
            "+",
            "+        # Verify second filter was applied",
            "+        new_head = self.repo.refs[b\"refs/heads/master\"]",
            "+        new_commit = self.repo[new_head]",
            "+        self.assertTrue(new_commit.message.startswith(b\"Second: First: \"))",
            "+",
            "+",
            "+class StashTests(PorcelainTestCase):",
            "+    def setUp(self) -> None:",
            "+        super().setUp()",
            "+        # Create initial commit",
            "+        with open(os.path.join(self.repo.path, \"initial.txt\"), \"wb\") as f:",
            "+            f.write(b\"initial content\")",
            "+        porcelain.add(repo=self.repo.path, paths=[\"initial.txt\"])",
            "+        porcelain.commit(",
            "+            repo=self.repo.path,",
            "+            message=b\"Initial commit\",",
            "+            author=b\"Test Author <test@example.com>\",",
            "+            committer=b\"Test Committer <test@example.com>\",",
            "+        )",
            "+",
            "+    def test_stash_push_and_pop(self) -> None:",
            "+        # Create a new file and stage it",
            "+        new_file = os.path.join(self.repo.path, \"new.txt\")",
            "+        with open(new_file, \"wb\") as f:",
            "+            f.write(b\"new file content\")",
            "+        porcelain.add(repo=self.repo.path, paths=[\"new.txt\"])",
            "+",
            "+        # Modify existing file",
            "+        with open(os.path.join(self.repo.path, \"initial.txt\"), \"wb\") as f:",
            "+            f.write(b\"modified content\")",
            "+",
            "+        # Push to stash",
            "+        porcelain.stash_push(self.repo.path)",
            "+",
            "+        # Verify files are reset",
            "+        self.assertFalse(os.path.exists(new_file))",
            "+        with open(os.path.join(self.repo.path, \"initial.txt\"), \"rb\") as f:",
            "+            self.assertEqual(b\"initial content\", f.read())",
            "+",
            "+        # Pop the stash",
            "+        porcelain.stash_pop(self.repo.path)",
            "+",
            "+        # Verify files are restored",
            "+        self.assertTrue(os.path.exists(new_file))",
            "+        with open(new_file, \"rb\") as f:",
            "+            self.assertEqual(b\"new file content\", f.read())",
            "+        with open(os.path.join(self.repo.path, \"initial.txt\"), \"rb\") as f:",
            "+            self.assertEqual(b\"modified content\", f.read())",
            "+",
            "+        # Verify new file is in the index",
            "+        from dulwich.index import Index",
            "+",
            "+        index = Index(os.path.join(self.repo.path, \".git\", \"index\"))",
            "+        self.assertIn(b\"new.txt\", index)",
            "+",
            "+    def test_stash_list(self) -> None:",
            "+        # Initially no stashes",
            "+        stashes = list(porcelain.stash_list(self.repo.path))",
            "+        self.assertEqual(0, len(stashes))",
            "+",
            "+        # Create a file and stash it",
            "+        test_file = os.path.join(self.repo.path, \"test.txt\")",
            "+        with open(test_file, \"wb\") as f:",
            "+            f.write(b\"test content\")",
            "+        porcelain.add(repo=self.repo.path, paths=[\"test.txt\"])",
            "+",
            "+        # Push first stash",
            "+        porcelain.stash_push(self.repo.path)",
            "+",
            "+        # Create another file and stash it",
            "+        test_file2 = os.path.join(self.repo.path, \"test2.txt\")",
            "+        with open(test_file2, \"wb\") as f:",
            "+            f.write(b\"test content 2\")",
            "+        porcelain.add(repo=self.repo.path, paths=[\"test2.txt\"])",
            "+",
            "+        # Push second stash",
            "+        porcelain.stash_push(self.repo.path)",
            "+",
            "+        # Check stash list",
            "+        stashes = list(porcelain.stash_list(self.repo.path))",
            "+        self.assertEqual(2, len(stashes))",
            "+",
            "+        # Stashes are returned in order (most recent first)",
            "+        self.assertEqual(0, stashes[0][0])",
            "+        self.assertEqual(1, stashes[1][0])",
            "+",
            "+    def test_stash_drop(self) -> None:",
            "+        # Create and stash some changes",
            "+        test_file = os.path.join(self.repo.path, \"test.txt\")",
            "+        with open(test_file, \"wb\") as f:",
            "+            f.write(b\"test content\")",
            "+        porcelain.add(repo=self.repo.path, paths=[\"test.txt\"])",
            "+        porcelain.stash_push(self.repo.path)",
            "+",
            "+        # Create another stash",
            "+        test_file2 = os.path.join(self.repo.path, \"test2.txt\")",
            "+        with open(test_file2, \"wb\") as f:",
            "+            f.write(b\"test content 2\")",
            "+        porcelain.add(repo=self.repo.path, paths=[\"test2.txt\"])",
            "+        porcelain.stash_push(self.repo.path)",
            "+",
            "+        # Verify we have 2 stashes",
            "+        stashes = list(porcelain.stash_list(self.repo.path))",
            "+        self.assertEqual(2, len(stashes))",
            "+",
            "+        # Drop the first stash (index 0)",
            "+        porcelain.stash_drop(self.repo.path, 0)",
            "+",
            "+        # Verify we have 1 stash left",
            "+        stashes = list(porcelain.stash_list(self.repo.path))",
            "+        self.assertEqual(1, len(stashes))",
            "+",
            "+        # The remaining stash should be the one we created first",
            "+        # Pop it and verify it's the first file",
            "+        porcelain.stash_pop(self.repo.path)",
            "+        self.assertTrue(os.path.exists(test_file))",
            "+        self.assertFalse(os.path.exists(test_file2))",
            "+",
            "+    def test_stash_pop_empty(self) -> None:",
            "+        # Attempting to pop from empty stash should raise an error",
            "+        with self.assertRaises(IndexError):",
            "+            porcelain.stash_pop(self.repo.path)",
            "+",
            "+    def test_stash_with_untracked_files(self) -> None:",
            "+        # Create an untracked file",
            "+        untracked_file = os.path.join(self.repo.path, \"untracked.txt\")",
            "+        with open(untracked_file, \"wb\") as f:",
            "+            f.write(b\"untracked content\")",
            "+",
            "+        # Create a tracked change",
            "+        tracked_file = os.path.join(self.repo.path, \"tracked.txt\")",
            "+        with open(tracked_file, \"wb\") as f:",
            "+            f.write(b\"tracked content\")",
            "+        porcelain.add(repo=self.repo.path, paths=[\"tracked.txt\"])",
            "+",
            "+        # Stash (by default, untracked files are not included)",
            "+        porcelain.stash_push(self.repo.path)",
            "+",
            "+        # Untracked file should still exist",
            "+        self.assertTrue(os.path.exists(untracked_file))",
            "+        # Tracked file should be gone",
            "+        self.assertFalse(os.path.exists(tracked_file))",
            "+",
            "+        # Pop the stash",
            "+        porcelain.stash_pop(self.repo.path)",
            "+",
            "+        # Tracked file should be restored",
            "+        self.assertTrue(os.path.exists(tracked_file))",
            "+",
            "+",
            "+class BisectTests(PorcelainTestCase):",
            "+    \"\"\"Tests for bisect porcelain functions.\"\"\"",
            "+",
            "+    def test_bisect_start(self):",
            "+        \"\"\"Test starting a bisect session.\"\"\"",
            "+        # Create some commits",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store,",
            "+            [[1], [2, 1], [3, 2]],",
            "+            attrs={",
            "+                1: {\"message\": b\"initial\"},",
            "+                2: {\"message\": b\"second\"},",
            "+                3: {\"message\": b\"third\"},",
            "+            },",
            "+        )",
            "+        self.repo.refs[b\"refs/heads/master\"] = c3.id",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Start bisect",
            "+        porcelain.bisect_start(self.repo_path)",
            "+",
            "+        # Check that bisect state files exist",
            "+        self.assertTrue(",
            "+            os.path.exists(os.path.join(self.repo.controldir(), \"BISECT_START\"))",
            "+        )",
            "+        self.assertTrue(",
            "+            os.path.exists(os.path.join(self.repo.controldir(), \"BISECT_TERMS\"))",
            "+        )",
            "+        self.assertTrue(",
            "+            os.path.exists(os.path.join(self.repo.controldir(), \"BISECT_NAMES\"))",
            "+        )",
            "+        self.assertTrue(",
            "+            os.path.exists(os.path.join(self.repo.controldir(), \"BISECT_LOG\"))",
            "+        )",
            "+",
            "+    def test_bisect_workflow(self):",
            "+        \"\"\"Test a complete bisect workflow.\"\"\"",
            "+        # Create some commits",
            "+        c1, c2, c3, c4 = build_commit_graph(",
            "+            self.repo.object_store,",
            "+            [[1], [2, 1], [3, 2], [4, 3]],",
            "+            attrs={",
            "+                1: {\"message\": b\"good commit 1\"},",
            "+                2: {\"message\": b\"good commit 2\"},",
            "+                3: {\"message\": b\"bad commit\"},",
            "+                4: {\"message\": b\"bad commit 2\"},",
            "+            },",
            "+        )",
            "+        self.repo.refs[b\"refs/heads/master\"] = c4.id",
            "+        self.repo.refs[b\"HEAD\"] = c4.id",
            "+",
            "+        # Start bisect with bad and good",
            "+        next_sha = porcelain.bisect_start(self.repo_path, bad=c4.id, good=c1.id)",
            "+",
            "+        # Should return the middle commit",
            "+        self.assertIsNotNone(next_sha)",
            "+        self.assertIn(next_sha, [c2.id, c3.id])",
            "+",
            "+        # Mark the middle commit as good or bad",
            "+        if next_sha == c2.id:",
            "+            # c2 is good, next should be c3",
            "+            next_sha = porcelain.bisect_good(self.repo_path)",
            "+            self.assertEqual(next_sha, c3.id)",
            "+            # Mark c3 as bad - bisect complete",
            "+            next_sha = porcelain.bisect_bad(self.repo_path)",
            "+            self.assertIsNone(next_sha)",
            "+        else:",
            "+            # c3 is bad, next should be c2",
            "+            next_sha = porcelain.bisect_bad(self.repo_path)",
            "+            self.assertEqual(next_sha, c2.id)",
            "+            # Mark c2 as good - bisect complete",
            "+            next_sha = porcelain.bisect_good(self.repo_path)",
            "+            self.assertIsNone(next_sha)",
            "+",
            "+    def test_bisect_log(self):",
            "+        \"\"\"Test getting bisect log.\"\"\"",
            "+        # Create some commits",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store,",
            "+            [[1], [2, 1], [3, 2]],",
            "+            attrs={",
            "+                1: {\"message\": b\"initial\"},",
            "+                2: {\"message\": b\"second\"},",
            "+                3: {\"message\": b\"third\"},",
            "+            },",
            "+        )",
            "+        self.repo.refs[b\"refs/heads/master\"] = c3.id",
            "+        self.repo.refs[b\"HEAD\"] = c3.id",
            "+",
            "+        # Start bisect and mark commits",
            "+        porcelain.bisect_start(self.repo_path)",
            "+        porcelain.bisect_bad(self.repo_path, c3.id)",
            "+        porcelain.bisect_good(self.repo_path, c1.id)",
            "+",
            "+        # Get log",
            "+        log = porcelain.bisect_log(self.repo_path)",
            "+",
            "+        self.assertIn(\"git bisect start\", log)",
            "+        self.assertIn(\"git bisect bad\", log)",
            "+        self.assertIn(\"git bisect good\", log)",
            "+",
            "+    def test_bisect_reset(self):",
            "+        \"\"\"Test resetting bisect state.\"\"\"",
            "+        # Create some commits",
            "+        c1, c2, c3 = build_commit_graph(",
            "+            self.repo.object_store,",
            "+            [[1], [2, 1], [3, 2]],",
            "+            attrs={",
            "+                1: {\"message\": b\"initial\"},",
            "+                2: {\"message\": b\"second\"},",
            "+                3: {\"message\": b\"third\"},",
            "+            },",
            "+        )",
            "+        self.repo.refs[b\"refs/heads/master\"] = c3.id",
            "+        self.repo.refs.set_symbolic_ref(b\"HEAD\", b\"refs/heads/master\")",
            "+",
            "+        # Start bisect",
            "+        porcelain.bisect_start(self.repo_path)",
            "+        porcelain.bisect_bad(self.repo_path)",
            "+        porcelain.bisect_good(self.repo_path, c1.id)",
            "+",
            "+        # Reset",
            "+        porcelain.bisect_reset(self.repo_path)",
            "+",
            "+        # Check that bisect state files are removed",
            "+        self.assertFalse(",
            "+            os.path.exists(os.path.join(self.repo.controldir(), \"BISECT_START\"))",
            "+        )",
            "+        self.assertFalse(",
            "+            os.path.exists(os.path.join(self.repo.controldir(), \"refs\", \"bisect\"))",
            "+        )",
            "+",
            "+        # HEAD should be back to being a symbolic ref to master",
            "+        head_target, _ = self.repo.refs.follow(b\"HEAD\")",
            "+        self.assertEqual(head_target[-1], b\"refs/heads/master\")",
            "+",
            "+    def test_bisect_skip(self):",
            "+        \"\"\"Test skipping commits during bisect.\"\"\"",
            "+        # Create some commits",
            "+        c1, c2, c3, c4, c5 = build_commit_graph(",
            "+            self.repo.object_store,",
            "+            [[1], [2, 1], [3, 2], [4, 3], [5, 4]],",
            "+            attrs={",
            "+                1: {\"message\": b\"good\"},",
            "+                2: {\"message\": b\"skip this\"},",
            "+                3: {\"message\": b\"bad\"},",
            "+                4: {\"message\": b\"bad\"},",
            "+                5: {\"message\": b\"bad\"},",
            "+            },",
            "+        )",
            "+        self.repo.refs[b\"refs/heads/master\"] = c5.id",
            "+        self.repo.refs[b\"HEAD\"] = c5.id",
            "+",
            "+        # Start bisect",
            "+        porcelain.bisect_start(self.repo_path, bad=c5.id, good=c1.id)",
            "+",
            "+        # Skip c2 if it's selected",
            "+        next_sha = porcelain.bisect_skip(self.repo_path, [c2.id])",
            "+        self.assertIsNotNone(next_sha)",
            "+        self.assertNotEqual(next_sha, c2.id)"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_refs.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_refs.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_refs.py",
            "@@ -32,14 +32,15 @@",
            " from dulwich.objects import ZERO_SHA",
            " from dulwich.refs import (",
            "     DictRefsContainer,",
            "     InfoRefsContainer,",
            "     SymrefLoop,",
            "     _split_ref_line,",
            "     check_ref_format,",
            "+    parse_remote_ref,",
            "     parse_symref_value,",
            "     read_packed_refs,",
            "     read_packed_refs_with_peeled,",
            "     split_peeled_refs,",
            "     strip_peeled_refs,",
            "     write_packed_refs,",
            " )",
            "@@ -377,14 +378,51 @@",
            "         # some way of injecting invalid refs.",
            "         self._refs._refs[b\"refs/stash\"] = b\"00\" * 20",
            "         expected_refs = dict(_TEST_REFS)",
            "         del expected_refs[b\"refs/heads/loop\"]",
            "         expected_refs[b\"refs/stash\"] = b\"00\" * 20",
            "         self.assertEqual(expected_refs, self._refs.as_dict())",
            " ",
            "+    def test_set_if_equals_with_symbolic_ref(self) -> None:",
            "+        # Test that set_if_equals only updates the requested ref,",
            "+        # not all refs in a symbolic reference chain",
            "+",
            "+        # The bug in the original implementation was that when follow()",
            "+        # was called on a ref, it would return all refs in the chain,",
            "+        # and set_if_equals would update ALL of them instead of just the",
            "+        # requested ref.",
            "+",
            "+        # Set up refs",
            "+        master_sha = b\"1\" * 40",
            "+        feature_sha = b\"2\" * 40",
            "+        new_sha = b\"3\" * 40",
            "+",
            "+        self._refs[b\"refs/heads/master\"] = master_sha",
            "+        self._refs[b\"refs/heads/feature\"] = feature_sha",
            "+        # Create a second symbolic ref pointing to feature",
            "+        self._refs.set_symbolic_ref(b\"refs/heads/other\", b\"refs/heads/feature\")",
            "+",
            "+        # Update refs/heads/other through set_if_equals",
            "+        # With the bug, this would update BOTH refs/heads/other AND refs/heads/feature",
            "+        # Without the bug, only refs/heads/other should be updated",
            "+        # Note: old_ref needs to be the actual stored value (the symref)",
            "+        self.assertTrue(",
            "+            self._refs.set_if_equals(",
            "+                b\"refs/heads/other\", b\"ref: refs/heads/feature\", new_sha",
            "+            )",
            "+        )",
            "+",
            "+        # refs/heads/other should now directly point to new_sha",
            "+        self.assertEqual(self._refs.read_ref(b\"refs/heads/other\"), new_sha)",
            "+",
            "+        # refs/heads/feature should remain unchanged",
            "+        # With the bug, refs/heads/feature would also be incorrectly updated to new_sha",
            "+        self.assertEqual(self._refs[b\"refs/heads/feature\"], feature_sha)",
            "+        self.assertEqual(self._refs[b\"refs/heads/master\"], master_sha)",
            "+",
            " ",
            " class DiskRefsContainerTests(RefsContainerTests, TestCase):",
            "     def setUp(self) -> None:",
            "         TestCase.setUp(self)",
            "         self._repo = open_repo(\"refs.git\")",
            "         self.addCleanup(tear_down_repo, self._repo)",
            "         self._refs = self._repo.refs",
            "@@ -753,14 +791,57 @@",
            "     b\"42d06bd4b77fed026b154d16493e5deab78f02ec\\trefs/heads/master\\n\"",
            "     b\"42d06bd4b77fed026b154d16493e5deab78f02ec\\trefs/heads/packed\\n\"",
            "     b\"df6800012397fb85c56e7418dd4eb9405dee075c\\trefs/tags/refs-0.1\\n\"",
            "     b\"3ec9c43c84ff242e3ef4a9fc5bc111fd780a76a8\\trefs/tags/refs-0.2\\n\"",
            " )",
            " ",
            " ",
            "+class DiskRefsContainerPathlibTests(TestCase):",
            "+    def test_pathlib_init(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        from dulwich.refs import DiskRefsContainer",
            "+",
            "+        # Create a temporary directory",
            "+        temp_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(os.rmdir, temp_dir)",
            "+",
            "+        # Test with pathlib.Path",
            "+        path_obj = Path(temp_dir)",
            "+        refs = DiskRefsContainer(path_obj)",
            "+        self.assertEqual(refs.path, temp_dir.encode())",
            "+",
            "+        # Test refpath with pathlib initialized container",
            "+        ref_path = refs.refpath(b\"HEAD\")",
            "+        self.assertTrue(isinstance(ref_path, bytes))",
            "+        self.assertEqual(ref_path, os.path.join(temp_dir.encode(), b\"HEAD\"))",
            "+",
            "+    def test_pathlib_worktree_path(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        from dulwich.refs import DiskRefsContainer",
            "+",
            "+        # Create temporary directories",
            "+        temp_dir = tempfile.mkdtemp()",
            "+        worktree_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(os.rmdir, temp_dir)",
            "+        self.addCleanup(os.rmdir, worktree_dir)",
            "+",
            "+        # Test with pathlib.Path for both paths",
            "+        path_obj = Path(temp_dir)",
            "+        worktree_obj = Path(worktree_dir)",
            "+        refs = DiskRefsContainer(path_obj, worktree_path=worktree_obj)",
            "+        self.assertEqual(refs.path, temp_dir.encode())",
            "+        self.assertEqual(refs.worktree_path, worktree_dir.encode())",
            "+",
            "+        # Test refpath returns worktree path for HEAD",
            "+        ref_path = refs.refpath(b\"HEAD\")",
            "+        self.assertEqual(ref_path, os.path.join(worktree_dir.encode(), b\"HEAD\"))",
            "+",
            "+",
            " class InfoRefsContainerTests(TestCase):",
            "     def test_invalid_refname(self) -> None:",
            "         text = _TEST_REFS_SERIALIZED + b\"00\" * 20 + b\"\\trefs/stash\\n\"",
            "         refs = InfoRefsContainer(BytesIO(text))",
            "         expected_refs = dict(_TEST_REFS)",
            "         del expected_refs[b\"HEAD\"]",
            "         expected_refs[b\"refs/stash\"] = b\"00\" * 20",
            "@@ -810,14 +891,44 @@",
            "     def test_valid(self) -> None:",
            "         self.assertEqual(b\"refs/heads/foo\", parse_symref_value(b\"ref: refs/heads/foo\"))",
            " ",
            "     def test_invalid(self) -> None:",
            "         self.assertRaises(ValueError, parse_symref_value, b\"foobar\")",
            " ",
            " ",
            "+class ParseRemoteRefTests(TestCase):",
            "+    def test_valid(self) -> None:",
            "+        # Test simple case",
            "+        remote, branch = parse_remote_ref(b\"refs/remotes/origin/main\")",
            "+        self.assertEqual(b\"origin\", remote)",
            "+        self.assertEqual(b\"main\", branch)",
            "+",
            "+        # Test with branch containing slashes",
            "+        remote, branch = parse_remote_ref(b\"refs/remotes/upstream/feature/new-ui\")",
            "+        self.assertEqual(b\"upstream\", remote)",
            "+        self.assertEqual(b\"feature/new-ui\", branch)",
            "+",
            "+    def test_invalid_not_remote_ref(self) -> None:",
            "+        # Not a remote ref",
            "+        with self.assertRaises(ValueError) as cm:",
            "+            parse_remote_ref(b\"refs/heads/main\")",
            "+        self.assertIn(\"Not a remote ref\", str(cm.exception))",
            "+",
            "+    def test_invalid_format(self) -> None:",
            "+        # Missing branch name",
            "+        with self.assertRaises(ValueError) as cm:",
            "+            parse_remote_ref(b\"refs/remotes/origin\")",
            "+        self.assertIn(\"Invalid remote ref format\", str(cm.exception))",
            "+",
            "+        # Just the prefix",
            "+        with self.assertRaises(ValueError) as cm:",
            "+            parse_remote_ref(b\"refs/remotes/\")",
            "+        self.assertIn(\"Invalid remote ref format\", str(cm.exception))",
            "+",
            "+",
            " class StripPeeledRefsTests(TestCase):",
            "     all_refs: ClassVar[dict[bytes, bytes]] = {",
            "         b\"refs/heads/master\": b\"8843d7f92416211de9ebb963ff4ce28125932878\",",
            "         b\"refs/heads/testing\": b\"186a005b134d8639a58b6731c7c1ea821a6eedba\",",
            "         b\"refs/tags/1.0.0\": b\"a93db4b0360cc635a2b93675010bac8d101f73f0\",",
            "         b\"refs/tags/1.0.0^{}\": b\"a93db4b0360cc635a2b93675010bac8d101f73f0\",",
            "         b\"refs/tags/2.0.0\": b\"0749936d0956c661ac8f8d3483774509c165f89e\","
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_repository.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_repository.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_repository.py",
            "@@ -118,28 +118,125 @@",
            "         tmp_dir = tempfile.mkdtemp()",
            "         target_dir = os.path.join(tmp_dir, \"target\")",
            "         self.addCleanup(shutil.rmtree, tmp_dir)",
            "         repo = Repo.init_bare(target_dir, mkdir=True)",
            "         self.assertEqual(target_dir, repo._controldir)",
            "         self._check_repo_contents(repo, True)",
            " ",
            "+    def test_create_disk_bare_pathlib(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        tmp_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+        repo_path = Path(tmp_dir)",
            "+        repo = Repo.init_bare(repo_path)",
            "+        self.assertEqual(tmp_dir, repo._controldir)",
            "+        self._check_repo_contents(repo, True)",
            "+        # Test that refpath works with pathlib",
            "+        ref_path = repo.refs.refpath(b\"refs/heads/master\")",
            "+        self.assertTrue(isinstance(ref_path, bytes))",
            "+        expected_path = os.path.join(tmp_dir.encode(), b\"refs\", b\"heads\", b\"master\")",
            "+        self.assertEqual(ref_path, expected_path)",
            "+",
            "+    def test_create_disk_non_bare_pathlib(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        tmp_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+        repo_path = Path(tmp_dir)",
            "+        repo = Repo.init(repo_path)",
            "+        self.assertEqual(os.path.join(tmp_dir, \".git\"), repo._controldir)",
            "+        self._check_repo_contents(repo, False)",
            "+",
            "+    def test_open_repo_pathlib(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        tmp_dir = tempfile.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+        # First create a repo",
            "+        repo = Repo.init_bare(tmp_dir)",
            "+        repo.close()",
            "+        # Now open it with pathlib",
            "+        repo_path = Path(tmp_dir)",
            "+        repo2 = Repo(repo_path)",
            "+        self.assertEqual(tmp_dir, repo2._controldir)",
            "+        self.assertTrue(repo2.bare)",
            "+        repo2.close()",
            "+",
            "+    def test_create_disk_bare_mkdir_pathlib(self) -> None:",
            "+        from pathlib import Path",
            "+",
            "+        tmp_dir = tempfile.mkdtemp()",
            "+        target_path = Path(tmp_dir) / \"target\"",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+        repo = Repo.init_bare(target_path, mkdir=True)",
            "+        self.assertEqual(str(target_path), repo._controldir)",
            "+        self._check_repo_contents(repo, True)",
            "+",
            " ",
            " class MemoryRepoTests(TestCase):",
            "     def test_set_description(self) -> None:",
            "         r = MemoryRepo.init_bare([], {})",
            "         description = b\"Some description\"",
            "         r.set_description(description)",
            "         self.assertEqual(description, r.get_description())",
            " ",
            "     def test_pull_into(self) -> None:",
            "         r = MemoryRepo.init_bare([], {})",
            "         repo = open_repo(\"a.git\")",
            "         self.addCleanup(tear_down_repo, repo)",
            "         repo.fetch(r)",
            " ",
            "+    def test_fetch_from_git_cloned_repo(self) -> None:",
            "+        \"\"\"Test fetching from a git-cloned repo into MemoryRepo (issue #1179).\"\"\"",
            "+        import tempfile",
            "+",
            "+        from dulwich.client import LocalGitClient",
            "+",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create initial repo using dulwich",
            "+            initial_path = os.path.join(tmpdir, \"initial\")",
            "+            initial_repo = Repo.init(initial_path, mkdir=True)",
            "+",
            "+            # Create some content",
            "+            test_file = os.path.join(initial_path, \"test.txt\")",
            "+            with open(test_file, \"w\") as f:",
            "+                f.write(\"test content\\n\")",
            "+",
            "+            # Stage and commit using dulwich",
            "+            initial_repo.stage([\"test.txt\"])",
            "+            initial_repo.do_commit(",
            "+                b\"Initial commit\\n\",",
            "+                committer=b\"Test Committer <test@example.com>\",",
            "+                author=b\"Test Author <test@example.com>\",",
            "+            )",
            "+",
            "+            # Clone using dulwich",
            "+            cloned_path = os.path.join(tmpdir, \"cloned\")",
            "+            cloned_repo = initial_repo.clone(cloned_path, mkdir=True)",
            "+",
            "+            initial_repo.close()",
            "+            cloned_repo.close()",
            "+",
            "+            # Fetch from the cloned repo into MemoryRepo",
            "+            memory_repo = MemoryRepo()",
            "+            client = LocalGitClient()",
            "+",
            "+            # This should not raise AssertionError",
            "+            result = client.fetch(cloned_path, memory_repo)",
            "+",
            "+            # Verify the fetch worked",
            "+            self.assertIn(b\"HEAD\", result.refs)",
            "+            self.assertIn(b\"refs/heads/master\", result.refs)",
            "+",
            "+            # Verify we can read the fetched objects",
            "+            head_sha = result.refs[b\"HEAD\"]",
            "+            commit = memory_repo[head_sha]",
            "+            self.assertEqual(commit.message, b\"Initial commit\\n\")",
            "+",
            " ",
            " class RepositoryRootTests(TestCase):",
            "     def mkdtemp(self):",
            "         return tempfile.mkdtemp()",
            " ",
            "     def open_repo(self, name):",
            "         temp_dir = self.mkdtemp()",
            "@@ -240,14 +337,43 @@",
            " ",
            "     def test_set_description(self) -> None:",
            "         r = self.open_repo(\"a.git\")",
            "         description = b\"Some description\"",
            "         r.set_description(description)",
            "         self.assertEqual(description, r.get_description())",
            " ",
            "+    def test_get_gitattributes(self) -> None:",
            "+        # Test when no .gitattributes file exists",
            "+        r = self.open_repo(\"a.git\")",
            "+        attrs = r.get_gitattributes()",
            "+        from dulwich.attrs import GitAttributes",
            "+",
            "+        self.assertIsInstance(attrs, GitAttributes)",
            "+        self.assertEqual(len(attrs), 0)",
            "+",
            "+        # Create .git/info/attributes file (which is read by get_gitattributes)",
            "+        info_dir = os.path.join(r.controldir(), \"info\")",
            "+        if not os.path.exists(info_dir):",
            "+            os.makedirs(info_dir)",
            "+        attrs_path = os.path.join(info_dir, \"attributes\")",
            "+        with open(attrs_path, \"wb\") as f:",
            "+            f.write(b\"*.txt text\\n\")",
            "+            f.write(b\"*.jpg -text binary\\n\")",
            "+",
            "+        # Test with attributes file",
            "+        attrs = r.get_gitattributes()",
            "+        self.assertEqual(len(attrs), 2)",
            "+",
            "+        # Test matching",
            "+        txt_attrs = attrs.match_path(b\"file.txt\")",
            "+        self.assertEqual(txt_attrs, {b\"text\": True})",
            "+",
            "+        jpg_attrs = attrs.match_path(b\"image.jpg\")",
            "+        self.assertEqual(jpg_attrs, {b\"text\": False, b\"binary\": True})",
            "+",
            "     def test_contains_missing(self) -> None:",
            "         r = self.open_repo(\"a.git\")",
            "         self.assertNotIn(b\"bar\", r)",
            " ",
            "     def test_get_peeled(self) -> None:",
            "         # unpacked ref",
            "         r = self.open_repo(\"a.git\")",
            "@@ -338,14 +464,50 @@",
            "         repo_dir = os.path.join(tmp_dir, repo_name)",
            " ",
            "         t = Repo.init(repo_dir, mkdir=True)",
            "         self.addCleanup(t.close)",
            "         self.assertEqual(os.listdir(repo_dir), [\".git\"])",
            "         self.assertFilesystemHidden(os.path.join(repo_dir, \".git\"))",
            " ",
            "+    def test_init_format(self) -> None:",
            "+        tmp_dir = self.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+",
            "+        # Test format 0",
            "+        t0 = Repo.init(tmp_dir + \"0\", mkdir=True, format=0)",
            "+        self.addCleanup(t0.close)",
            "+        self.assertEqual(t0.get_config().get(\"core\", \"repositoryformatversion\"), b\"0\")",
            "+",
            "+        # Test format 1",
            "+        t1 = Repo.init(tmp_dir + \"1\", mkdir=True, format=1)",
            "+        self.addCleanup(t1.close)",
            "+        self.assertEqual(t1.get_config().get(\"core\", \"repositoryformatversion\"), b\"1\")",
            "+",
            "+        # Test default format",
            "+        td = Repo.init(tmp_dir + \"d\", mkdir=True)",
            "+        self.addCleanup(td.close)",
            "+        self.assertEqual(td.get_config().get(\"core\", \"repositoryformatversion\"), b\"0\")",
            "+",
            "+        # Test invalid format",
            "+        with self.assertRaises(ValueError):",
            "+            Repo.init(tmp_dir + \"bad\", mkdir=True, format=99)",
            "+",
            "+    def test_init_bare_format(self) -> None:",
            "+        tmp_dir = self.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+",
            "+        # Test format 1 for bare repo",
            "+        t = Repo.init_bare(tmp_dir + \"bare\", mkdir=True, format=1)",
            "+        self.addCleanup(t.close)",
            "+        self.assertEqual(t.get_config().get(\"core\", \"repositoryformatversion\"), b\"1\")",
            "+",
            "+        # Test invalid format for bare repo",
            "+        with self.assertRaises(ValueError):",
            "+            Repo.init_bare(tmp_dir + \"badbr\", mkdir=True, format=2)",
            "+",
            "     @skipIf(sys.platform == \"win32\", \"fails on Windows\")",
            "     def test_fetch(self) -> None:",
            "         r = self.open_repo(\"a.git\")",
            "         tmp_dir = self.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, tmp_dir)",
            "         t = Repo.init(tmp_dir)",
            "         self.addCleanup(t.close)",
            "@@ -469,14 +631,43 @@",
            " ",
            "         t = o.clone(os.path.join(tmp_dir, \"t\"), symlinks=False)",
            "         with open(os.path.join(tmp_dir, \"t\", \"bar\")) as f:",
            "             self.assertEqual(\"foo\", f.read())",
            " ",
            "         t.close()",
            " ",
            "+    def test_reset_index_protect_hfs(self) -> None:",
            "+        tmp_dir = self.mkdtemp()",
            "+        self.addCleanup(shutil.rmtree, tmp_dir)",
            "+",
            "+        repo = Repo.init(tmp_dir)",
            "+        self.addCleanup(repo.close)",
            "+        config = repo.get_config()",
            "+",
            "+        # Test with protectHFS enabled",
            "+        config.set(b\"core\", b\"core.protectHFS\", b\"true\")",
            "+        config.write_to_path()",
            "+",
            "+        # Create a file with HFS+ Unicode attack vector",
            "+        # This uses a zero-width non-joiner to create \".g\\u200cit\"",
            "+        attack_name = b\".g\\xe2\\x80\\x8cit\"",
            "+        attack_path = os.path.join(tmp_dir, attack_name.decode(\"utf-8\"))",
            "+        os.mkdir(attack_path)",
            "+",
            "+        # Try to stage the malicious path - should be rejected",
            "+        with self.assertRaises(ValueError):",
            "+            repo.stage([attack_name])",
            "+",
            "+        # Test with protectHFS disabled",
            "+        config.set(b\"core\", b\"core.protectHFS\", b\"false\")",
            "+        config.write_to_path()",
            "+",
            "+        # Now it should work (though still dangerous!)",
            "+        # We're not actually staging it to avoid creating a dangerous repo",
            "+",
            "     def test_clone_bare(self) -> None:",
            "         r = self.open_repo(\"a.git\")",
            "         tmp_dir = self.mkdtemp()",
            "         self.addCleanup(shutil.rmtree, tmp_dir)",
            "         t = r.clone(tmp_dir, mkdir=False)",
            "         t.close()",
            " ",
            "@@ -836,16 +1027,15 @@",
            "             author=b\"Test Author <test@nodomain.com>\",",
            "             commit_timestamp=12345,",
            "             commit_timezone=0,",
            "             author_timestamp=12345,",
            "             author_timezone=0,",
            "         )",
            "         expected_warning = UserWarning(",
            "-            \"post-commit hook failed: Hook post-commit exited with \"",
            "-            \"non-zero status 1\",",
            "+            \"post-commit hook failed: Hook post-commit exited with non-zero status 1\",",
            "         )",
            "         for w in warnings_list:",
            "             if type(w) is type(expected_warning) and w.args == expected_warning.args:",
            "                 break",
            "         else:",
            "             raise AssertionError(",
            "                 f\"Expected warning {expected_warning!r} not in {warnings_list!r}\"",
            "@@ -1561,7 +1751,226 @@",
            "         )",
            "         self.assertRaises(",
            "             InvalidUserIdentity, check_user_identity, b\"Contains\\0null byte <>\"",
            "         )",
            "         self.assertRaises(",
            "             InvalidUserIdentity, check_user_identity, b\"Contains\\nnewline byte <>\"",
            "         )",
            "+",
            "+",
            "+class RepoConfigIncludeIfTests(TestCase):",
            "+    \"\"\"Test includeIf functionality in repository config loading.\"\"\"",
            "+",
            "+    def test_repo_config_includeif_gitdir(self) -> None:",
            "+        \"\"\"Test that includeIf gitdir conditions work when loading repo config.\"\"\"",
            "+        import tempfile",
            "+",
            "+        from dulwich.repo import Repo",
            "+",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create a repository",
            "+            repo_path = os.path.join(tmpdir, \"myrepo\")",
            "+            r = Repo.init(repo_path, mkdir=True)",
            "+            # Use realpath to resolve any symlinks (important on macOS)",
            "+            repo_path = os.path.realpath(repo_path)",
            "+",
            "+            # Create an included config file",
            "+            included_path = os.path.join(tmpdir, \"work.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = work@example.com\\n\")",
            "+",
            "+            # Add includeIf to the repo config",
            "+            config_path = os.path.join(repo_path, \".git\", \"config\")",
            "+            with open(config_path, \"ab\") as f:",
            "+                f.write(f'\\n[includeIf \"gitdir:{repo_path}/.git/\"]\\n'.encode())",
            "+                escaped_path = included_path.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"    path = {escaped_path}\\n\".encode())",
            "+",
            "+            # Close and reopen to reload config",
            "+            r.close()",
            "+            r = Repo(repo_path)",
            "+",
            "+            # Check if include was processed",
            "+            config = r.get_config()",
            "+            self.assertEqual(b\"work@example.com\", config.get((b\"user\",), b\"email\"))",
            "+            r.close()",
            "+",
            "+    def test_repo_config_includeif_gitdir_pattern(self) -> None:",
            "+        \"\"\"Test includeIf gitdir pattern matching in repository config.\"\"\"",
            "+        import tempfile",
            "+",
            "+        from dulwich.repo import Repo",
            "+",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create a repository under \"work\" directory",
            "+            work_dir = os.path.join(tmpdir, \"work\", \"project1\")",
            "+            os.makedirs(os.path.dirname(work_dir), exist_ok=True)",
            "+            r = Repo.init(work_dir, mkdir=True)",
            "+",
            "+            # Create an included config file",
            "+            included_path = os.path.join(tmpdir, \"work.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = work@company.com\\n\")",
            "+",
            "+            # Add includeIf with pattern to the repo config",
            "+            config_path = os.path.join(work_dir, \".git\", \"config\")",
            "+            with open(config_path, \"ab\") as f:",
            "+                # Use a pattern that will match paths containing /work/",
            "+                f.write(b'\\n[includeIf \"gitdir:**/work/**\"]\\n')",
            "+                escaped_path = included_path.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"    path = {escaped_path}\\n\".encode())",
            "+",
            "+            # Close and reopen to reload config",
            "+            r.close()",
            "+            r = Repo(work_dir)",
            "+",
            "+            # Check if include was processed",
            "+            config = r.get_config()",
            "+            self.assertEqual(b\"work@company.com\", config.get((b\"user\",), b\"email\"))",
            "+            r.close()",
            "+",
            "+    def test_repo_config_includeif_no_match(self) -> None:",
            "+        \"\"\"Test that includeIf doesn't include when condition doesn't match.\"\"\"",
            "+        import tempfile",
            "+",
            "+        from dulwich.repo import Repo",
            "+",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create a repository",
            "+            repo_path = os.path.join(tmpdir, \"personal\", \"project\")",
            "+            os.makedirs(os.path.dirname(repo_path), exist_ok=True)",
            "+            r = Repo.init(repo_path, mkdir=True)",
            "+",
            "+            # Create an included config file",
            "+            included_path = os.path.join(tmpdir, \"work.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    email = work@company.com\\n\")",
            "+",
            "+            # Add includeIf that won't match",
            "+            config_path = os.path.join(repo_path, \".git\", \"config\")",
            "+            with open(config_path, \"ab\") as f:",
            "+                f.write(b'\\n[includeIf \"gitdir:**/work/**\"]\\n')",
            "+                escaped_path = included_path.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"    path = {escaped_path}\\n\".encode())",
            "+",
            "+            # Close and reopen to reload config",
            "+            r.close()",
            "+            r = Repo(repo_path)",
            "+",
            "+            # Check that include was NOT processed",
            "+            config = r.get_config()",
            "+            with self.assertRaises(KeyError):",
            "+                config.get((b\"user\",), b\"email\")",
            "+            r.close()",
            "+",
            "+    def test_bare_repo_config_includeif(self) -> None:",
            "+        \"\"\"Test includeIf in bare repository.\"\"\"",
            "+        import tempfile",
            "+",
            "+        from dulwich.repo import Repo",
            "+",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create a bare repository",
            "+            repo_path = os.path.join(tmpdir, \"bare.git\")",
            "+            r = Repo.init_bare(repo_path, mkdir=True)",
            "+            # Use realpath to resolve any symlinks (important on macOS)",
            "+            repo_path = os.path.realpath(repo_path)",
            "+",
            "+            # Create an included config file",
            "+            included_path = os.path.join(tmpdir, \"server.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[receive]\\n    denyNonFastForwards = true\\n\")",
            "+",
            "+            # Add includeIf to the repo config",
            "+            config_path = os.path.join(repo_path, \"config\")",
            "+            with open(config_path, \"ab\") as f:",
            "+                f.write(f'\\n[includeIf \"gitdir:{repo_path}/\"]\\n'.encode())",
            "+                escaped_path = included_path.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"    path = {escaped_path}\\n\".encode())",
            "+",
            "+            # Close and reopen to reload config",
            "+            r.close()",
            "+            r = Repo(repo_path)",
            "+",
            "+            # Check if include was processed",
            "+            config = r.get_config()",
            "+            self.assertEqual(b\"true\", config.get((b\"receive\",), b\"denyNonFastForwards\"))",
            "+            r.close()",
            "+",
            "+    def test_repo_config_includeif_hasconfig(self) -> None:",
            "+        \"\"\"Test includeIf hasconfig conditions in repository config.\"\"\"",
            "+        import tempfile",
            "+",
            "+        from dulwich.repo import Repo",
            "+",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create a repository",
            "+            repo_path = os.path.join(tmpdir, \"myrepo\")",
            "+            r = Repo.init(repo_path, mkdir=True)",
            "+",
            "+            # Create an included config file",
            "+            included_path = os.path.join(tmpdir, \"work.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[user]\\n    name = WorkUser\\n\")",
            "+",
            "+            # Add a remote and includeIf hasconfig to the repo config",
            "+            config_path = os.path.join(repo_path, \".git\", \"config\")",
            "+            with open(config_path, \"ab\") as f:",
            "+                f.write(b'\\n[remote \"origin\"]\\n')",
            "+                f.write(b\"    url = ssh://org-work@github.com/company/project\\n\")",
            "+                f.write(",
            "+                    b'[includeIf \"hasconfig:remote.*.url:ssh://org-*@github.com/**\"]\\n'",
            "+                )",
            "+                escaped_path = included_path.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"    path = {escaped_path}\\n\".encode())",
            "+",
            "+            # Close and reopen to reload config",
            "+            r.close()",
            "+            r = Repo(repo_path)",
            "+",
            "+            # Check if include was processed",
            "+            config = r.get_config()",
            "+            self.assertEqual(b\"WorkUser\", config.get((b\"user\",), b\"name\"))",
            "+            r.close()",
            "+",
            "+    def test_repo_config_includeif_onbranch(self) -> None:",
            "+        \"\"\"Test includeIf onbranch conditions in repository config.\"\"\"",
            "+        import tempfile",
            "+",
            "+        from dulwich.repo import Repo",
            "+",
            "+        with tempfile.TemporaryDirectory() as tmpdir:",
            "+            # Create a repository",
            "+            repo_path = os.path.join(tmpdir, \"myrepo\")",
            "+            r = Repo.init(repo_path, mkdir=True)",
            "+",
            "+            # Create HEAD pointing to main branch",
            "+            refs_heads_dir = os.path.join(repo_path, \".git\", \"refs\", \"heads\")",
            "+            os.makedirs(refs_heads_dir, exist_ok=True)",
            "+            main_ref_path = os.path.join(refs_heads_dir, \"main\")",
            "+            with open(main_ref_path, \"wb\") as f:",
            "+                f.write(b\"0123456789012345678901234567890123456789\\n\")",
            "+",
            "+            head_path = os.path.join(repo_path, \".git\", \"HEAD\")",
            "+            with open(head_path, \"wb\") as f:",
            "+                f.write(b\"ref: refs/heads/main\\n\")",
            "+",
            "+            # Create an included config file",
            "+            included_path = os.path.join(tmpdir, \"main.config\")",
            "+            with open(included_path, \"wb\") as f:",
            "+                f.write(b\"[core]\\n    autocrlf = true\\n\")",
            "+",
            "+            # Add includeIf onbranch to the repo config",
            "+            config_path = os.path.join(repo_path, \".git\", \"config\")",
            "+            with open(config_path, \"ab\") as f:",
            "+                f.write(b'\\n[includeIf \"onbranch:main\"]\\n')",
            "+                escaped_path = included_path.replace(\"\\\\\", \"\\\\\\\\\")",
            "+                f.write(f\"    path = {escaped_path}\\n\".encode())",
            "+",
            "+            # Close and reopen to reload config",
            "+            r.close()",
            "+            r = Repo(repo_path)",
            "+",
            "+            # Check if include was processed",
            "+            config = r.get_config()",
            "+            self.assertEqual(b\"true\", config.get((b\"core\",), b\"autocrlf\"))",
            "+            r.close()"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_server.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_server.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_server.py",
            "@@ -29,29 +29,28 @@",
            " ",
            " from dulwich.errors import (",
            "     GitProtocolError,",
            "     HangupException,",
            "     NotGitRepository,",
            "     UnexpectedCommandError,",
            " )",
            "-from dulwich.object_store import MemoryObjectStore",
            "+from dulwich.object_store import MemoryObjectStore, find_shallow",
            " from dulwich.objects import Tree",
            " from dulwich.protocol import ZERO_SHA, format_capability_line",
            " from dulwich.repo import MemoryRepo, Repo",
            " from dulwich.server import (",
            "     Backend,",
            "     DictBackend,",
            "     FileSystemBackend,",
            "     MultiAckDetailedGraphWalkerImpl,",
            "     MultiAckGraphWalkerImpl,",
            "     PackHandler,",
            "     ReceivePackHandler,",
            "     SingleAckGraphWalkerImpl,",
            "     UploadPackHandler,",
            "-    _find_shallow,",
            "     _ProtocolGraphWalker,",
            "     _split_proto_line,",
            "     serve_command,",
            "     update_server_info,",
            " )",
            " from dulwich.tests.utils import make_commit, make_tag",
            " ",
            "@@ -267,72 +266,72 @@",
            " ",
            "     def assertSameElements(self, expected, actual) -> None:",
            "         self.assertEqual(set(expected), set(actual))",
            " ",
            "     def test_linear(self) -> None:",
            "         c1, c2, c3 = self.make_linear_commits(3)",
            " ",
            "-        self.assertEqual(({c3.id}, set()), _find_shallow(self._store, [c3.id], 1))",
            "+        self.assertEqual(({c3.id}, set()), find_shallow(self._store, [c3.id], 1))",
            "         self.assertEqual(",
            "             ({c2.id}, {c3.id}),",
            "-            _find_shallow(self._store, [c3.id], 2),",
            "+            find_shallow(self._store, [c3.id], 2),",
            "         )",
            "         self.assertEqual(",
            "             ({c1.id}, {c2.id, c3.id}),",
            "-            _find_shallow(self._store, [c3.id], 3),",
            "+            find_shallow(self._store, [c3.id], 3),",
            "         )",
            "         self.assertEqual(",
            "             (set(), {c1.id, c2.id, c3.id}),",
            "-            _find_shallow(self._store, [c3.id], 4),",
            "+            find_shallow(self._store, [c3.id], 4),",
            "         )",
            " ",
            "     def test_multiple_independent(self) -> None:",
            "         a = self.make_linear_commits(2, message=b\"a\")",
            "         b = self.make_linear_commits(2, message=b\"b\")",
            "         c = self.make_linear_commits(2, message=b\"c\")",
            "         heads = [a[1].id, b[1].id, c[1].id]",
            " ",
            "         self.assertEqual(",
            "             ({a[0].id, b[0].id, c[0].id}, set(heads)),",
            "-            _find_shallow(self._store, heads, 2),",
            "+            find_shallow(self._store, heads, 2),",
            "         )",
            " ",
            "     def test_multiple_overlapping(self) -> None:",
            "         # Create the following commit tree:",
            "         # 1--2",
            "         #  \\",
            "         #   3--4",
            "         c1, c2 = self.make_linear_commits(2)",
            "         c3 = self.make_commit(parents=[c1.id])",
            "         c4 = self.make_commit(parents=[c3.id])",
            " ",
            "         # 1 is shallow along the path from 4, but not along the path from 2.",
            "         self.assertEqual(",
            "             ({c1.id}, {c1.id, c2.id, c3.id, c4.id}),",
            "-            _find_shallow(self._store, [c2.id, c4.id], 3),",
            "+            find_shallow(self._store, [c2.id, c4.id], 3),",
            "         )",
            " ",
            "     def test_merge(self) -> None:",
            "         c1 = self.make_commit()",
            "         c2 = self.make_commit()",
            "         c3 = self.make_commit(parents=[c1.id, c2.id])",
            " ",
            "         self.assertEqual(",
            "             ({c1.id, c2.id}, {c3.id}),",
            "-            _find_shallow(self._store, [c3.id], 2),",
            "+            find_shallow(self._store, [c3.id], 2),",
            "         )",
            " ",
            "     def test_tag(self) -> None:",
            "         c1, c2 = self.make_linear_commits(2)",
            "         tag = make_tag(c2, name=b\"tag\")",
            "         self._store.add_object(tag)",
            " ",
            "         self.assertEqual(",
            "             ({c1.id}, {c2.id}),",
            "-            _find_shallow(self._store, [tag.id], 2),",
            "+            find_shallow(self._store, [tag.id], 2),",
            "         )",
            " ",
            " ",
            " class TestUploadPackHandler(UploadPackHandler):",
            "     @classmethod",
            "     def required_capabilities(self):",
            "         return []",
            "@@ -350,15 +349,15 @@",
            "     def test_apply_pack_del_ref(self) -> None:",
            "         refs = {b\"refs/heads/master\": TWO, b\"refs/heads/fake-branch\": ONE}",
            "         self._repo.refs._update(refs)",
            "         update_refs = [",
            "             [ONE, ZERO_SHA, b\"refs/heads/fake-branch\"],",
            "         ]",
            "         self._handler.set_client_capabilities([b\"delete-refs\"])",
            "-        status = self._handler._apply_pack(update_refs)",
            "+        status = list(self._handler._apply_pack(update_refs))",
            "         self.assertEqual(status[0][0], b\"unpack\")",
            "         self.assertEqual(status[0][1], b\"ok\")",
            "         self.assertEqual(status[1][0], b\"refs/heads/fake-branch\")",
            "         self.assertEqual(status[1][1], b\"ok\")",
            " ",
            " ",
            " class ProtocolGraphWalkerEmptyTestCase(TestCase):",
            "@@ -1089,14 +1088,23 @@",
            "         )",
            " ",
            "     def test_bad_repo_path(self) -> None:",
            "         backend = FileSystemBackend()",
            " ",
            "         self.assertRaises(NotGitRepository, lambda: backend.open_repository(\"/ups\"))",
            " ",
            "+    def test_bytes_path(self) -> None:",
            "+        # Test that FileSystemBackend can handle bytes paths (issue #973)",
            "+        repo = self.backend.open_repository(self.path.encode(\"utf-8\"))",
            "+        self.assertTrue(",
            "+            os.path.samefile(",
            "+                os.path.abspath(repo.path), os.path.abspath(self.repo.path)",
            "+            )",
            "+        )",
            "+",
            " ",
            " class DictBackendTests(TestCase):",
            "     \"\"\"Tests for DictBackend.\"\"\"",
            " ",
            "     def test_nonexistant(self) -> None:",
            "         repo = MemoryRepo.init_bare([], {})",
            "         backend = DictBackend({b\"/\": repo})"
          ]
        },
        {
          "file": "/home/dulwich-dulwich-0.23.2/tests/test_stash.py",
          "change": [
            "--- /home/dulwich-dulwich-0.22.7/tests/test_stash.py",
            "+++ /home/dulwich-dulwich-0.23.2/tests/test_stash.py",
            "@@ -17,20 +17,202 @@",
            " # <http://www.gnu.org/licenses/> for a copy of the GNU General Public License",
            " # and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache",
            " # License, Version 2.0.",
            " #",
            " ",
            " \"\"\"Tests for stashes.\"\"\"",
            " ",
            "-from dulwich.repo import MemoryRepo",
            "-from dulwich.stash import Stash",
            "+import os",
            "+import shutil",
            "+import tempfile",
            "+",
            "+from dulwich.objects import Blob, Tree",
            "+from dulwich.repo import Repo",
            "+from dulwich.stash import DEFAULT_STASH_REF, Stash",
            " ",
            " from . import TestCase",
            " ",
            " ",
            " class StashTests(TestCase):",
            "     \"\"\"Tests for stash.\"\"\"",
            " ",
            "+    def setUp(self):",
            "+        self.test_dir = tempfile.mkdtemp()",
            "+        self.repo_dir = os.path.join(self.test_dir, \"repo\")",
            "+        os.makedirs(self.repo_dir)",
            "+        self.repo = Repo.init(self.repo_dir)",
            "+",
            "+        # Create initial commit so we can have a HEAD",
            "+        blob = Blob()",
            "+        blob.data = b\"initial data\"",
            "+        self.repo.object_store.add_object(blob)",
            "+",
            "+        tree = Tree()",
            "+        tree.add(b\"initial.txt\", 0o100644, blob.id)",
            "+        tree_id = self.repo.object_store.add_object(tree)",
            "+",
            "+        self.commit_id = self.repo.do_commit(b\"Initial commit\", tree=tree_id)",
            "+",
            "+    def tearDown(self):",
            "+        shutil.rmtree(self.test_dir)",
            "+",
            "     def test_obtain(self) -> None:",
            "-        repo = MemoryRepo()",
            "-        stash = Stash.from_repo(repo)",
            "+        stash = Stash.from_repo(self.repo)",
            "         self.assertIsInstance(stash, Stash)",
            "+",
            "+    def test_empty_stash(self) -> None:",
            "+        stash = Stash.from_repo(self.repo)",
            "+        # Make sure logs directory exists for reflog",
            "+        os.makedirs(os.path.join(self.repo.commondir(), \"logs\"), exist_ok=True)",
            "+",
            "+        self.assertEqual(0, len(stash))",
            "+        self.assertEqual([], list(stash.stashes()))",
            "+",
            "+    def test_push_stash(self) -> None:",
            "+        stash = Stash.from_repo(self.repo)",
            "+",
            "+        # Make sure logs directory exists for reflog",
            "+        os.makedirs(os.path.join(self.repo.commondir(), \"logs\"), exist_ok=True)",
            "+",
            "+        # Create a file and add it to the index",
            "+        file_path = os.path.join(self.repo_dir, \"testfile.txt\")",
            "+        with open(file_path, \"wb\") as f:",
            "+            f.write(b\"test data\")",
            "+        self.repo.stage([\"testfile.txt\"])",
            "+",
            "+        # Push to stash",
            "+        commit_id = stash.push(message=b\"Test stash message\")",
            "+        self.assertIsNotNone(commit_id)",
            "+",
            "+        # Verify stash was created",
            "+        self.assertEqual(1, len(stash))",
            "+",
            "+        # Verify stash entry",
            "+        entry = stash[0]",
            "+        self.assertEqual(commit_id, entry.new_sha)",
            "+        self.assertTrue(b\"Test stash message\" in entry.message)",
            "+",
            "+    def test_drop_stash(self) -> None:",
            "+        stash = Stash.from_repo(self.repo)",
            "+",
            "+        # Make sure logs directory exists for reflog",
            "+        logs_dir = os.path.join(self.repo.commondir(), \"logs\")",
            "+        os.makedirs(logs_dir, exist_ok=True)",
            "+",
            "+        # Create a couple of files and stash them",
            "+        file1_path = os.path.join(self.repo_dir, \"testfile1.txt\")",
            "+        with open(file1_path, \"wb\") as f:",
            "+            f.write(b\"test data 1\")",
            "+        self.repo.stage([\"testfile1.txt\"])",
            "+        commit_id1 = stash.push(message=b\"Test stash 1\")",
            "+",
            "+        file2_path = os.path.join(self.repo_dir, \"testfile2.txt\")",
            "+        with open(file2_path, \"wb\") as f:",
            "+            f.write(b\"test data 2\")",
            "+        self.repo.stage([\"testfile2.txt\"])",
            "+        stash.push(message=b\"Test stash 2\")",
            "+",
            "+        self.assertEqual(2, len(stash))",
            "+",
            "+        # Drop the newest stash",
            "+        stash.drop(0)",
            "+        self.assertEqual(1, len(stash))",
            "+        self.assertEqual(commit_id1, stash[0].new_sha)",
            "+",
            "+        # Drop the remaining stash",
            "+        stash.drop(0)",
            "+        self.assertEqual(0, len(stash))",
            "+        self.assertNotIn(DEFAULT_STASH_REF, self.repo.refs)",
            "+",
            "+    def test_custom_ref(self) -> None:",
            "+        custom_ref = b\"refs/custom_stash\"",
            "+        stash = Stash(self.repo, ref=custom_ref)",
            "+        self.assertEqual(custom_ref, stash._ref)",
            "+",
            "+    def test_pop_stash(self) -> None:",
            "+        stash = Stash.from_repo(self.repo)",
            "+",
            "+        # Make sure logs directory exists for reflog",
            "+        os.makedirs(os.path.join(self.repo.commondir(), \"logs\"), exist_ok=True)",
            "+",
            "+        # Create a file and add it to the index",
            "+        file_path = os.path.join(self.repo_dir, \"testfile.txt\")",
            "+        with open(file_path, \"wb\") as f:",
            "+            f.write(b\"test data\")",
            "+        self.repo.stage([\"testfile.txt\"])",
            "+",
            "+        # Push to stash",
            "+        stash.push(message=b\"Test stash message\")",
            "+        self.assertEqual(1, len(stash))",
            "+",
            "+        # After stash push, the file should be removed from working tree",
            "+        # (matching git's behavior)",
            "+        self.assertFalse(os.path.exists(file_path))",
            "+",
            "+        # Pop the stash",
            "+        stash.pop(0)",
            "+",
            "+        # Verify file is restored",
            "+        self.assertTrue(os.path.exists(file_path))",
            "+        with open(file_path, \"rb\") as f:",
            "+            self.assertEqual(b\"test data\", f.read())",
            "+",
            "+        # Verify stash is empty",
            "+        self.assertEqual(0, len(stash))",
            "+",
            "+        # Verify the file is in the index",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"testfile.txt\", index)",
            "+",
            "+    def test_pop_stash_with_index_changes(self) -> None:",
            "+        stash = Stash.from_repo(self.repo)",
            "+",
            "+        # Make sure logs directory exists for reflog",
            "+        os.makedirs(os.path.join(self.repo.commondir(), \"logs\"), exist_ok=True)",
            "+",
            "+        # First commit a file so we have tracked files",
            "+        tracked_path = os.path.join(self.repo_dir, \"tracked.txt\")",
            "+        with open(tracked_path, \"wb\") as f:",
            "+            f.write(b\"original content\")",
            "+        self.repo.stage([\"tracked.txt\"])",
            "+        self.repo.do_commit(b\"Add tracked file\")",
            "+",
            "+        # Modify the tracked file and stage it",
            "+        with open(tracked_path, \"wb\") as f:",
            "+            f.write(b\"staged changes\")",
            "+        self.repo.stage([\"tracked.txt\"])",
            "+",
            "+        # Modify it again but don't stage",
            "+        with open(tracked_path, \"wb\") as f:",
            "+            f.write(b\"working tree changes\")",
            "+",
            "+        # Create a new file and stage it",
            "+        new_file_path = os.path.join(self.repo_dir, \"new.txt\")",
            "+        with open(new_file_path, \"wb\") as f:",
            "+            f.write(b\"new file content\")",
            "+        self.repo.stage([\"new.txt\"])",
            "+",
            "+        # Push to stash",
            "+        stash.push(message=b\"Test stash with index\")",
            "+        self.assertEqual(1, len(stash))",
            "+",
            "+        # After stash push, new file should be removed and tracked file reset",
            "+        self.assertFalse(os.path.exists(new_file_path))",
            "+        with open(tracked_path, \"rb\") as f:",
            "+            self.assertEqual(b\"original content\", f.read())",
            "+",
            "+        # Pop the stash",
            "+        stash.pop(0)",
            "+",
            "+        # Verify tracked file has working tree changes",
            "+        self.assertTrue(os.path.exists(tracked_path))",
            "+        with open(tracked_path, \"rb\") as f:",
            "+            self.assertEqual(b\"working tree changes\", f.read())",
            "+",
            "+        # Verify new file is restored",
            "+        self.assertTrue(os.path.exists(new_file_path))",
            "+        with open(new_file_path, \"rb\") as f:",
            "+            self.assertEqual(b\"new file content\", f.read())",
            "+",
            "+        # Verify index has the staged changes",
            "+        index = self.repo.open_index()",
            "+        self.assertIn(b\"new.txt\", index)"
          ]
        }
      ]
    }
  }
}